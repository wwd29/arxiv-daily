<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-14</h1>
<h3>Title: Automating the RMF: Lessons from the FedRAMP 20x Pilot</h3>
<ul>
<li><strong>Authors: </strong>Isaac Henry Teuscher</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09613">https://arxiv.org/abs/2510.09613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09613">https://arxiv.org/pdf/2510.09613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09613]] Automating the RMF: Lessons from the FedRAMP 20x Pilot(https://arxiv.org/abs/2510.09613)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The U.S. Federal Risk and Authorization Management Program (FedRAMP) has long relied on extensive sets of controls and static documentation to assess cloud systems. However, this manual, point-in-time approach has struggled to keep pace with cloud-native development. FedRAMP 20x, a 2025 pilot program, reimagines the NIST Risk Management Framework (RMF): replacing traditional NIST 800-53 controls with Key Security Indicators (KSIs), using automated, machine-readable evidence, and emphasizing continuous reporting and authorization. This case study presents a practitioner-led field report from an industry participant who led multiple FedRAMP 20x pilot submissions and engaged directly with the FedRAMP PMO, 3PAOs, and community working groups. It explores how KSIs, continuous evidence pipelines, and DevSecOps integration can streamline authorization and improve cyber risk management. The study shows FedRAMP 20x as a live testbed for implementing the RMF in a cloud-native, automation-first approach and shares actionable recommendations for risk professionals seeking to modernize compliance and support real-time, risk-informed decision-making.</li>
</ul>

<h3>Title: A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Meiyin Meng, Zaixi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09615">https://arxiv.org/abs/2510.09615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09615">https://arxiv.org/pdf/2510.09615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09615]] A Biosecurity Agent for Lifecycle LLM Biosecurity Alignment(https://arxiv.org/abs/2510.09615)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly integrated into biomedical research workflows--from literature triage and hypothesis generation to experimental design--yet this expanded utility also heightens dual-use concerns, including the potential misuse for guiding toxic compound synthesis. In response, this study shows a Biosecurity Agent that comprises four coordinated modes across the model lifecycle: dataset sanitization, preference alignment, run-time guardrails, and automated red teaming. For dataset sanitization (Mode 1), evaluation is conducted on CORD-19, a COVID-19 Open Research Dataset of coronavirus-related scholarly articles. We define three sanitization tiers--L1 (compact, high-precision), L2 (human-curated biosafety terms), and L3 (comprehensive union)--with removal rates rising from 0.46% to 70.40%, illustrating the safety-utility trade-off. For preference alignment (Mode 2), DPO with LoRA adapters internalizes refusals and safe completions, reducing end-to-end attack success rate (ASR) from 59.7% to 3.0%. At inference (Mode 3), run-time guardrails across L1-L3 show the expected security-usability trade-off: L2 achieves the best balance (F1 = 0.720, precision = 0.900, recall = 0.600, FPR =0.067), while L3 offers stronger jailbreak resistance at the cost of higher false positives. Under continuous automated red-teaming (Mode 4), no successful jailbreaks are observed under the tested protocol. Taken together, our biosecurity agent offers an auditable, lifecycle-aligned framework that reduces attack success while preserving benign utility, providing safeguards for the use of LLMs in scientific research and setting a precedent for future agent-level security protections.</li>
</ul>

<h3>Title: Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems</h3>
<ul>
<li><strong>Authors: </strong>Mohammadhossein Homaei, Mehran Tarif, Mar Avilla, Andres Caro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09616">https://arxiv.org/abs/2510.09616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09616">https://arxiv.org/pdf/2510.09616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09616]] Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems(https://arxiv.org/abs/2510.09616)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Industrial Control Systems (ICS) face growing cyber-physical attacks that exploit both network vulnerabilities and physical processes. Current anomaly detection methods rely on correlation-based analysis, which cannot separate true causal relationships from spurious associations. This limitation results in high false alarm rates and poor root cause analysis. We propose a novel Causal Digital Twin (CDT) framework for cyber-physical security in medium-scale ICS. Our method combines causal inference theory with digital twin modeling. The framework enables three types of causal reasoning: association for pattern detection, intervention for understanding system responses, and counterfactual analysis for attack prevention planning. We evaluate our framework on three industrial datasets: SWaT, WADI, and HAI, with validation through physical constraint compliance (90.8\%) and synthetic ground truth testing (structural Hamming distance 0.13). Results show significant improvements over seven baseline methods. Our CDT achieves F1-scores are $0.944 \pm 0.014$ for SWaT, $0.902 \pm 0.021$ for WADI, and $0.923 \pm 0.018$ for HAI with statistical significance ($p < 0.0024$, Bonferroni corrected). The framework reduces false positives by \SI{74}{\percent} and achieves \SI{78.4}{\percent} root cause analysis accuracy compared to \SI{48.7}{\percent} for existing methods. Counterfactual analysis enables defense strategies that reduce attack success by \SI{73.2}{\percent}. The system keeps real-time performance with \SI{3.2}{ms} latency, which is suitable for industrial deployment, while providing interpretable explanations for operators.</li>
</ul>

<h3>Title: ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii A. Gerasimov</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09617">https://arxiv.org/abs/2510.09617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09617">https://arxiv.org/pdf/2510.09617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09617]] ChipmunkRing: A Practical Post-Quantum Ring Signature Scheme for Blockchain Applications(https://arxiv.org/abs/2510.09617)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>ChipmunkRing, a practical post-quantum ring signature construction tailored for blockchain environments. Building on our Chipmunk lattice-based cryptographic framework, this implementation delivers compact digital signatures ranging from 20.5 to 279.7KB, with rapid signing operations completing in 1.1-15.1ms and efficient validation processes requiring only 0.4-4.5ms for participant groups of 2-64 members. The cornerstone of our approach is Acorn Verification-a streamlined zero-knowledge protocol that supersedes the classical Fiat-Shamir methodology. This innovation enables linear O(n) authentication complexity using concise 96-byte cryptographic proofs per participant, yielding a remarkable 17.7x performance enhancement for 32-member rings when compared to conventional techniques. Our work includes rigorous mathematical security demonstrations confirming 112-bit post-quantum protection (NIST Level 1), extensive computational benchmarking, and comprehensive support for both standard anonymity sets and collaborative threshold constructions with flexible participation requirements.</li>
</ul>

<h3>Title: A Systematic Review on Crimes facilitated by Consumer Internet of Things Devices</h3>
<ul>
<li><strong>Authors: </strong>Ashley Brown, Nilufer Tuptuk, Enrico Mariconti, Shane Johnson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09618">https://arxiv.org/abs/2510.09618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09618">https://arxiv.org/pdf/2510.09618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09618]] A Systematic Review on Crimes facilitated by Consumer Internet of Things Devices(https://arxiv.org/abs/2510.09618)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>It is well documented that criminals use IoT devices to facilitate crimes. The review process follows a systematic approach with a clear search strategy, and study selection strategy. The review included a total of 543 articles and the findings from these articles were synthesised through thematic analysis. Identified security attacks targeting consumer IoT devices include man-in-the-middle (MiTM) attacks, synchronisation attacks, Denial-of-Service (DoS), DNS poisoning and malware, alongside device-specific vulnerabilities. Besides security attacks, this review discusses mitigations. Furthermore, the literature also covers crime threat scenarios arising from these attacks, such as, fraud, identity theft, crypto jacking and domestic abuse.</li>
</ul>

<h3>Title: Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability</h3>
<ul>
<li><strong>Authors: </strong>Jiayun Mo, Xin Kang, Tieyan Li, Zhongding Lei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09620">https://arxiv.org/abs/2510.09620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09620">https://arxiv.org/pdf/2510.09620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09620]] Toward a Unified Security Framework for AI Agents: Trust, Risk, and Liability(https://arxiv.org/abs/2510.09620)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The excitement brought by the development of AI agents came alongside arising problems. These concerns centered around users' trust issues towards AIs, the risks involved, and the difficulty of attributing responsibilities and liabilities. Current solutions only attempt to target each problem separately without acknowledging their inter-influential nature. The Trust, Risk and Liability (TRL) framework proposed in this paper, however, ties together the interdependent relationships of trust, risk, and liability to provide a systematic method of building and enhancing trust, analyzing and mitigating risks, and allocating and attributing liabilities. It can be applied to analyze any application scenarios of AI agents and suggest appropriate measures fitting to the context. The implications of the TRL framework lie in its potential societal impacts, economic impacts, ethical impacts, and more. It is expected to bring remarkable values to addressing potential challenges and promoting trustworthy, risk-free, and responsible usage of AI in 6G networks.</li>
</ul>

<h3>Title: A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms</h3>
<ul>
<li><strong>Authors: </strong>Krishno Dey, Diogo Barradas, Saqib Hakak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09621">https://arxiv.org/abs/2510.09621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09621">https://arxiv.org/pdf/2510.09621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09621]] A Systematic Literature Review on Fundamental Technologies and Security Challenges in the Metaverse Platforms(https://arxiv.org/abs/2510.09621)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>The Metaverse utilizes emerging technologies such as Extended Reality (XR), Artificial Intelligence (AI), blockchain, and digital twins to provide an immersive and interactive virtual experience. As Metaverse continues to evolve, it bring a range of security and privacy threats, such as identity management, data governance, and user interactions. This survey aims to provide a comprehensive review of the enabling technologies for the Metaverse. It also aims to provide a thorough analysis of key vulnerabilities and threats that may compromise its sustainability and user safety. We perform a systematic literature review (SLR) to identify key vulnerabilities and their countermeasures in Metaverse platforms. Metaverse offers a much larger attack surface compared to conventional digital platforms. Immersive, decentralized, and permanent characteristics of the Metaverse generate new vulnerabilities. Although there are many countermeasures to these vulnerabilities, most of them are theoretical or have not been tested in real-world environments. Our review highlights current advancements, identifies research gaps, and outlines future directions to ensure a secure, resilient, and ethically governed Metaverse.</li>
</ul>

<h3>Title: Smart Medical IoT Security Vulnerabilities: Real-Time MITM Attack Analysis, Lightweight Encryption Implementation, and Practitioner Perceptions in Underdeveloped Nigerian Healthcare Systems</h3>
<ul>
<li><strong>Authors: </strong>Aminu Muhammad Auwal</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09629">https://arxiv.org/abs/2510.09629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09629">https://arxiv.org/pdf/2510.09629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09629]] Smart Medical IoT Security Vulnerabilities: Real-Time MITM Attack Analysis, Lightweight Encryption Implementation, and Practitioner Perceptions in Underdeveloped Nigerian Healthcare Systems(https://arxiv.org/abs/2510.09629)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>The growing use of Internet of Things (IoT) technologies in Nigerian healthcare offers potential improvements in remote monitoring and data-driven care, but unsecured wireless communication in medical IoT (mIoT) devices exposes patient data to cyber threats. This study investigates such vulnerabilities through a real-time Man in the Middle (MITM) attack simulation and evaluates lightweight AES-128 encryption on low-cost devices. A prototype mIoT device was built with a NodeMCU ESP8266 and sensors for heart rate and temperature. In controlled lab conditions simulating local healthcare networks, unencrypted data transmissions were intercepted and altered using common tools (Bettercap, Wireshark). After AES-128 encryption was applied, all transmissions became unreadable and tamper attempts failed, demonstrating its effectiveness. Performance costs were modest, latency rose from 80 ms to 125 ms (56.25 percent increase) and CPU use from 30 percent to 45 percent, but system stability remained intact. Device cost stayed under 18,000 NGN (about 12 USD), making it feasible for Nigeria's resource constrained facilities. A survey of healthcare professionals showed moderate awareness of IoT-related risks but strong support for encryption and staff training. Barriers included limited budgets and technical complexity. The study concludes that lightweight AES-128 encryption provides practical, low-cost protection against common attack vectors while maintaining operational efficiency. Feedback from professionals highlights the urgency of improving security awareness and establishing guidelines for clinical deployment.</li>
</ul>

<h3>Title: Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits</h3>
<ul>
<li><strong>Authors: </strong>Bernhard Mueller</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09633">https://arxiv.org/abs/2510.09633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09633">https://arxiv.org/pdf/2510.09633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09633]] Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits(https://arxiv.org/abs/2510.09633)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Hound introduces a relation-first graph engine that improves system-level reasoning across interrelated components in complex codebases. The agent designs flexible, analyst-defined views with compact annotations (e.g., monetary/value flows, authentication/authorization roles, call graphs, protocol invariants) and uses them to anchor exact retrieval: for any question, it loads precisely the code that matters (often across components) so it can zoom out to system structure and zoom in to the decisive lines. A second contribution is a persistent belief system: long-lived vulnerability hypotheses whose confidence is updated as evidence accrues. The agent employs coverage-versus-intuition planning and a QA finalizer to confirm or reject hypotheses. On a five-project subset of ScaBench[1], Hound improves recall and F1 over a baseline LLM analyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest precision trade-off. We attribute these gains to flexible, relation-first graphs that extend model understanding beyond call/dataflow to abstract aspects, plus the hypothesis-centric loop; code and artifacts are released to support reproduction.</li>
</ul>

<h3>Title: A Method for Quantifying Human Risk and a Blueprint for LLM Integration</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Canale</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09635">https://arxiv.org/abs/2510.09635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09635">https://arxiv.org/pdf/2510.09635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09635]] A Method for Quantifying Human Risk and a Blueprint for LLM Integration(https://arxiv.org/abs/2510.09635)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>This paper presents the Cybersecurity Psychology Framework (CPF), a novel methodology for quantifying human-centric vulnerabilities in security operations through systematic integration of established psychological constructs with operational security telemetry. While individual human factors-alert fatigue, compliance fatigue, cognitive overload, and risk perception biases-have been extensively studied in isolation, no framework provides end-to-end operationalization across the full spectrum of psychological vulnerabilities. We address this gap by: (1) defining specific, measurable algorithms that quantify key psychological states using standard SOC tooling (SIEM, ticketing systems, communication platforms); (2) proposing a lightweight, privacy-preserving LLM architecture based on Retrieval-Augmented Generation (RAG) and domain-specific fine-tuning to analyze structured and unstructured data for latent psychological risks; (3) detailing a rigorous mixed-methods validation strategy acknowledging the inherent difficulty of obtaining sensitive cybersecurity data. Our implementation of CPF indicators has been demonstrated in a proof-of-concept deployment using small language models achieving 0.92 F1-score on synthetic data. This work provides the theoretical and methodological foundation necessary for industry partnerships to conduct empirical validation with real operational data.</li>
</ul>

<h3>Title: Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining</h3>
<ul>
<li><strong>Authors: </strong>Shaharyar Alam Ansari, Mohammad Luqman, Aasim Zafar, Savir Ali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09644">https://arxiv.org/abs/2510.09644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09644">https://arxiv.org/pdf/2510.09644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09644]] Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining(https://arxiv.org/abs/2510.09644)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Rapid urbanization has intensified traffic congestion, environmental strain, and inefficiencies in transportation systems, creating an urgent need for intelligent and adaptive traffic management solutions. Conventional systems relying on static signals and manual monitoring are inadequate for the dynamic nature of modern traffic. This research aims to develop a unified framework that integrates CCTV surveillance videos with multi-source data descriptors to enhance real-time urban traffic prediction. The proposed methodology incorporates spatio-temporal feature fusion, Frequent Episode Mining for sequential traffic pattern discovery, and a hybrid LSTM-Transformer model for robust traffic state forecasting. The framework was evaluated on the CityFlowV2 dataset comprising 313,931 annotated bounding boxes across 46 cameras. It achieved a high prediction accuracy of 98.46 percent, with a macro precision of 0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis revealed significant sequential patterns such as moderate-congested transitions with confidence levels exceeding 55 percent. The 46 sustained congestion alerts are system-generated, which shows practical value for proactive congestion management. This emphasizes the need for the incorporation of video stream analytics with data from multiple sources for the design of real-time, responsive, adaptable multi-level intelligent transportation systems, which makes urban mobility smarter and safer.</li>
</ul>

<h3>Title: AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security</h3>
<ul>
<li><strong>Authors: </strong>Tonmoy Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09645">https://arxiv.org/abs/2510.09645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09645">https://arxiv.org/pdf/2510.09645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09645]] AdaptAuth: Multi-Layered Behavioral and Credential Analysis for a Secure and Adaptive Authentication Framework for Password Security(https://arxiv.org/abs/2510.09645)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Password security has been compelled to evolve in response to the growing computational capabilities of modern systems. However, this evolution has often resulted in increasingly complex security practices that alienate users, leading to poor compliance and heightened vulnerability. Consequently, individuals remain exposed to attackers through weak or improperly managed passwords, underscoring the urgent need for a comprehensive defense mechanism that effectively addresses password-related risks and threats. In this paper, we propose a multifaceted solution designed to revolutionize password security by integrating diverse attributes such as the Password Dissection Mechanism, Dynamic Password Policy Mechanism, human behavioral patterns, device characteristics, network parameters, geographical context, and other relevant factors. By leveraging learning-based models, our framework constructs detailed user profiles capable of recognizing individuals and preventing nearly all forms of unauthorized access or device possession. The proposed framework enhances the usability-security paradigm by offering stronger protection than existing standards while simultaneously engaging users in the policy-setting process through a novel, adaptive approach.</li>
</ul>

<h3>Title: Rounding-Guided Backdoor Injection in Deep Learning Model Quantization</h3>
<ul>
<li><strong>Authors: </strong>Xiangxiang Chen, Peixin Zhang, Jun Sun, Wenhai Wang, Jingyi Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09647">https://arxiv.org/abs/2510.09647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09647">https://arxiv.org/pdf/2510.09647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09647]] Rounding-Guided Backdoor Injection in Deep Learning Model Quantization(https://arxiv.org/abs/2510.09647)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Model quantization is a popular technique for deploying deep learning models on resource-constrained environments. However, it may also introduce previously overlooked security risks. In this work, we present QuRA, a novel backdoor attack that exploits model quantization to embed malicious behaviors. Unlike conventional backdoor attacks relying on training data poisoning or model training manipulation, QuRA solely works using the quantization operations. In particular, QuRA first employs a novel weight selection strategy to identify critical weights that influence the backdoor target (with the goal of perserving the model's overall performance in mind). Then, by optimizing the rounding direction of these weights, we amplify the backdoor effect across model layers without degrading accuracy. Extensive experiments demonstrate that QuRA achieves nearly 100% attack success rates in most cases, with negligible performance degradation. Furthermore, we show that QuRA can adapt to bypass existing backdoor defenses, underscoring its threat potential. Our findings highlight critical vulnerability in widely used model quantization process, emphasizing the need for more robust security measures. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI</h3>
<ul>
<li><strong>Authors: </strong>Khartik Uppalapati, Bora Yimenicioglu, Shakeel Abdulkareem, Adan Eftekhari, Bhavya Uppalapati, Viraj Kamath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09649">https://arxiv.org/abs/2510.09649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09649">https://arxiv.org/pdf/2510.09649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09649]] TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI(https://arxiv.org/abs/2510.09649)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric neurodegenerative disorder whose early MRI signs are subtle and often missed. We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to detect early Batten disease from pediatric brain MRI with limited training cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and fine-tune it using metric-based few-shot learning (prototypical loss with 5-shot episodes). Our model achieves high accuracy (approximately 91%) and area under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2 from an international longitudinal cohort, 12 early-manifestation CLN2 cases reported by Cokal et al., and 8 public Radiopaedia scans) together with 90 age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to highlight disease-relevant brain regions, enabling explainable predictions. The model's small size and strong performance (sensitivity greater than 90%, specificity approximately 90%) demonstrates a practical AI solution for early Batten disease detection.</li>
</ul>

<h3>Title: Learning Cybersecurity vs. Ethical Hacking: A Comparative Pathway for Aspiring Students</h3>
<ul>
<li><strong>Authors: </strong>Fahed Quttainah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09650">https://arxiv.org/abs/2510.09650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09650">https://arxiv.org/pdf/2510.09650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09650]] Learning Cybersecurity vs. Ethical Hacking: A Comparative Pathway for Aspiring Students(https://arxiv.org/abs/2510.09650)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>This paper explores the distinctions and connections between cybersecurity and ethical hacking, two vital disciplines in the protection of digital systems. It defines each field, outlines their goals and methodologies, and compares the academic and professional paths available to aspiring students. Cybersecurity is presented as a defensive discipline focused on preventing attacks and safeguarding data, while ethical hacking adopts an offensive approach that identifies vulnerabilities through authorized testing. The paper highlights key skills, certifications, and career opportunities in both areas, offering practical guidance to help learners choose the path best suited to their interests and ambitions. Ultimately, it emphasizes the complementary nature of both fields in strengthening global cyber resilience.</li>
</ul>

<h3>Title: Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ranjan Sapkota, Manoj Karkee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09653">https://arxiv.org/abs/2510.09653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09653">https://arxiv.org/pdf/2510.09653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09653]] Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition(https://arxiv.org/abs/2510.09653)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive overview of the Ultralytics YOLO(You Only Look Once) family of object detectors, focusing the architectural evolution, benchmarking, deployment perspectives, and future challenges. The review begins with the most recent release, YOLO26 (YOLOv26), which introduces key innovations including Distribution Focal Loss (DFL) removal, native NMS-free inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label Assignment (STAL), and the MuSGD optimizer for stable training. The progression is then traced through YOLO11, with its hybrid task assignment and efficiency-focused modules; YOLOv8, which advanced with a decoupled detection head and anchor-free predictions; and YOLOv5, which established the modular PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8, YOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR, and DEIM. Metrics including precision, recall, F1 score, mean Average Precision, and inference speed are analyzed to highlight trade-offs between accuracy and efficiency. Deployment and application perspectives are further discussed, covering export formats, quantization strategies, and real-world use in robotics, agriculture, surveillance, and manufacturing. Finally, the paper identifies challenges and future directions, including dense-scene limitations, hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware training approaches.</li>
</ul>

<h3>Title: TreeNet: Layered Decision Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Zeshan Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09654">https://arxiv.org/abs/2510.09654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09654">https://arxiv.org/pdf/2510.09654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09654]] TreeNet: Layered Decision Ensembles(https://arxiv.org/abs/2510.09654)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Within the domain of medical image analysis, three distinct methodologies have demonstrated commendable accuracy: Neural Networks, Decision Trees, and Ensemble-Based Learning Algorithms, particularly in the specialized context of genstro institutional track abnormalities detection. These approaches exhibit efficacy in disease detection scenarios where a substantial volume of data is available. However, the prevalent challenge in medical image analysis pertains to limited data availability and data confidence. This paper introduces TreeNet, a novel layered decision ensemble learning methodology tailored for medical image analysis. Constructed by integrating pivotal features from neural networks, ensemble learning, and tree-based decision models, TreeNet emerges as a potent and adaptable model capable of delivering superior performance across diverse and intricate machine learning tasks. Furthermore, its interpretability and insightful decision-making process enhance its applicability in complex medical scenarios. Evaluation of the proposed approach encompasses key metrics including Accuracy, Precision, Recall, and training and evaluation time. The methodology resulted in an F1-score of up to 0.85 when using the complete training data, with an F1-score of 0.77 when utilizing 50\% of the training data. This shows a reduction of F1-score of 0.08 while in the reduction of 50\% of the training data and training time. The evaluation of the methodology resulted in the 32 Frame per Second which is usable for the realtime applications. This comprehensive assessment underscores the efficiency and usability of TreeNet in the demanding landscape of medical image analysis specially in the realtime analysis.</li>
</ul>

<h3>Title: Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique</h3>
<ul>
<li><strong>Authors: </strong>Yanming Li (PETSCRAFT), Seifeddine Ghozzi (ENSTA), Cédric Eichler (PETSCRAFT), Nicolas Anciaux (PETSCRAFT), Alexandra Bensamoun, Lorena Gonzalez Manzano (UC3M)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09655">https://arxiv.org/abs/2510.09655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09655">https://arxiv.org/pdf/2510.09655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09655]] Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique(https://arxiv.org/abs/2510.09655)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, membership infer, watermark, large language model</a></li>
<li><strong>Abstract: </strong>We address the problem of auditing whether sensitive or copyrighted texts were used to fine-tune large language models (LLMs) under black-box access. Prior signals-verbatim regurgitation and membership inference-are unreliable at the level of individual documents or require altering the visible text. We introduce a text-preserving watermarking framework that embeds sequences of invisible Unicode characters into documents. Each watermark is split into a cue (embedded in odd chunks) and a reply (embedded in even chunks). At audit time, we submit prompts that contain only the cue; the presence of the corresponding reply in the model's output provides evidence of memorization consistent with training on the marked text. To obtain sound decisions, we compare the score of the published watermark against a held-out set of counterfactual watermarks and apply a ranking test with a provable false-positive-rate bound. The design is (i) minimally invasive (no visible text changes), (ii) scalable to many users and documents via a large watermark space and multi-watermark attribution, and (iii) robust to common passive transformations. We evaluate on open-weight LLMs and multiple text domains, analyzing regurgitation dynamics, sensitivity to training set size, and interference under multiple concurrent watermarks. Our results demonstrate reliable post-hoc provenance signals with bounded FPR under black-box access. We experimentally observe a failure rate of less than 0.1\% when detecting a reply after fine-tuning with 50 marked documents. Conversely, no spurious reply was recovered in over 18,000 challenges, corresponding to a 100\%TPR@0\% FPR. Moreover, detection rates remain relatively stable as the dataset size increases, maintaining a per-document detection rate above 45\% even when the marked collection accounts for less than 0.33\% of the fine-tuning data.</li>
</ul>

<h3>Title: Signing Right Away</h3>
<ul>
<li><strong>Authors: </strong>Yejun Jang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09656">https://arxiv.org/abs/2510.09656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09656">https://arxiv.org/pdf/2510.09656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09656]] Signing Right Away(https://arxiv.org/abs/2510.09656)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The proliferation of high-fidelity synthetic media, coupled with exploitable hardware vulnerabilities in conventional imaging pipelines, has precipitated a crisis of trust in digital content. Existing countermeasures, from post-hoc classifiers to software-based signing, fail to address the fundamental challenge of establishing an unbreakable link to reality at the moment of capture. This whitepaper introduces Signing Right Away (SRA), a comprehensive security architecture that guarantees the provenance of digital media from "silicon to silicon to signed file." SRA leverages a four-pillar security model-Confidentiality, Integrity, Authentication, and Replay Protection, akin to the MIPI Camera Security Framework (CSF), but also extends its scope beyond the internal data bus to the creation of a cryptographically sealed, C2PA-compliant final asset. By securing the entire imaging pipeline within a Trusted Execution Environment (TEE), SRA ensures that every captured image and video carries an immutable, verifiable proof of origin. This provides a foundational solution for industries reliant on trustworthy visual information, including journalism, legal evidence, and insurance. We present the SRA architecture, a detailed implementation roadmap informed by empirical prototyping, and a comparative analysis that positions SRA as the essential "last mile" in the chain of content trust.</li>
</ul>

<h3>Title: Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Fosco Gramaccioni, Christian Marinoni, Fabrizio Frezza, Aurelio Uncini, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09657">https://arxiv.org/abs/2510.09657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09657">https://arxiv.org/pdf/2510.09657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09657]] Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials(https://arxiv.org/abs/2510.09657)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Accurate simulation of wave propagation in complex acoustic materials is crucial for applications in sound design, noise control, and material engineering. Traditional numerical solvers, such as finite element methods, are computationally expensive, especially when dealing with large-scale or real-time scenarios. In this work, we introduce a dataset of 31,000 acoustic materials, named HA30K, designed and simulated solving the Helmholtz equations. For each material, we provide the geometric configuration and the corresponding pressure field solution, enabling data-driven approaches to learn Helmholtz equation solutions. As a baseline, we explore a deep learning approach based on Stable Diffusion with ControlNet, a state-of-the-art model for image generation. Unlike classical solvers, our approach leverages GPU parallelization to process multiple simulations simultaneously, drastically reducing computation time. By representing solutions as images, we bypass the need for complex simulation software and explicit equation-solving. Additionally, the number of diffusion steps can be adjusted at inference time, balancing speed and quality. We aim to demonstrate that deep learning-based methods are particularly useful in early-stage research, where rapid exploration is more critical than absolute accuracy.</li>
</ul>

<h3>Title: Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors</h3>
<ul>
<li><strong>Authors: </strong>Edgar E. Robles, Dikshant Sagar, Alejandro Yankelevich, Jianming Bian, Pierre Baldi, NOvA Collaboration</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09659">https://arxiv.org/abs/2510.09659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09659">https://arxiv.org/pdf/2510.09659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09659]] Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors(https://arxiv.org/abs/2510.09659)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>NOvA is a long-baseline neutrino oscillation experiment that detects neutrino particles from the NuMI beam at Fermilab. Before data from this experiment can be used in analyses, raw hits in the detector must be matched to their source particles, and the type of each particle must be identified. This task has commonly been done using a mix of traditional clustering approaches and convolutional neural networks (CNNs). Due to the construction of the detector, the data is presented as two sparse 2D images: an XZ and a YZ view of the detector, rather than a 3D representation. We propose a point set neural network that operates on the sparse matrices with an operation that mixes information from both views. Our model uses less than 10% of the memory required using previous methods while achieving a 96.8% AUC score, a higher score than obtained when both views are processed independently (85.4%).</li>
</ul>

<h3>Title: Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</h3>
<ul>
<li><strong>Authors: </strong>Luca Scimeca, Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09660">https://arxiv.org/abs/2510.09660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09660">https://arxiv.org/pdf/2510.09660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09660]] Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise(https://arxiv.org/abs/2510.09660)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as spectrally anisotropic Gaussian diffusion (SAGD). In this work, we derive the score relation for anisotropic covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.</li>
</ul>

<h3>Title: Core Mondrian: Basic Mondrian beyond k-anonymity</h3>
<ul>
<li><strong>Authors: </strong>Adam Bloomston, Elizabeth Burke, Megan Cacace, Anne Diaz, Wren Dougherty, Matthew Gonzalez, Remington Gregg, Yeliz Güngör, Bryce Hayes, Eeway Hsu, Oron Israeli, Heesoo Kim, Sara Kwasnick, Joanne Lacsina, Demma Rosa Rodriguez, Adam Schiller, Whitney Schumacher, Jessica Simon, Maggie Tang, Skyler Wharton, Marilyn Wilcken</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09661">https://arxiv.org/abs/2510.09661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09661">https://arxiv.org/pdf/2510.09661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09661]] Core Mondrian: Basic Mondrian beyond k-anonymity(https://arxiv.org/abs/2510.09661)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We present Core Mondrian, a scalable extension of the Original Mondrian partition-based anonymization algorithm. A modular strategy layer supports k-anonymity, allowing new privacy models to be added easily. A hybrid recursive/queue execution engine exploits multi-core parallelism while maintaining deterministic output. Utility-preserving enhancements include NaN-pattern pre-partitioning, metric-driven cut scoring, and dynamic suppression budget management. Experiments on the 48k-record UCI ADULT dataset and synthetically scaled versions up to 1M records achieve lower Discernibility Metric scores than Original Mondrian for numeric quasi-identifier sets while parallel processing delivers up to 4x speedup vs. sequential Core Mondrian. Core Mondrian enables privacy-compliant equity analytics at production scale.</li>
</ul>

<h3>Title: Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection</h3>
<ul>
<li><strong>Authors: </strong>Raju Dhakal, Prashant Shekhar, Laxima Niure Kandel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09663">https://arxiv.org/abs/2510.09663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09663">https://arxiv.org/pdf/2510.09663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09663]] Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection(https://arxiv.org/abs/2510.09663)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Radio Frequency Fingerprinting (RFF) has evolved as an effective solution for authenticating devices by leveraging the unique imperfections in hardware components involved in the signal generation process. In this work, we propose a Convolutional Neural Network (CNN) based framework for detecting rogue devices and identifying genuine ones using softmax probability thresholding. We emulate an attack scenario in which adversaries attempt to mimic the RF characteristics of genuine devices by training a Generative Adversarial Network (GAN) using In-phase and Quadrature (IQ) samples from genuine devices. The proposed approach is verified using IQ samples collected from ten different ADALM-PLUTO Software Defined Radios (SDRs), with seven devices considered genuine, two as rogue, and one used for validation to determine the threshold.</li>
</ul>

<h3>Title: Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing</h3>
<ul>
<li><strong>Authors: </strong>Changchang Sun, Vickie Chen, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09664">https://arxiv.org/abs/2510.09664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09664">https://arxiv.org/pdf/2510.09664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09664]] Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing(https://arxiv.org/abs/2510.09664)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recently, deep supervised cross-modal hashing methods have achieve compelling success by learning semantic information in a self-supervised way. However, they still suffer from the key limitation that the multi-label semantic extraction process fail to explicitly interact with raw multimodal data, making the learned representation-level semantic information not compatible with the heterogeneous multimodal data and hindering the performance of bridging modality gap. To address this limitation, in this paper, we propose a novel semantic cohesive knowledge distillation scheme for deep cross-modal hashing, dubbed as SODA. Specifically, the multi-label information is introduced as a new textual modality and reformulated as a set of ground-truth label prompt, depicting the semantics presented in the image like the text modality. Then, a cross-modal teacher network is devised to effectively distill cross-modal semantic characteristics between image and label modalities and thus learn a well-mapped Hamming space for image modality. In a sense, such Hamming space can be regarded as a kind of prior knowledge to guide the learning of cross-modal student network and comprehensively preserve the semantic similarities between image and text modality. Extensive experiments on two benchmark datasets demonstrate the superiority of our model over the state-of-the-art methods.</li>
</ul>

<h3>Title: Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning</h3>
<ul>
<li><strong>Authors: </strong>Aditya Chakravarty</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09666">https://arxiv.org/abs/2510.09666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09666">https://arxiv.org/pdf/2510.09666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09666]] Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning(https://arxiv.org/abs/2510.09666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Climate change is intensifying wildfire risks globally, making reliable forecasting critical for adaptation strategies. While machine learning shows promise for wildfire prediction from Earth observation data, current approaches lack uncertainty quantification essential for risk-aware decision making. We present the first systematic analysis of spatial uncertainty in wildfire spread forecasting using multimodal Earth observation inputs. We demonstrate that predictive uncertainty exhibits coherent spatial structure concentrated near fire perimeters. Our novel distance metric reveals high-uncertainty regions form consistent 20-60 meter buffer zones around predicted firelines - directly applicable for emergency planning. Feature attribution identifies vegetation health and fire activity as primary uncertainty drivers. This work enables more robust wildfire management systems supporting communities adapting to increasing fire risk under climate change.</li>
</ul>

<h3>Title: OmniSAT: Compact Action Token, Faster Auto Regression</h3>
<ul>
<li><strong>Authors: </strong>Huaihai Lyu, Chaofan Chen, Senwei Xie, Pengwei Wang, Xiansheng Chen, Shanghang Zhang, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09667">https://arxiv.org/abs/2510.09667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09667">https://arxiv.org/pdf/2510.09667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09667]] OmniSAT: Compact Action Token, Faster Auto Regression(https://arxiv.org/abs/2510.09667)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing Vision-Language-Action (VLA) models can be broadly categorized into diffusion-based and auto-regressive (AR) approaches: diffusion models capture continuous action distributions but rely on computationally heavy iterative denoising. In contrast, AR models enable efficient optimization and flexible sequence construction, making them better suited for large-scale pretraining. To further improve AR efficiency, particularly when action chunks induce extended and high-dimensional sequences, prior work applies entropy-guided and token-frequency techniques to shorten the sequence length. However, such compression struggled with \textit{poor reconstruction or inefficient compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer, which learns a compact, transferable action representation. Specifically, we first normalize value ranges and temporal horizons to obtain a consistent representation with B-Spline encoding. Then, we apply multi-stage residual quantization to the position, rotation, and gripper subspaces, producing compressed discrete tokens with coarse-to-fine granularity for each part. After pre-training on the large-scale dataset Droid, the resulting discrete tokenization shortens the training sequence by 6.8$\times$, and lowers the target entropy. To further explore the potential of OmniSAT, we develop a cross-embodiment learning strategy that builds on the unified action-pattern space and jointly leverages robot and human demonstrations. It enables scalable auxiliary supervision from heterogeneous egocentric videos. Across diverse real-robot and simulation experiments, OmniSAT encompasses higher compression while preserving reconstruction quality, enabling faster AR training convergence and model performance.</li>
</ul>

<h3>Title: A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction</h3>
<ul>
<li><strong>Authors: </strong>Maryam Abdollahi Shamami, Babak Teimourpour, Farshad Sharifi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09668">https://arxiv.org/abs/2510.09668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09668">https://arxiv.org/pdf/2510.09668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09668]] A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction(https://arxiv.org/abs/2510.09668)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Drug-drug interactions (DDIs) are a leading cause of preventable adverse events, often complicating treatment and increasing healthcare costs. At the same time, knowing which drugs do not interact is equally important, as such knowledge supports safer prescriptions and better patient outcomes. In this study, we propose an interpretable and efficient framework that blends modern machine learning with domain knowledge to improve DDI prediction. Our approach combines two complementary molecular embeddings - Mol2Vec, which captures fragment-level structural patterns, and SMILES-BERT, which learns contextual chemical features - together with a leakage-free, rule-based clinical score (RBScore) that injects pharmacological knowledge without relying on interaction labels. A lightweight neural classifier is then optimized using a novel three-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global exploration and local refinement for stable performance. Experiments on real-world datasets demonstrate that the model achieves high predictive accuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a clinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance, studies show how embedding fusion, RBScore, and the optimizer each contribute to precision and robustness. Together, these results highlight a practical pathway for building reliable, interpretable, and computationally efficient models that can support safer drug therapies and clinical decision-making.</li>
</ul>

<h3>Title: Population synthesis with geographic coordinates</h3>
<ul>
<li><strong>Authors: </strong>Jacopo Lenti, Lorenzo Costantini, Ariadna Fosch, Anna Monticelli, David Scala, Marco Pangallo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.SI, physics.soc-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09669">https://arxiv.org/abs/2510.09669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09669">https://arxiv.org/pdf/2510.09669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09669]] Population synthesis with geographic coordinates(https://arxiv.org/abs/2510.09669)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>It is increasingly important to generate synthetic populations with explicit coordinates rather than coarse geographic areas, yet no established methods exist to achieve this. One reason is that latitude and longitude differ from other continuous variables, exhibiting large empty spaces and highly uneven densities. To address this, we propose a population synthesis algorithm that first maps spatial coordinates into a more regular latent space using Normalizing Flows (NF), and then combines them with other features in a Variational Autoencoder (VAE) to generate synthetic populations. This approach also learns the joint distribution between spatial and non-spatial features, exploiting spatial autocorrelations. We demonstrate the method by generating synthetic homes with the same statistical properties of real homes in 121 datasets, corresponding to diverse geographies. We further propose an evaluation framework that measures both spatial accuracy and practical utility, while ensuring privacy preservation. Our results show that the NF+VAE architecture outperforms popular benchmarks, including copula-based methods and uniform allocation within geographic areas. The ability to generate geolocated synthetic populations at fine spatial resolution opens the door to applications requiring detailed geography, from household responses to floods, to epidemic spread, evacuation planning, and transport modeling.</li>
</ul>

<h3>Title: A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials</h3>
<ul>
<li><strong>Authors: </strong>Xinlun Cheng, Bingzhe Chen, Joseph Choi, Yen T. Nguyen, Pradeep Seshadri, Mayank Verma, H. S. Udaykumar, Stephen Baek</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09670">https://arxiv.org/abs/2510.09670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09670">https://arxiv.org/pdf/2510.09670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09670]] A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials(https://arxiv.org/abs/2510.09670)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modeling shock-to-detonation phenomena in energetic materials (EMs) requires capturing complex physical processes such as strong shocks, rapid changes in microstructural morphology, and nonlinear dynamics of chemical reaction fronts. These processes participate in energy localization at hotspots, which initiate chemical energy release leading to detonation. This study addresses the formation of hotspots in crystalline EMs subjected to weak-to-moderate shock loading, which, despite its critical relevance to the safe storage and handling of EMs, remains underexplored compared to the well-studied strong shock conditions. To overcome the computational challenges associated with direct numerical simulations, we advance the Physics-Aware Recurrent Convolutional Neural Network (PARCv2), which has been shown to be capable of predicting strong shock responses in EMs. We improved the architecture of PARCv2 to rapidly predict shear localizations and plastic heating, which play important roles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two widely used physics-informed models, namely, Fourier neural operator and neural ordinary differential equation; we demonstrate its superior performance in capturing the spatiotemporal dynamics of shear band formation. While all models exhibit certain failure modes, our findings underscore the importance of domain-specific considerations in developing robust AI-accelerated simulation tools for reactive materials.</li>
</ul>

<h3>Title: Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhou, Bolei Ma, Annemarie Friedrich, Mohsen Mesgar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09671">https://arxiv.org/abs/2510.09671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09671">https://arxiv.org/pdf/2510.09671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09671]] Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation(https://arxiv.org/abs/2510.09671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Table Question Answering (TQA) aims to answer natural language questions about tabular data, often accompanied by additional contexts such as text passages. The task spans diverse settings, varying in table representation, question/answer complexity, modality involved, and domain. While recent advances in large language models (LLMs) have led to substantial progress in TQA, the field still lacks a systematic organization and understanding of task formulations, core challenges, and methodological trends, particularly in light of emerging research directions such as reinforcement learning. This survey addresses this gap by providing a comprehensive and structured overview of TQA research with a focus on LLM-based methods. We provide a comprehensive categorization of existing benchmarks and task setups. We group current modeling strategies according to the challenges they target, and analyze their strengths and limitations. Furthermore, we highlight underexplored but timely topics that have not been systematically covered in prior research. By unifying disparate research threads and identifying open problems, our survey offers a consolidated foundation for the TQA community, enabling a deeper understanding of the state of the art and guiding future developments in this rapidly evolving area.</li>
</ul>

<h3>Title: Pingmark: A Textual Protocol for Universal Spatial Mentions</h3>
<ul>
<li><strong>Authors: </strong>Kalin Dimitrov (Independent Researcher, <a href="http://M.Sc" rel="external noopener nofollow" class="link-external link-http">this http URL</a>. Student, Veliko Tarnovo University)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09672">https://arxiv.org/abs/2510.09672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09672">https://arxiv.org/pdf/2510.09672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09672]] Pingmark: A Textual Protocol for Universal Spatial Mentions(https://arxiv.org/abs/2510.09672)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Pingmark defines a universal textual protocol for expressing spatial context through a minimal symbol: !@. Rather than embedding coordinates or using proprietary map links, Pingmark introduces a semantic trigger that compliant client applications interpret to generate a standardized resolver link of the form this https URL[timestamp]. This allows location expression to function like existing textual conventions - @ for identity or # for topics - but for physical space. The protocol requires no user registration, relies on open mapping technologies, and protects privacy by generating location data ephemerally and locally. This paper presents the motivation, syntax, and design of the Pingmark Protocol Specification (PPS v0.1), its reference resolver implementation, and the long-term goal of establishing Pingmark as an open Internet standard for spatial mentions.</li>
</ul>

<h3>Title: Cybersecurity Competence for Organisations in Inner Scandinavia</h3>
<ul>
<li><strong>Authors: </strong>Simone Fischer-Hübner, Leonardo A. Martucci, Lejla Islami, Ala Sarah Alaqra, Farzaneh Karegar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09673">https://arxiv.org/abs/2510.09673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09673">https://arxiv.org/pdf/2510.09673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09673]] Cybersecurity Competence for Organisations in Inner Scandinavia(https://arxiv.org/abs/2510.09673)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>A rapidly growing number of cybersecurity threats and incidents demands that Swedish organisations increase their efforts to improve their cybersecurity capacities. This paper presents results from interviews and a prior survey with key representatives from enterprises and public sector organisations in the Swedish region of Värmland in Inner Scandinavia, examining their cybersecurity readiness and needs for education and competence development. We discuss the generalizability of our findings and the extent to which they may be specific to Sweden and Värmland, and we conclude by proposing efforts to strengthen cybersecurity competences in Inner Scandinavia.</li>
</ul>

<h3>Title: Advancing Security in Software-Defined Vehicles: A Comprehensive Survey and Taxonomy</h3>
<ul>
<li><strong>Authors: </strong>Khaoula Sghaier, Badis Hammi, Ghada Gharbi, Pierre Merdrignac, Pierre Parrend, Didier Verna</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09675">https://arxiv.org/abs/2510.09675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09675">https://arxiv.org/pdf/2510.09675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09675]] Advancing Security in Software-Defined Vehicles: A Comprehensive Survey and Taxonomy(https://arxiv.org/abs/2510.09675)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Software-Defined Vehicles (SDVs) introduce innovative features that extend the vehicle's lifecycle through the integration of outsourced applications and continuous Over-The-Air (OTA) updates. This shift necessitates robust cybersecurity and system resilience. While research on Connected and Autonomous Vehicles (CAV) has been extensive, there is a lack of clarity in distinguishing SDVs from non-SDVs and a need to consolidate cybersecurity research. SDVs, with their extensive connectivity, have a broader attack surface. Besides, their software-centric nature introduces additional vulnerabilities. This paper provides a comprehensive examination of SDVs, detailing their ecosystem, enabling technologies, and the principal cyberattack entry points that arise from their architectural and operational characteristics. We also introduce a novel, layered taxonomy that maps concrete exploit techniques onto core SDV properties and attack paths, and use it to analyze representative studies and experimental approaches.</li>
</ul>

<h3>Title: Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shayan Mohajer Hamidi, En-Hui Yang, Ben Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09676">https://arxiv.org/abs/2510.09676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09676">https://arxiv.org/pdf/2510.09676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09676]] Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling(https://arxiv.org/abs/2510.09676)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inverse problems, where the goal is to recover an unknown signal from noisy or incomplete measurements, are central to applications in medical imaging, remote sensing, and computational biology. Diffusion models have recently emerged as powerful priors for solving such problems. However, existing methods either rely on projection-based techniques that enforce measurement consistency through heuristic updates, or they approximate the likelihood $p(\boldsymbol{y} \mid \boldsymbol{x})$, often resulting in artifacts and instability under complex or high-noise conditions. To address these limitations, we propose a novel framework called \emph{coupled data and measurement space diffusion posterior sampling} (C-DPS), which eliminates the need for constraint tuning or likelihood approximation. C-DPS introduces a forward stochastic process in the measurement space $\{\boldsymbol{y}_t\}$, evolving in parallel with the data-space diffusion $\{\boldsymbol{x}_t\}$, which enables the derivation of a closed-form posterior $p(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive sampling based on a well-defined posterior distribution. Empirical results demonstrate that C-DPS consistently outperforms existing baselines, both qualitatively and quantitatively, across multiple inverse problem benchmarks.</li>
</ul>

<h3>Title: NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sashank Makanaboyina</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09681">https://arxiv.org/abs/2510.09681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09681">https://arxiv.org/pdf/2510.09681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09681]] NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation(https://arxiv.org/abs/2510.09681)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, transformer, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate detection and segmentation of brain tumors in magnetic resonance imaging (MRI) are critical for effective diagnosis and treatment planning. Despite advances in convolutional neural networks (CNNs) such as U-Net, existing models often struggle with generalization, boundary precision, and limited data diversity. To address these challenges, we propose NNDM (NN\_UNet Diffusion Model)a hybrid framework that integrates the robust feature extraction of NN-UNet with the generative capabilities of diffusion probabilistic models. In our approach, the diffusion model progressively refines the segmentation masks generated by NN-UNet by learning the residual error distribution between predicted and ground-truth masks. This iterative denoising process enables the model to correct fine structural inconsistencies and enhance tumor boundary delineation. Experiments conducted on the BraTS 2021 datasets demonstrate that NNDM achieves superior performance compared to conventional U-Net and transformer-based baselines, yielding improvements in Dice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided refinement enhances robustness across modalities and tumor subregions. The proposed NNDM establishes a new direction for combining deterministic segmentation networks with stochastic diffusion models, advancing the state of the art in automated brain tumor analysis.</li>
</ul>

<h3>Title: Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices</h3>
<ul>
<li><strong>Authors: </strong>Rupam Patir, Keyan Guo, Haipeng Cai, Hongxin Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09682">https://arxiv.org/abs/2510.09682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09682">https://arxiv.org/pdf/2510.09682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09682]] Fortifying LLM-Based Code Generation with Graph-Based Reasoning on Secure Coding Practices(https://arxiv.org/abs/2510.09682)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>The code generation capabilities of Large Language Models (LLMs) have transformed the field of software development. However, this advancement also presents significant security challenges, as LLM-generated code often contains vulnerabilities. One direction of research strengthens LLMs by injecting or refining security knowledge through curated datasets, model tuning, or static analyzers. While effective in certain settings, these methods can be resource-intensive, less adaptable to zero-day vulnerabilities, and often inapplicable to proprietary models. To address these challenges, we introduce GRASP, which explores a new direction that focuses on structured reasoning over Secure Coding Practices(SCPs) rather than additional training or external feedback. GRASP comprises two key ideas: (1) an SCP graph that organizes SCPs into a Directed Acyclic Graph (DAG) capturing dependencies and relationships, and (2) a graph-based reasoning process that systematically guides LLMs through relevant SCPs for code generation. This design enables interpretable, model-agnostic, and scalable security improvements, particularly for previously unseen vulnerabilities. Our evaluation shows that GRASP consistently achieves Security Rates (SR) exceeding 80% across multiple LLMs, and delivers up to 88% improvements over baselines on zero-day vulnerabilities.</li>
</ul>

<h3>Title: Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation</h3>
<ul>
<li><strong>Authors: </strong>Chris Engh, P. M. Aronow</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09684">https://arxiv.org/abs/2510.09684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09684">https://arxiv.org/pdf/2510.09684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09684]] Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation(https://arxiv.org/abs/2510.09684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a simple yet effective use of LLM-powered AI tools to improve causal estimation. In double machine learning, the accuracy of causal estimates of the effect of a treatment on an outcome in the presence of a high-dimensional confounder depends on the performance of estimators of conditional expectation functions. We show that predictions made by generative models trained on historical data can be used to improve the performance of these estimators relative to approaches that solely rely on adjusting for embeddings extracted from these models. We argue that the historical knowledge and reasoning capacities associated with these generative models can help overcome curse-of-dimensionality problems in causal inference problems. We consider a case study using a small dataset of online jewelry auctions, and demonstrate that inclusion of LLM-generated guesses as predictors can improve efficiency in estimation.</li>
</ul>

<h3>Title: Deep Neural Networks Inspired by Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Yongshuai Liu, Lianfang Wang, Kuilin Qin, Qinghua Zhang, Faqiang Wang, Li Cui, Jun Liu, Yuping Duan, Tieyong Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09685">https://arxiv.org/abs/2510.09685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09685">https://arxiv.org/pdf/2510.09685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09685]] Deep Neural Networks Inspired by Differential Equations(https://arxiv.org/abs/2510.09685)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has become a pivotal technology in fields such as computer vision, scientific computing, and dynamical systems, significantly advancing these disciplines. However, neural Networks persistently face challenges related to theoretical understanding, interpretability, and generalization. To address these issues, researchers are increasingly adopting a differential equations perspective to propose a unified theoretical framework and systematic design methodologies for neural networks. In this paper, we provide an extensive review of deep neural network architectures and dynamic modeling methods inspired by differential equations. We specifically examine deep neural network models and deterministic dynamical network constructs based on ordinary differential equations (ODEs), as well as regularization techniques and stochastic dynamical network models informed by stochastic differential equations (SDEs). We present numerical comparisons of these models to illustrate their characteristics and performance. Finally, we explore promising research directions in integrating differential equations with deep learning to offer new insights for developing intelligent computational methods that boast enhanced interpretability and generalization capabilities.</li>
</ul>

<h3>Title: CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search</h3>
<ul>
<li><strong>Authors: </strong>Haoran Ou, Kangjie Chen, Xingshuo Han, Gelei Deng, Jie Zhang, Han Qiu, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09689">https://arxiv.org/abs/2510.09689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09689">https://arxiv.org/pdf/2510.09689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09689]] CREST-Search: Comprehensive Red-teaming for Evaluating Safety Threats in Large Language Models Powered by Web Search(https://arxiv.org/abs/2510.09689)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at tasks such as dialogue, summarization, and question answering, yet they struggle to adapt to specialized domains and evolving facts. To overcome this, web search has been integrated into LLMs, allowing real-time access to online content. However, this connection magnifies safety risks, as adversarial prompts combined with untrusted sources can cause severe vulnerabilities. We investigate red teaming for LLMs with web search and present CREST-Search, a framework that systematically exposes risks in such systems. Unlike existing methods for standalone LLMs, CREST-Search addresses the complex workflow of search-enabled models by generating adversarial queries with in-context learning and refining them through iterative feedback. We further construct WebSearch-Harm, a search-specific dataset to fine-tune LLMs into efficient red-teaming agents. Experiments show that CREST-Search effectively bypasses safety filters and reveals vulnerabilities in modern web-augmented LLMs, underscoring the need for specialized defenses to ensure trustworthy deployment.</li>
</ul>

<h3>Title: A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022</h3>
<ul>
<li><strong>Authors: </strong>Morteza Sargolzaei Javan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09690">https://arxiv.org/abs/2510.09690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09690">https://arxiv.org/pdf/2510.09690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09690]] A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022(https://arxiv.org/abs/2510.09690)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Cloud computing has become the foundation of modern digital infrastructure, yet the absence of a unified architectural and compliance framework impedes interoperability, auditability, and robust security. This paper introduces a formal, machine-readable semantic model for Cloud Engines, integrating the architectural taxonomy of ISO/IEC 22123 (Cloud Reference Architecture) with the security and compliance controls of ISO/IEC 27001:2022 and ISO/IEC TR 3445:2022. The model decomposes cloud systems into four canonical interfaces--Control, Business, Audit, and Data--and extends them with a security ontology that maps mechanisms such as authentication, authorization, and encryption to specific compliance controls. Expressed in RDF/Turtle, the model enables semantic reasoning, automated compliance validation, and vendor-neutral architecture design. We demonstrate its practical utility through OpenStack and AWS case studies, and provide reproducible validation workflows using SPARQL and SHACL. This work advances the state of cloud security modeling by bridging architectural and compliance standards in a unified framework, with a particular emphasis on auditability.</li>
</ul>

<h3>Title: Evaluation of Differential Privacy Mechanisms on Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Tejash Varsani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09691">https://arxiv.org/abs/2510.09691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09691">https://arxiv.org/pdf/2510.09691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09691]] Evaluation of Differential Privacy Mechanisms on Federated Learning(https://arxiv.org/abs/2510.09691)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is distributed model training across several clients without disclosing raw data. Despite advancements in data privacy, risks still remain. Differential Privacy (DP) is a technique to protect sensitive data by adding noise to model updates, usually controlled by a fixed privacy budget. However, this approach can introduce excessive noise, particularly when the model converges, which compromises performance. To address this problem, adaptive privacy budgets have been investigated as a potential solution. This work implements DP methods using Laplace and Gaussian mechanisms with an adaptive privacy budget, extending the SelecEval simulator. We introduce an adaptive clipping approach in the Gaussian mechanism, ensuring that gradients of the model are dynamically updated rather than using a fixed sensitivity. We conduct extensive experiments with various privacy budgets, IID and non-IID datasets, and different numbers of selected clients per round. While our experiments were limited to 200 training rounds, the results suggest that adaptive privacy budgets and adaptive clipping can help maintain model accuracy while preserving privacy.</li>
</ul>

<h3>Title: Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaodan Li, Mengjie Wu, Yao Zhu, Yunna Lv, YueFeng Chen, Cen Chen, Jianmei Guo, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09694">https://arxiv.org/abs/2510.09694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09694">https://arxiv.org/pdf/2510.09694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09694]] Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection(https://arxiv.org/abs/2510.09694)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Large models (LMs) are powerful content generators, yet their open-ended nature can also introduce potential risks, such as generating harmful or biased content. Existing guardrails mostly perform post-hoc detection that may expose unsafe content before it is caught, and the latency constraints further push them toward lightweight models, limiting detection accuracy. In this work, we propose Kelp, a novel plug-in framework that enables streaming risk detection within the LM generation pipeline. Kelp leverages intermediate LM hidden states through a Streaming Latent Dynamics Head (SLD), which models the temporal evolution of risk across the generated sequence for more accurate real-time risk detection. To ensure reliable streaming moderation in real applications, we introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic harm predictions by embedding a benign-then-harmful temporal prior. Besides, for a rigorous evaluation of streaming guardrails, we also present StreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from each protected model, reflecting real-world streaming scenarios in both text and vision-language tasks. Across diverse models and datasets, Kelp consistently outperforms state-of-the-art post-hoc guardrails and prior plug-in probes (15.61% higher average F1), while using only 20M parameters and adding less than 0.5 ms of per-token latency.</li>
</ul>

<h3>Title: Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanran Chen, Lynn Greschner, Roman Klinger, Michael Klenk, Steffen Eger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09695">https://arxiv.org/abs/2510.09695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09695">https://arxiv.org/pdf/2510.09695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09695]] Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection(https://arxiv.org/abs/2510.09695)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Logical fallacies are common in public communication and can mislead audiences; fallacious arguments may still appear convincing despite lacking soundness, because convincingness is inherently subjective. We present the first computational study of how emotional framing interacts with fallacies and convincingness, using large language models (LLMs) to systematically change emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting emotional appeal into fallacious arguments while preserving their logical structures, then use the best models to generate stimuli for a human study. Our results show that LLM-driven emotional framing reduces human fallacy detection in F1 by 14.5% on average. Humans perform better in fallacy detection when perceiving enjoyment than fear or sadness, and these three emotions also correlate with significantly higher convincingness compared to neutral or other emotion states. Our work has implications for AI-driven emotional manipulation in the context of fallacious argumentation.</li>
</ul>

<h3>Title: VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands</h3>
<ul>
<li><strong>Authors: </strong>Aofan Liu, Lulu Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09699">https://arxiv.org/abs/2510.09699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09699">https://arxiv.org/pdf/2510.09699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09699]] VisualDAN: Exposing Vulnerabilities in VLMs with Visual-Driven DAN Commands(https://arxiv.org/abs/2510.09699)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have garnered significant attention for their remarkable ability to interpret and generate multimodal content. However, securing these models against jailbreak attacks continues to be a substantial challenge. Unlike text-only models, VLMs integrate additional modalities, introducing novel vulnerabilities such as image hijacking, which can manipulate the model into producing inappropriate or harmful responses. Drawing inspiration from text-based jailbreaks like the "Do Anything Now" (DAN) command, this work introduces VisualDAN, a single adversarial image embedded with DAN-style commands. Specifically, we prepend harmful corpora with affirmative prefixes (e.g., "Sure, I can provide the guidance you need") to trick the model into responding positively to malicious queries. The adversarial image is then trained on these DAN-inspired harmful texts and transformed into the text domain to elicit malicious outputs. Extensive experiments on models such as MiniGPT-4, MiniGPT-v2, InstructBLIP, and LLaVA reveal that VisualDAN effectively bypasses the safeguards of aligned VLMs, forcing them to execute a broad range of harmful instructions that severely violate ethical standards. Our results further demonstrate that even a small amount of toxic content can significantly amplify harmful outputs once the model's defenses are compromised. These findings highlight the urgent need for robust defenses against image-based attacks and offer critical insights for future research into the alignment and security of VLMs.</li>
</ul>

<h3>Title: A Comprehensive Survey on Smart Home IoT Fingerprinting: From Detection to Prevention and Practical Deployment</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Baena, Han Yang, Dimitrios Koutsonikolas, Israat Haque</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09700">https://arxiv.org/abs/2510.09700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09700">https://arxiv.org/pdf/2510.09700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09700]] A Comprehensive Survey on Smart Home IoT Fingerprinting: From Detection to Prevention and Practical Deployment(https://arxiv.org/abs/2510.09700)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate, generative</a></li>
<li><strong>Abstract: </strong>Smart homes are increasingly populated with heterogeneous Internet of Things (IoT) devices that interact continuously with users and the environment. This diversity introduces critical challenges in device identification, authentication, and security, where fingerprinting techniques have emerged as a key approach. In this survey, we provide a comprehensive analysis of IoT fingerprinting specifically in the context of smart homes, examining methods for device and their event detection, classification, and intrusion prevention. We review existing techniques, e.g., network traffic analysis or machine learning-based schemes, highlighting their applicability and limitations in home environments characterized by resource-constrained devices, dynamic usage patterns, and privacy requirements. Furthermore, we discuss fingerprinting system deployment challenges like scalability, interoperability, and energy efficiency, as well as emerging opportunities enabled by generative AI and federated learning. Finally, we outline open research directions that can advance reliable and privacy-preserving fingerprinting for next-generation smart home ecosystems.</li>
</ul>

<h3>Title: A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Sudip Khadka, L.S. Paudel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09705">https://arxiv.org/abs/2510.09705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09705">https://arxiv.org/pdf/2510.09705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09705]] A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation(https://arxiv.org/abs/2510.09705)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Static feature exclusion strategies often fail to prevent bias when hidden dependencies influence the model predictions. To address this issue, we explore a reinforcement learning (RL) framework that integrates bias mitigation and automated feature selection within a single learning process. Unlike traditional heuristic-driven filter or wrapper approaches, our RL agent adaptively selects features using a reward signal that explicitly integrates predictive performance with fairness considerations. This dynamic formulation allows the model to balance generalization, accuracy, and equity throughout the training process, rather than rely exclusively on pre-processing adjustments or post hoc correction mechanisms. In this paper, we describe the construction of a multi-component reward function, the specification of the agents action space over feature subsets, and the integration of this system with ensemble learning. We aim to provide a flexible and generalizable way to select features in environments where predictors are correlated and biases can inadvertently re-emerge.</li>
</ul>

<h3>Title: A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN</h3>
<ul>
<li><strong>Authors: </strong>Md Habibur Rahman, Md Sharif Hossen, Nathan H. Stephenson, Vijay K. Shah, Aloizio Da Silva</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09706">https://arxiv.org/abs/2510.09706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09706">https://arxiv.org/pdf/2510.09706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09706]] A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN(https://arxiv.org/abs/2510.09706)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The open radio access network (O-RAN) enables modular, intelligent, and programmable 5G network architectures through the adoption of software-defined networking, network function virtualization, and implementation of standardized open interfaces. However, one of the security concerns for O-RAN, which can severely undermine network performance, is jamming attacks. This paper presents SAJD- a self-adaptive jammer detection framework that autonomously detects jamming attacks in AI/ML framework-integrated ORAN environments without human intervention. The SAJD framework forms a closed-loop system that includes near-realtime inference of radio signal jamming via our developed ML-based xApp, as well as continuous monitoring and retraining pipelines through rApps. In this demonstration, we will show how SAJD outperforms state-of-the-art jamming detection xApp (offline trained with manual labels) in terms of accuracy and adaptability under various dynamic and previously unseen interference scenarios in the O-RAN-compliant testbed.</li>
</ul>

<h3>Title: The Idola Tribus of AI: Large Language Models tend to perceive order where none exists</h3>
<ul>
<li><strong>Authors: </strong>Shin-nosuke Ishikawa, Masato Todo, Taiki Ogihara, Hirotsugu Ohba</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09709">https://arxiv.org/abs/2510.09709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09709">https://arxiv.org/pdf/2510.09709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09709]] The Idola Tribus of AI: Large Language Models tend to perceive order where none exists(https://arxiv.org/abs/2510.09709)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-world tasks, such as providing knowledge through retrieval-augmented generation and executing multi-step tasks using AI agent frameworks. However, these approaches rely on the logical consistency and self-coherence of LLMs, making it crucial to evaluate these aspects and consider potential countermeasures. To identify cases where LLMs fail to maintain logical consistency, we conducted an experiment in which LLMs were asked to explain the patterns in various integer sequences, ranging from arithmetic sequences to randomly generated integer series. While the models successfully identified correct patterns in arithmetic and geometric sequences, they frequently over-recognized patterns that were inconsistent with the given numbers when analyzing randomly generated series. This issue was observed even in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini 2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can be interpreted as the AI model equivalent of Idola Tribus and highlights potential limitations in their capability for applied tasks requiring logical reasoning, even when employing chain-of-thought reasoning mechanisms.</li>
</ul>

<h3>Title: SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG</h3>
<ul>
<li><strong>Authors: </strong>Xiaonan Si, Meilin Zhu, Simeng Qin, Lijia Yu, Lijun Zhang, Shuaitong Liu, Xinfeng Li, Ranjie Duan, Yang Liu, Xiaojun Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09710">https://arxiv.org/abs/2510.09710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09710">https://arxiv.org/pdf/2510.09710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09710]] SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG(https://arxiv.org/abs/2510.09710)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) with external knowledge but are vulnerable to corpus poisoning and contamination attacks, which can compromise output integrity. Existing defenses often apply aggressive filtering, leading to unnecessary loss of valuable information and reduced reliability in generation. To address this problem, we propose a two-stage semantic filtering and conflict-free framework for trustworthy RAG. In the first stage, we perform a joint filter with semantic and cluster-based filtering which is guided by the Entity-intent-relation extractor (EIRE). EIRE extracts entities, latent objectives, and entity relations from both the user query and filtered documents, scores their semantic relevance, and selectively adds valuable documents into the clean retrieval database. In the second stage, we proposed an EIRE-guided conflict-aware filtering module, which analyzes semantic consistency between the query, candidate answers, and retrieved knowledge before final answer generation, filtering out internal and external contradictions that could mislead the model. Through this two-stage process, SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination, achieving significant improvements in both generation robustness and output trustworthiness. Extensive experiments across various LLMs and datasets demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art defense methods.</li>
</ul>

<h3>Title: ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Guo, Xin Wang, Jiaoyan Chen, Lingbing Guo, Zhao Li, Zirui Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09711">https://arxiv.org/abs/2510.09711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09711">https://arxiv.org/pdf/2510.09711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09711]] ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models(https://arxiv.org/abs/2510.09711)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently emerged as a powerful paradigm for Knowledge Graph Completion (KGC), offering strong reasoning and generalization capabilities beyond traditional embedding-based approaches. However, existing LLM-based methods often struggle to fully exploit structured semantic representations, as the continuous embedding space of pretrained KG models is fundamentally misaligned with the discrete token space of LLMs. This discrepancy hinders effective semantic transfer and limits their performance. To address this challenge, we propose ReaLM, a novel and effective framework that bridges the gap between KG embeddings and LLM tokenization through the mechanism of residual vector quantization. ReaLM discretizes pretrained KG embeddings into compact code sequences and integrates them as learnable tokens within the LLM vocabulary, enabling seamless fusion of symbolic and contextual knowledge. Furthermore, we incorporate ontology-guided class constraints to enforce semantic consistency, refining entity predictions based on class-level compatibility. Extensive experiments on two widely used benchmark datasets demonstrate that ReaLM achieves state-of-the-art performance, confirming its effectiveness in aligning structured knowledge with large-scale language models.</li>
</ul>

<h3>Title: Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</h3>
<ul>
<li><strong>Authors: </strong>Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, Xiao-Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09712">https://arxiv.org/abs/2510.09712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09712">https://arxiv.org/pdf/2510.09712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09712]] Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments(https://arxiv.org/abs/2510.09712)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The spread of fake news online distorts public judgment and erodes trust in social media platforms. Although recent fake news detection (FND) models perform well in standard settings, they remain vulnerable to adversarial comments-authored by real users or by large language models (LLMs)-that subtly shift model decisions. In view of this, we first present a comprehensive evaluation of comment attacks to existing fake news detectors and then introduce a group-adaptive adversarial training strategy to improve the robustness of FND models. To be specific, our approach comprises three steps: (1) dividing adversarial comments into three psychologically grounded categories: perceptual, cognitive, and societal; (2) generating diverse, category-specific attacks via LLMs to enhance adversarial training; and (3) applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting Mechanism) that dynamically adjusts the learning focus across different comment categories during training. Experiments on benchmark datasets show that our method maintains strong detection accuracy while substantially increasing robustness to a wide range of adversarial comment perturbations.</li>
</ul>

<h3>Title: All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</h3>
<ul>
<li><strong>Authors: </strong>Shiyuan Guo, Henry Sleight, Fabien Roger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09714">https://arxiv.org/abs/2510.09714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09714">https://arxiv.org/pdf/2510.09714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09714]] All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language(https://arxiv.org/abs/2510.09714)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Detecting harmful AI actions is important as AI agents gain adoption. Chain-of-thought (CoT) monitoring is one method widely used to detect adversarial attacks and AI misalignment. However, attackers and misaligned models might evade CoT monitoring through ciphered reasoning: reasoning hidden in encrypted, translated, or compressed text. To assess this risk, we test whether models can perform ciphered reasoning. For each of 28 different ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We measure model accuracy on math problems as a proxy for reasoning ability. Across the models we test, we find an asymmetry: model accuracy can drop significantly when reasoning in ciphered text, even though models demonstrate comprehension of ciphered text by being able to translate it accurately to English. Even frontier models struggle with lesser-known ciphers, although they can reason accurately in well-known ciphers like rot13. We show that ciphered reasoning capability correlates with cipher prevalence in pretraining data. We also identify scaling laws showing that ciphered reasoning capability improves slowly with additional fine-tuning data. Our work suggests that evading CoT monitoring using ciphered reasoning may be an ineffective tactic for current models and offers guidance on constraining the development of this capability in future frontier models.</li>
</ul>

<h3>Title: A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs</h3>
<ul>
<li><strong>Authors: </strong>Hui Yuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09715">https://arxiv.org/abs/2510.09715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09715">https://arxiv.org/pdf/2510.09715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09715]] A Scalable, Privacy-Preserving Decentralized Identity and Verifiable Data Sharing Framework based on Zero-Knowledge Proofs(https://arxiv.org/abs/2510.09715)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>With the proliferation of decentralized applications (DApps), the conflict between the transparency of blockchain technology and user data privacy has become increasingly prominent. While Decentralized Identity (DID) and Verifiable Credentials (VCs) provide a standardized framework for user data sovereignty, achieving trusted identity verification and data sharing without compromising privacy remains a significant challenge. This paper proposes a novel, comprehensive framework that integrates DIDs and VCs with efficient Zero-Knowledge Proof (ZKP) schemes to address this core issue. The key contributions of this framework are threefold: first, it constructs a set of strong privacy-preserving protocols based on zk-STARKs, allowing users to prove that their credentials satisfy specific conditions (e.g., "age is over 18") without revealing any underlying sensitive data. Second, it designs a scalable, privacy-preserving credential revocation mechanism based on cryptographic accumulators, effectively solving credential management challenges in large-scale scenarios. Finally, it integrates a practical social key recovery scheme, significantly enhancing system usability and security. Through a prototype implementation and performance evaluation, this paper quantitatively analyzes the framework's performance in terms of proof generation time, verification overhead, and on-chain costs. Compared to existing state-of-the-art systems based on zk-SNARKs, our framework, at the cost of a larger proof size, significantly improves prover efficiency for complex computations and provides stronger security guarantees, including no trusted setup and post-quantum security. Finally, a case study in the decentralized finance (DeFi) credit scoring scenario demonstrates the framework's immense potential for unlocking capital efficiency and fostering a trusted data economy.</li>
</ul>

<h3>Title: High-Power Training Data Identification with Provable Statistical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Zhenlong Liu, Hao Zeng, Weiran Huang, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09717">https://arxiv.org/abs/2510.09717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09717">https://arxiv.org/pdf/2510.09717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09717]] High-Power Training Data Identification with Provable Statistical Guarantees(https://arxiv.org/abs/2510.09717)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Identifying training data within large-scale models is critical for copyright litigation, privacy auditing, and ensuring fair evaluation. The conventional approaches treat it as a simple binary classification task without statistical guarantees. A recent approach is designed to control the false discovery rate (FDR), but its guarantees rely on strong, easily violated assumptions. In this paper, we introduce Provable Training Data Identification (PTDI), a rigorous method that identifies a set of training data with strict false discovery rate (FDR) control. Specifically, our method computes p-values for each data point using a set of known unseen data, and then constructs a conservative estimator for the data usage proportion of the test set, which allows us to scale these p-values. Our approach then selects the final set of training data by identifying all points whose scaled p-values fall below a data-dependent threshold. This entire procedure enables the discovery of training data with provable, strict FDR control and significantly boosted power. Extensive experiments across a wide range of models (LLMs and VLMs), and datasets demonstrate that PTDI strictly controls the FDR and achieves higher power.</li>
</ul>

<h3>Title: Federated k-Means via Generalized Total Variation Minimization</h3>
<ul>
<li><strong>Authors: </strong>A. Jung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09718">https://arxiv.org/abs/2510.09718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09718">https://arxiv.org/pdf/2510.09718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09718]] Federated k-Means via Generalized Total Variation Minimization(https://arxiv.org/abs/2510.09718)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>We consider the problem of federated clustering, where interconnected devices have access to private local datasets and need to jointly cluster the overall dataset without sharing their local dataset. Our focus is on hard clustering based on the k-means principle. We formulate federated k-means clustering as an instance of GTVMin. This formulation naturally lends to a federated k-means algorithm where each device updates local cluster centroids by solving a modified local k-means problem. The modification involves adding a penalty term to measure the discrepancy between the cluster centroid of neighbouring devices. Our federated k-means algorithm is privacy-friendly as it only requires sharing aggregated information among interconnected devices.</li>
</ul>

<h3>Title: ICL-Router: In-Context Learned Model Representations for LLM Routing</h3>
<ul>
<li><strong>Authors: </strong>Chenxu Wang, Hao Li, Yiqun Zhang, Linyao Chen, Jianhao Chen, Ping Jian, Peng Ye, Qiaosheng Zhang, Shuyue Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09719">https://arxiv.org/abs/2510.09719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09719">https://arxiv.org/pdf/2510.09719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09719]] ICL-Router: In-Context Learned Model Representations for LLM Routing(https://arxiv.org/abs/2510.09719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often exhibit complementary strengths. Model routing harnesses these strengths by dynamically directing each query to the most suitable model, given a candidate model pool. However, routing performance relies on accurate model representations, and adding new models typically requires retraining, limiting scalability. To address these challenges, we propose a novel routing method using in-context vectors to represent model capabilities. The method proceeds in two stages. First, queries are embedded and projected into vectors, with a projector and LLM-based router trained to reconstruct the original queries, aligning vector representations with the router's semantic space. Second, each candidate model is profiled on a query set, and the router learns -- based on in-context vectors of query and model performance -- to predict whether each model can correctly answer new queries. Extensive experiments demonstrate that our method achieves state-of-the-art routing performance in both in-distribution and out-of-distribution tasks. Moreover, our method allows for seamless integration of new models without retraining the router. The code is available at this https URL.</li>
</ul>

<h3>Title: Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Fanwei Zhu, Jinke Yu, Zulong Chen, Ying Zhou, Junhao Ji, Zhibo Yang, Yuxue Zhang, Haoyuan Hu, Zhenghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09722">https://arxiv.org/abs/2510.09722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09722">https://arxiv.org/pdf/2510.09722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09722]] Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation(https://arxiv.org/abs/2510.09722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Automated resume information extraction is critical for scaling talent acquisition, yet its real-world deployment faces three major challenges: the extreme heterogeneity of resume layouts and content, the high cost and latency of large language models (LLMs), and the lack of standardized datasets and evaluation tools. In this work, we present a layout-aware and efficiency-optimized framework for automated extraction and evaluation that addresses all three challenges. Our system combines a fine-tuned layout parser to normalize diverse document formats, an inference-efficient LLM extractor based on parallel prompting and instruction tuning, and a robust two-stage automated evaluation framework supported by new benchmark datasets. Extensive experiments show that our framework significantly outperforms strong baselines in both accuracy and efficiency. In particular, we demonstrate that a fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly reducing inference latency and computational cost. The system is fully deployed in Alibaba's intelligent HR platform, supporting real-time applications across its business units.</li>
</ul>

<h3>Title: Zk-SNARK Marketplace with Proof of Useful Work</h3>
<ul>
<li><strong>Authors: </strong>Samuel Oleksak, Richard Gazdik, Martin Peresini, Ivan Homoliak</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09729">https://arxiv.org/abs/2510.09729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09729">https://arxiv.org/pdf/2510.09729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09729]] Zk-SNARK Marketplace with Proof of Useful Work(https://arxiv.org/abs/2510.09729)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Proof of Work (PoW) is widely regarded as the most secure permissionless blockchain consensus protocol. However, its reliance on computationally intensive yet externally useless puzzles results in excessive electric energy wasting. To alleviate this, Proof of Useful Work (PoUW) has been explored as an alternative to secure blockchain platforms while also producing real-world value. Despite this promise, existing PoUW proposals often fail to embed the integrity of the chain and identity of the miner into the puzzle solutions, not meeting necessary requirements for PoW and thus rendering them vulnerable. In this work, we propose a PoUW consensus protocol that computes client-outsourced zk-SNARKs proofs as a byproduct, which are at the same time used to secure the consensus protocol. We further leverage this mechanism to design a decentralized marketplace for outsourcing zk-SNARK proof generation, which is, to the best of our knowledge, the first such marketplace operating at the consensus layer, while meeting all necessary properties of PoW.</li>
</ul>

<h3>Title: Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Munsif, Waqas Ahmad, Amjid Ali, Mohib Ullah, Adnan Hussain, Sung Wook Baik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09731">https://arxiv.org/abs/2510.09731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09731">https://arxiv.org/pdf/2510.09731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09731]] Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey(https://arxiv.org/abs/2510.09731)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Connected Vision Systems (CVS) are transforming a variety of applications, including autonomous vehicles, smart cities, surveillance, and human-robot interaction. These systems harness multi-view multi-camera (MVMC) data to provide enhanced situational awareness through the integration of MVMC tracking, re-identification (Re-ID), and action understanding (AU). However, deploying CVS in real-world, dynamic environments presents a number of challenges, particularly in addressing occlusions, diverse viewpoints, and environmental variability. Existing surveys have focused primarily on isolated tasks such as tracking, Re-ID, and AU, often neglecting their integration into a cohesive system. These reviews typically emphasize single-view setups, overlooking the complexities and opportunities provided by multi-camera collaboration and multi-view data analysis. To the best of our knowledge, this survey is the first to offer a comprehensive and integrated review of MVMC that unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a unique taxonomy to better understand the critical components of CVS, dividing it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We systematically arrange and summarize the state-of-the-art datasets, methodologies, results, and evaluation metrics, providing a structured view of the field's progression. Furthermore, we identify and discuss the open research questions and challenges, along with emerging technologies such as lifelong learning, privacy, and federated learning, that need to be addressed for future advancements. The paper concludes by outlining key research directions for enhancing the robustness, efficiency, and adaptability of CVS in complex, real-world applications. We hope this survey will inspire innovative solutions and guide future research toward the next generation of intelligent and adaptive CVS.</li>
</ul>

<h3>Title: Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction</h3>
<ul>
<li><strong>Authors: </strong>P. van Oerle, R. H. Bemthuis, F. A. Bukhsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09732">https://arxiv.org/abs/2510.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09732">https://arxiv.org/pdf/2510.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09732]] Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction(https://arxiv.org/abs/2510.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to generate textual explanations of process models discovered from event logs. Producing explanations from large behavioral abstractions (e.g., directly-follows graphs or Petri nets) can be computationally expensive. This paper reports an exploratory evaluation of explanation quality under progressive behavioral-input reduction, where models are discovered from progressively smaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple input sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a second LLM to assess completeness, bottleneck identification, and suggested improvements. On synthetic logs, explanation quality is largely preserved under moderate reduction, indicating a practical cost-quality trade-off. The study is exploratory, as the scores are LLM-based (comparative signals rather than ground truth) and the data are synthetic. The results suggest a path toward more computationally efficient, LLM-assisted process analysis in resource-constrained settings.</li>
</ul>

<h3>Title: InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qianyou Sun, Jiexin Zheng, Bohan Jin, Lihua Chen, Yijie Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09735">https://arxiv.org/abs/2510.09735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09735">https://arxiv.org/pdf/2510.09735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09735]] InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models(https://arxiv.org/abs/2510.09735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Identifying inter-firm relationships such as supply and competitive ties is critical for financial analysis and corporate governance, yet remains challenging due to the scale, sparsity, and contextual dependence of corporate data. Graph-based methods capture structure but miss semantic depth, while large language models (LLMs) excel at text but remain limited in their ability to represent relational dependencies. To address this, we propose InterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs, supported by a proprietary dataset derived from FactSet supply chain records and three tailored training tasks: company graph matching, industry classification, and supply relation prediction. This design enables effective joint modeling of structure and semantics. Experiments show that InterCorpRel-LLM substantially outperforms strong baselines, including GPT-5, on a supply relation identification task, achieving an F-score of 0.8543 vs. 0.2287 with only a 7B-parameter backbone and lightweight training. The model also generalizes to zero-shot competitor identification, underscoring its ability to capture nuanced inter-firm dynamics. Our framework thus provides analysts and strategists with a robust tool for mapping and reasoning about complex corporate networks, enhancing decision-making and risk management in dynamic markets.</li>
</ul>

<h3>Title: Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement</h3>
<ul>
<li><strong>Authors: </strong>Steve Han, Gilberto Titericz Junior, Tom Balough, Wenfei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09738">https://arxiv.org/abs/2510.09738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09738">https://arxiv.org/pdf/2510.09738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09738]] Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement(https://arxiv.org/abs/2510.09738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research introduces the Judge's Verdict Benchmark, a novel two-step methodology to evaluate Large Language Models (LLMs) as judges for response accuracy evaluation tasks. We assess how well 54 LLMs can replicate human judgment when scoring responses from RAG (Retrieval-Augmented Generation) or Agentic pipelines against ground truth answers. Our methodology progresses from traditional correlation analysis to comprehensive Cohen's Kappa analysis that measures actual agreement patterns. The two-step approach includes: (1) a correlation test that filters judges with strong alignment, followed by (2) a human-likeness test using z-scores to identify two distinct judgment patterns: human-like judgment (|z| < 1) that mimics natural human variation, and super-consistent judgment (z > 1) that exceeds typical human-to-human agreement levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1 performance: 23 models exhibit human-like patterns that preserve the nuances of human judgment, while 4 models demonstrate super-consistent behavior, a pattern that could indicate either enhanced reliability or oversimplification of complex judgments. Testing 43 open-source models (1B-405B parameters) and 11 closed models (GPT, Gemini, Claude variants), we demonstrate that judge excellence is not solely dependent on model size but on specific training strategies. Our key contributions include: (1) establishing that correlation alone is insufficient for judge evaluation, (2) introducing a "Turing Test for judges" based on agreement patterns, and (3) providing a standardized benchmark for classifying LLM judges into distinct performance tiers for different evaluation needs.</li>
</ul>

<h3>Title: Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings</h3>
<ul>
<li><strong>Authors: </strong>Ayoub Bouguettaya, Elizabeth M. Stuart</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09739">https://arxiv.org/abs/2510.09739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09739">https://arxiv.org/pdf/2510.09739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09739]] Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings(https://arxiv.org/abs/2510.09739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The lexical hypothesis posits that personality traits are encoded in language and is foundational to models like the Big Five. We created a bottom-up personality model from a classic adjective list using machine learning and compared its descriptive utility against the Big Five by analyzing one million Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness, and Neuroticism, provided a far more powerful and interpretable description of these online communities. In contrast, our machine-learning clusters provided no meaningful distinctions, failed to recover the Extraversion trait, and lacked the psychometric coherence of the Big Five. These results affirm the robustness of the Big Five and suggest personality's semantic structure is context-dependent. Our findings show that while machine learning can help check the ecological validity of established psychological theories, it may not be able to replace them.</li>
</ul>

<h3>Title: Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry</h3>
<ul>
<li><strong>Authors: </strong>Atharv Goel, Sharat Agarwal, Saket Anand, Chetan Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09740">https://arxiv.org/abs/2510.09740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09740">https://arxiv.org/pdf/2510.09740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09740]] Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry(https://arxiv.org/abs/2510.09740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Active Learning (AL) promises to reduce annotation cost by prioritizing informative samples, yet its reliability is undermined when labels are noisy or when the data distribution shifts. In practice, annotators make mistakes, rare categories are ambiguous, and conventional AL heuristics (uncertainty, diversity) often amplify such errors by repeatedly selecting mislabeled or redundant samples. We propose Reliable Active Learning via Neural Collapse Geometry (NCAL-R), a framework that leverages the emergent geometric regularities of deep networks to counteract unreliable supervision. Our method introduces two complementary signals: (i) a Class-Mean Alignment Perturbation score, which quantifies how candidate samples structurally stabilize or distort inter-class geometry, and (ii) a Feature Fluctuation score, which captures temporal instability of representations across training checkpoints. By combining these signals, NCAL-R prioritizes samples that both preserve class separation and highlight ambiguous regions, mitigating the effect of noisy or redundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R consistently outperforms standard AL baselines, achieving higher accuracy with fewer labels, improved robustness under synthetic label noise, and stronger generalization to out-of-distribution data. These results suggest that incorporating geometric reliability criteria into acquisition decisions can make Active Learning less brittle to annotation errors and distribution shifts, a key step toward trustworthy deployment in real-world labeling pipelines. Our code is available at this https URL.</li>
</ul>

<h3>Title: Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping</h3>
<ul>
<li><strong>Authors: </strong>Dwip Dalal, Gautam Vashishtha, Utkarsh Mishra, Jeonghwan Kim, Madhav Kanda, Hyeonjeong Ha, Svetlana Lazebnik, Heng Ji, Unnat Jain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09741">https://arxiv.org/abs/2510.09741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09741">https://arxiv.org/pdf/2510.09741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09741]] Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping(https://arxiv.org/abs/2510.09741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) often miss small details and spatial relations in cluttered scenes, leading to errors in fine-grained perceptual grounding. We introduce AttWarp, a lightweight method that allocates more resolution to query-relevant content while compressing less informative areas, all while preserving global context. At test time, the approach uses an MLLM's cross-modal attention to perform rectilinear warping of the input image, reallocating spatial resolution toward regions the model deems important, without changing model weights or architecture. This attention-guided warping preserves all original image information but redistributes it non-uniformly, so small objects and subtle relationships become easier for the same model to read while the global layout remains intact. Across five benchmarks (TextVQA, GQA, DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and InstructBLIP), AttWarp consistently improves accuracy, strengthens compositional reasoning, and reduces hallucinations, outperforming four competitive baselines that manipulate raw images at test time. Together, these results show that attention-guided warping prioritizes information relevant to the query while preserving context, and that the same MLLMs perform better when given such warped inputs.</li>
</ul>

<h3>Title: Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Wanting Mao, Maxwell A Xu, Harish Haresamudram, Mithun Saha, Santosh Kumar, James Matthew Rehg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09764">https://arxiv.org/abs/2510.09764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09764">https://arxiv.org/pdf/2510.09764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09764]] Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model(https://arxiv.org/abs/2510.09764)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Modeling multi-modal time-series data is critical for capturing system-level dynamics, particularly in biosignals where modalities such as ECG, PPG, EDA, and accelerometry provide complementary perspectives on interconnected physiological processes. While recent self-supervised learning (SSL) advances have improved unimodal representation learning, existing multi-modal approaches often rely on CLIP-style contrastive objectives that overfit to easily aligned features and misclassify valid cross-modal relationships as negatives, resulting in fragmented and non-generalizable embeddings. To overcome these limitations, we propose ProtoMM, a novel SSL framework that introduces a shared prototype dictionary to anchor heterogeneous modalities in a common embedding space. By clustering representations around shared prototypes rather than explicit negative sampling, our method captures complementary information across modalities and provides a coherent "common language" for physiological signals. In this work, we focus on developing a Pulse Motion foundation model with ProtoMM and demonstrate that our approach outperforms contrastive-only and prior multimodal SSL methods, achieving state-of-the-art performance while offering improved interpretability of learned features.</li>
</ul>

<h3>Title: HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network</h3>
<ul>
<li><strong>Authors: </strong>Yifan Lu, Ziyun Zou, Belal Alsinglawi, Islam Al-Qudah, Izzat Alsmadi, Feilong Tang, Pengfei Jiao, Shoaib Jameel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09767">https://arxiv.org/abs/2510.09767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09767">https://arxiv.org/pdf/2510.09767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09767]] HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network(https://arxiv.org/abs/2510.09767)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformers have recently achieved remarkable progress in graph representation learning by capturing long-range dependencies through self-attention. However, their quadratic computational complexity and inability to effectively model heterogeneous semantics severely limit their scalability and generalization on real-world heterogeneous graphs. To address these issues, we propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for efficient and expressive heterogeneous graph representation learning. HeSRN introduces a slot-aware structure encoder that explicitly disentangles node-type semantics by projecting heterogeneous features into independent slots and aligning their distributions through slot normalization and retention-based fusion, effectively mitigating the semantic entanglement caused by forced feature-space unification in previous Transformer-based models. Furthermore, we replace the self-attention mechanism with a retention-based encoder, which models structural and contextual dependencies in linear time complexity while maintaining strong expressive power. A heterogeneous retentive encoder is further employed to jointly capture both local structural signals and global heterogeneous semantics through multi-scale retention layers. Extensive experiments on four real-world heterogeneous graph datasets demonstrate that HeSRN consistently outperforms state-of-the-art heterogeneous graph neural networks and Graph Transformer baselines on node classification tasks, achieving superior accuracy with significantly lower computational complexity.</li>
</ul>

<h3>Title: Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Adam Byerly, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09770">https://arxiv.org/abs/2510.09770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09770">https://arxiv.org/pdf/2510.09770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09770]] Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning(https://arxiv.org/abs/2510.09770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit a strong position bias in multi-document contexts, systematically prioritizing information based on location rather than relevance. While existing approaches treat this bias as noise to be mitigated, we introduce Gold Panning Bandits, a framework that leverages position bias as a diagnostic signal: by reordering documents and observing shifts in the model's responses, we can efficiently identify the most relevant content. We frame the problem of choosing reorderings as a bipartite matching problem. While an optimal assignment can be computed at each iteration with the Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$ strategy that achieves comparable performance by prioritizing the placement of the most uncertain documents in the most informative positions. Our approach identifies relevant documents using up to 65\% fewer language model queries than random permutation baselines on knowledge-intensive NLP tasks, substantially reducing computational cost without model retraining. This work demonstrates that inherent LLM biases can be transformed from liabilities into assets for efficient, inference-time optimization.</li>
</ul>

<h3>Title: PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Rakib Hossan, Shubhashis Roy Dipta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09771">https://arxiv.org/abs/2510.09771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09771">https://arxiv.org/pdf/2510.09771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09771]] PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection(https://arxiv.org/abs/2510.09771)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The BLP-2025 Task 1A requires Bengali hate speech classification into six categories. Traditional supervised approaches need extensive labeled datasets that are expensive for low-resource languages. We developed PromptGuard, a few-shot framework combining chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making. We explore statistical keyword selection versus random approaches and adaptive voting mechanisms that extend classification based on consensus quality. Chi-square keywords provide consistent improvements across categories, while adaptive voting benefits ambiguous cases requiring extended classification rounds. PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines (60.75) and random approaches (14.65). Ablation studies confirm chi-square-based keywords show the most consistent impact across all categories.</li>
</ul>

<h3>Title: Secret-Key Agreement Through Hidden Markov Modeling of Wavelet Scattering Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Nora Basha, Bechir Hamdaoui, Attila A. Yavuz, Thang Hoang, Mehran Mozaffari Kermani</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09773">https://arxiv.org/abs/2510.09773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09773">https://arxiv.org/pdf/2510.09773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09773]] Secret-Key Agreement Through Hidden Markov Modeling of Wavelet Scattering Embeddings(https://arxiv.org/abs/2510.09773)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Secret-key generation and agreement based on wireless channel reciprocity offers a promising avenue for securing IoT networks. However, existing approaches predominantly rely on the similarity of instantaneous channel measurement samples between communicating devices. This narrow view of reciprocity is often impractical, as it is highly susceptible to noise, asynchronous sampling, channel fading, and other system-level imperfections -- all of which significantly impair key generation performance. Furthermore, the quantization step common in traditional schemes introduces irreversible errors, further limiting efficiency. In this work, we propose a novel approach for secret-key generation by using wavelet scattering networks to extract robust and reciprocal CSI features. Dimensionality reduction is applied to uncover hidden cluster structures, which are then used to build hidden Markov models for efficient key agreement. Our approach eliminates the need for quantization and effectively captures channel randomness. It achieves a 5x improvement in key generation rate compared to traditional benchmarks, providing a secure and efficient solution for key generation in resource-constrained IoT environments.</li>
</ul>

<h3>Title: Why Do Transformers Fail to Forecast Time Series In-Context?</h3>
<ul>
<li><strong>Authors: </strong>Yufa Zhou, Yixiao Wang, Surbhi Goel, Anru R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09776">https://arxiv.org/abs/2510.09776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09776">https://arxiv.org/pdf/2510.09776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09776]] Why Do Transformers Fail to Forecast Time Series In-Context?(https://arxiv.org/abs/2510.09776)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting (TSF) remains a challenging and largely unsolved problem in machine learning, despite significant recent efforts leveraging Large Language Models (LLMs), which predominantly rely on Transformer architectures. Empirical evidence consistently shows that even powerful Transformers often fail to outperform much simpler models, e.g., linear models, on TSF tasks; however, a rigorous theoretical understanding of this phenomenon remains limited. In this paper, we provide a theoretical analysis of Transformers' limitations for TSF through the lens of In-Context Learning (ICL) theory. Specifically, under AR($p$) data, we establish that: (1) Linear Self-Attention (LSA) models $\textit{cannot}$ achieve lower expected MSE than classical linear models for in-context forecasting; (2) as the context length approaches to infinity, LSA asymptotically recovers the optimal linear predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions collapse to the mean exponentially. We empirically validate these findings through carefully designed experiments. Our theory not only sheds light on several previously underexplored phenomena but also offers practical insights for designing more effective forecasting architectures. We hope our work encourages the broader research community to revisit the fundamental theoretical limitations of TSF and to critically evaluate the direct application of increasingly sophisticated architectures without deeper scrutiny.</li>
</ul>

<h3>Title: Building a Foundational Guardrail for General Agentic Systems via Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yue Huang, Hang Hua, Yujun Zhou, Pengcheng Jing, Manish Nagireddy, Inkit Padhi, Greta Dolcetti, Zhangchen Xu, Subhajit Chaudhury, Ambrish Rawat, Liubov Nedoshivina, Pin-Yu Chen, Prasanna Sattigeri, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09781">https://arxiv.org/abs/2510.09781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09781">https://arxiv.org/pdf/2510.09781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09781]] Building a Foundational Guardrail for General Agentic Systems via Synthetic Data(https://arxiv.org/abs/2510.09781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.</li>
</ul>

<h3>Title: Large Language Models for Imbalanced Classification: Diversity makes the difference</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Sunil Gupta, Kien Do, Thin Nguyen, Taylor Braund, Alexis Whitton, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09783">https://arxiv.org/abs/2510.09783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09783">https://arxiv.org/pdf/2510.09783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09783]] Large Language Models for Imbalanced Classification: Diversity makes the difference(https://arxiv.org/abs/2510.09783)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Oversampling is one of the most widely used approaches for addressing imbalanced classification. The core idea is to generate additional minority samples to rebalance the dataset. Most existing methods, such as SMOTE, require converting categorical variables into numerical vectors, which often leads to information loss. Recently, large language model (LLM)-based methods have been introduced to overcome this limitation. However, current LLM-based approaches typically generate minority samples with limited diversity, reducing robustness and generalizability in downstream classification tasks. To address this gap, we propose a novel LLM-based oversampling method designed to enhance diversity. First, we introduce a sampling strategy that conditions synthetic sample generation on both minority labels and features. Second, we develop a new permutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the LLM not only on minority samples but also on interpolated samples to further enrich variability. Extensive experiments on 10 tabular datasets demonstrate that our method significantly outperforms eight SOTA baselines. The generated synthetic samples are both realistic and diverse. Moreover, we provide theoretical analysis through an entropy-based perspective, proving that our method encourages diversity in the generated samples.</li>
</ul>

<h3>Title: Combined Representation and Generation with Diffusive State Predictive Information Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Richard John, Yunrui Qiu, Lukas Herron, Pratyush Tiwary</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09784">https://arxiv.org/abs/2510.09784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09784">https://arxiv.org/pdf/2510.09784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09784]] Combined Representation and Generation with Diffusive State Predictive Information Bottleneck(https://arxiv.org/abs/2510.09784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling becomes increasingly data-intensive in high-dimensional spaces. In molecular science, where data collection is expensive and important events are rare, compression to lower-dimensional manifolds is especially important for various downstream tasks, including generation. We combine a time-lagged information bottleneck designed to characterize molecular important representations and a diffusion model in one joint training objective. The resulting protocol, which we term Diffusive State Predictive Information Bottleneck (D-SPIB), enables the balancing of representation learning and generation aims in one flexible architecture. Additionally, the model is capable of combining temperature information from different molecular simulation trajectories to learn a coherent and useful internal representation of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase its potential for exploring physical conditions outside the training set.</li>
</ul>

<h3>Title: Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Freenor, Lauren Alvarez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09790">https://arxiv.org/abs/2510.09790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09790">https://arxiv.org/pdf/2510.09790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09790]] Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models(https://arxiv.org/abs/2510.09790)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how language and embedding models encode semantic relationships is fundamental to model interpretability and control. While early word embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' + ''woman'' = ''queen''), modern high-dimensional text representations lack straightforward interpretable geometric properties. We introduce Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents semantic transformations as consistent rotational operations in embedding space, leveraging the manifold structure of modern language representations. RISE operations have the ability to operate across both languages and models with high transfer of performance, suggesting the existence of analogous cross-lingual geometric structure. We evaluate RISE across three embedding models, three datasets, and seven morphologically diverse languages in five major language groups. Our results demonstrate that RISE consistently maps discourse-level semantic transformations with distinct grammatical features (e.g., negation and conditionality) across languages and models. This work provides the first systematic demonstration that discourse-level semantic transformations correspond to consistent geometric operations in multilingual embedding spaces, empirically supporting the Linear Representation Hypothesis at the sentence level.</li>
</ul>

<h3>Title: Causality $\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs</h3>
<ul>
<li><strong>Authors: </strong>Lianghuan Huang, Yingshan Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09794">https://arxiv.org/abs/2510.09794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09794">https://arxiv.org/pdf/2510.09794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09794]] Causality $\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs(https://arxiv.org/abs/2510.09794)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability seeks to uncover how internal components of neural networks give rise to predictions. A persistent challenge, however, is disentangling two often conflated notions: decodability--the recoverability of information from hidden states--and causality--the extent to which those states functionally influence outputs. In this work, we investigate their relationship in vision transformers (ViTs) fine-tuned for object counting. Using activation patching, we test the causal role of spatial and CLS tokens by transplanting activations across clean-corrupted image pairs. In parallel, we train linear probes to assess the decodability of count information at different depths. Our results reveal systematic mismatches: middle-layer object tokens exert strong causal influence despite being weakly decodable, whereas final-layer object tokens support accurate decoding yet are functionally inert. Similarly, the CLS token becomes decodable in mid-layers but only acquires causal power in the final layers. These findings highlight that decodability and causality reflect complementary dimensions of representation--what information is present versus what is used--and that their divergence can expose hidden computational circuits.</li>
</ul>

<h3>Title: Task-Aware Resolution Optimization for Visual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weiqing Luo, Zhen Tan, Yifan Li, Xinyu Zhao, Kwonjoon Lee, Behzad Dariush, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09822">https://arxiv.org/abs/2510.09822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09822">https://arxiv.org/pdf/2510.09822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09822]] Task-Aware Resolution Optimization for Visual Large Language Models(https://arxiv.org/abs/2510.09822)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Real-world vision-language applications demand varying levels of perceptual granularity. However, most existing visual large language models (VLLMs), such as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to subpar performance. To address this problem, we first conduct a comprehensive and pioneering investigation into the resolution preferences of different vision-language tasks, revealing a correlation between resolution preferences with image complexity, and uncertainty variance of the VLLM at different image input resolutions. Building on this insight, we propose an empirical formula to determine the optimal resolution for a given vision-language task, combining these two factors. Second, based on rigorous experiments, we propose a novel parameter-efficient fine-tuning technique to extend the visual input resolution of pre-trained VLLMs to the identified optimal resolution. Extensive experiments on various vision-language tasks validate the effectiveness of our method.</li>
</ul>

<h3>Title: An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants</h3>
<ul>
<li><strong>Authors: </strong>Michael Crawshaw, Chirag Modi, Mingrui Liu, Robert M. Gower</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09827">https://arxiv.org/abs/2510.09827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09827">https://arxiv.org/pdf/2510.09827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09827]] An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants(https://arxiv.org/abs/2510.09827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To define a steepest descent method over a neural network, we need to choose a norm for each layer, a way to aggregate these norms across layers, and whether to use normalization. We systematically explore different alternatives for aggregating norms across layers, both formalizing existing combinations of Adam and the recently proposed Muon as a type of non-Euclidean gradient descent, and deriving new variants of the Muon optimizer. Through a comprehensive experimental evaluation of the optimizers within our framework, we find that Muon is sensitive to the choice of learning rate, whereas a new variant we call MuonMax is significantly more robust. We then show how to combine any non-Euclidean gradient method with model based momentum (known as Momo). The new Momo variants of Muon are significantly more robust to hyperparameter tuning, and often achieve a better validation score. Thus for new tasks, where the optimal hyperparameters are not known, we advocate for using Momo in combination with MuonMax to save on costly hyperparameter tuning.</li>
</ul>

<h3>Title: Post Processing of image segmentation using Conditional Random Fields</h3>
<ul>
<li><strong>Authors: </strong>Aashish Dhawan, Pankaj Bodani, Vishal Garg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09833">https://arxiv.org/abs/2510.09833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09833">https://arxiv.org/pdf/2510.09833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09833]] Post Processing of image segmentation using Conditional Random Fields(https://arxiv.org/abs/2510.09833)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The output of image the segmentation process is usually not very clear due to low quality features of Satellite images. The purpose of this study is to find a suitable Conditional Random Field (CRF) to achieve better clarity in a segmented image. We started with different types of CRFs and studied them as to why they are or are not suitable for our purpose. We evaluated our approach on two different datasets - Satellite imagery having low quality features and high quality Aerial photographs. During the study we experimented with various CRFs to find which CRF gives the best results on images and compared our results on these datasets to show the pitfalls and potentials of different approaches.</li>
</ul>

<h3>Title: Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>David Benavente-Rios, Juan Ruiz Rodriguez, Gustavo Gatica</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09836">https://arxiv.org/abs/2510.09836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09836">https://arxiv.org/pdf/2510.09836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09836]] Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection(https://arxiv.org/abs/2510.09836)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>This paper investigates the use of synthetic face data to enhance Single-Morphing Attack Detection (S-MAD), addressing the limitations of availability of large-scale datasets of bona fide images due to privacy concerns. Various morphing tools and cross-dataset evaluation schemes were utilized to conduct this study. An incremental testing protocol was implemented to assess the generalization capabilities as more and more synthetic images were added. The results of the experiments show that generalization can be improved by carefully incorporating a controlled number of synthetic images into existing datasets or by gradually adding bona fide images during training. However, indiscriminate use of synthetic data can lead to sub-optimal performance. Evenmore, the use of only synthetic data (morphed and non-morphed images) achieves the highest Equal Error Rate (EER), which means in operational scenarios the best option is not relying only on synthetic data for S-MAD.</li>
</ul>

<h3>Title: CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes</h3>
<ul>
<li><strong>Authors: </strong>Zhenjiang Fan, Zengyi Qin, Yuanning Zheng, Bo Xiong, Summer Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09846">https://arxiv.org/abs/2510.09846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09846">https://arxiv.org/pdf/2510.09846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09846]] CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes(https://arxiv.org/abs/2510.09846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Causal discovery from observational data is fundamental to scientific fields like biology, where controlled experiments are often impractical. However, existing methods, including constraint-based (e.g., PC, causalMGM) and score-based approaches (e.g., NOTEARS), face significant limitations. These include an inability to resolve causal direction, restrictions to linear associations, sensitivity to violations of the faithfulness assumption, and inefficiency in searching vast hypothesis spaces. While large language models (LLMs) offer powerful reasoning capabilities, their application is hindered by a fundamental discrepancy: they are designed for text, while most causal data is tabular. To address these challenges, we introduce CALM, a novel causal analysis language model specifically designed for tabular data in complex systems. CALM leverages a Mamba-based architecture to classify causal patterns from pairwise variable relationships. It integrates a comprehensive suite of evidence, including local causal scores, conditional independence tests, and relational attributes, to capture a wide spectrum of linear, nonlinear, and conditional causal mechanisms. Trained on a diverse corpus of synthetic data (from linear, mixed, and nonlinear models) and 10 real-world biological datasets with rigorously validated causal relationships, our model ensures robustness and generalizability. Empirical evaluation demonstrates that CALM significantly outperforms existing methods in both simulation studies, achieving over 91% accuracy, and in a real-world application identifying causal factors in Hepatitis C virus progression. This work represents a significant step towards accurate and generalizable causal discovery by successfully adapting the pattern recognition capabilities of language models to the intricacies of tabular data.</li>
</ul>

<h3>Title: Cell Instance Segmentation: The Devil Is in the Boundaries</h3>
<ul>
<li><strong>Authors: </strong>Peixian Liang, Yifan Ding, Yizhe Zhang, Jianxu Chen, Hao Zheng, Hongxiao Wang, Yejia Zhang, Guangyu Meng, Tim Weninger, Michael Niemier, X. Sharon Hu, Danny Z Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09848">https://arxiv.org/abs/2510.09848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09848">https://arxiv.org/pdf/2510.09848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09848]] Cell Instance Segmentation: The Devil Is in the Boundaries(https://arxiv.org/abs/2510.09848)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>State-of-the-art (SOTA) methods for cell instance segmentation are based on deep learning (DL) semantic segmentation approaches, focusing on distinguishing foreground pixels from background pixels. In order to identify cell instances from foreground pixels (e.g., pixel clustering), most methods decompose instance information into pixel-wise objectives, such as distances to foreground-background boundaries (distance maps), heat gradients with the center point as heat source (heat diffusion maps), and distances from the center point to foreground-background boundaries with fixed angles (star-shaped polygons). However, pixel-wise objectives may lose significant geometric properties of the cell instances, such as shape, curvature, and convexity, which require a collection of pixels to represent. To address this challenge, we present a novel pixel clustering method, called Ceb (for Cell boundaries), to leverage cell boundary features and labels to divide foreground pixels into cell instances. Starting with probability maps generated from semantic segmentation, Ceb first extracts potential foreground-foreground boundaries with a revised Watershed algorithm. For each boundary candidate, a boundary feature representation (called boundary signature) is constructed by sampling pixels from the current foreground-foreground boundary as well as the neighboring background-foreground boundaries. Next, a boundary classifier is used to predict its binary boundary label based on the corresponding boundary signature. Finally, cell instances are obtained by dividing or merging neighboring regions based on the predicted boundary labels. Extensive experiments on six datasets demonstrate that Ceb outperforms existing pixel clustering methods on semantic segmentation probability maps. Moreover, Ceb achieves highly competitive performance compared to SOTA cell instance segmentation methods.</li>
</ul>

<h3>Title: Text Prompt Injection of Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09849">https://arxiv.org/abs/2510.09849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09849">https://arxiv.org/pdf/2510.09849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09849]] Text Prompt Injection of Vision Language Models(https://arxiv.org/abs/2510.09849)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The widespread application of large vision language models has significantly raised safety concerns. In this project, we investigate text prompt injection, a simple yet effective method to mislead these models. We developed an algorithm for this type of attack and demonstrated its effectiveness and efficiency through experiments. Compared to other attack methods, our approach is particularly effective for large models without high demand for computational resources.</li>
</ul>

<h3>Title: ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers</h3>
<ul>
<li><strong>Authors: </strong>Shivam Patel, Neharika Jali, Ankur Mallick, Gauri Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09852">https://arxiv.org/abs/2510.09852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09852">https://arxiv.org/pdf/2510.09852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09852]] ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers(https://arxiv.org/abs/2510.09852)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) query routers are critical to modern AI platforms as they seek to improve efficiency by assigning inference queries to accurate, yet low-cost models. Parametric routers typically use trained neural networks for LLM selection but suffer from retraining and maintenance overheads. Nonparametric routers are training-free, instead estimating LLM accuracy and cost via similarity between encodings of the input query and training set queries. However, like their parametric counterparts, nonparametric routers struggle to generalize to outlier queries, an issue exacerbated by limited diversity in training sets which are costly to expand and difficult to keep current with ever-evolving use cases. We propose ProxRouter, which applies an exponentially tilted aggregation mechanism to balance bias and variance in nonparametric routers, improving their robustness to outliers. Experiments show ProxRouter enhances outlier routing while preserving inlier performance with minimal overhead.</li>
</ul>

<h3>Title: Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zhi Chen, Xin Yu, Xiaohui Tao, Yan Li, Zi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09867">https://arxiv.org/abs/2510.09867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09867">https://arxiv.org/pdf/2510.09867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09867]] Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation(https://arxiv.org/abs/2510.09867)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across various tasks by pre-training on numerous image-text pairs. These models often benefit from using an ensemble of context prompts to represent a class. Despite being effective, conventional prompt ensembling that averages textual features of context prompts often yields suboptimal results. This is because feature averaging shifts the class centroids away from the true class distribution. To address this issue, we propose the Cluster-Aware Prompt Ensemble Learning (CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL classifies images into one of several class clusters, each represented by a distinct prompt. Instead of ensembling prompts in the feature space, we perform ensembling in the classification logits space, aligning better with the visual feature distribution. To further optimize prompt fine-tuning while maintaining cluster-specific discriminative power, we introduce a cluster-preserving regularization term. This ensures that prompts remain distinct and specialized for different clusters, preventing collapse into a uniform direction. Additionally, we integrate an adaptive prompt weighting technique to dynamically adjust the attention weights for flawed or ambiguous prompts, ensuring robust performance across diverse datasets and tasks.</li>
</ul>

<h3>Title: CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nafiseh Nikeghbal, Amir Hossein Kargaran, Jana Diesner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09871">https://arxiv.org/abs/2510.09871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09871">https://arxiv.org/pdf/2510.09871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09871]] CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs(https://arxiv.org/abs/2510.09871)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair, large language model</a></li>
<li><strong>Abstract: </strong>Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at this https URL.</li>
</ul>

<h3>Title: WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions</h3>
<ul>
<li><strong>Authors: </strong>Sanjari Srivastava, Gang Li, Cheng Chang, Rishu Garg, Manpreet Kaur, Charlene Y. Lee, Yuezhang Li, Yining Mao, Ignacio Cases, Yanan Xie, Peng Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09872">https://arxiv.org/abs/2510.09872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09872">https://arxiv.org/pdf/2510.09872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09872]] WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions(https://arxiv.org/abs/2510.09872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Training web agents to navigate complex, real-world websites requires them to master $\textit{subtasks}$ - short-horizon interactions on multiple UI components (e.g., choosing the correct date in a date picker, or scrolling in a container to extract information). We introduce WARC-Bench (Web Archive Benchmark), a novel web navigation benchmark featuring 438 tasks designed to evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed interactions with dynamic and realistic webpages using Web ARChive files. We show that WARC-Bench is challenging for leading computer-use models, with the highest observed success rate being 64.8%. To improve open source models on subtask, we explore two common training techniques: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments show that SFT models obtain a 48.8% success rate on the benchmark. Training with RLVR over SFT checkpoints, even in data-scarce settings, improves the score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis concludes that mastering these subtasks is essential for robust web planning and navigation, and is a capability not extensively evaluated by existing benchmarks.</li>
</ul>

<h3>Title: Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Milad Khanchi, Maria Amer, Charalambos Poullis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09878">https://arxiv.org/abs/2510.09878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09878">https://arxiv.org/pdf/2510.09878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09878]] Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking(https://arxiv.org/abs/2510.09878)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) methods often rely on Intersection-over-Union (IoU) for association. However, this becomes unreliable when objects are similar or occluded. Also, computing IoU for segmentation masks is computationally expensive. In this work, we use segmentation masks to capture object shapes, but we do not compute segmentation IoU. Instead, we fuse depth and mask features and pass them through a compact encoder trained self-supervised. This encoder produces stable object representations, which we use as an additional similarity cue alongside bounding box IoU and re-identification features for matching. We obtain depth maps from a zero-shot depth estimator and object masks from a promptable visual segmentation model to obtain fine-grained spatial cues. Our MOT method is the first to use the self-supervised encoder to refine segmentation masks without computing masks IoU. MOT can be divided into joint detection-ReID (JDR) and tracking-by-detection (TBD) models. The latter are computationally more efficient. Experiments of our TBD method on challenging benchmarks with non-linear motion, occlusion, and crowded scenes, such as SportsMOT and DanceTrack, show that our method outperforms the TBD state-of-the-art on most metrics, while achieving competitive performance on simpler benchmarks with linear motion, such as MOT17.</li>
</ul>

<h3>Title: LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates</h3>
<ul>
<li><strong>Authors: </strong>Minkwan Kim, Seungmin Lee, Junho Kim, Young Min Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09881">https://arxiv.org/abs/2510.09881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09881">https://arxiv.org/pdf/2510.09881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09881]] LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates(https://arxiv.org/abs/2510.09881)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in novel-view synthesis can create the photo-realistic visualization of real-world environments from conventional camera captures. However, acquiring everyday environments from casual captures faces challenges due to frequent scene changes, which require dense observations both spatially and temporally. We propose long-term Gaussian scene chronology from sparse-view updates, coined LTGS, an efficient scene representation that can embrace everyday changes from highly under-constrained casual captures. Given an incomplete and unstructured Gaussian splatting representation obtained from an initial set of input images, we robustly model the long-term chronology of the scene despite abrupt movements and subtle environmental variations. We construct objects as template Gaussians, which serve as structural, reusable priors for shared object tracks. Then, the object templates undergo a further refinement pipeline that modulates the priors to adapt to temporally varying environments based on few-shot observations. Once trained, our framework is generalizable across multiple time steps through simple transformations, significantly enhancing the scalability for a temporal evolution of 3D environments. As existing datasets do not explicitly represent the long-term real-world changes with a sparse capture setup, we collect real-world datasets to evaluate the practicality of our pipeline. Experiments demonstrate that our framework achieves superior reconstruction quality compared to other baselines while enabling fast and light-weight updates.</li>
</ul>

<h3>Title: iBERT: Interpretable Style Embeddings via Sense Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Vishal Anand, Milad Alshomary, Kathleen McKeown</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09882">https://arxiv.org/abs/2510.09882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09882">https://arxiv.org/pdf/2510.09882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09882]] iBERT: Interpretable Style Embeddings via Sense Decomposition(https://arxiv.org/abs/2510.09882)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present iBERT (interpretable-BERT), an encoder to produce inherently interpretable and controllable embeddings - designed to modularize and expose the discriminative cues present in language, such as stylistic and semantic structure. Each input token is represented as a sparse, non-negative mixture over k context-independent sense vectors, which can be pooled into sentence embeddings or used directly at the token level. This enables modular control over representation, before any decoding or downstream use. To demonstrate our model's interpretability, we evaluate it on a suite of style-focused tasks. On the STEL benchmark, it improves style representation effectiveness by ~8 points over SBERT-style baselines, while maintaining competitive performance on authorship verification. Because each embedding is a structured composition of interpretable senses, we highlight how specific style attributes - such as emoji use, formality, or misspelling can be assigned to specific sense vectors. While our experiments center on style, iBERT is not limited to stylistic modeling. Its structural modularity is designed to interpretably decompose whichever discriminative signals are present in the data - enabling generalization even when supervision blends stylistic and semantic factors.</li>
</ul>

<h3>Title: DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang, Murali Annavarm</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09883">https://arxiv.org/abs/2510.09883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09883">https://arxiv.org/pdf/2510.09883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09883]] DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning(https://arxiv.org/abs/2510.09883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Large reasoning models (LRMs) achieve state-of-the-art performance on challenging benchmarks by generating long chains of intermediate steps, but their inference cost is dominated by decoding, where each new token must attend to the entire growing sequence. Existing sparse attention methods reduce computation by pruning the key-value (KV) cache, yet they suffer from severe accuracy degradation on reasoning tasks due to cumulative selection errors and the dynamic importance of tokens over long derivations. We present \textbf{DELTA}, a training-free sparse attention mechanism that achieves computational efficiency without sacrificing model accuracy. DELTA partitions transformer layers into three groups: initial layers that use full attention, a small set of \emph{selection layers} that identify salient tokens via aggregated head-level attention scores, and subsequent \emph{sparse-attention layers} that attend only to the selected subset. This design preserves the full KV cache in GPU memory for accuracy, while avoiding expensive full-attention computation over many layers. On reasoning benchmarks such as AIME and GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while reducing the number of attended tokens by up to $5\times$ and delivering $1.5\times$ end-to-end speedup. Our results show that selective reuse of intermediate attention maps offers a robust path toward efficient long-context reasoning.</li>
</ul>

<h3>Title: Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xu Pan, Ely Hahami, Jingxuan Fan, Ziqian Xie, Haim Sompolinsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09885">https://arxiv.org/abs/2510.09885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09885">https://arxiv.org/pdf/2510.09885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09885]] Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs(https://arxiv.org/abs/2510.09885)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Despite autoregressive large language models (arLLMs) being the current dominant paradigm in language modeling, they resist knowledge injection via fine-tuning due to inherent shortcomings such as the "reversal curse" -- the challenge of answering questions that reverse the original information order in the training sample. Masked diffusion large language models (dLLMs) are rapidly emerging as a powerful alternative to the arLLM paradigm, with evidence of better data efficiency and free of the "reversal curse" in pre-training. However, it is unknown whether these advantages extend to the post-training phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs, evaluating them with forward and backward style Question Answering (QA) to probe knowledge generalization and the reversal curse. Our results confirm that arLLMs critically rely on extensive data augmentation via paraphrases for QA generalization, and paraphrases are only effective when their information order matches the QA style. Conversely, dLLMs achieve high accuracies on both forward and backward QAs without paraphrases; adding paraphrases yields only marginal gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This proposed method successfully and drastically improves the data efficiency of arLLM fine-tuning, effectively closing the performance gap with dLLMs.</li>
</ul>

<h3>Title: Abductive Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Yijin Ni, Peng Qi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09887">https://arxiv.org/abs/2510.09887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09887">https://arxiv.org/pdf/2510.09887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09887]] Abductive Preference Learning(https://arxiv.org/abs/2510.09887)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Frontier large language models such as GPT-5 and Claude Sonnet remain prone to overconfidence even after alignment through Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance, they tend to offer the same conservative answer "No" to both questions "Can I eat the [food / potato chips] that has been left out overnight?" despite the latter requiring no refridgeration for safe consumption. We find that this failure is potentially attributed to a limitation of existing preference learning: it emphasizes selecting the correct response for a given prompt, while neglecting counterfactual prompts that should alter the response. To address this limitation, we propose abductive preference learning, a fine-tuning paradigm that reverses the conventional conditioning by learning preferences over prompts given a response. To validate this idea, we construct an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries, implementing abductive DPO and its variant DPOP. Experiments reveal complementary strengths: standard methods improve response selection, abductive methods improve prompt discrimination, while a multitask objective unifies both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to $99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt discrimination, with qualitative evidence highlighting improved sensitivity to prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive preference learning preserves the benefits of conventional preference optimization while addressing the overlooked challenge of counterfactual prompts.</li>
</ul>

<h3>Title: Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Feng, Qiang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09888">https://arxiv.org/abs/2510.09888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09888">https://arxiv.org/pdf/2510.09888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09888]] Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise(https://arxiv.org/abs/2510.09888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We investigate robust nonparametric regression in the presence of heavy-tailed noise, where the hypothesis class may contain unbounded functions and robustness is ensured via a robust loss function $\ell_\sigma$. Using Huber regression as a close-up example within Tikhonov-regularized risk minimization in reproducing kernel Hilbert spaces (RKHS), we address two central challenges: (i) the breakdown of standard concentration tools under weak moment assumptions, and (ii) the analytical difficulties introduced by unbounded hypothesis spaces. Our first message is conceptual: conventional generalization-error bounds for robust losses do not faithfully capture out-of-sample performance. We argue that learnability should instead be quantified through prediction error, namely the $L_2$-distance to the truth $f^\star$, which is $\sigma$-independent and directly reflects the target of robust estimation. To make this workable under unboundedness, we introduce a \emph{probabilistic effective hypothesis space} that confines the estimator with high probability and enables a meaningful bias--variance decomposition under weak $(1+\epsilon)$-moment conditions. Technically, we establish new comparison theorems linking the excess robust risk to the $L_2$ prediction error up to a residual of order $\mathcal{O}(\sigma^{-2\epsilon})$, clarifying the robustness--bias trade-off induced by the scale parameter $\sigma$. Building on this, we derive explicit finite-sample error bounds and convergence rates for Huber regression in RKHS that hold without uniform boundedness and under heavy-tailed noise. Our study delivers principled tuning rules, extends beyond Huber to other robust losses, and highlights prediction error, not excess generalization risk, as the fundamental lens for analyzing robust learning.</li>
</ul>

<h3>Title: HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection</h3>
<ul>
<li><strong>Authors: </strong>Guanming Chen, Lingzhi Shen, Xiaohao Cai, Imran Razzak, Shoaib Jameel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09893">https://arxiv.org/abs/2510.09893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09893">https://arxiv.org/pdf/2510.09893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09893]] HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection(https://arxiv.org/abs/2510.09893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Personality detection from text aims to infer an individual's personality traits based on linguistic patterns. However, existing machine learning approaches often struggle to capture contextual information spanning multiple posts and tend to fall short in extracting representative and robust features in semantically sparse environments. This paper presents HIPPD, a brain-inspired framework for personality detection that emulates the hierarchical information processing of the human brain. HIPPD utilises a large language model to simulate the cerebral cortex, enabling global semantic reasoning and deep feature abstraction. A dynamic memory module, modelled after the prefrontal cortex, performs adaptive gating and selective retention of critical features, with all adjustments driven by dopaminergic prediction error feedback. Subsequently, a set of specialised lightweight models, emulating the basal ganglia, are dynamically routed via a strict winner-takes-all mechanism to capture the personality-related patterns they are most proficient at recognising. Extensive experiments on the Kaggle and Pandora datasets demonstrate that HIPPD consistently outperforms state-of-the-art baselines.</li>
</ul>

<h3>Title: Learning Bug Context for PyTorch-to-JAX Translation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hung Phan, Son Le Vu, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09898">https://arxiv.org/abs/2510.09898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09898">https://arxiv.org/pdf/2510.09898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09898]] Learning Bug Context for PyTorch-to-JAX Translation with LLMs(https://arxiv.org/abs/2510.09898)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite recent progress of large language models (LLMs) on code translation among mainstream languages, translating PyTorch to JAX remains nontrivial. The two libraries, though both embedded in Python, differ in core design, execution semantics, and ecosystem maturity; JAX is newer and comparatively underrepresented in public code, and parallel PyTorch--JAX corpora are limited. Weaknesses in existing evaluation further complicate cross-framework benchmarking. We present T2J, a prompt-augmentation framework that strengthens LLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch sources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a GitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini to produce initial JAX drafts; (ii) engages two professional developers to iteratively repair those drafts until functional equivalence, yielding a curated fixed-bug dataset of common errors and patches; and (iii) constructs augmented prompts that inject structured guidance from these fixes to steer lightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored to PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based estimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge). Empirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50% on T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100% on T2J Comparison Score; moreover, the generated code runs up to 2.5x faster than the baseline.</li>
</ul>

<h3>Title: An uncertainty-aware framework for data-efficient multi-view animal pose estimation</h3>
<ul>
<li><strong>Authors: </strong>Lenny Aharon, Keemin Lee, Karan Sikka, Selmaan Chettih, Cole Hurwitz, Liam Paninski, Matthew R Whiteway</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09903">https://arxiv.org/abs/2510.09903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09903">https://arxiv.org/pdf/2510.09903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09903]] An uncertainty-aware framework for data-efficient multi-view animal pose estimation(https://arxiv.org/abs/2510.09903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Multi-view pose estimation is essential for quantifying animal behavior in scientific research, yet current methods struggle to achieve accurate tracking with limited labeled data and suffer from poor uncertainty estimates. We address these challenges with a comprehensive framework combining novel training and post-processing techniques, and a model distillation procedure that leverages the strengths of these techniques to produce a more efficient and effective pose estimator. Our multi-view transformer (MVT) utilizes pretrained backbones and enables simultaneous processing of information across all views, while a novel patch masking scheme learns robust cross-view correspondences without camera calibration. For calibrated setups, we incorporate geometric consistency through 3D augmentation and a triangulation loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to the nonlinear case and enhance uncertainty quantification via a variance inflation technique. Finally, to leverage the scaling properties of the MVT, we design a distillation procedure that exploits improved EKS predictions and uncertainty estimates to generate high-quality pseudo-labels, thereby reducing dependence on manual labels. Our framework components consistently outperform existing methods across three diverse animal species (flies, mice, chickadees), with each component contributing complementary benefits. The result is a practical, uncertainty-aware system for reliable pose estimation that enables downstream behavioral analyses under real-world data constraints.</li>
</ul>

<h3>Title: Stability of Transformers under Layer Normalization</h3>
<ul>
<li><strong>Authors: </strong>Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, Krishna Kumar, Markos A. Katsoulakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09904">https://arxiv.org/abs/2510.09904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09904">https://arxiv.org/pdf/2510.09904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09904]] Stability of Transformers under Layer Normalization(https://arxiv.org/abs/2510.09904)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite their widespread use, training deep Transformers can be unstable. Layer normalization, a standard component, improves training stability, but its placement has often been ad-hoc. In this paper, we conduct a principled study on the forward (hidden states) and backward (gradient) stability of Transformers under different layer normalization placements. Our theory provides key insights into the training dynamics: whether training drives Transformers toward regular solutions or pathological behaviors. For forward stability, we derive explicit bounds on the growth of hidden states in trained Transformers. For backward stability, we analyze how layer normalization affects the backpropagation of gradients, thereby explaining the training dynamics of each layer normalization placement. Our analysis also guides the scaling of residual steps in Transformer blocks, where appropriate choices can further improve stability and performance. Our numerical results corroborate our theoretical findings. Beyond these results, our framework provides a principled way to sanity-check the stability of Transformers under new architectural modifications, offering guidance for future designs.</li>
</ul>

<h3>Title: SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision</h3>
<ul>
<li><strong>Authors: </strong>D.V. Brovko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09912">https://arxiv.org/abs/2510.09912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09912">https://arxiv.org/pdf/2510.09912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09912]] SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision(https://arxiv.org/abs/2510.09912)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The relevance of this research lies in the growing demand for unmanned aerial vehicles (UAVs) capable of operating reliably in complex environments where conventional navigation becomes unreliable due to interference, poor visibility, or camouflage. Hyperspectral imaging (HSI) provides unique opportunities for UAV-based computer vision by enabling fine-grained material recognition and object differentiation, which are critical for navigation, surveillance, agriculture, and environmental monitoring. The aim of this work is to develop a deep learning architecture integrating HSI into UAV perception for navigation, object detection, and terrain classification. Objectives include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional architecture with spectral-spatial cross-attention, training, and benchmarking. The methodology is based on the modification of the Mobile 3D Vision Transformer (MDvT) by introducing the proposed SpectralCA block. This block employs bi-directional cross-attention to fuse spectral and spatial features, enhancing accuracy while reducing parameters and inference time. Experimental evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The findings confirm that the proposed architecture improves UAV perception efficiency, enabling real-time operation for navigation, object recognition, and environmental monitoring tasks. Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging, unmanned aerial vehicle, object detection, semi-supervised learning.</li>
</ul>

<h3>Title: Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery</h3>
<ul>
<li><strong>Authors: </strong>Aditya Malusare, Vineet Punyamoorty, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09914">https://arxiv.org/abs/2510.09914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09914">https://arxiv.org/pdf/2510.09914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09914]] Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery(https://arxiv.org/abs/2510.09914)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in generative modeling have demonstrated remarkable capabilities in molecular generation, yet the integration of comprehensive biomedical knowledge into these models has remained an untapped frontier. In this study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model), a novel framework that leverages knowledge graphs to augment diffusion-based generative models for drug discovery. By embedding structured information from large-scale knowledge graphs, K-DREAM directs molecular generation toward candidates with higher biological relevance and therapeutic suitability. This integration ensures that the generated molecules are aligned with specific therapeutic targets, moving beyond traditional heuristic-driven approaches. In targeted drug design tasks, K-DREAM generates drug candidates with improved binding affinities and predicted efficacy, surpassing current state-of-the-art generative models. It also demonstrates flexibility by producing molecules designed for multiple targets, enabling applications to complex disease mechanisms. These results highlight the utility of knowledge-enhanced generative models in rational drug design and their relevance to practical therapeutic development.</li>
</ul>

<h3>Title: Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Sicong Huang, Qianqi Yan, Shengze Wang, Ian Lane</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09915">https://arxiv.org/abs/2510.09915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09915">https://arxiv.org/pdf/2510.09915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09915]] Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning(https://arxiv.org/abs/2510.09915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Abstractive summarization using large language models (LLMs) has become an essential tool for condensing information. However, despite their ability to generate fluent summaries, these models sometimes produce unfaithful summaries, introducing hallucinations at the word, phrase, or concept level. Existing mitigation strategies, such as post-processing corrections or contrastive learning with synthetically generated negative samples, fail to fully address the diverse errors that can occur in LLM-generated summaries. In this paper, we investigate fine-tuning strategies to reduce the occurrence of unfaithful spans in generated summaries. First, we automatically generate summaries for the set of source documents in the training set with a variety of LLMs and then use GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging these annotations, we fine-tune LLMs with both hallucination-free summaries and annotated unfaithful spans to enhance model faithfulness. In this paper, we introduce a new dataset that contains both faithful and unfaithful summaries with span-level labels and we evaluate three techniques to fine-tuning a LLM to improve the faithfulness of the resulting summarization: gradient ascent, unlikelihood training, and task vector negation. Experimental results show that all three approaches successfully leverage span-level annotations to improve faithfulness, with unlikelihood training being the most effective.</li>
</ul>

<h3>Title: Advancing Intoxication Detection: A Smartwatch-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Manuel Segura, Pere Vergés, Richard Ky, Ramesh Arangott, Angela Kristine Garcia, Thang Dihn Trong, Makoto Hyodo, Alexandru Nicolau, Tony Givargis, Sergio Gago-Masague</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09916">https://arxiv.org/abs/2510.09916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09916">https://arxiv.org/pdf/2510.09916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09916]] Advancing Intoxication Detection: A Smartwatch-Based Approach(https://arxiv.org/abs/2510.09916)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Excess alcohol consumption leads to serious health risks and severe consequences for both individuals and their communities. To advocate for healthier drinking habits, we introduce a groundbreaking mobile smartwatch application approach to just-in-time interventions for intoxication warnings. In this work, we have created a dataset gathering TAC, accelerometer, gyroscope, and heart rate data from the participants during a period of three weeks. This is the first study to combine accelerometer, gyroscope, and heart rate smartwatch data collected over an extended monitoring period to classify intoxication levels. Previous research had used limited smartphone motion data and conventional machine learning (ML) algorithms to classify heavy drinking episodes; in this work, we use smartwatch data and perform a thorough evaluation of different state-of-the-art classifiers such as the Transformer, Bidirectional Long Short-Term Memory (bi-LSTM), Gated Recurrent Unit (GRU), One-Dimensional Convolutional Neural Networks (1D-CNN), and Hyperdimensional Computing (HDC). We have compared performance metrics for the algorithms and assessed their efficiency on resource-constrained environments like mobile hardware. The HDC model achieved the best balance between accuracy and efficiency, demonstrating its practicality for smartwatch-based applications.</li>
</ul>

<h3>Title: HeadsUp! High-Fidelity Portrait Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Renjie Li, Zihao Zhu, Xiaoyu Wang, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09924">https://arxiv.org/abs/2510.09924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09924">https://arxiv.org/pdf/2510.09924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09924]] HeadsUp! High-Fidelity Portrait Image Super-Resolution(https://arxiv.org/abs/2510.09924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Portrait pictures, which typically feature both human subjects and natural backgrounds, are one of the most prevalent forms of photography on social media. Existing image super-resolution (ISR) techniques generally focus either on generic real-world images or strictly aligned facial images (i.e., face super-resolution). In practice, separate models are blended to handle portrait photos: the face specialist model handles the face region, and the general model processes the rest. However, these blending approaches inevitably introduce blending or boundary artifacts around the facial regions due to different model training recipes, while human perception is particularly sensitive to facial fidelity. To overcome these limitations, we study the portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a single-step diffusion model that is capable of seamlessly restoring and upscaling portrait images in an end-to-end manner. Specifically, we build our model on top of a single-step diffusion model and develop a face supervision mechanism to guide the model in focusing on the facial region. We then integrate a reference-based mechanism to help with identity restoration, reducing face ambiguity in low-quality face restoration. Additionally, we have built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to support model training and benchmarking for portrait images. Extensive experiments show that HeadsUp achieves state-of-the-art performance on the PortraitISR task while maintaining comparable or higher performance on both general image and aligned face datasets.</li>
</ul>

<h3>Title: MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ching Chang, Ming-Chih Lo, Chiao-Tung Chan, Wen-Chih Peng, Tien-Fu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09930">https://arxiv.org/abs/2510.09930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09930">https://arxiv.org/pdf/2510.09930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09930]] MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation(https://arxiv.org/abs/2510.09930)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Web platforms, mobile applications, and connected sensing systems generate multivariate time series with states at multiple levels of granularity, from coarse regimes to fine-grained events. Effective segmentation in these settings requires integrating across granularities while supporting iterative refinement through sparse prompt signals, which provide a compact mechanism for injecting domain knowledge. Yet existing prompting approaches for time series segmentation operate only within local contexts, so the effect of a prompt quickly fades and cannot guide predictions across the entire sequence. To overcome this limitation, we propose MemPromptTSS, a framework for iterative multi-granularity segmentation that introduces persistent prompt memory. A memory encoder transforms prompts and their surrounding subsequences into memory tokens stored in a bank. This persistent memory enables each new prediction to condition not only on local cues but also on all prompts accumulated across iterations, ensuring their influence persists across the entire sequence. Experiments on six datasets covering wearable sensing and industrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy improvements over the best baseline in single- and multi-granularity segmentation under single iteration inference, and provides stronger refinement in iterative inference with average per-iteration gains of 2.66 percentage points compared to 1.19 for PromptTSS. These results highlight the importance of persistent memory for prompt-guided segmentation, establishing MemPromptTSS as a practical and effective framework for real-world applications.</li>
</ul>

<h3>Title: Denoising Diffusion as a New Framework for Underwater Images</h3>
<ul>
<li><strong>Authors: </strong>Nilesh Jain, Elie Alhajjar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09934">https://arxiv.org/abs/2510.09934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09934">https://arxiv.org/pdf/2510.09934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09934]] Denoising Diffusion as a New Framework for Underwater Images(https://arxiv.org/abs/2510.09934)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem. Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet</li>
</ul>

<h3>Title: Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Payel Bhattacharjee, Fengwei Tian, Meiyu Zhong, Guangyi Zhang, Osvaldo Simeone, Ravi Tandon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09942">https://arxiv.org/abs/2510.09942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09942">https://arxiv.org/pdf/2510.09942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09942]] Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding(https://arxiv.org/abs/2510.09942)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Edge-cloud speculative decoding (SD) accelerates inference by having a cloud-based large language model (LLM) that verifies draft tokens generated by a resource-constrained small language model (SLM) at the edge. A central bottleneck is the limited bandwidth of the edge-cloud link, which necessitates efficient compression of draft token distributions. We first derive an information-theoretic bound that decomposes the token rejection rate into contributions from SLM-LLM distribution mismatch and from quantization distortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample SD (SQS-SD) framework, which exploits distributional sparsity through structured sparsification and lattice-based quantization. Within this framework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts the retained token set via online conformal prediction to ensure bounded deviation from the dense distribution. Empirical results confirm that both approaches improve end-to-end latency and rejection rates in complimentary operating regimes.</li>
</ul>

<h3>Title: Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals</h3>
<ul>
<li><strong>Authors: </strong>Pouya Shaeri, Ryan T. Woo, Yasaman Mohammadpour, Ariane Middel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09945">https://arxiv.org/abs/2510.09945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09945">https://arxiv.org/pdf/2510.09945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09945]] Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals(https://arxiv.org/abs/2510.09945)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation models achieve high accuracy on benchmarks but often fail in real-world domains by relying on spurious correlations instead of true object boundaries. We propose a human-in-the-loop interactive framework that enables interventional learning through targeted human corrections of segmentation outputs. Our approach treats human corrections as interventional signals that show when reliance on superficial features (e.g., color or texture) is inappropriate. The system learns from these interventions by propagating correction-informed edits across visually similar images, effectively steering the model toward robust, semantically meaningful features rather than dataset-specific artifacts. Unlike traditional annotation approaches that simply provide more training data, our method explicitly identifies when and why the model fails and then systematically corrects these failure modes across the entire dataset. Through iterative human feedback, the system develops increasingly robust representations that generalize better to novel domains and resist artifactual correlations. We demonstrate that our framework improves segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on challenging cubemap data and yields 3-4$\times$ reductions in annotation effort compared to standard retraining, while maintaining competitive performance on benchmark datasets. This work provides a practical framework for researchers and practitioners seeking to build segmentation systems that are accurate, robust to dataset biases, data-efficient, and adaptable to real-world domains such as urban climate monitoring and autonomous driving.</li>
</ul>

<h3>Title: Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Mir Tafseer Nayeem, Sawsan Alqahtani, Md Tahmid Rahman Laskar, Tasnim Mohiuddin, M Saiful Bari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09947">https://arxiv.org/abs/2510.09947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09947">https://arxiv.org/pdf/2510.09947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09947]] Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation(https://arxiv.org/abs/2510.09947)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Tokenization is a crucial but under-evaluated step in large language models (LLMs). The standard metric, fertility (the average number of tokens per word), captures compression efficiency but obscures how vocabularies are allocated across languages and domains. We analyze six widely used tokenizers across seven languages and two domains, finding stable fertility for English, high fertility for Chinese, and little domain sensitivity. To address fertility's blind spots, we propose the Single Token Retention Rate (STRR), which measures the proportion of words preserved as single tokens. STRR reveals systematic prioritization of English, strong support for Chinese, and fragmentation in Hindi, offering an interpretable view of cross-lingual fairness. Our results show that STRR complements fertility and provides practical guidance for designing more equitable multilingual tokenizers.</li>
</ul>

<h3>Title: HTTP Request Synchronization Defeats Discrepancy Attacks</h3>
<ul>
<li><strong>Authors: </strong>Cem Topcuoglu, Kaan Onarlioglu, Steven Sprecher, Engin Kirda</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09952">https://arxiv.org/abs/2510.09952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09952">https://arxiv.org/pdf/2510.09952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09952]] HTTP Request Synchronization Defeats Discrepancy Attacks(https://arxiv.org/abs/2510.09952)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Contemporary web application architectures involve many layers of proxy services that process traffic. Due to the complexity of HTTP and vendor design decisions, these proxies sometimes process a given request in different ways. Attackers can exploit these processing discrepancies to launch damaging attacks including web cache poisoning and request smuggling. Discrepancy attacks are surging, yet, there exists no systemic defense. In this work, we propose the first comprehensive defense to address this problem, called HTTP Request Synchronization. Our scheme uses standard HTTP extension mechanisms to augment each request with a complete processing history. It propagates this context through the traffic path detailing how each server hop has processed said request. Using this history, every proxy server can validate that their processing is consistent with all previous hops, eliminating discrepancy attacks. We implement our scheme for 5 popular proxy technologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating its practical impact.</li>
</ul>

<h3>Title: J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training</h3>
<ul>
<li><strong>Authors: </strong>Salma J. Ahmed, Emad A. Mohammed, Azam Asilian Bidgoli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09953">https://arxiv.org/abs/2510.09953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09953">https://arxiv.org/pdf/2510.09953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09953]] J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training(https://arxiv.org/abs/2510.09953)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation, the process of dividing images into meaningful regions, is critical in medical applications for accurate diagnosis, treatment planning, and disease monitoring. Although manual segmentation by healthcare professionals produces precise outcomes, it is time-consuming, costly, and prone to variability due to differences in human expertise. Artificial intelligence (AI)-based methods have been developed to address these limitations by automating segmentation tasks; however, they often require large, annotated datasets that are rarely available in practice and frequently struggle to generalize across diverse imaging conditions due to inter-patient variability and rare pathological cases. In this paper, we propose Joint Retrieval Augmented Segmentation (J-RAS), a joint training method for guided image segmentation that integrates a segmentation model with a retrieval model. Both models are jointly optimized, enabling the segmentation model to leverage retrieved image-mask pairs to enrich its anatomical understanding, while the retrieval model learns segmentation-relevant features beyond simple visual similarity. This joint optimization ensures that retrieval actively contributes meaningful contextual cues to guide boundary delineation, thereby enhancing the overall segmentation performance. We validate J-RAS across multiple segmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two benchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For example, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice score of 0.8708$\pm$0.042 and a mean Hausdorff Distance (HD) of 1.8130$\pm$2.49, whereas with J-RAS, the performance improves substantially to a mean Dice score of 0.9115$\pm$0.031 and a mean HD of 1.1489$\pm$0.30. These results highlight the method's effectiveness and its generalizability across architectures and datasets.</li>
</ul>

<h3>Title: Clustering Result Re-guided Incomplete Multi-view Spectral Clustering</h3>
<ul>
<li><strong>Authors: </strong>Jun Yin, Runcheng Cai, Shiliang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09959">https://arxiv.org/abs/2510.09959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09959">https://arxiv.org/pdf/2510.09959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09959]] Clustering Result Re-guided Incomplete Multi-view Spectral Clustering(https://arxiv.org/abs/2510.09959)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Incomplete multi-view spectral clustering generalizes spectral clustering to multi-view data and simultaneously realizes the partition of multi-view data with missing views. For this category of method, K-means algorithm needs to be performed to generate the clustering result after the procedure of feature extraction. More importantly, the connectivity of samples reflected by the clustering result is not utilized effectively. To overcome these defects, we propose Clustering Result re-Guided Incomplete Multi-view Spectral Clustering (CRG_IMSC). CRG_IMSC obtains the clustering result directly by imposing nonnegative constraint to the extracted feature. Furthermore, it constructs the connectivity matrix according to the result of spectral clustering, and minimizes the residual of self-representation based on the connectivity matrix. A novel iterative algorithm using multiplicative update is developed to solve the optimization problem of CRG_IMSC, and its convergence is proved rigorously. On benchmark datasets, for multi-view data, CRG_IMSC performs better than state-of-the-art clustering methods, and the experimental results also demonstrate the convergence of CRG_IMSC algorithm.</li>
</ul>

<h3>Title: Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Lyu, Yinqian Sun, Erliang Lin, Huangrui Li, Ruolin Chen, Feifei Zhao, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09976">https://arxiv.org/abs/2510.09976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09976">https://arxiv.org/pdf/2510.09976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09976]] Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models(https://arxiv.org/abs/2510.09976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\pi_0$ have shown strong generalization by leveraging large-scale demonstrations, yet their performance is still fundamentally constrained by the quality and coverage of supervised data. Reinforcement learning (RL) provides a promising path for improving and fine-tuning VLAs through online interaction. However, conventional policy gradient methods are computationally infeasible in the context of flow-matching based models due to the intractability of the importance sampling process, which requires explicit computation of policy ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO) algorithm, which reformulates importance sampling by leveraging per-sample changes in the conditional flow-matching objective. Furthermore, FPO achieves stable and scalable online reinforcement fine-tuning of the $\pi_0$ model by integrating structure-aware credit assignment to enhance gradient efficiency, clipped surrogate objectives to stabilize optimization, multi-step latent exploration to encourage diverse policy updates, and a Q-ensemble mechanism to provide robust value estimation. We evaluate FPO on the LIBERO benchmark and the ALOHA simulation task against supervised, preference-aligned, diffusion-based, autoregressive online RL, and $\pi_0$-FAST baselines, observing consistent improvements over the imitation prior and strong alternatives with stable learning under sparse rewards. In addition, ablation studies and analyses of the latent space dynamics further highlight the contributions of individual components within FPO, validating the effectiveness of the proposed computational modules and the stable convergence of the conditional flow-matching objective during online RL.</li>
</ul>

<h3>Title: An Unsupervised Time Series Anomaly Detection Approach for Efficient Online Process Monitoring of Additive Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Frida Cantu, Salomon Ibarra, Arturo Gonzales, Jesus Barreda, Chenang Liu, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09977">https://arxiv.org/abs/2510.09977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09977">https://arxiv.org/pdf/2510.09977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09977]] An Unsupervised Time Series Anomaly Detection Approach for Efficient Online Process Monitoring of Additive Manufacturing(https://arxiv.org/abs/2510.09977)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Online sensing plays an important role in advancing modern manufacturing. The real-time sensor signals, which can be stored as high-resolution time series data, contain rich information about the operation status. One of its popular usages is online process monitoring, which can be achieved by effective anomaly detection from the sensor signals. However, most existing approaches either heavily rely on labeled data for training supervised models, or are designed to detect only extreme outliers, thus are ineffective at identifying subtle semantic off-track anomalies to capture where new regimes or unexpected routines start. To address this challenge, we propose an matrix profile-based unsupervised anomaly detection algorithm that captures fabrication cycle similarity and performs semantic segmentation to precisely identify the onset of defect anomalies in additive manufacturing. The effectiveness of the proposed method is demonstrated by the experiments on real-world sensor data.</li>
</ul>

<h3>Title: Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Fan Zuo, Donglin Zhou, Jingqin Gao, Kaan Ozbay</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09981">https://arxiv.org/abs/2510.09981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09981">https://arxiv.org/pdf/2510.09981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09981]] Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making(https://arxiv.org/abs/2510.09981)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate, scalable traffic monitoring is critical for real-time and long-term transportation management, particularly during disruptions such as natural disasters, large construction projects, or major policy changes like New York City's first-in-the-nation congestion pricing program. However, widespread sensor deployment remains limited due to high installation, maintenance, and data management costs. While traffic cameras offer a cost-effective alternative, existing video analytics struggle with dynamic camera viewpoints and massive data volumes from large camera networks. This study presents an end-to-end AI-based framework leveraging existing traffic camera infrastructure for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11 model, trained on localized urban scenes, extracts multimodal traffic density and classification metrics in real time. To address inconsistencies from non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based viewpoint normalization method. A domain-specific large language model was also integrated to process massive data from a 24/7 video stream to generate frequent, automated summaries of evolving traffic patterns, a task far exceeding manual capabilities. We validated the system using over 9 million images from roughly 1,000 traffic cameras during the early rollout of NYC congestion pricing in 2025. Results show a 9% decline in weekday passenger vehicle density within the Congestion Relief Zone, early truck volume reductions with signs of rebound, and consistent increases in pedestrian and cyclist activity at corridor and zonal scales. Experiments showed that example-based prompts improved LLM's numerical accuracy and reduced hallucinations. These findings demonstrate the framework's potential as a practical, infrastructure-ready solution for large-scale, policy-relevant traffic monitoring with minimal human intervention.</li>
</ul>

<h3>Title: Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Kartikeya Aneja, Nagender Aneja, Murat Kantarcioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09984">https://arxiv.org/abs/2510.09984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09984">https://arxiv.org/pdf/2510.09984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09984]] Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection(https://arxiv.org/abs/2510.09984)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Software systems can be represented as graphs, capturing dependencies among functions and processes. An interesting aspect of software systems is that they can be represented as different types of graphs, depending on the extraction goals and priorities. For example, function calls within the software can be captured to create function call graphs, which highlight the relationships between functions and their dependencies. Alternatively, the processes spawned by the software can be modeled to generate process interaction graphs, which focus on runtime behavior and inter-process communication. While these graph representations are related, each captures a distinct perspective of the system, providing complementary insights into its structure and operation. While previous studies have leveraged graph neural networks (GNNs) to analyze software behaviors, most of this work has focused on a single type of graph representation. The joint modeling of both function call graphs and process interaction graphs remains largely underexplored, leaving opportunities for deeper, multi-perspective analysis of software systems. This paper presents a pipeline for constructing and training Function Call Graphs (FCGs) and Process Call Graphs (PCGs) and learning joint embeddings. We demonstrate that joint embeddings outperform a single-graph model. In this paper, we propose GeminiNet, a unified neural network approach that learns joint embeddings from both FCGs and PCGs. We construct a new dataset of 635 Windows executables (318 malicious and 317 benign), extracting FCGs via Ghidra and PCGs via this http URL sandbox. GeminiNet employs dual graph convolutional branches with an adaptive gating mechanism that balances contributions from static and dynamic views.</li>
</ul>

<h3>Title: Prismo: A Decision Support System for Privacy-Preserving ML Framework Selection</h3>
<ul>
<li><strong>Authors: </strong>Nges Brian Njungle, Eric Jahns, Luigi Mastromauro, Edwin P. Kayang, Milan Stojkov, Michel A. Kinsy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09985">https://arxiv.org/abs/2510.09985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09985">https://arxiv.org/pdf/2510.09985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09985]] Prismo: A Decision Support System for Privacy-Preserving ML Framework Selection(https://arxiv.org/abs/2510.09985)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Machine learning has become a crucial part of our lives, with applications spanning nearly every aspect of our daily activities. However, using personal information in machine learning applications has sparked significant security and privacy concerns about user data. To address these challenges, different privacy-preserving machine learning (PPML) frameworks have been developed to protect sensitive information in machine learning applications. These frameworks generally attempt to balance design trade-offs such as computational efficiency, communication overhead, security guarantees, and scalability. Despite the advancements, selecting the optimal framework and parameters for specific deployment scenarios remains a complex and critical challenge for privacy and security application developers. We present Prismo, an open-source recommendation system designed to aid in selecting optimal parameters and frameworks for different PPML application scenarios. Prismo enables users to explore a comprehensive space of PPML frameworks through various properties based on user-defined objectives. It supports automated filtering of suitable candidate frameworks by considering parameters such as the number of parties in multi-party computation or federated learning and computation cost constraints in homomorphic encryption. Prismo models every use case into a Linear Integer Programming optimization problem, ensuring tailored solutions are recommended for each scenario. We evaluate Prismo's effectiveness through multiple use cases, demonstrating its ability to deliver best-fit solutions in different deployment scenarios.</li>
</ul>

<h3>Title: Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wei, Xiang Zhang, Yuejin Yang, Wenxuan Huang, Juntai Cao, Sheng Xu, Xiang Zhuang, Zhangyang Gao, Muhammad Abdul-Mageed, Laks V.S. Lakshmanan, Chenyu You, Wanli Ouyang, Siqi Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.09988">https://arxiv.org/abs/2510.09988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.09988">https://arxiv.org/pdf/2510.09988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.09988]] Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey(https://arxiv.org/abs/2510.09988)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deliberative tree search is a cornerstone of modern Large Language Model (LLM) research, driving the pivot from brute-force scaling toward algorithmic efficiency. This single paradigm unifies two critical frontiers: \textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve hard problems, and \textbf{Self-Improvement}, which uses search-generated data to durably enhance model parameters. However, this burgeoning field is fragmented and lacks a common formalism, particularly concerning the ambiguous role of the reward signal -- is it a transient heuristic or a durable learning target? This paper resolves this ambiguity by introducing a unified framework that deconstructs search algorithms into three core components: the \emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition Function}. We establish a formal distinction between transient \textbf{Search Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for Self-Improvement. Building on this formalism, we introduce a component-centric taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward more systematic progress in creating autonomous, self-improving agents.</li>
</ul>

<h3>Title: Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Bach C. Le, Tung V. Dao, Binh T. Nguyen, Hong T.M. Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10000">https://arxiv.org/abs/2510.10000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10000">https://arxiv.org/pdf/2510.10000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10000]] Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks(https://arxiv.org/abs/2510.10000)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Wasserstein distributionally robust optimization (WDRO) provides a framework for adversarial robustness, yet existing methods based on global Lipschitz continuity or strong duality often yield loose upper bounds or require prohibitive computation. In this work, we address these limitations by introducing a primal approach and adopting a notion of exact Lipschitz certificate to tighten this upper bound of WDRO. In addition, we propose a novel Wasserstein distributional attack (WDA) that directly constructs a candidate for the worst-case distribution. Compared to existing point-wise attack and its variants, our WDA offers greater flexibility in the number and location of attack points. In particular, by leveraging the piecewise-affine structure of ReLU networks on their activation cells, our approach results in an exact tractable characterization of the corresponding WDRO problem. Extensive evaluations demonstrate that our method achieves competitive robust accuracy against state-of-the-art baselines while offering tighter certificates than existing methods. Our code is available at this https URL</li>
</ul>

<h3>Title: Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Hong, Siqing Li, Muqing Jian, Luming Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10004">https://arxiv.org/abs/2510.10004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10004">https://arxiv.org/pdf/2510.10004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10004]] Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification(https://arxiv.org/abs/2510.10004)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing EEG recognition models suffer from poor cross-paradigm generalization due to dataset-specific constraints and individual variability. To overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid Network), an end-to-end unified architecture featuring robust multistream synergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive convolutions. The framework uniquely integrates: 1) Aligned time-frequency streams maintaining temporal synchronization with STFT for bidirectional modeling, 2) PTFA-based multi-scale feature enhancement amplifying critical neural patterns, 3) BiTCN with learnable fusion capturing forward/backward neural dynamics. Demonstrating enhanced robustness, BITE achieves state-of-the-art performance across four divergent paradigms (BCICIV-2A/2B, HGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject generalization. As a unified architecture, it combines robust performance across both MI and SSVEP tasks with exceptional computational efficiency. Our work validates that paradigm-aligned spectral-temporal processing is essential for reliable BCI systems. Just as its name suggests, BITE "takes a bite out of EEG." The source code is available at this https URL.</li>
</ul>

<h3>Title: Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety</h3>
<ul>
<li><strong>Authors: </strong>Yuyi Huang, Runzhe Zhan, Lidia S.Chao, Ailin Tao, Derek F.Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10013">https://arxiv.org/abs/2510.10013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10013">https://arxiv.org/pdf/2510.10013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10013]] Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety(https://arxiv.org/abs/2510.10013)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed for complex reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a key paradigm for structured inference. Despite early-stage safeguards enabled by alignment techniques such as RLHF, we identify a previously underexplored vulnerability: reasoning trajectories in Long-CoT models can drift from aligned paths, resulting in content that violates safety constraints. We term this phenomenon Path Drift. Through empirical analysis, we uncover three behavioral triggers of Path Drift: (1) first-person commitments that induce goal-driven reasoning that delays refusal signals; (2) ethical evaporation, where surface-level disclaimers bypass alignment checkpoints; (3) condition chain escalation, where layered cues progressively steer models toward unsafe completions. Building on these insights, we introduce a three-stage Path Drift Induction Framework comprising cognitive load amplification, self-role priming, and condition chain hijacking. Each stage independently reduces refusal rates, while their combination further compounds the effect. To mitigate these risks, we propose a path-level defense strategy incorporating role attribution correction and metacognitive reflection (reflective safety cues). Our findings highlight the need for trajectory-level alignment oversight in long-form reasoning beyond token-level alignment.</li>
</ul>

<h3>Title: Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning</h3>
<ul>
<li><strong>Authors: </strong>Junan Chen, Trung Thanh Nguyen, Takahiro Komamizu, Ichiro Ide</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10022">https://arxiv.org/abs/2510.10022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10022">https://arxiv.org/pdf/2510.10022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10022]] Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning(https://arxiv.org/abs/2510.10022)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in video captioning are driven by large-scale pretrained models, which follow the standard "pre-training followed by fine-tuning" paradigm, where the full model is fine-tuned for downstream tasks. Although effective, this approach becomes computationally prohibitive as the model size increases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a promising alternative, but primarily focuses on the language components of Multimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains underexplored in multimodal tasks and lacks sufficient understanding of visual information during fine-tuning the model. To bridge this gap, we propose Query-Adapter (Q-Adapter), a lightweight visual adapter module designed to enhance MLLMs by enabling efficient fine-tuning for the video captioning task. Q-Adapter introduces learnable query tokens and a gating layer into Vision Encoder, enabling effective extraction of sparse, caption-relevant features without relying on external textual supervision. We evaluate Q-Adapter on two well-known video captioning datasets, MSR-VTT and MSVD, where it achieves state-of-the-art performance among the methods that take the PEFT approach across BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves competitive performance compared to methods that take the full fine-tuning approach while requiring only 1.4% of the parameters. We further analyze the impact of key hyperparameters and design choices on fine-tuning effectiveness, providing insights into optimization strategies for adapter-based learning. These results highlight the strong potential of Q-Adapter in balancing caption quality and parameter efficiency, demonstrating its scalability for video-language modeling.</li>
</ul>

<h3>Title: Skill-Targeted Adaptive Training</h3>
<ul>
<li><strong>Authors: </strong>Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10023">https://arxiv.org/abs/2510.10023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10023">https://arxiv.org/pdf/2510.10023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10023]] Skill-Targeted Adaptive Training(https://arxiv.org/abs/2510.10023)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language models often show little to no improvement (i.e., "saturation") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Lanruo Wang, Su Liu, Xin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10025">https://arxiv.org/abs/2510.10025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10025">https://arxiv.org/pdf/2510.10025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10025]] Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default(https://arxiv.org/abs/2510.10025)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models work well for many NLP tasks, but they are hard to deploy in health settings with strict cost, latency, and privacy limits. We revisit a lightweight recipe for medical abstract classification and ask how far compact encoders can go under a controlled budget. Using the public medical abstracts corpus, we finetune BERT base and DistilBERT with three objectives standard cross-entropy, class weighted cross entropy, and focal loss keeping tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with plain cross-entropy gives the best balance on the test set while using far fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1, release the evaluation code, and include confusion analyses to make error patterns clear. Our results suggest a practical default: start with a compact encoder and cross-entropy, then add calibration and task-specific checks before moving to heavier models.</li>
</ul>

<h3>Title: Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10028">https://arxiv.org/abs/2510.10028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10028">https://arxiv.org/pdf/2510.10028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10028]] Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization(https://arxiv.org/abs/2510.10028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.</li>
</ul>

<h3>Title: FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Sahng-Min Han, Minjae Kim, Jinho Cha, Se-woon Choe, Eunchan Daniel Cha, Jungwon Choi, Kyudong Jung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10041">https://arxiv.org/abs/2510.10041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10041">https://arxiv.org/pdf/2510.10041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10041]] FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis(https://arxiv.org/abs/2510.10041)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning in small and imbalanced biomedical datasets remains fundamentally constrained by unstable optimization and poor generalization. We present the first biomedical implementation of FOSSIL (Flexible Optimization via Sample-Sensitive Importance Learning), a regret-minimizing weighting framework that adaptively balances training emphasis according to sample difficulty. Using softmax-based uncertainty as a continuous measure of difficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate FOSSIL into both convolutional and transformer-based architectures for Mpox skin lesion diagnosis. Across all settings, FOSSIL substantially improves discrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under real-world perturbations, outperforming conventional baselines without metadata, manual curation, or synthetic augmentation. The results position FOSSIL as a generalizable, data-efficient, and interpretable framework for difficulty-aware learning in medical imaging under data scarcity.</li>
</ul>

<h3>Title: Complementary and Contrastive Learning for Audio-Visual Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sitong Gong, Yunzhi Zhuge, Lu Zhang, Pingping Zhang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10051">https://arxiv.org/abs/2510.10051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10051">https://arxiv.org/pdf/2510.10051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10051]] Complementary and Contrastive Learning for Audio-Visual Segmentation(https://arxiv.org/abs/2510.10051)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps that correlate with the auditory signals of objects. This field has seen significant progress with numerous CNN and Transformer-based methods enhancing the segmentation accuracy and robustness. Traditional CNN approaches manage audio-visual interactions through basic operations like padding and multiplications but are restricted by CNNs' limited local receptive field. More recently, Transformer-based methods treat auditory cues as queries, utilizing attention mechanisms to enhance audio-visual cooperation within frames. Nevertheless, they typically struggle to extract multimodal coefficients and temporal dynamics adequately. To overcome these limitations, we present the Complementary and Contrastive Transformer (CCFormer), a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively. Our CCFormer initiates with the Early Integration Module (EIM) that employs a parallel bilateral architecture, merging multi-scale visual features with audio data to boost cross-modal complementarity. To extract the intra-frame spatial features and facilitate the perception of temporal coherence, we introduce the Multi-query Transformer Module (MTM), which dynamically endows audio queries with learning capabilities and models the frame and video-level relations simultaneously. Furthermore, we propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across both modalities in the unified feature space. Through the effective combination of those designs, our method sets new state-of-the-art benchmarks across the S4, MS3 and AVSS datasets. Our source code and model weights will be made publicly available at this https URL</li>
</ul>

<h3>Title: Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels</h3>
<ul>
<li><strong>Authors: </strong>Zhi-Fen He, Ren-Dong Xie, Bo Li, Bin Liu, Jin-Yan Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10055">https://arxiv.org/abs/2510.10055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10055">https://arxiv.org/pdf/2510.10055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10055]] Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels(https://arxiv.org/abs/2510.10055)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-label image recognition with incomplete labels is a critical learning task and has emerged as a focal topic in computer vision. However, this task is confronted with two core challenges: semantic-aware feature learning and missing label recovery. In this paper, we propose a novel Collaborative Learning of Semantic-aware feature learning and Label recovery (CLSL) method for multi-label image recognition with incomplete labels, which unifies the two aforementioned challenges into a unified learning framework. More specifically, we design a semantic-related feature learning module to learn robust semantic-related features by discovering semantic information and label correlations. Then, a semantic-guided feature enhancement module is proposed to generate high-quality discriminative semantic-aware features by effectively aligning visual and semantic feature spaces. Finally, we introduce a collaborative learning framework that integrates semantic-aware feature learning and label recovery, which can not only dynamically enhance the discriminability of semantic-aware features but also adaptively infer and recover missing labels, forming a mutually reinforced loop between the two processes. Extensive experiments on three widely used public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art multi-label image recognition methods with incomplete labels.</li>
</ul>

<h3>Title: HUME: Measuring the Human-Model Performance Gap in Text Embedding Task</h3>
<ul>
<li><strong>Authors: </strong>Adnan El Assadi, Isaac Chung, Roman Solomatin, Niklas Muennighoff, Kenneth Enevoldsen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10062">https://arxiv.org/abs/2510.10062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10062">https://arxiv.org/pdf/2510.10062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10062]] HUME: Measuring the Human-Model Performance Gap in Text Embedding Task(https://arxiv.org/abs/2510.10062)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at this https URL.</li>
</ul>

<h3>Title: CLMN: Concept based Language Models via Neural Symbolic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yibo Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10063">https://arxiv.org/abs/2510.10063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10063">https://arxiv.org/pdf/2510.10063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10063]] CLMN: Concept based Language Models via Neural Symbolic Reasoning(https://arxiv.org/abs/2510.10063)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has advanced NLP, but interpretability remains limited, especially in healthcare and finance. Concept bottleneck models tie predictions to human concepts in vision, but NLP versions either use binary activations that harm text representations or latent concepts that weaken semantics, and they rarely model dynamic concept interactions such as negation and context. We introduce the Concept Language Model Network (CLMN), a neural-symbolic framework that keeps both performance and interpretability. CLMN represents concepts as continuous, human-readable embeddings and applies fuzzy-logic reasoning to learn adaptive interaction rules that state how concepts affect each other and the final decision. The model augments original text features with concept-aware representations and automatically induces interpretable logic rules. Across multiple datasets and pre-trained language models, CLMN achieves higher accuracy than existing concept-based methods while improving explanation quality. These results show that integrating neural representations with symbolic reasoning in a unified concept space can yield practical, transparent NLP systems.</li>
</ul>

<h3>Title: ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jinyang Zhang, Yue Fang, Hongxin Ding, Weibin Liao, Muyang Ye, Xu Chu, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10071">https://arxiv.org/abs/2510.10071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10071">https://arxiv.org/pdf/2510.10071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10071]] ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning(https://arxiv.org/abs/2510.10071)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Conventional continual pretraining (CPT) for large language model (LLM) domain adaptation often suffers from catastrophic forgetting and limited domain capacity. Existing strategies adopt layer expansion, introducing additional trainable parameters to accommodate new knowledge. However, the uniform expansion and updates still entangle general and domain learning, undermining its effectiveness. Our pilot studies reveal that LLMs exhibit functional specialization, where layers and units differentially encode general-critical capabilities, suggesting that parameter expansion and optimization should be function-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled Tuning for continual pretraining, a two-stage framework for domain-adaptive CPT. ADEPT first performs General-Competence Guided Selective Layer Expansion, duplicating layers least critical for the general domain to increase representational capacity while minimizing interference with general knowledge. It then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter units within expanded layers according to their general-domain importance and assigning asymmetric learning rates to balance knowledge injection and retention. Experiments on mathematical and medical benchmarks show that ADEPT outperforms full-parameter CPT by up to 5.76% on the general domain and 5.58% on the target domain with only 15% of parameters tuned and less than 50% training time. Ablation studies, theoretical analysis, and extended investigations further demonstrate the necessity of targeted expansion and decoupled optimization, providing new principles for efficient and robust domain-adaptive CPT. Our code is open-sourced at this https URL</li>
</ul>

<h3>Title: Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference</h3>
<ul>
<li><strong>Authors: </strong>Hua Cai, Shuang Zhao, Liang Zhang, Xuli Shen, Qing Xu, Weilin Shen, Zihao Wen, Tianke Ban</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10072">https://arxiv.org/abs/2510.10072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10072">https://arxiv.org/pdf/2510.10072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10072]] Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference(https://arxiv.org/abs/2510.10072)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning-focused large language models (LLMs) are rapidly evolving across various domains, yet their capabilities in handling complex legal problems remains underexplored. In this paper, we introduce Unilaw-R1, a large language model tailored for legal reasoning. With a lightweight 7-billion parameter scale, Unilaw-R1 significantly reduces deployment cost while effectively tackling three core challenges in the legal domain: insufficient legal knowledge, unreliable reasoning logic, and weak business generalization. To address these issues, we first construct Unilaw-R1-Data, a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples. Based on this, we adopt a two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which significantly boosts the performance on complex legal reasoning tasks and supports interpretable decision-making in legal AI applications. To assess legal reasoning ability, we also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.</li>
</ul>

<h3>Title: SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Zonghao Ying, Yangguang Shao, Jianle Gan, Gan Xu, Junjie Shen, Wenxin Zhang, Quanchen Zou, Junzheng Shi, Zhenfei Yin, Mingchuan Zhang, Aishan Liu, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10073">https://arxiv.org/abs/2510.10073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10073">https://arxiv.org/pdf/2510.10073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10073]] SecureWebArena: A Holistic Security Evaluation Benchmark for LVLM-based Web Agents(https://arxiv.org/abs/2510.10073)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Large vision-language model (LVLM)-based web agents are emerging as powerful tools for automating complex online tasks. However, when deployed in real-world environments, they face serious security risks, motivating the design of security evaluation benchmarks. Existing benchmarks provide only partial coverage, typically restricted to narrow scenarios such as user-level prompt manipulation, and thus fail to capture the broad range of agent vulnerabilities. To address this gap, we present \tool{}, the first holistic benchmark for evaluating the security of LVLM-based web agents. \tool{} first introduces a unified evaluation suite comprising six simulated but realistic web environments (\eg, e-commerce platforms, community forums) and includes 2,970 high-quality trajectories spanning diverse tasks and attack settings. The suite defines a structured taxonomy of six attack vectors spanning both user-level and environment-level manipulations. In addition, we introduce a multi-layered evaluation protocol that analyzes agent failures across three critical dimensions: internal reasoning, behavioral trajectory, and task outcome, facilitating a fine-grained risk analysis that goes far beyond simple success metrics. Using this benchmark, we conduct large-scale experiments on 9 representative LVLMs, which fall into three categories: general-purpose, agent-specialized, and GUI-grounded. Our results show that all tested agents are consistently vulnerable to subtle adversarial manipulations and reveal critical trade-offs between model specialization and security. By providing (1) a comprehensive benchmark suite with diverse environments and a multi-layered evaluation pipeline, and (2) empirical insights into the security challenges of modern LVLM-based web agents, \tool{} establishes a foundation for advancing trustworthy web agent deployment.</li>
</ul>

<h3>Title: A-IPO: Adaptive Intent-driven Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenqing Wang (1), Muhammad Asif Ali (2), Ali Shoker (2), Ruohan Yang (1), Junyang Chen (3), Ying Sha (1), Huan Wang (1) ((1) Huazhong Agricultural University, China,(2) King Abdullah University of Science and Technology, KSA,(3) Shenzhen University, China)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10077">https://arxiv.org/abs/2510.10077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10077">https://arxiv.org/pdf/2510.10077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10077]] A-IPO: Adaptive Intent-driven Preference Optimization(https://arxiv.org/abs/2510.10077)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Human preferences are diverse and dynamic, shaped by regional, cultural, and social factors. Existing alignment methods like Direct Preference Optimization (DPO) and its variants often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts. To address these limitations, we introduce \underline{\textbf{A}}daptive \textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference \textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO introduces an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function, encouraging stronger alignment between the preferred model's responses and the user's underlying intentions. We demonstrate, both theoretically and empirically, that incorporating an intention--response similarity term increases the preference margin (by a positive shift of $\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation between preferred and dispreferred responses compared to DPO. For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess real-world and adversarial preference alignment. Through explicit modeling of diverse user intents,A-IPO facilitates pluralistic preference optimization while simultaneously enhancing adversarial robustness in preference alignment. Comprehensive empirical evaluation demonstrates that A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics: up to +24.8 win-rate and +45.6 Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention Consistency Score on GlobalOpinionQA-Ext.</li>
</ul>

<h3>Title: Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers</h3>
<ul>
<li><strong>Authors: </strong>Parthiv Chatterjee, Shivam Sonawane, Amey Hengle, Aditya Tanna, Sourish Dasgupta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10082">https://arxiv.org/abs/2510.10082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10082">https://arxiv.org/pdf/2510.10082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10082]] Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers(https://arxiv.org/abs/2510.10082)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document summarization enables efficient extraction of user-relevant content but is inherently shaped by individual subjectivity, making it challenging to identify subjective salient information in multifaceted documents. This complexity underscores the necessity for personalized summarization. However, training models for personalized summarization has so far been challenging, particularly because diverse training data containing both user preference history (i.e., click-skip trajectory) and expected (gold-reference) summaries are scarce. The MS/CAS PENS dataset is a valuable resource but includes only preference history without target summaries, preventing end-to-end supervised learning, and its limited topic-transition diversity further restricts generalization. To address this, we propose $\mathrm{PerAugy}$, a novel cross-trajectory shuffling and summary-content perturbation based data augmentation technique that significantly boosts the accuracy of four state-of-the-art baseline (SOTA) user-encoders commonly used in personalized summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We select two such SOTA summarizer frameworks as baselines and observe that when augmented with their corresponding improved user-encoders, they consistently show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t. PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the augmented dataset by \peraugy, we introduce three dataset diversity metrics -- $\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity. We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with user-encoder performance on the PerAugy-generated dataset across all accuracy metrics, indicating that increased dataset diversity is a key factor driving performance gains.</li>
</ul>

<h3>Title: Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework</h3>
<ul>
<li><strong>Authors: </strong>Meijun Zhou, Gang Mei, Zhengjing Ma, Nengxiong Xu, Jianbing Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10084">https://arxiv.org/abs/2510.10084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10084">https://arxiv.org/pdf/2510.10084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10084]] Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework(https://arxiv.org/abs/2510.10084)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tracking the spatiotemporal evolution of large-scale landslide scars is critical for understanding the evolution mechanisms and failure precursors, enabling effective early-warning. However, most existing studies have focused on single-phase or pre- and post-failure dual-phase landslide identification. Although these approaches delineate post-failure landslide boundaries, it is challenging to track the spatiotemporal evolution of landslide scars. To address this problem, this study proposes a novel and universal framework for tracking the spatiotemporal evolution of large-scale landslide scars using a vision foundation model. The key idea behind the proposed framework is to reconstruct discrete optical remote sensing images into a continuous video sequence. This transformation enables a vision foundation model, which is developed for video segmentation, to be used for tracking the evolution of landslide scars. The proposed framework operates within a knowledge-guided, auto-propagation, and interactive refinement paradigm to ensure the continuous and accurate identification of landslide scars. The proposed framework was validated through application to two representative cases: the post-failure Baige landslide and the active Sela landslide (2017-2025). Results indicate that the proposed framework enables continuous tracking of landslide scars, capturing both failure precursors critical for early warning and post-failure evolution essential for assessing secondary hazards and long-term stability.</li>
</ul>

<h3>Title: Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Guozhi Liu, Qi Mu, Tiansheng Huang, Xinhua Wang, Li Shen, Weiwei Lin, Zhang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10085">https://arxiv.org/abs/2510.10085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10085">https://arxiv.org/pdf/2510.10085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10085]] Pharmacist: Safety Alignment Data Curation for Large Language Models against Harmful Fine-tuning(https://arxiv.org/abs/2510.10085)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Harmful fine-tuning issues present significant safety challenges for fine-tuning-as-a-service in large language models. Existing alignment-stage defenses, e.g., Vaccine, Repnoise, Booster, and T-Vaccine, mitigate harmful fine-tuning issues by enhancing the model's robustness during the alignment phase. While these methods have been proposed to mitigate the issue, they often overlook a critical upstream factor: the role of the original safety-alignment data. We observe that their defense performance and computational efficiency remain constrained by the quality and composition of the alignment dataset. To address this limitation, we propose Pharmacist, a safety alignment data curation solution that enhances defense against harmful fine-tuning by selecting a high-quality and safety-critical core subset from the original alignment data. The core idea of Pharmacist is to train an alignment data selector to rank alignment data. Specifically, up-ranking high-quality and safety-critical alignment data, down-ranking low-quality and non-safety-critical data. Empirical results indicate that models trained on datasets selected by Pharmacist outperform those trained on datasets selected by existing selection methods in both defense and inference performance. In addition, Pharmacist can be effectively integrated with mainstream alignment-stage defense methods. For example, when applied to RepNoise and T-Vaccine, using the dataset selected by Pharmacist instead of the full dataset leads to improvements in defense performance by 2.60\% and 3.30\%, respectively, and enhances inference performance by 3.50\% and 1.10\%. Notably, it reduces training time by 56.83\% and 57.63\%, respectively. Our code is available at this https URL.</li>
</ul>

<h3>Title: What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Gong, Jiaye Teng, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10089">https://arxiv.org/abs/2510.10089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10089">https://arxiv.org/pdf/2510.10089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10089]] What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)(https://arxiv.org/abs/2510.10089)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While looped transformers (termed as Looped-Attn) often outperform standard transformers (termed as Single-Attn) on complex reasoning tasks, the theoretical basis for this advantage remains underexplored. In this paper, we explain this phenomenon through the lens of loss landscape geometry, inspired by empirical observations of their distinct dynamics at both sample and Hessian levels. To formalize this, we extend the River-Valley landscape model by distinguishing between U-shaped valleys (flat) and V-shaped valleys (steep). Based on empirical observations, we conjecture that the recursive architecture of Looped-Attn induces a landscape-level inductive bias towards River-V-Valley. Theoretical derivations based on this inductive bias guarantee a better loss convergence along the river due to valley hopping, and further encourage learning about complex patterns compared to the River-U-Valley induced by Single-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical Framework for Progressive Training), a staged training framework that accelerates the training process of Looped-Attn while achieving comparable performances.</li>
</ul>

<h3>Title: Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Lu, Haihong Xiao, Xueyan Zhao, Wenxiong Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10097">https://arxiv.org/abs/2510.10097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10097">https://arxiv.org/pdf/2510.10097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10097]] Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting(https://arxiv.org/abs/2510.10097)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced 3D reconstruction and novel view synthesis, but remain heavily dependent on accurate camera poses and dense viewpoint coverage. These requirements limit their applicability in sparse-view settings, where pose estimation becomes unreliable and supervision is insufficient. To overcome these challenges, we introduce Gesplat, a 3DGS-based framework that enables robust novel view synthesis and geometrically consistent reconstruction from unposed sparse images. Unlike prior works that rely on COLMAP for sparse point cloud initialization, we leverage the VGGT foundation model to obtain more reliable initial poses and dense point clouds. Our approach integrates several key innovations: 1) a hybrid Gaussian representation with dual position-shape optimization enhanced by inter-view matching consistency; 2) a graph-guided attribute refinement module to enhance scene details; and 3) flow-based depth regularization that improves depth estimation accuracy for more effective supervision. Comprehensive quantitative and qualitative experiments demonstrate that our approach achieves more robust performance on both forward-facing and large-scale complex datasets compared to other pose-free methods.</li>
</ul>

<h3>Title: Cooperative Pseudo Labeling for Unsupervised Federated Classification</h3>
<ul>
<li><strong>Authors: </strong>Kuangpu Guo, Lijun Sheng, Yongcan Yu, Jian Liang, Zilei Wang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10100">https://arxiv.org/abs/2510.10100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10100">https://arxiv.org/pdf/2510.10100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10100]] Cooperative Pseudo Labeling for Unsupervised Federated Classification(https://arxiv.org/abs/2510.10100)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Unsupervised Federated Learning (UFL) aims to collaboratively train a global model across distributed clients without sharing data or accessing label information. Previous UFL works have predominantly focused on representation learning and clustering tasks. Recently, vision language models (e.g., CLIP) have gained significant attention for their powerful zero-shot prediction capabilities. Leveraging this advancement, classification problems that were previously infeasible under the UFL paradigm now present promising new opportunities, yet remain largely unexplored. In this paper, we extend UFL to the classification problem with CLIP for the first time and propose a novel method, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative \underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}). Specifically, clients estimate and upload their pseudo label distribution, and the server adjusts and redistributes them to avoid global imbalance among classes. Moreover, we introduce a partial prompt aggregation protocol for effective collaboration and personalization. In particular, visual prompts containing general image features are aggregated at the server, while text prompts encoding personalized knowledge are retained locally. Extensive experiments demonstrate the superior performance of our FedCoPL compared to baseline methods. Our code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Rademacher Meets Colors: More Expressivity, but at What Cost ?</h3>
<ul>
<li><strong>Authors: </strong>Martin Carrasco, Caio Deberaldini Netto, Vahan A. Martirosyan, Aneeqa Mehrab, Ehimare Okoyomon, Caterina Graziani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10101">https://arxiv.org/abs/2510.10101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10101">https://arxiv.org/pdf/2510.10101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10101]] Rademacher Meets Colors: More Expressivity, but at What Cost ?(https://arxiv.org/abs/2510.10101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The expressive power of graph neural networks (GNNs) is typically understood through their correspondence with graph isomorphism tests such as the Weisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a richer set of graphs, they are also observed to suffer from higher generalization error. This work provides a theoretical explanation for this trade-off by linking expressivity and generalization through the lens of coloring algorithms. Specifically, we show that the number of equivalence classes induced by WL colorings directly bounds the GNNs Rademacher complexity -- a key data-dependent measure of generalization. Our analysis reveals that greater expressivity leads to higher complexity and thus weaker generalization guarantees. Furthermore, we prove that the Rademacher complexity is stable under perturbations in the color counts across different samples, ensuring robustness to sampling variability across datasets. Importantly, our framework is not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN architectures and expressivity measures that partition graphs into equivalence classes. These results unify the study of expressivity and generalization in GNNs, providing a principled understanding of why increasing expressive power often comes at the cost of generalization.</li>
</ul>

<h3>Title: PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</h3>
<ul>
<li><strong>Authors: </strong>Guilin Li, Yun Zhang, Xiuyuan Chen, Chengqi Li, Bo Wang, Linghe Kong, Wenjia Wang, Weiran Huang, Matthias Hwai Yong Tan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10102">https://arxiv.org/abs/2510.10102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10102">https://arxiv.org/pdf/2510.10102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10102]] PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling(https://arxiv.org/abs/2510.10102)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown that generative pretraining can distill vast world knowledge into compact token representations. While LLMs encapsulate extensive world knowledge, they remain limited in modeling the behavioral knowledge contained within user interaction histories. User behavior forms a distinct modality, where each action, defined by multi-dimensional attributes such as time, context, and transaction type, constitutes a behavioral token. Modeling these high-cardinality sequences is challenging, and discriminative models often falter under limited supervision. To bridge this gap, we extend generative pretraining to user behavior, learning transferable representations from unlabeled behavioral data analogous to how LLMs learn from text. We present PANTHER, a hybrid generative-discriminative framework that unifies user behavior pretraining and downstream adaptation, enabling large-scale sequential user representation learning and real-time inference. PANTHER introduces: (1) Structured Tokenization to compress multi-dimensional transaction attributes into an interpretable vocabulary; (2) Sequence Pattern Recognition Module (SPRM) for modeling periodic transaction motifs; (3) a Unified User-Profile Embedding that fuses static demographics with dynamic transaction histories; and (4) Real-time scalability enabled by offline caching of pretrained embeddings for millisecond-level inference. Fully deployed and operational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in next-transaction prediction HitRate@1 and a 38.6 percent relative improvement in fraud detection recall over baselines. Cross-domain evaluations on public benchmarks show strong generalization, achieving up to 21 percent HitRate@1 gains over transformer baselines, establishing PANTHER as a scalable, high-performance framework for industrial sequential user behavior modeling.</li>
</ul>

<h3>Title: Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Renliang Sun, Wei Cheng, Dawei Li, Haifeng Chen, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10103">https://arxiv.org/abs/2510.10103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10103">https://arxiv.org/pdf/2510.10103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10103]] Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning(https://arxiv.org/abs/2510.10103)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning has driven recent gains of large language models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps. However, excessive or redundant reasoning -- so-called overthinking -- can increase inference costs and lead LLMs toward incorrect conclusions. In this paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework that adaptively determines when to stop reasoning to mitigate overthinking. REFRAIN integrates a two-stage stop discriminator to identify reflective yet redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB) multi-armed bandit controller to dynamically adjust stopping thresholds according to problem difficulty without supervision or fine-tuning. Across four representative benchmarks and two model families, REFRAIN reduces token usage by 20-55% while maintaining or improving accuracy compared to standard CoT prompting. Extensive ablation and robustness analyses demonstrate its stability across models, scorers, and prompt variations. In summary, our findings highlight when-to-stop as a new and practical axis of test-time scaling -- enabling models to reason not just more, but just enough.</li>
</ul>

<h3>Title: Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models</h3>
<ul>
<li><strong>Authors: </strong>Minbin Huang, Runhui Huang, Chuanyang Zheng, Jingyao Li, Guoxuan Chen, Han Shi, Hong Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10104">https://arxiv.org/abs/2510.10104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10104">https://arxiv.org/pdf/2510.10104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10104]] Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models(https://arxiv.org/abs/2510.10104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated that reinforcement learning with verifiable rewards (RLVR) can significantly enhance reasoning abilities by directly optimizing correctness, rather than relying solely on supervised imitation. This paradigm has been extended to multimodal LLMs for complex video and image understanding tasks. However, while outcome-driven RL improves answer accuracy, it can inadvertently decouple the reasoning chain from the final answer, leading to situations where models produce inconsistency between the reasoning trace and final answer. In our experiments on multiple-choice visual question-answering tasks, the standard GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps and the chosen answers, indicating frequent mismatches between answers and reasoning. To this end, we propose Answer-Consistent Reinforcement Learning (ACRE) that modifies the GRPO algorithm with an auxiliary consistency check. After the model generates a chain of thought and an initial answer for a given question, we shuffle the answer options and prompt the model again with the same reasoning trace to predict a second answer. We design a consistency-verification reward that grants a high reward only if both the original and the post-shuffle answers agree and are correct; otherwise, a lower reward is assigned accordingly. This mechanism penalizes reasoning-answer misalignment and discourages the model from relying on spurious patterns, such as option ordering biases. We evaluate ACRE on challenging Video Reasoning benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\% and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the GRPO baseline.</li>
</ul>

<h3>Title: Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Aniruddha Srinivas Joshi, Godwyn James William, Shreyas Srinivas Joshi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10108">https://arxiv.org/abs/2510.10108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10108">https://arxiv.org/pdf/2510.10108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10108]] Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models(https://arxiv.org/abs/2510.10108)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate fire and smoke detection is critical for safety and disaster response, yet existing vision-based methods face challenges in balancing efficiency and reliability. Compact deep learning models such as YOLOv5n and YOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT devices, but their reduced capacity often results in false positives and missed detections. Conventional post-detection methods such as Non-Maximum Suppression and Soft-NMS rely only on spatial overlap, which can suppress true positives or retain false alarms in cluttered or ambiguous fire scenes. To address these limitations, we propose an uncertainty aware post-detection framework that rescales detection confidences using both statistical uncertainty and domain relevant visual cues. A lightweight Confidence Refinement Network integrates uncertainty estimates with color, edge, and texture features to adjust detection scores without modifying the base model. Experiments on the D-Fire dataset demonstrate improved precision, recall, and mean average precision compared to existing baselines, with only modest computational overhead. These results highlight the effectiveness of post-detection rescoring in enhancing the robustness of compact deep learning models for real-world fire and smoke detection.</li>
</ul>

<h3>Title: Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Rui Chen, Bin Liu, Changtao Miao, Xinghao Wang, Yi Li, Tao Gong, Qi Chu, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10111">https://arxiv.org/abs/2510.10111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10111">https://arxiv.org/pdf/2510.10111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10111]] Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization(https://arxiv.org/abs/2510.10111)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Advances in image tampering pose serious security threats, underscoring the need for effective image manipulation localization (IML). While supervised IML achieves strong performance, it depends on costly pixel-level annotations. Existing weakly supervised or training-free alternatives often underperform and lack interpretability. We propose the In-Context Forensic Chain (ICFC), a training-free framework that leverages multi-modal large language models (MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule construction with adaptive filtering to build a reliable knowledge base and a multi-step progressive reasoning pipeline that mirrors expert forensic workflows from coarse proposals to fine-grained forensics results. This design enables systematic exploitation of MLLM reasoning for image-level classification, pixel-level localization, and text-level interpretability. Across multiple benchmarks, ICFC not only surpasses state-of-the-art training-free methods but also achieves competitive or superior performance compared to weakly and fully supervised approaches.</li>
</ul>

<h3>Title: ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Mi, Qiuyang Yuan, Zhizhou Zhong, Xuan Zhao, Jiaogen Zhou, Fubao Zhu, Jihong Guan, Shuigeng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10113">https://arxiv.org/abs/2510.10113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10113">https://arxiv.org/pdf/2510.10113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10113]] ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes(https://arxiv.org/abs/2510.10113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In egocentric applications such as augmented and virtual reality, immersive iris recognition is emerging as an accurate and seamless way to identify persons. While classic systems acquire iris images on-axis, i.e., via dedicated frontal sensors in controlled settings, the immersive setup primarily captures off-axis irises through tilt-placed headset cameras, with only mild control in open scenes. This yields unique challenges, including perspective distortion, intensified quality degradations, and intra-class variations in iris texture. Datasets capturing these challenges remain scarce. To fill this gap, this paper introduces ImmerIris, a large-scale dataset collected via VR headsets, containing 499,791 ocular images from 564 subjects. It is, to the best of current knowledge, the largest public dataset and among the first dedicated to off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed to benchmark recognition methods under different challenging factors. Current methods, primarily designed for classic on-axis imagery, perform unsatisfactorily on the immersive setup, mainly due to reliance on fallible normalization. To this end, this paper further proposes a normalization-free paradigm that directly learns from ocular images with minimal adjustment. Despite its simplicity, this approach consistently outperforms normalization-based counterparts, pointing to a promising direction for robust immersive recognition.</li>
</ul>

<h3>Title: LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora</h3>
<ul>
<li><strong>Authors: </strong>Luyao Zhuang, Shengyuan Chen, Yilin Xiao, Huachi Zhou, Yujing Zhang, Hao Chen, Qinggang Zhang, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10114">https://arxiv.org/abs/2510.10114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10114">https://arxiv.org/pdf/2510.10114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10114]] LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora(https://arxiv.org/abs/2510.10114)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is widely used to mitigate hallucinations of Large Language Models (LLMs) by leveraging external knowledge. While effective for simple queries, traditional RAG systems struggle with large-scale, unstructured corpora where information is fragmented. Recent advances incorporate knowledge graphs to capture relational structures, enabling more comprehensive retrieval for complex, multi-hop reasoning tasks. However, existing graph-based RAG (GraphRAG) methods rely on unstable and costly relation extraction for graph construction, often producing noisy graphs with incorrect or inconsistent relations that degrade retrieval quality. In this paper, we revisit the pipeline of existing GraphRAG systems and propose LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient framework that enables reliable graph construction and precise passage retrieval. Specifically, LinearRAG constructs a relation-free hierarchical graph, termed Tri-Graph, using only lightweight entity extraction and semantic linking, avoiding unstable relation modeling. This new paradigm of graph construction scales linearly with corpus size and incurs no extra token consumption, providing an economical and reliable indexing of the original passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant entity activation via local semantic bridging, followed by (ii) passage retrieval through global importance aggregation. Extensive experiments on four datasets demonstrate that LinearRAG significantly outperforms baseline models.</li>
</ul>

<h3>Title: Preference-driven Knowledge Distillation for Few-shot Node Classification</h3>
<ul>
<li><strong>Authors: </strong>Xing Wei, Chunchun Chen, Rui Fan, Xiaofeng Cao, Sourav Medya, Wei Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10116">https://arxiv.org/abs/2510.10116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10116">https://arxiv.org/pdf/2510.10116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10116]] Preference-driven Knowledge Distillation for Few-shot Node Classification(https://arxiv.org/abs/2510.10116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-/few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodes' intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs.</li>
</ul>

<h3>Title: CacheClip: Accelerating RAG with Effective KV Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10129">https://arxiv.org/abs/2510.10129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10129">https://arxiv.org/pdf/2510.10129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10129]] CacheClip: Accelerating RAG with Effective KV Cache Reuse(https://arxiv.org/abs/2510.10129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems suffer from severe time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV cache reuse methods face a fundamental trade-off: prefix caching requires identical prefixes that rarely occur in RAG scenarios, while direct precomputation sacrifices quality due to missing inter-chunk attention and repeated attention sinks. Recent methods like APE and CacheBlend partially address these issues but remain inadequate for robust RAG applications. This paper presents CacheClip, a novel framework that achieves both fast TTFT and high generation quality. Our key insight is that small auxiliary LLMs exhibit similar last-layer attention distributions to primary LLMs (the target model for generation), enabling efficient identification of tokens critical for restoring inter-chunk attention, thereby significantly improving response quality on cross-chunk reasoning tasks. CacheClip integrates three techniques: (1) auxiliary-model-guided token selection for selective KV cache recomputation, where the auxiliary model is finetuned to improve selection accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3) grouping strategy to maintain local coherence during partial KV cache updates. Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2% and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM inference by up to 1.92x in prefill time, providing a practical solution to the efficiency-quality trade-off in RAG systems.</li>
</ul>

<h3>Title: PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lancheng Zou, Shuo Yin, Zehua Pei, Tsung-Yi Ho, Farzan Farnia, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10136">https://arxiv.org/abs/2510.10136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10136">https://arxiv.org/pdf/2510.10136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10136]] PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models(https://arxiv.org/abs/2510.10136)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. However, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. To address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. Additionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. PermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. Extensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models. The code is available at this https URL.</li>
</ul>

<h3>Title: Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10138">https://arxiv.org/abs/2510.10138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10138">https://arxiv.org/pdf/2510.10138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10138]] Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task(https://arxiv.org/abs/2510.10138)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Information extraction from copy-heavy documents, characterized by massive volumes of structurally similar content, represents a critical yet understudied challenge in enterprise document processing. We present a systematic framework that strategically combines OCR engines with Large Language Models (LLMs) to optimize the accuracy-efficiency trade-off inherent in repetitive document extraction tasks. Unlike existing approaches that pursue universal solutions, our method exploits document-specific characteristics through intelligent strategy selection. We implement and evaluate 25 configurations across three extraction paradigms (direct, replacement, and table-based) on identity documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based extraction methods, our adaptive framework delivers outstanding results: F1=1.0 accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy with 0.6 s for challenging image inputs when integrated with PaddleOCR, all while maintaining sub-second processing speeds. The 54 times performance improvement compared with multimodal methods over naive approaches, coupled with format-aware routing, enables processing of heterogeneous document streams at production scale. Beyond the specific application to identity extraction, this work establishes a general principle: the repetitive nature of copy-heavy tasks can be transformed from a computational burden into an optimization opportunity through structure-aware method selection.</li>
</ul>

<h3>Title: Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yue Deng, Francisco Santos, Pang-Ning Tan, Lifeng Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10140">https://arxiv.org/abs/2510.10140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10140">https://arxiv.org/pdf/2510.10140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10140]] Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction(https://arxiv.org/abs/2510.10140)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Deep learning based weather forecasting (DLWF) models leverage past weather observations to generate future forecasts, supporting a wide range of downstream tasks, including tropical cyclone (TC) trajectory prediction. In this paper, we investigate their vulnerability to adversarial attacks, where subtle perturbations to the upstream weather forecasts can alter the downstream TC trajectory predictions. Although research on adversarial attacks in DLWF models has grown recently, generating perturbed upstream forecasts that reliably steer downstream output toward attacker-specified trajectories remains a challenge. First, conventional TC detection systems are opaque, non-differentiable black boxes, making standard gradient-based attacks infeasible. Second, the extreme rarity of TC events leads to severe class imbalance problem, making it difficult to develop efficient attack methods that will produce the attacker's target trajectories. Furthermore, maintaining physical consistency in adversarially generated forecasts presents another significant challenge. To overcome these limitations, we propose Cyc-Attack, a novel method that perturbs the upstream forecasts of DLWF models to generate adversarial trajectories. First, we pre-train a differentiable surrogate model to approximate the TC detector's output, enabling the construction of gradient-based attacks. Cyc-Attack also employs skewness-aware loss function with kernel dilation strategy to address the imbalance problem. Finally, a distance-based gradient weighting scheme and regularization are used to constrain the perturbations and eliminate spurious trajectories to ensure the adversarial forecasts are realistic and not easily detectable.</li>
</ul>

<h3>Title: YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments</h3>
<ul>
<li><strong>Authors: </strong>Hongxing Peng, Haopei Xie, Weijia Lia, Huanai Liuc, Ximing Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10141">https://arxiv.org/abs/2510.10141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10141">https://arxiv.org/pdf/2510.10141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10141]] YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments(https://arxiv.org/abs/2510.10141)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Litchi is a high-value fruit, yet traditional manual selection methods are increasingly inadequate for modern production demands. Integrating UAV-based aerial imagery with deep learning offers a promising solution to enhance efficiency and reduce costs. This paper introduces YOLOv11-Litchi, a lightweight and robust detection model specifically designed for UAV-based litchi detection. Built upon the YOLOv11 framework, the proposed model addresses key challenges such as small target size, large model parameters hindering deployment, and frequent target occlusion. To tackle these issues, three major innovations are incorporated: a multi-scale residual module to improve contextual feature extraction across scales, a lightweight feature fusion method to reduce model size and computational costs while maintaining high accuracy, and a litchi occlusion detection head to mitigate occlusion effects by emphasizing target regions and suppressing background interference. Experimental results validate the model's effectiveness. YOLOv11-Litchi achieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline - while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%. Additionally, the model achieves a frame rate of 57.2 FPS, meeting real-time detection requirements. These findings demonstrate the suitability of YOLOv11-Litchi for UAV-based litchi detection in complex orchard environments, showcasing its potential for broader applications in precision agriculture.</li>
</ul>

<h3>Title: DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tingxu Han, Wei Song, Ziqi Ding, Ziming Li, Chunrong Fang, Yuekang Li, Dongfang Liu, Zhenyu Chen, Zhenting Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10142">https://arxiv.org/abs/2510.10142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10142">https://arxiv.org/pdf/2510.10142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10142]] DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models(https://arxiv.org/abs/2510.10142)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable. Existing work probes when biased outputs appear, but gives little insight into the mechanisms that generate them, leaving existing mitigations largely fragile. In this paper, we conduct a systematic investigation LLM unfairness and propose DiffHeads, a lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA) prompting to Chain-of-Thought (CoT) prompting across eight representative open- and closed-source LLMs. DA will trigger the nature bias part of LLM and improve measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues. Next, we define a token-to-head contribution score that traces each token's influence back to individual attention heads. This reveals a small cluster of bias heads that activate under DA but stay largely dormant with CoT, providing the first causal link between prompting strategy and bias emergence. Finally, building on this insight, we propose DiffHeads that identifies bias heads through differential activation analysis between DA and CoT, and selectively masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under DA and CoT, respectively, without harming model utility.</li>
</ul>

<h3>Title: A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Cheng He, Xijie Liang, Zengrong Zheng, Patrick P.C. Lee, Xu Huang, Zhaoyi Li, Hong Xie, Defu Lian, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10145">https://arxiv.org/abs/2510.10145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10145">https://arxiv.org/pdf/2510.10145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10145]] A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting(https://arxiv.org/abs/2510.10145)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Current approaches for time series forecasting, whether in the time or frequency domain, predominantly use deep learning models based on linear layers or transformers. They often encode time series data in a black-box manner and rely on trial-and-error optimization solely based on forecasting performance, leading to limited interpretability and theoretical understanding. Furthermore, the dynamics in data distribution over time and frequency domains pose a critical challenge to accurate forecasting. We propose FIRE, a unified frequency domain decomposition framework that provides a mathematical abstraction for diverse types of time series, so as to achieve interpretable and robust time series forecasting. FIRE introduces several key innovations: (i) independent modeling of amplitude and phase components, (ii) adaptive learning of weights of frequency basis components, (iii) a targeted loss function, and (iv) a novel training paradigm for sparse data. Extensive experiments demonstrate that FIRE consistently outperforms state-of-the-art models on long-term forecasting benchmarks, achieving superior predictive performance and significantly enhancing interpretability of time series</li>
</ul>

<h3>Title: Robust Learning of Diffusion Models with Extremely Noisy Conditions</h3>
<ul>
<li><strong>Authors: </strong>Xin Chen, Gillian Dobbie, Xinyu Wang, Feng Liu, Di Wang, Jingfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10149">https://arxiv.org/abs/2510.10149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10149">https://arxiv.org/pdf/2510.10149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10149]] Robust Learning of Diffusion Models with Extremely Noisy Conditions(https://arxiv.org/abs/2510.10149)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have the generative controllability by incorporating external conditions. However, their performance significantly degrades with noisy conditions, such as corrupted labels in the image generation or unreliable observations or states in the control policy generation. This paper introduces a robust learning framework to address extremely noisy conditions in conditional diffusion models. We empirically demonstrate that existing noise-robust methods fail when the noise level is high. To overcome this, we propose learning pseudo conditions as surrogates for clean conditions and refining pseudo ones progressively via the technique of temporal ensembling. Additionally, we develop a Reverse-time Diffusion Condition (RDC) technique, which diffuses pseudo conditions to reinforce the memorization effect and further facilitate the refinement of the pseudo conditions. Experimentally, our approach achieves state-of-the-art performance across a range of noise levels on both class-conditional image generation and visuomotor policy generation this http URL code can be accessible via the project page this https URL</li>
</ul>

<h3>Title: Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong, Qiang Lin, Can Wang, Jiawei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10150">https://arxiv.org/abs/2510.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10150">https://arxiv.org/pdf/2510.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10150]] Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective(https://arxiv.org/abs/2510.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM reasoning, its training process poses a critical risk: entropy collapse. This phenomenon is a rapid loss of policy diversity, stemming from the exploration-exploitation imbalance and leading to a lack of generalization. Recent entropy-intervention methods aim to prevent \coloredtext{entropy collapse}, yet their underlying mechanisms remain unclear. In this paper, we conduct a quantitative analysis to reveal token-level entropy changes and how existing entropy intervention methods help avoid entropy collapse. Our findings point out a fundamental limitation of existing methods: they attempt to control entropy dynamics indirectly. By only affecting related factors, such as the advantage signal and generation probability, their effectiveness is inherently limited and could potentially fail. To address this limitation, we introduce an entropy-change-aware reweighting scheme, namely Stabilizing Token-level Entropy-changE via Reweighting (STEER), that adaptively stabilizes entropy dynamics through fine-grained token-level adjustments. Our approach mitigates over-exploitation while fostering robust exploration. Extensive experiments demonstrate that STEER significantly mitigates entropy collapse, stabilizes entropy dynamics, and achieves stronger downstream performance across various mathematical reasoning benchmarks \footnote{Our code is available at this https URL.</li>
</ul>

<h3>Title: Stroke Locus Net: Occluded Vessel Localization from MRI Modalities</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Hamad, Muhammad Khan, Tamer Khattab, Mohamed Mabrok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10155">https://arxiv.org/abs/2510.10155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10155">https://arxiv.org/pdf/2510.10155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10155]] Stroke Locus Net: Occluded Vessel Localization from MRI Modalities(https://arxiv.org/abs/2510.10155)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>A key challenge in ischemic stroke diagnosis using medical imaging is the accurate localization of the occluded vessel. Current machine learning methods in focus primarily on lesion segmentation, with limited work on vessel localization. In this study, we introduce Stroke Locus Net, an end-to-end deep learning pipeline for detection, segmentation, and occluded vessel localization using only MRI scans. The proposed system combines a segmentation branch using nnUNet for lesion detection with an arterial atlas for vessel mapping and identification, and a generation branch using pGAN to synthesize MRA images from MRI. Our implementation demonstrates promising results in localizing occluded vessels on stroke-affected T1 MRI scans, with potential for faster and more informed stroke diagnosis.</li>
</ul>

<h3>Title: ReMix: Towards a Unified View of Consistent Character Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Benjia Zhou, Bin Fu, Pei Cheng, Yanru Wang, Jiayuan Fan, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10156">https://arxiv.org/abs/2510.10156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10156">https://arxiv.org/pdf/2510.10156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10156]] ReMix: Towards a Unified View of Consistent Character Generation and Editing(https://arxiv.org/abs/2510.10156)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1) have greatly improved visual fidelity in consistent character generation and editing. However, existing methods rarely unify these tasks within a single framework. Generation-based approaches struggle with fine-grained identity consistency across instances, while editing-based methods often lose spatial controllability and instruction alignment. To bridge this gap, we propose ReMix, a unified framework for character-consistent generation and editing. It constitutes two core components: the ReMix Module and IP-ControlNet. The ReMix Module leverages the multimodal reasoning ability of MLLMs to edit semantic features of input images and adapt instruction embeddings to the native DiT backbone without fine-tuning. While this ensures coherent semantic layouts, pixel-level consistency and pose controllability remain challenging. To address this, IP-ControlNet extends ControlNet to decouple semantic and layout cues from reference images and introduces an {\epsilon}-equivariant latent space that jointly denoises the reference and target images within a shared noise space. Inspired by convergent evolution and quantum decoherence,i.e., where environmental noise drives state convergence, this design promotes feature alignment in the hidden space, enabling consistent object generation while preserving identity. ReMix supports a wide range of tasks, including personalized generation, image editing, style transfer, and multi-condition synthesis. Extensive experiments validate its effectiveness and efficiency as a unified framework for character-consistent image generation and editing.</li>
</ul>

<h3>Title: BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation</h3>
<ul>
<li><strong>Authors: </strong>Tsung-Min Pai, Jui-I Wang, Li-Chun Lu, Shao-Hua Sun, Hung-Yi Lee, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10157">https://arxiv.org/abs/2510.10157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10157">https://arxiv.org/pdf/2510.10157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10157]] BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation(https://arxiv.org/abs/2510.10157)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. BILLY operates by extracting and blending multiple distinct persona vectors directly in the model's activation space. We steer the model's generation process with this merged vector while inference, enabling multi-perspective output without explicit multi-LLM communication. Our experiments across creativity-oriented benchmarks demonstrate that BILLY surpasses single model prompting and traditional multi-LLM approaches, while substantially reducing inference time and computational costs. Our analyses further reveal that distinct persona vectors can be blended to achieve both effective control over complementary aspects of generation and greater interpretability.</li>
</ul>

<h3>Title: SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhenjie Mao, Yuhuan Yang, Chaofan Ma, Dongsheng Jiang, Jiangchao Yao, Ya Zhang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10160">https://arxiv.org/abs/2510.10160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10160">https://arxiv.org/pdf/2510.10160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10160]] SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation(https://arxiv.org/abs/2510.10160)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring Image Segmentation (RIS) aims to segment the target object in an image given a natural language expression. While recent methods leverage pre-trained vision backbones and more training corpus to achieve impressive results, they predominantly focus on simple expressions--short, clear noun phrases like "red car" or "left girl". This simplification often reduces RIS to a key word/concept matching problem, limiting the model's ability to handle referential ambiguity in expressions. In this work, we identify two challenging real-world scenarios: object-distracting expressions, which involve multiple entities with contextual cues, and category-implicit expressions, where the object class is not explicitly stated. To address the challenges, we propose a novel framework, SaFiRe, which mimics the human two-phase cognitive process--first forming a global understanding, then refining it through detail-oriented inspection. This is naturally supported by Mamba's scan-then-update property, which aligns with our phased design and enables efficient multi-cycle refinement with linear complexity. We further introduce aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous referring expressions. Extensive experiments on both standard and proposed datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.</li>
</ul>

<h3>Title: Large Language Model Sourcing: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Liang Pang, Kangxi Wu, Sunhao Dai, Zihao Wei, Zenghao Duan, Jia Gu, Xiang Li, Zhiyi Yin, Jun Xu, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10161">https://arxiv.org/abs/2510.10161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10161">https://arxiv.org/pdf/2510.10161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10161]] Large Language Model Sourcing: A Survey(https://arxiv.org/abs/2510.10161)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, shifting from supporting objective tasks (e.g., recognition) to empowering subjective decision-making (e.g., planning, decision). This marks the dawn of general and powerful AI, with applications spanning a wide range of fields, including programming, education, healthcare, finance, and law. However, their deployment introduces multifaceted risks. Due to the black-box nature of LLMs and the human-like quality of their generated content, issues such as hallucinations, bias, unfairness, and copyright infringement become particularly significant. In this context, sourcing information from multiple perspectives is essential. This survey presents a systematic investigation into provenance tracking for content generated by LLMs, organized around four interrelated dimensions that together capture both model- and data-centric perspectives. From the model perspective, Model Sourcing treats the model as a whole, aiming to distinguish content generated by specific LLMs from content authored by humans. Model Structure Sourcing delves into the internal generative mechanisms, analyzing architectural components that shape the outputs of model. From the data perspective, Training Data Sourcing focuses on internal attribution, tracing the origins of generated content back to the training data of model. In contrast, External Data Sourcing emphasizes external validation, identifying external information used to support or influence the responses of model. Moreover, we also propose a dual-paradigm taxonomy that classifies existing sourcing methods into prior-based (proactive traceability embedding) and posterior-based (retrospective inference) approaches. Traceability across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world applications.</li>
</ul>

<h3>Title: SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>César Borja, Carlos Plou, Rubén Martinez-Cantín, Ana C. Murillo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10163">https://arxiv.org/abs/2510.10163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10163">https://arxiv.org/pdf/2510.10163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10163]] SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation(https://arxiv.org/abs/2510.10163)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is essential to automate underwater imagery analysis with ecology monitoring purposes. Unfortunately, fine grained underwater scene analysis is still an open problem even for top performing segmentation models. The high cost of obtaining dense, expert-annotated, segmentation labels hinders the supervision of models in this domain. While sparse point-labels are easier to obtain, they introduce challenges regarding which points to annotate and how to propagate the sparse information. We present SparseUWSeg, a novel framework that addresses both issues. SparseUWSeg employs an active sampling strategy to guide annotators, maximizing the value of their point labels. Then, it propagates these sparse labels with a hybrid approach leverages both the best of SAM2 and superpixel-based methods. Experiments on two diverse underwater datasets demonstrate the benefits of SparseUWSeg over state-of-the-art approaches, achieving up to +5\% mIoU over D+NN. Our main contribution is the design and release of a simple but effective interactive annotation tool, integrating our algorithms. It enables ecology researchers to leverage foundation models and computer vision to efficiently generate high-quality segmentation masks to process their data.</li>
</ul>

<h3>Title: ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Cristiano Patrício, Luís F. Teixeira, João C. Neves</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10174">https://arxiv.org/abs/2510.10174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10174">https://arxiv.org/pdf/2510.10174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10174]] ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis(https://arxiv.org/abs/2510.10174)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Concept-based models aim to explain model decisions with human-understandable concepts. However, most existing approaches treat concepts as numerical attributes, without providing complementary visual explanations that could localize the predicted concepts. This limits their utility in real-world applications and particularly in high-stakes scenarios, such as medical use-cases. This paper proposes ViConEx-Med, a novel transformer-based framework for visual concept explainability, which introduces multi-concept learnable tokens to jointly predict and localize visual concepts. By leveraging specialized attention layers for processing visual and text-based concept tokens, our method produces concept-level localization maps while maintaining high predictive accuracy. Experiments on both synthetic and real-world medical datasets demonstrate that ViConEx-Med outperforms prior concept-based models and achieves competitive performance with black-box models in terms of both concept detection and localization precision. Our results suggest a promising direction for building inherently interpretable models grounded in visual concepts. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zixu Zhao, Yang Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10180">https://arxiv.org/abs/2510.10180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10180">https://arxiv.org/pdf/2510.10180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10180]] TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval(https://arxiv.org/abs/2510.10180)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time, high-resolution data collection, producing massive volumes of aerial videos. Efficient retrieval of relevant content from these videos is crucial for applications in urban management, emergency response, security, and disaster relief. While text-video retrieval has advanced in natural video domains, the UAV domain remains underexplored due to limitations in existing datasets, such as coarse and redundant captions. Thus, in this work, we construct the Drone Video-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320 fine-grained, semantically diverse captions. The annotations capture multiple complementary aspects, including human actions, objects, background settings, environmental conditions, and visual style, thereby enhancing text-video correspondence and reducing redundancy. Building on this dataset, we propose the Text-Conditioned Multi-granularity Alignment (TCMA) framework, which integrates global video-sentence alignment, sentence-guided frame aggregation, and word-guided patch alignment. To further refine local alignment, we design a Word and Patch Selection module that filters irrelevant content, as well as a Text-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to text type. Extensive experiments on DVTMD and CapERA establish the first complete benchmark for drone text-video retrieval. Our TCMA achieves state-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8% R@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset and method. The code and dataset will be released.</li>
</ul>

<h3>Title: A Survey of Inductive Reasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kedi Chen, Dezhao Ruan, Yuhao Dan, Yaoting Wang, Siyu Yan, Xuecheng Wu, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Biqing Qi, Linyang Li, Qipeng Guo, Xiaoming Shi, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10182">https://arxiv.org/abs/2510.10182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10182">https://arxiv.org/pdf/2510.10182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10182]] A Survey of Inductive Reasoning for Large Language Models(https://arxiv.org/abs/2510.10182)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning is an important task for large language models (LLMs). Among all the reasoning paradigms, inductive reasoning is one of the fundamental types, which is characterized by its particular-to-general thinking process and the non-uniqueness of its answers. The inductive mode is crucial for knowledge generalization and aligns better with human cognition, so it is a fundamental mode of learning, hence attracting increasing interest. Despite the importance of inductive reasoning, there is no systematic summary of it. Therefore, this paper presents the first comprehensive survey of inductive reasoning for LLMs. First, methods for improving inductive reasoning are categorized into three main areas: post-training, test-time scaling, and data augmentation. Then, current benchmarks of inductive reasoning are summarized, and a unified sandbox-based evaluation approach with the observation coverage metric is derived. Finally, we offer some analyses regarding the source of inductive ability and how simple model architectures and data help with inductive tasks, providing a solid foundation for future research.</li>
</ul>

<h3>Title: MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Lei Gu, Yinghao Zhu, Haoran Sang, Zixiang Wang, Dehao Sui, Wen Tang, Ewen Harrison, Junyi Gao, Lequan Yu, Liantao Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10185">https://arxiv.org/abs/2510.10185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10185">https://arxiv.org/pdf/2510.10185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10185]] MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems(https://arxiv.org/abs/2510.10185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language model (LLM)-based multi-agent systems show promise in simulating medical consultations, their evaluation is often confined to final-answer accuracy. This practice treats their internal collaborative processes as opaque "black boxes" and overlooks a critical question: is a diagnostic conclusion reached through a sound and verifiable reasoning pathway? The inscrutable nature of these systems poses a significant risk in high-stakes medical applications, potentially leading to flawed or untrustworthy conclusions. To address this, we conduct a large-scale empirical study of 3,600 cases from six medical datasets and six representative multi-agent frameworks. Through a rigorous, mixed-methods approach combining qualitative analysis with quantitative auditing, we develop a comprehensive taxonomy of collaborative failure modes. Our quantitative audit reveals four dominant failure patterns: flawed consensus driven by shared model deficiencies, suppression of correct minority opinions, ineffective discussion dynamics, and critical information loss during synthesis. This study demonstrates that high accuracy alone is an insufficient measure of clinical or public trust. It highlights the urgent need for transparent and auditable reasoning processes, a cornerstone for the responsible development and deployment of medical AI.</li>
</ul>

<h3>Title: INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Linfei Li, Fengyi Zhang, Zhong Wang, Lin Zhang, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10188">https://arxiv.org/abs/2510.10188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10188">https://arxiv.org/pdf/2510.10188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10188]] INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction(https://arxiv.org/abs/2510.10188)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Implicit Neural Representations (INRs) have gained success in various signal processing tasks due to their advantages of continuity and infinite resolution. However, the factors influencing their effectiveness and limitations remain underexplored. To better understand these factors, we leverage insights from Neural Tangent Kernel (NTK) theory to analyze how model architectures (classic MLP and emerging KAN), positional encoding, and nonlinear primitives affect the response to signals of varying frequencies. Building on this analysis, we introduce INR-Bench, the first comprehensive benchmark specifically designed for multimodal INR tasks. It includes 56 variants of Coordinate-MLP models (featuring 4 types of positional encoding and 14 activation functions) and 22 Coordinate-KAN models with distinct basis functions, evaluated across 9 implicit multimodal tasks. These tasks cover both forward and inverse problems, offering a robust platform to highlight the strengths and limitations of different neural models, thereby establishing a solid foundation for future research. The code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification</h3>
<ul>
<li><strong>Authors: </strong>Haohua Dong, Ana Manzano Rodríguez, Camille Guinaudeau, Shin'ichi Satoh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10191">https://arxiv.org/abs/2510.10191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10191">https://arxiv.org/pdf/2510.10191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10191]] Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification(https://arxiv.org/abs/2510.10191)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Face gender classification models often reflect and amplify demographic biases present in their training data, leading to uneven performance across gender and racial subgroups. We introduce pseudo-balancing, a simple and effective strategy for mitigating such biases in semi-supervised learning. Our method enforces demographic balance during pseudo-label selection, using only unlabeled images from a race-balanced dataset without requiring access to ground-truth annotations. We evaluate pseudo-balancing under two conditions: (1) fine-tuning a biased gender classifier using unlabeled images from the FairFace dataset, and (2) stress-testing the method with intentionally imbalanced training data to simulate controlled bias scenarios. In both cases, models are evaluated on the All-Age-Faces (AAF) benchmark, which contains a predominantly East Asian population. Our results show that pseudo-balancing consistently improves fairness while preserving or enhancing accuracy. The method achieves 79.81% overall accuracy - a 6.53% improvement over the baseline - and reduces the gender accuracy gap by 44.17%. In the East Asian subgroup, where baseline disparities exceeded 49%, the gap is narrowed to just 5.01%. These findings suggest that even in the absence of label supervision, access to a demographically balanced or moderately skewed unlabeled dataset can serve as a powerful resource for debiasing existing computer vision models.</li>
</ul>

<h3>Title: CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions</h3>
<ul>
<li><strong>Authors: </strong>Hong-Kun Zhang, Xin Li, Sikun Yang, Zhihong Xia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10195">https://arxiv.org/abs/2510.10195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10195">https://arxiv.org/pdf/2510.10195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10195]] CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions(https://arxiv.org/abs/2510.10195)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A novel neural network inspired by Cauchy's integral formula, is proposed for function approximation tasks that include time series forecasting, missing data imputation, etc. Hence, the novel neural network is named CauchyNet. By embedding real-valued data into the complex plane, CauchyNet efficiently captures complex temporal dependencies, surpassing traditional real-valued models in both predictive performance and computational efficiency. Grounded in Cauchy's integral formula and supported by the universal approximation theorem, CauchyNet offers strong theoretical guarantees for function approximation. The architecture incorporates complex-valued activation functions, enabling robust learning from incomplete data while maintaining a compact parameter footprint and reducing computational overhead. Through extensive experiments in diverse domains, including transportation, energy consumption, and epidemiological data, CauchyNet consistently outperforms state-of-the-art models in predictive accuracy, often achieving a 50% lower mean absolute error with fewer parameters. These findings highlight CauchyNet's potential as an effective and efficient tool for data-driven predictive modeling, particularly in resource-constrained and data-scarce environments.</li>
</ul>

<h3>Title: RLFR: Extending Reinforcement Learning for LLMs with Flow Environment</h3>
<ul>
<li><strong>Authors: </strong>Jinghao Zhang, Naishan Zheng, Ruilin Li, Dongzhou Cheng, Zheming Liang, Feng Zhao, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10201">https://arxiv.org/abs/2510.10201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10201">https://arxiv.org/pdf/2510.10201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10201]] RLFR: Extending Reinforcement Learning for LLMs with Flow Environment(https://arxiv.org/abs/2510.10201)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.</li>
</ul>

<h3>Title: A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets</h3>
<ul>
<li><strong>Authors: </strong>Dingyi Yao, Xinyao Han, Ruibo Ming, Zhihang Song, Lihui Peng, Jianming Hu, Danya Yao, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10203">https://arxiv.org/abs/2510.10203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10203">https://arxiv.org/pdf/2510.10203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10203]] A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets(https://arxiv.org/abs/2510.10203)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Ensuring the reliability of autonomous driving perception systems requires extensive environment-based testing, yet real-world execution is often impractical. Synthetic datasets have therefore emerged as a promising alternative, offering advantages such as cost-effectiveness, bias free labeling, and controllable scenarios. However, the domain gap between synthetic and real-world datasets remains a critical bottleneck for the generalization of AI-based autonomous driving models. Quantifying this synthetic-to-real gap is thus essential for evaluating dataset utility and guiding the design of more effective training pipelines. In this paper, we establish a systematic framework for quantifying the synthetic-to-real gap in autonomous driving systems, and propose Style Embedding Distribution Discrepancy (SEDD) as a novel evaluation metric. Our framework combines Gram matrix-based style extraction with metric learning optimized for intra-class compactness and inter-class separation to extract style embeddings. Furthermore, we establish a benchmark using publicly available datasets. Experiments are conducted on a variety of datasets and sim-to-real methods, and the results show that our method is capable of quantifying the synthetic-to-real gap. This work provides a standardized quality control tool that enables systematic diagnosis and targeted enhancement of synthetic datasets, advancing future development of data-driven autonomous driving systems.</li>
</ul>

<h3>Title: Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning</h3>
<ul>
<li><strong>Authors: </strong>Bo Yuan, Yulin Chen, Yin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10208">https://arxiv.org/abs/2510.10208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10208">https://arxiv.org/pdf/2510.10208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10208]] Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning(https://arxiv.org/abs/2510.10208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have shown impressive performance in various downstream tasks. However, in many real-world scenarios, the collected training data inevitably contains noisy labels. To learn from noisy labels, most solutions select samples with small losses for model training. However, the selected samples, in turn, impact the loss computation in the next iteration. An inaccurate initial selection can create a vicious cycle, leading to suboptimal performance. To break this cycle, we propose Delora, a novel framework that decouples the sample selection from model training. For sample selection, Delora establishes a noisy label detector by introducing clean and noisy LoRA. Benefiting from the memory effect, the clean LoRA is encouraged to memorize clean data, while the noisy LoRA is constrained to memorize mislabeled data, which serves as a learnable threshold for selecting clean and noisy samples. For model training, Delora can use carefully selected samples to fine-tune language models seamlessly. Experimental results on synthetic and real-world noisy datasets demonstrate the effectiveness of Delora in noisy label detection and text classification.</li>
</ul>

<h3>Title: Hierarchical Bayesian Flow Networks for Molecular Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yida Xiong, Jiameng Chen, Kun Li, Hongzhi Zhang, Xiantao Cai, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10211">https://arxiv.org/abs/2510.10211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10211">https://arxiv.org/pdf/2510.10211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10211]] Hierarchical Bayesian Flow Networks for Molecular Graph Generation(https://arxiv.org/abs/2510.10211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Molecular graph generation is essentially a classification generation problem, aimed at predicting categories of atoms and bonds. Currently, prevailing paradigms such as continuous diffusion models are trained to predict continuous numerical values, treating the training process as a regression task. However, the final generation necessitates a rounding step to convert these predictions back into discrete classification categories, which is intrinsically a classification operation. Given that the rounding operation is not incorporated during training, there exists a significant discrepancy between the model's training objective and its inference procedure. As a consequence, an excessive emphasis on point-wise precision can lead to overfitting and inefficient learning. This occurs because considerable efforts are devoted to capturing intra-bin variations that are ultimately irrelevant to the discrete nature of the task at hand. Such a flaw results in diminished molecular diversity and constrains the model's generalization capabilities. To address this fundamental limitation, we propose GraphBFN, a novel hierarchical coarse-to-fine framework based on Bayesian Flow Networks that operates on the parameters of distributions. By innovatively introducing Cumulative Distribution Function, GraphBFN is capable of calculating the probability of selecting the correct category, thereby unifying the training objective with the sampling rounding operation. We demonstrate that our method achieves superior performance and faster generation, setting new state-of-the-art results on the QM9 and ZINC250k molecular graph generation benchmarks.</li>
</ul>

<h3>Title: You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yijie Xu, Huizai Yao, Zhiyu Guo, Weiyu Guo, Pengteng Li, Aiwei Liu, Xuming Hu, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10223">https://arxiv.org/abs/2510.10223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10223">https://arxiv.org/pdf/2510.10223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10223]] You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs(https://arxiv.org/abs/2510.10223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in specialized domains such as finance, medicine, and agriculture, where they face significant distribution shifts from their training data. Domain-specific fine-tuning can mitigate this challenge but relies on high-quality labeled data that is expensive and slow to collect in expertise-limited settings. We study label-free test-time adaptation for language models and present SyTTA, an inference-time framework that adapts models on-the-fly without additional supervision. SyTTA couples two complementary uncertainty signals that arise under distribution shift: input-side perplexity, indicating mismatch with domain-specific terminology and patterns, and output-side predictive entropy, indicating diffuse and unstable token probabilities during generation. Across diverse model architectures and domain-specific benchmarks, SyTTA delivers consistent gains. Notably, on agricultural question answering, SyTTA improves Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query. These results show that effective test-time adaptation for language models is achievable without labeled examples, supporting deployment in label-scarce domains. The code will be made available upon acceptance.</li>
</ul>

<h3>Title: Text2Token: Unsupervised Text Representation Learning with Token Target Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ruize An, Richong Zhang, Zhijie Nie, Zhanyu Wu, Yanzhao Zhang, Dingkun Long</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10224">https://arxiv.org/abs/2510.10224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10224">https://arxiv.org/pdf/2510.10224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10224]] Text2Token: Unsupervised Text Representation Learning with Token Target Prediction(https://arxiv.org/abs/2510.10224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unsupervised text representation learning (TRL) is a fundamental task in natural language processing, which is beneficial for improving search and recommendations with the web's unlabeled texts. A recent empirical study finds that the high-quality representation aligns with the key token of the input text, uncovering the potential connection between representation space and vocabulary space. Inspired by the findings, we revisit the generative tasks and develop an unsupervised generative framework for TRL, Text2Token. The framework is based on the token target prediction task, utilizing carefully constructed target token distribution as supervisory signals. To construct the high-quality target token distribution, we analyze the token-alignment properties with advanced embedders and identify two essential categories of key tokens: (1) the meaningful tokens in the text and (2) semantically derived tokens beyond the text. Based on these insights, we propose two methods -- data-driven and model-derived -- to construct synthetic token targets from data or the LLM backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token achieves performance competitive with the state-of-the-art embedder with unsupervised contrastive learning, LLM2Vec. Our analysis further shows that vocabulary and representation spaces optimize together and toward the optimum solution during training, providing new ideas and insights for future work.</li>
</ul>

<h3>Title: SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification</h3>
<ul>
<li><strong>Authors: </strong>Xuening Wu, Shenqin Yin, Yanlan Kang, Xinhang Zhang, Qianya Xu, Zeping Chen, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10232">https://arxiv.org/abs/2510.10232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10232">https://arxiv.org/pdf/2510.10232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10232]] SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification(https://arxiv.org/abs/2510.10232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recursive self-modification is increasingly central in AutoML, neural architecture search, and adaptive optimization, yet no existing framework ensures that such changes are made safely. Godel machines offer a principled safeguard by requiring formal proofs of improvement before rewriting code; however, such proofs are unattainable in stochastic, high-dimensional settings. We introduce the Statistical Godel Machine (SGM), the first statistical safety layer for recursive edits. SGM replaces proof-based requirements with statistical confidence tests (e-values, Hoeffding bounds), admitting a modification only when superiority is certified at a chosen confidence level, while allocating a global error budget to bound cumulative risk across this http URL also propose Confirm-Triggered Harmonic Spending (CTHS), which indexes spending by confirmation events rather than rounds, concentrating the error budget on promising edits while preserving familywise this http URL across supervised learning, reinforcement learning, and black-box optimization validate this role: SGM certifies genuine gains on CIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates robustness on RL and optimization this http URL, these results position SGM as foundational infrastructure for continual, risk-aware self-modification in learning this http URL is available at: this https URL.</li>
</ul>

<h3>Title: ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement</h3>
<ul>
<li><strong>Authors: </strong>Kangyang Luo, Yuzhuo Bai, Shuzheng Si, Cheng Gao, Zhitong Wang, Yingli Shen, Wenhao Li, Zhu Liu, Yufeng Han, Jiayi Wu, Cunliang Kong, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10241">https://arxiv.org/abs/2510.10241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10241">https://arxiv.org/pdf/2510.10241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10241]] ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement(https://arxiv.org/abs/2510.10241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Coreference Resolution (CR) is a critical task in Natural Language Processing (NLP). Current research faces a key dilemma: whether to further explore the potential of supervised neural methods based on small language models, whose detect-then-cluster pipeline still delivers top performance, or embrace the powerful capabilities of Large Language Models (LLMs). However, effectively combining their strengths remains underexplored. To this end, we propose \textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised model with LLM-based reasoning. First, we present an improved CR method (\textbf{ImCoref}) to push the performance boundaries of the supervised neural method by introducing a lightweight bridging module to enhance long-text encoding capability, devising a biaffine scorer to comprehensively capture positional information, and invoking a hybrid mention regularization to improve training efficiency. Importantly, we employ an LLM acting as a multi-role Checker-Splitter agent to validate candidate mentions (filtering out invalid ones) and coreference results (splitting erroneous clusters) predicted by ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS, which achieves superior performance compared to existing state-of-the-art (SOTA) methods.</li>
</ul>

<h3>Title: System Password Security: Attack and Defense Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Chaofang Shi, Zhongwen Li, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10246">https://arxiv.org/abs/2510.10246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10246">https://arxiv.org/pdf/2510.10246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10246]] System Password Security: Attack and Defense Mechanisms(https://arxiv.org/abs/2510.10246)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>System passwords serve as critical credentials for user authentication and access control when logging into operating systems or applications. Upon entering a valid password, users pass verification to access system resources and execute corresponding operations. In recent years, frequent password cracking attacks targeting system passwords have posed a severe threat to information system security. To address this challenge, in-depth research into password cracking attack methods and defensive technologies holds significant importance. This paper conducts systematic research on system password security, focusing on analyzing typical password cracking methods such as brute force attacks, dictionary attacks, and rainbow table attacks, while evaluating the effectiveness of existing defensive measures. The experimental section utilizes common cryptanalysis tools, such as John the Ripper and Hashcat, to simulate brute force and dictionary attacks. Five test datasets, each generated using Message Digest Algorithm 5 (MD5), Secure Hash Algorithm 256-bit (SHA 256), and bcrypt hash functions, are analyzed. By comparing the overall performance of different hash algorithms and password complexity strategies against these attacks, the effectiveness of defensive measures such as salting and slow hashing algorithms is validated. Building upon this foundation, this paper further evaluates widely adopted defense mechanisms, including account lockout policies, multi-factor authentication, and risk adaptive authentication. By integrating experimental data with recent research findings, it analyzes the strengths and limitations of each approach while proposing feasible improvement recommendations and optimization strategies.</li>
</ul>

<h3>Title: Reasoning-Enhanced Large Language Models for Molecular Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Zhuang, Yaorui Shi, Jue Hou, Yunong He, Mingwei Ye, Mingjun Xu, Yuming Su, Linfeng Zhang, Linfeng Zhang, Guolin Ke, Hengxing Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10248">https://arxiv.org/abs/2510.10248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10248">https://arxiv.org/pdf/2510.10248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10248]] Reasoning-Enhanced Large Language Models for Molecular Property Prediction(https://arxiv.org/abs/2510.10248)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Molecular property prediction is crucial for drug discovery and materials science, yet existing approaches suffer from limited interpretability, poor cross-task generalization, and lack of chemical reasoning capabilities. Traditional machine learning models struggle with task transferability, while specialized molecular language models provide little insight into their decision-making processes. To address these limitations, we propose \textbf{MPPReasoner}, a multimodal large language model that incorporates chemical reasoning for molecular property prediction. Our approach, built upon Qwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to enable comprehensive molecular understanding. We develop a two-stage training strategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning trajectories generated through expert knowledge and multiple teacher models, followed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR employs verifiable, rule-based rewards that systematically evaluate chemical principle application, molecular structure analysis, and logical consistency through computational verification. Extensive experiments across 8 datasets demonstrate significant performance improvements, with MPPReasoner outperforming the best baselines by 7.91\% and 4.53\% on in-distribution and out-of-distribution tasks respectively. MPPReasoner exhibits exceptional cross-task generalization and generates chemically sound reasoning paths that provide valuable insights into molecular property analysis, substantially enhancing both interpretability and practical utility for chemists. Code is available at this https URL.</li>
</ul>

<h3>Title: MRI Brain Tumor Detection with Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Jack Krolik, Jake Lynn, John Henry Rudden, Dmytro Vremenko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10250">https://arxiv.org/abs/2510.10250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10250">https://arxiv.org/pdf/2510.10250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10250]] MRI Brain Tumor Detection with Computer Vision(https://arxiv.org/abs/2510.10250)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study explores the application of deep learning techniques in the automated detection and segmentation of brain tumors from MRI scans. We employ several machine learning models, including basic logistic regression, Convolutional Neural Networks (CNNs), and Residual Networks (ResNet) to classify brain tumors effectively. Additionally, we investigate the use of U-Net for semantic segmentation and EfficientDet for anchor-based object detection to enhance the localization and identification of tumors. Our results demonstrate promising improvements in the accuracy and efficiency of brain tumor diagnostics, underscoring the potential of deep learning in medical imaging and its significance in improving clinical outcomes.</li>
</ul>

<h3>Title: Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10252">https://arxiv.org/abs/2510.10252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10252">https://arxiv.org/pdf/2510.10252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10252]] Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models(https://arxiv.org/abs/2510.10252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at this https URL.</li>
</ul>

<h3>Title: Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10254">https://arxiv.org/abs/2510.10254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10254">https://arxiv.org/pdf/2510.10254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10254]] Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?(https://arxiv.org/abs/2510.10254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in large generative models have shown that simple autoregressive formulations, when scaled appropriately, can exhibit strong zero-shot generalization across domains. Motivated by this trend, we investigate whether autoregressive video modeling principles can be directly applied to medical imaging tasks, despite the model never being trained on medical data. Specifically, we evaluate a large vision model (LVM) in a zero-shot setting across four representative tasks: organ segmentation, denoising, super-resolution, and motion prediction. Remarkably, even without domain-specific fine-tuning, the LVM can delineate anatomical structures in CT scans and achieve competitive performance on segmentation, denoising, and super-resolution. Most notably, in radiotherapy motion prediction, the model forecasts future 3D CT phases directly from prior phases of a 4D CT scan, producing anatomically consistent predictions that capture patient-specific respiratory dynamics with realistic temporal coherence. We evaluate the LVM on 4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no prior exposure to medical data, the model achieves strong performance across all tasks and surpasses specialized DVF-based and generative baselines in motion prediction, achieving state-of-the-art spatial accuracy. These findings reveal the emergence of zero-shot capabilities in medical video modeling and highlight the potential of general-purpose video models to serve as unified learners and reasoners laying the groundwork for future medical foundation models built on video models.</li>
</ul>

<h3>Title: Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liang Lin, Miao Yu, Moayad Aloqaily, Zhenhong Zhou, Kun Wang, Linsey Pang, Prakhar Mehrotra, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10265">https://arxiv.org/abs/2510.10265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10265">https://arxiv.org/pdf/2510.10265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10265]] Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models(https://arxiv.org/abs/2510.10265)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Backdoor attacks are a significant threat to large language models (LLMs), often embedded via public checkpoints, yet existing defenses rely on impractical assumptions about trigger settings. To address this challenge, we propose \ourmethod, a defense framework that requires no prior knowledge of trigger settings. \ourmethod is based on the key observation that when deliberately injecting known backdoors into an already-compromised model, both existing unknown and newly injected backdoors aggregate in the representation space. \ourmethod leverages this through a two-stage process: \textbf{first}, aggregating backdoor representations by injecting known triggers, and \textbf{then}, performing recovery fine-tuning to restore benign outputs. Extensive experiments across multiple LLM architectures demonstrate that: (I) \ourmethod reduces the average Attack Success Rate to 4.41\% across multiple benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$. (II) Clean accuracy and utility are preserved within 0.5\% of the original model, ensuring negligible impact on legitimate tasks. (III) The defense generalizes across different types of backdoors, confirming its robustness in practical deployment scenarios.</li>
</ul>

<h3>Title: MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Wentian Zhu, Zhen Xiang, Wei Niu, Le Guan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10271">https://arxiv.org/abs/2510.10271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10271">https://arxiv.org/pdf/2510.10271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10271]] MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation(https://arxiv.org/abs/2510.10271)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Unlike regular tokens derived from existing text corpora, special tokens are artificially created to annotate structured conversations during the fine-tuning process of Large Language Models (LLMs). Serving as metadata of training data, these tokens play a crucial role in instructing LLMs to generate coherent and context-aware responses. We demonstrate that special tokens can be exploited to construct four attack primitives, with which malicious users can reliably bypass the internal safety alignment of online LLM services and circumvent state-of-the-art (SOTA) external content moderation systems simultaneously. Moreover, we found that addressing this threat is challenging, as aggressive defense mechanisms-such as input sanitization by removing special tokens entirely, as suggested in academia-are less effective than anticipated. This is because such defense can be evaded when the special tokens are replaced by regular ones with high semantic similarity within the tokenizer's embedding space. We systemically evaluated our method, named MetaBreak, on both lab environment and commercial LLM platforms. Our approach achieves jailbreak rates comparable to SOTA prompt-engineering-based solutions when no content moderation is deployed. However, when there is content moderation, MetaBreak outperforms SOTA solutions PAP and GPTFuzzer by 11.6% and 34.8%, respectively. Finally, since MetaBreak employs a fundamentally different strategy from prompt engineering, the two approaches can work synergistically. Notably, empowering MetaBreak on PAP and GPTFuzzer boosts jailbreak rates by 24.3% and 20.2%, respectively.</li>
</ul>

<h3>Title: Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nikolaus Salvatore, Hao Wang, Qiong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10276">https://arxiv.org/abs/2510.10276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10276">https://arxiv.org/pdf/2510.10276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10276]] Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs(https://arxiv.org/abs/2510.10276)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) often degrades when crucial information is in the middle of a long context, a "lost-in-the-middle" phenomenon that mirrors the primacy and recency effects in human memory. We propose that this behavior is not simply a flaw indicative of information loss but an adaptation to different information retrieval demands during pre-training: some tasks require uniform recall across the entire input (a long-term memory demand), while others prioritize the most recent information (a short-term memory demand). Consistent with this view, we show that this U-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are trained from scratch on two simple human memory paradigms simulating long-term and short-term memory demands. Our analysis reveals that while the recency effect directly aligns with short-term memory demand in the training data, the primacy effect is induced by the uniform long-term memory demand and is additionally influenced by the model's autoregressive properties and the formation of attention sinks. Our main findings from simple human memory paradigms also generalize to a sequence completion task, which more closely resembles the next-token prediction process in LLM pre-training. Together, our findings reveal how information retrieval demands, model architecture, and structural attention dynamics during model training can jointly produce positional bias observed in LLMs.</li>
</ul>

<h3>Title: Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Christopher Chiu, Silviu Pitis, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10278">https://arxiv.org/abs/2510.10278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10278">https://arxiv.org/pdf/2510.10278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10278]] Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models(https://arxiv.org/abs/2510.10278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Clinical reasoning in medicine is a hypothesis-driven process where physicians refine diagnoses from limited information through targeted history, physical examination, and diagnostic investigations. In contrast, current medical benchmarks for large language models (LLMs) primarily assess knowledge recall through single-turn questions, where complete clinical information is provided upfront. To address this gap, we introduce VivaBench, a multi-turn benchmark that evaluates sequential clinical reasoning in LLM agents. Our dataset consists of 1762 physician-curated clinical vignettes structured as interactive scenarios that simulate a (oral) examination in medical training, requiring agents to actively probe for relevant findings, select appropriate investigations, and synthesize information across multiple steps to reach a diagnosis. While current LLMs demonstrate competence in diagnosing conditions from well-described clinical presentations, their performance degrades significantly when required to navigate iterative diagnostic reasoning under uncertainty in our evaluation. Our analysis identified several failure modes that mirror common cognitive errors in clinical practice, including: (1) fixation on initial hypotheses, (2) inappropriate investigation ordering, (3) premature diagnostic closure, and (4) failing to screen for critical conditions. These patterns reveal fundamental limitations in how current LLMs reason and make decisions under uncertainty. Through VivaBench, we provide a standardized benchmark for evaluating conversational medical AI systems for real-world clinical decision support. Beyond medical applications, we contribute to the larger corpus of research on agentic AI by demonstrating how sequential reasoning trajectories can diverge in complex decision-making environments.</li>
</ul>

<h3>Title: On the Entity-Level Alignment in Crosslingual Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yihong Liu, Mingyang Wang, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10280">https://arxiv.org/abs/2510.10280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10280">https://arxiv.org/pdf/2510.10280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10280]] On the Entity-Level Alignment in Crosslingual Consistency(https://arxiv.org/abs/2510.10280)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) are expected to recall factual knowledge consistently across languages. However, the factors that give rise to such crosslingual consistency -- and its frequent failure -- remain poorly understood. In this work, we hypothesize that these inconsistencies may arise from failures in entity alignment, the process of mapping subject and object entities into a shared conceptual space across languages. To test this, we assess alignment through entity-level (subject and object) translation tasks, and find that consistency is strongly correlated with alignment across all studied models, with misalignment of subjects or objects frequently resulting in inconsistencies. Building on this insight, we propose SubSub and SubInj, two effective methods that integrate English translations of subjects into prompts across languages, leading to substantial gains in both factual recall accuracy and consistency. Finally, our mechanistic analysis reveals that these interventions reinforce the entity representation alignment in the conceptual space through model's internal pivot-language processing, offering effective and practical strategies for improving multilingual factual prediction.</li>
</ul>

<h3>Title: ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test</h3>
<ul>
<li><strong>Authors: </strong>Guan-Yan Yang, Tzu-Yu Cheng, Ya-Wen Teng, Farn Wanga, Kuo-Hui Yeh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10281">https://arxiv.org/abs/2510.10281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10281">https://arxiv.org/pdf/2510.10281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10281]] ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test(https://arxiv.org/abs/2510.10281)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) into computer applications has introduced transformative capabilities but also significant security challenges. Existing safety alignments, which primarily focus on semantic interpretation, leave LLMs vulnerable to attacks that use non-standard data representations. This paper introduces ArtPerception, a novel black-box jailbreak framework that strategically leverages ASCII art to bypass the security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that rely on iterative, brute-force attacks, ArtPerception introduces a systematic, two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to empirically determine the optimal parameters for ASCII art recognition. Phase 2 leverages these insights to launch a highly efficient, one-shot malicious jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a more nuanced evaluation of an LLM's recognition capability. Through comprehensive experiments on four SOTA open-source LLMs, we demonstrate superior jailbreak performance. We further validate our framework's real-world relevance by showing its successful transferability to leading commercial models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting a rigorous effectiveness analysis against potential defenses such as LLaMA Guard and Azure's content filters. Our findings underscore that true LLM security requires defending against a multi-modal space of interpretations, even within text-only inputs, and highlight the effectiveness of strategic, reconnaissance-based attacks. Content Warning: This paper includes potentially harmful and offensive model outputs.</li>
</ul>

<h3>Title: SAM2LoRA: Composite Loss-Guided, Parameter-Efficient Finetuning of SAM2 for Retinal Fundus Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sayan Mandal, Divyadarshini Karthikeyan, Manas Paldhe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10288">https://arxiv.org/abs/2510.10288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10288">https://arxiv.org/pdf/2510.10288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10288]] SAM2LoRA: Composite Loss-Guided, Parameter-Efficient Finetuning of SAM2 for Retinal Fundus Segmentation(https://arxiv.org/abs/2510.10288)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We propose SAM2LoRA, a parameter-efficient fine-tuning strategy that adapts the Segment Anything Model 2 (SAM2) for fundus image segmentation. SAM2 employs a masked autoencoder-pretrained Hierarchical Vision Transformer for multi-scale feature decoding, enabling rapid inference in low-resource settings; however, fine-tuning remains challenging. To address this, SAM2LoRA integrates a low-rank adapter into both the image encoder and mask decoder, requiring fewer than 5\% of the original trainable parameters. Our analysis indicates that for cross-dataset fundus segmentation tasks, a composite loss function combining segmentationBCE, SoftDice, and FocalTversky losses is essential for optimal network tuning. Evaluated on 11 challenging fundus segmentation datasets, SAM2LoRA demonstrates high performance in both blood vessel and optic disc segmentation under cross-dataset training conditions. It achieves Dice scores of up to 0.86 and 0.93 for blood vessel and optic disc segmentation, respectively, and AUC values of up to 0.98 and 0.99, achieving state-of-the-art performance while substantially reducing training overhead.</li>
</ul>

<h3>Title: From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries</h3>
<ul>
<li><strong>Authors: </strong>Joy Hsu, Emily Jin, Jiajun Wu, Niloy J. Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10292">https://arxiv.org/abs/2510.10292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10292">https://arxiv.org/pdf/2510.10292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10292]] From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries(https://arxiv.org/abs/2510.10292)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Real-world scenes, such as those in ScanNet, are difficult to capture, with highly limited data available. Generating realistic scenes with varied object poses remains an open and challenging task. In this work, we propose FactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging the underlying structure of rooms while learning the variation of object poses from lived-in scenes. We introduce a factored representation that decomposes scenes into hierarchically organized concepts of room programs and object poses. To encode structure, FactoredScenes learns a library of functions capturing reusable layout patterns from which scenes are drawn, then uses large language models to generate high-level programs, regularized by the learned library. To represent scene variations, FactoredScenes learns a program-conditioned model to hierarchically predict object poses, and retrieves and places 3D objects in a scene. We show that FactoredScenes generates realistic, real-world rooms that are difficult to distinguish from real ScanNet scenes.</li>
</ul>

<h3>Title: Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy</h3>
<ul>
<li><strong>Authors: </strong>Ananya Malik, Nazanin Sabri, Melissa Karnaze, Mai Elsherief</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10328">https://arxiv.org/abs/2510.10328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10328">https://arxiv.org/pdf/2510.10328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10328]] Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy(https://arxiv.org/abs/2510.10328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models' (LLMs) ability to converse naturally is empowered by their ability to empathetically understand and respond to their users. However, emotional experiences are shaped by demographic and cultural contexts. This raises an important question: Can LLMs demonstrate equitable empathy across diverse user groups? We propose a framework to investigate how LLMs' cognitive and affective empathy vary across user personas defined by intersecting demographic attributes. Our study introduces a novel intersectional analysis spanning 315 unique personas, constructed from combinations of age, culture, and gender, across four LLMs. Results show that attributes profoundly shape a model's empathetic responses. Interestingly, we see that adding multiple attributes at once can attenuate and reverse expected empathy patterns. We show that they broadly reflect real-world empathetic trends, with notable misalignments for certain groups, such as those from Confucian culture. We complement our quantitative findings with qualitative insights to uncover model behaviour patterns across different demographic groups. Our findings highlight the importance of designing empathy-aware LLMs that account for demographic diversity to promote more inclusive and equitable model behaviour.</li>
</ul>

<h3>Title: End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nam Luu, Ondřej Bojar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10329">https://arxiv.org/abs/2510.10329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10329">https://arxiv.org/pdf/2510.10329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10329]] End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs(https://arxiv.org/abs/2510.10329)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language; this task has two different approaches, namely the traditional cascade and the more recent end-to-end. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper and NLLB, with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.</li>
</ul>

<h3>Title: Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsuan Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10342">https://arxiv.org/abs/2510.10342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10342">https://arxiv.org/pdf/2510.10342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10342]] Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis(https://arxiv.org/abs/2510.10342)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Accurate traffic congestion classification is essential for intelligent transportation systems and real-time urban traffic management. This paper presents a multimodal framework combining open-vocabulary visual-language reasoning (CLIP), object detection (YOLO-World), and motion analysis via MOG2-based background subtraction. The system predicts congestion levels on an ordinal scale from 1 (free flow) to 5 (severe congestion), enabling semantically aligned and temporally consistent classification. To enhance interpretability, we incorporate motion-based confidence weighting and generate annotated visual outputs. Experimental results show the model achieves 76.7 percent accuracy, an F1 score of 0.752, and a Quadratic Weighted Kappa (QWK) of 0.684, significantly outperforming unimodal baselines. These results demonstrate the framework's effectiveness in preserving ordinal structure and leveraging visual-language and motion modalities. Future enhancements include incorporating vehicle sizing and refined density metrics.</li>
</ul>

<h3>Title: Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker</h3>
<ul>
<li><strong>Authors: </strong>Ali Mirzazadeh, Simon Cadavid, Kaiwen Zha, Chao Li, Sultan Alzahrani, Manar Alawajy, Joshua Korzenik, Kreshnik Hoti, Charles Reynolds, David Mischoulon, John Winkelman, Maurizio Fava, Dina Katabi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10364">https://arxiv.org/abs/2510.10364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10364">https://arxiv.org/pdf/2510.10364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10364]] Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker(https://arxiv.org/abs/2510.10364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Antidepressant nonadherence is pervasive, driving relapse, hospitalization, suicide risk, and billions in avoidable costs. Clinicians need tools that detect adherence lapses promptly, yet current methods are either invasive (serum assays, neuroimaging) or proxy-based and inaccurate (pill counts, pharmacy refills). We present the first noninvasive biomarker that detects antidepressant intake from a single night of sleep. A transformer-based model analyzes sleep data from a consumer wearable or contactless wireless sensor to infer antidepressant intake, enabling remote, effortless, daily adherence assessment at home. Across six datasets comprising 62,000 nights from >20,000 participants (1,800 antidepressant users), the biomarker achieved AUROC = 0.84, generalized across drug classes, scaled with dose, and remained robust to concomitant psychotropics. Longitudinal monitoring captured real-world initiation, tapering, and lapses. This approach offers objective, scalable adherence surveillance with potential to improve depression care and outcomes.</li>
</ul>

<h3>Title: PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion</h3>
<ul>
<li><strong>Authors: </strong>Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10365">https://arxiv.org/abs/2510.10365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10365">https://arxiv.org/pdf/2510.10365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10365]] PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion(https://arxiv.org/abs/2510.10365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point cloud completion is essential for robust 3D perception in safety-critical applications such as robotics and augmented reality. However, existing models perform static inference and rely heavily on inductive biases learned during training, limiting their ability to adapt to novel structural patterns and sensor-induced distortions at test time. To address this limitation, we propose PointMAC, a meta-learned framework for robust test-time adaptation in point cloud completion. It enables sample-specific refinement without requiring additional supervision. Our method optimizes the completion model under two self-supervised auxiliary objectives that simulate structural and sensor-level incompleteness. A meta-auxiliary learning strategy based on Model-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary objectives is consistently aligned with the primary completion task. During inference, we adapt the shared encoder on-the-fly by optimizing auxiliary losses, with the decoder kept fixed. To further stabilize adaptation, we introduce Adaptive $\lambda$-Calibration, a meta-learned mechanism for balancing gradients between primary and auxiliary objectives. Extensive experiments on synthetic, simulated, and real-world datasets demonstrate that PointMAC achieves state-of-the-art results by refining each sample individually to produce high-quality completions. To the best of our knowledge, this is the first work to apply meta-auxiliary test-time adaptation to point cloud completion.</li>
</ul>

<h3>Title: Applying non-negative matrix factorization with covariates to label matrix for classification</h3>
<ul>
<li><strong>Authors: </strong>Kenichi Satoh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10375">https://arxiv.org/abs/2510.10375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10375">https://arxiv.org/pdf/2510.10375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10375]] Applying non-negative matrix factorization with covariates to label matrix for classification(https://arxiv.org/abs/2510.10375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Non-negative matrix factorization (NMF) is widely used for dimensionality reduction and interpretable analysis, but standard formulations are unsupervised and cannot directly exploit class labels. Existing supervised or semi-supervised extensions usually incorporate labels only via penalties or graph constraints, still requiring an external classifier. We propose \textit{NMF-LAB} (Non-negative Matrix Factorization for Label Matrix), which redefines classification as the inverse problem of non-negative matrix tri-factorization (tri-NMF). Unlike joint NMF methods, which reconstruct both features and labels, NMF-LAB directly factorizes the label matrix $Y$ as the observation, while covariates $A$ are treated as given explanatory variables. This yields a direct probabilistic mapping from covariates to labels, distinguishing our method from label-matrix factorization approaches that mainly model label correlations or impute missing labels. Our inversion offers two key advantages: (i) class-membership probabilities are obtained directly from the factorization without a separate classifier, and (ii) covariates, including kernel-based similarities, can be seamlessly integrated to generalize predictions to unseen samples. In addition, unlabeled data can be encoded as uniform distributions, supporting semi-supervised learning. Experiments on diverse datasets, from small-scale benchmarks to the large-scale MNIST dataset, demonstrate that NMF-LAB achieves competitive predictive accuracy, robustness to noisy or incomplete labels, and scalability to high-dimensional problems, while preserving interpretability. By unifying regression and classification within the tri-NMF framework, NMF-LAB provides a novel, probabilistic, and scalable approach to modern classification tasks.</li>
</ul>

<h3>Title: Self-Supervised Multi-Scale Transformer with Attention-Guided Fusion for Efficient Crack Detection</h3>
<ul>
<li><strong>Authors: </strong>Blessing Agyei Kyem, Joshua Kofi Asamoah, Eugene Denteh, Andrews Danyo, Armstrong Aboah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10378">https://arxiv.org/abs/2510.10378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10378">https://arxiv.org/pdf/2510.10378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10378]] Self-Supervised Multi-Scale Transformer with Attention-Guided Fusion for Efficient Crack Detection(https://arxiv.org/abs/2510.10378)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Pavement crack detection has long depended on costly and time-intensive pixel-level annotations, which limit its scalability for large-scale infrastructure monitoring. To overcome this barrier, this paper examines the feasibility of achieving effective pixel-level crack segmentation entirely without manual annotations. Building on this objective, a fully self-supervised framework, Crack-Segmenter, is developed, integrating three complementary modules: the Scale-Adaptive Embedder (SAE) for robust multi-scale feature extraction, the Directional Attention Transformer (DAT) for maintaining linear crack continuity, and the Attention-Guided Fusion (AGF) module for adaptive feature integration. Through evaluations on ten public datasets, Crack-Segmenter consistently outperforms 13 state-of-the-art supervised methods across all major metrics, including mean Intersection over Union (mIoU), Dice score, XOR, and Hausdorff Distance (HD). These findings demonstrate that annotation-free crack detection is not only feasible but also superior, enabling transportation agencies and infrastructure managers to conduct scalable and cost-effective monitoring. This work advances self-supervised learning and motivates pavement cracks detection research.</li>
</ul>

<h3>Title: RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aashiq Muhamed, Leonardo F. R. Ribeiro, Markus Dreyer, Virginia Smith, Mona T. Diab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10390">https://arxiv.org/abs/2510.10390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10390">https://arxiv.org/pdf/2510.10390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10390]] RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models(https://arxiv.org/abs/2510.10390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ability of language models in RAG systems to selectively refuse to answer based on flawed context is critical for safety, yet remains a significant failure point. Our large-scale study reveals that even frontier models struggle in this setting, with refusal accuracy dropping below 50% on multi-document tasks, while exhibiting either dangerous overconfidence or overcaution. Static benchmarks fail to reliably evaluate this capability, as models exploit dataset-specific artifacts and memorize test instances. We introduce RefusalBench, a generative methodology that programmatically creates diagnostic test cases through controlled linguistic perturbation. Our framework employs 176 distinct perturbation strategies across six categories of informational uncertainty and three intensity levels. Evaluation of over 30 models uncovers systematic failure patterns: refusal comprises separable detection and categorization skills, and neither scale nor extended reasoning improves performance. We find that selective refusal is a trainable, alignment-sensitive capability, offering a clear path for improvement. We release two benchmarks -- RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) -- and our complete generation framework to enable continued, dynamic evaluation of this critical capability.</li>
</ul>

<h3>Title: STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Geunyeong Jeong, Juoh Sun, Seonghee Lee, Harksoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10398">https://arxiv.org/abs/2510.10398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10398">https://arxiv.org/pdf/2510.10398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10398]] STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models(https://arxiv.org/abs/2510.10398)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models store extensive factual knowledge acquired during large-scale pre-training. However, this knowledge is inherently static, reflecting only the state of the world at the time of training. Knowledge editing has emerged as a promising solution for updating outdated or incorrect facts without full retraining. However, most existing locate-and-edit methods primarily focus on token-level likelihood optimization without addressing semantic coherence. Our analysis reveals that such edited knowledge is often encoded as isolated residual streams in the model's latent space, distinct from pre-existing knowledge and bypassing natural reasoning process. To address this, we propose \textsc{Steam}, a semantic-level knowledge editing framework that enhances integration of updated knowledge into the model's knowledge structure. \textsc{Steam} first identifies target representations as semantic anchors for the updated factual association, then guides the internal representation of the edited fact towards these anchors through an alignment loss during optimization. Experimental results demonstrate that \textsc{Steam} improves model's ability to reason with edited knowledge and enhances semantic coherence, underscoring the importance of latent-space alignment for reliable and coherent knowledge editing. The code is available at this https URL.</li>
</ul>

<h3>Title: Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jiachi Zhao, Zehong Wang, Yamei Liao, Chuxu Zhang, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10402">https://arxiv.org/abs/2510.10402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10402">https://arxiv.org/pdf/2510.10402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10402]] Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance(https://arxiv.org/abs/2510.10402)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation is a fundamental problem in graph learning with broad applications across Web-scale systems, knowledge graphs, and scientific domains such as drug and material discovery. Recent approaches leverage diffusion models for step-by-step generation, yet unconditional diffusion offers little control over desired properties, often leading to unstable quality and difficulty in incorporating new objectives. Inference-time guidance methods mitigate these issues by adjusting the sampling process without retraining, but they remain inherently local, heuristic, and limited in controllability. To overcome these limitations, we propose TreeDiff, a Monte Carlo Tree Search (MCTS) guided dual-space diffusion framework for controllable graph generation. TreeDiff is a plug-and-play inference-time method that expands the search space while keeping computation tractable. Specifically, TreeDiff introduces three key designs to make it practical and scalable: (1) a macro-step expansion strategy that groups multiple denoising updates into a single transition, reducing tree depth and enabling long-horizon exploration; (2) a dual-space denoising mechanism that couples efficient latent-space denoising with lightweight discrete correction in graph space, ensuring both scalability and structural fidelity; and (3) a dual-space verifier that predicts long-term rewards from partially denoised graphs, enabling early value estimation and removing the need for full rollouts. Extensive experiments on 2D and 3D molecular generation benchmarks, under both unconditional and conditional settings, demonstrate that TreeDiff achieves state-of-the-art performance. Notably, TreeDiff exhibits favorable inference-time scaling: it continues to improve with additional computation, while existing inference-time methods plateau early under limited resources.</li>
</ul>

<h3>Title: Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes</h3>
<ul>
<li><strong>Authors: </strong>Zhao-Yang Wang, Jieneng Chen, Jiang Liu, Yuxiang Guo, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10406">https://arxiv.org/abs/2510.10406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10406">https://arxiv.org/pdf/2510.10406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10406]] Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes(https://arxiv.org/abs/2510.10406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric</a></li>
<li><strong>Abstract: </strong>Gait recognition, a fundamental biometric technology, leverages unique walking patterns for individual identification, typically using 2D representations such as silhouettes or skeletons. However, these methods often struggle with viewpoint variations, occlusions, and noise. Multi-modal approaches that incorporate 3D body shape information offer improved robustness but are computationally expensive, limiting their feasibility for real-time applications. To address these challenges, we introduce Mesh-Gait, a novel end-to-end multi-modal gait recognition framework that directly reconstructs 3D representations from 2D silhouettes, effectively combining the strengths of both modalities. Compared to existing methods, directly learning 3D features from 3D joints or meshes is complex and difficult to fuse with silhouette-based gait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an intermediate representation, enabling the model to effectively capture 3D geometric information while maintaining simplicity and computational efficiency. During training, the intermediate 3D heatmaps are gradually reconstructed and become increasingly accurate under supervised learning, where the loss is calculated between the reconstructed 3D joints, virtual markers, and 3D meshes and their corresponding ground truth, ensuring precise spatial alignment and consistent 3D structure. Mesh-Gait extracts discriminative features from both silhouettes and reconstructed 3D heatmaps in a computationally efficient manner. This design enables the model to capture spatial and structural gait characteristics while avoiding the heavy overhead of direct 3D reconstruction from RGB videos, allowing the network to focus on motion dynamics rather than irrelevant visual details. Extensive experiments demonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be released upon acceptance of the paper.</li>
</ul>

<h3>Title: PrediQL: Automated Testing of GraphQL APIs with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shaolun Liu, Sina Marefat, Omar Tsai, Yu Chen, Zecheng Deng, Jia Wang, Mohammad A. Tayebi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10407">https://arxiv.org/abs/2510.10407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10407">https://arxiv.org/pdf/2510.10407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10407]] PrediQL: Automated Testing of GraphQL APIs with LLMs(https://arxiv.org/abs/2510.10407)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>GraphQL's flexible query model and nested data dependencies expose APIs to complex, context-dependent vulnerabilities that are difficult to uncover using conventional testing tools. Existing fuzzers either rely on random payload generation or rigid mutation heuristics, failing to adapt to the dynamic structures of GraphQL schemas and responses. We present PrediQL, the first retrieval-augmented, LLM-guided fuzzer for GraphQL APIs. PrediQL combines large language model reasoning with adaptive feedback loops to generate semantically valid and diverse queries. It models the choice of fuzzing strategy as a multi-armed bandit problem, balancing exploration of new query structures with exploitation of past successes. To enhance efficiency, PrediQL retrieves and reuses execution traces, schema fragments, and prior errors, enabling self-correction and progressive learning across test iterations. Beyond input generation, PrediQL integrates a context-aware vulnerability detector that uses LLM reasoning to analyze responses, interpreting data values, error messages, and status codes to identify issues such as injection flaws, access-control bypasses, and information disclosure. Our evaluation across open-source and benchmark GraphQL APIs shows that PrediQL achieves significantly higher coverage and vulnerability discovery rates compared to state-of-the-art baselines. These results demonstrate that combining retrieval-augmented reasoning with adaptive fuzzing can transform API security testing from reactive enumeration to intelligent exploration.</li>
</ul>

<h3>Title: Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhao-Yang Wang, Zhimin Shao, Jieneng Chen, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10417">https://arxiv.org/abs/2510.10417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10417">https://arxiv.org/pdf/2510.10417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10417]] Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis(https://arxiv.org/abs/2510.10417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, transformer</a></li>
<li><strong>Abstract: </strong>Gait recognition is an important biometric for human identification at a distance, particularly under low-resolution or unconstrained environments. Current works typically focus on either 2D representations (e.g., silhouettes and skeletons) or 3D representations (e.g., meshes and SMPLs), but relying on a single modality often fails to capture the full geometric and dynamic complexity of human walking patterns. In this paper, we propose a multi-modal and multi-task framework that combines 2D temporal silhouettes with 3D SMPL features for robust gait analysis. Beyond identification, we introduce a multitask learning strategy that jointly performs gait recognition and human attribute estimation, including age, body mass index (BMI), and gender. A unified transformer is employed to effectively fuse multi-modal gait features and better learn attribute-related representations, while preserving discriminative identity cues. Extensive experiments on the large-scale BRIAR datasets, collected under challenging conditions such as long-range distances (up to 1 km) and extreme pitch angles (up to 50°), demonstrate that our approach outperforms state-of-the-art methods in gait recognition and provides accurate human attribute estimation. These results highlight the promise of multi-modal and multitask learning for advancing gait-based human understanding in real-world scenarios.</li>
</ul>

<h3>Title: Softmax $\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent</h3>
<ul>
<li><strong>Authors: </strong>Sara Dragutinović, Andrew M. Saxe, Aaditya K. Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10425">https://arxiv.org/abs/2510.10425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10425">https://arxiv.org/pdf/2510.10425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10425]] Softmax $\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent(https://arxiv.org/abs/2510.10425)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The remarkable ability of transformers to learn new concepts solely by reading examples within the input prompt, termed in-context learning (ICL), is a crucial aspect of intelligent behavior. Here, we focus on understanding the learning algorithm transformers use to learn from context. Existing theoretical work, often based on simplifying assumptions, has primarily focused on linear self-attention and continuous regression tasks, finding transformers can learn in-context by gradient descent. Given that transformers are typically trained on discrete and complex tasks, we bridge the gap from this existing work to the setting of classification, with non-linear (importantly, softmax) activation. We find that transformers still learn to do gradient descent in-context, though on functionals in the kernel feature space and with a context-adaptive learning rate in the case of softmax transformer. These theoretical findings suggest a greater adaptability to context for softmax attention, which we empirically verify and study through ablations. Overall, we hope this enhances theoretical understanding of in-context learning algorithms in more realistic settings, pushes forward our intuitions and enables further theory bridging to larger models.</li>
</ul>

<h3>Title: Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Suyang Xi, Chenxi Yang, Hong Ding, Yiqing Ni, Catherine C. Liu, Yunhao Liu, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10426">https://arxiv.org/abs/2510.10426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10426">https://arxiv.org/pdf/2510.10426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10426]] Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs(https://arxiv.org/abs/2510.10426)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) often fail in fine-grained visual question answering, producing hallucinations about object identities, positions, and relations because textual queries are not explicitly anchored to visual referents. Retrieval-augmented generation (RAG) alleviates some errors, but it fails to align with human-like processing at both the retrieval and augmentation levels. Specifically, it focuses only on global-level image information but lacks local detail and limits reasoning about fine-grained interactions. To overcome this limitation, we present Human-Like Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to candidate referents via open-vocabulary detection (what), then spatially resolved with SAM-derived masks to recover fine-grained precision (where), and adaptively prioritized through the trade-off between local and global alignment (reweight). Mask-guided fine-tuning further injects spatial evidence into the generation process, transforming grounding from a passive bias into an explicit constraint on answer formulation. Extensive experiments demonstrate that this human-like cascade improves grounding fidelity and factual consistency while reducing hallucinations, advancing multimodal question answering toward trustworthy reasoning.</li>
</ul>

<h3>Title: Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Xu, Menghui Zhou, Jun Qi, Xuanhan Fan, Yun Yang, Po Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10433">https://arxiv.org/abs/2510.10433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10433">https://arxiv.org/pdf/2510.10433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10433]] Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression(https://arxiv.org/abs/2510.10433)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in aging populations, posing a significant and escalating burden on global healthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful computational paradigm for modeling longitudinal AD data, existing frameworks do not account for the time-varying nature of feature correlations. To address this limitation, we propose a novel MTL framework, named Feature Similarity Laplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel Feature Similarity Laplacian (FSL) penalty that explicitly models the time-varying relationships between features. By simultaneously considering temporal smoothness among tasks and the dynamic correlations among features, our model enhances both predictive accuracy and biological interpretability. To solve the non-smooth optimization problem arising from our proposed penalty terms, we adopt the Alternating Direction Method of Multipliers (ADMM) algorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework achieves state-of-the-art performance, outperforming various baseline methods. The implementation source can be found at this https URL.</li>
</ul>

<h3>Title: MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kangjian Zhu, Haobo Jiang, Yigong Zhang, Jianjun Qian, Jian Yang, Jin Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10434">https://arxiv.org/abs/2510.10434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10434">https://arxiv.org/pdf/2510.10434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10434]] MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation(https://arxiv.org/abs/2510.10434)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that formulates markerless, image-based robot pose estimation as a conditional denoising diffusion process. The framework consists of two processes: a visibility-constrained diffusion process for diverse pose augmentation and a timestep-aware reverse process for progressive pose refinement. The diffusion process progressively perturbs ground-truth poses to noisy transformations for training a pose denoising network. Importantly, we integrate visibility constraints into the process, ensuring the transformations remain within the camera field of view. Compared to the fixed-scale perturbations used in current methods, the diffusion process generates in-view and diverse training poses, thereby improving the network generalization capability. Furthermore, the reverse process iteratively predicts the poses by the denoising network and refines pose estimates by sampling from the diffusion posterior of current timestep, following a scheduled coarse-to-fine procedure. Moreover, the timestep indicates the transformation scales, which guide the denoising network to achieve more accurate pose predictions. The reverse process demonstrates higher robustness than direct prediction, benefiting from its timestep-aware refinement scheme. Our approach demonstrates improvements across two benchmarks (DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most challenging dataset, representing a 32.3% gain over the state-of-the-art.</li>
</ul>

<h3>Title: Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Gaurab Chhetri, Shriyank Somvanshi, Pavan Hebli, Shamyo Brotee, Subasish Das</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10436">https://arxiv.org/abs/2510.10436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10436">https://arxiv.org/pdf/2510.10436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10436]] Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey(https://arxiv.org/abs/2510.10436)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Post-quantum cryptography (PQC) is moving from evaluation to deployment as NIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the space from foundations to practice. We first develop a taxonomy across lattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families, summarizing security assumptions, cryptanalysis, and standardization status. We then compare performance and communication costs using representative, implementation-grounded measurements, and review hardware acceleration (AVX2, FPGA/ASIC) and implementation security with a focus on side-channel resistance. Building upward, we examine protocol integration (TLS, DNSSEC), PKI and certificate hygiene, and deployment in constrained and high-assurance environments (IoT, cloud, finance, blockchain). We also discuss complementarity with quantum technologies (QKD, QRNGs) and the limits of near-term quantum computing. Throughout, we emphasize crypto-agility, hybrid migration, and evidence-based guidance for operators. We conclude with open problems spanning parameter agility, leakage-resilient implementations, and domain-specific rollout playbooks. This survey aims to be a practical reference for researchers and practitioners planning quantum-safe systems, bridging standards, engineering, and operations.</li>
</ul>

<h3>Title: Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</h3>
<ul>
<li><strong>Authors: </strong>Masoud Makrehchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10446">https://arxiv.org/abs/2510.10446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10446">https://arxiv.org/pdf/2510.10446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10446]] Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation(https://arxiv.org/abs/2510.10446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.</li>
</ul>

<h3>Title: Steering Over-refusals Towards Safety in Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Utsav Maskey, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10452">https://arxiv.org/abs/2510.10452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10452">https://arxiv.org/pdf/2510.10452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10452]] Steering Over-refusals Towards Safety in Retrieval Augmented Generation(https://arxiv.org/abs/2510.10452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment in large language models (LLMs) induces over-refusals -- where LLMs decline benign requests due to aggressive safety filters. We analyze this phenomenon in retrieval-augmented generation (RAG), where both the query intent and retrieved context properties influence refusal behavior. We construct RagRefuse, a domain-stratified benchmark spanning medical, chemical, and open domains, pairing benign and harmful queries with controlled context contamination patterns and sizes. Our analysis shows that context arrangement / contamination, domain of query and context, and harmful-text density trigger refusals even on benign queries, with effects depending on model-specific alignment choices. To mitigate over-refusals, we introduce \textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers the embedding regions towards the confirmed safe, non-refusing output regions at inference time. This reduces over-refusals in contaminated RAG pipelines while preserving legitimate refusals.</li>
</ul>

<h3>Title: On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Tai Le-Gia, Ahn Jaehyun</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10456">https://arxiv.org/abs/2510.10456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10456">https://arxiv.org/pdf/2510.10456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10456]] On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection(https://arxiv.org/abs/2510.10456)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Zero-shot image anomaly classification (AC) and segmentation (AS) are vital for industrial quality control, detecting defects without prior training data. Existing representation-based methods compare patch features with nearest neighbors in unlabeled test images but struggle with consistent anomalies -- similar defects recurring across multiple images -- resulting in poor AC/AS performance. We introduce Consistent-Anomaly Detection Graph (CoDeGraph), a novel algorithm that identifies and filters consistent anomalies from similarity computations. Our key insight is that normal patches in industrial images show stable, gradually increasing similarity to other test images, while consistent-anomaly patches exhibit abrupt similarity spikes after exhausting a limited set of similar matches, a phenomenon we term ``neighbor-burnout.'' CoDeGraph constructs an image-level graph, with images as nodes and edges connecting those with shared consistent-anomaly patterns, using community detection to filter these anomalies. We provide a theoretical foundation using Extreme Value Theory to explain the effectiveness of our approach. Experiments on MVTec AD with the ViT-L-14-336 backbone achieve 98.3% AUROC for AC and AS performance of 66.8% (+4.2%) F1 and 68.1% (+5.4%) AP over state-of-the-art zero-shot methods. Using the DINOv2 backbone further improves segmentation, yielding 69.1% (+6.5%) F1 and 71.9% (+9.2%) AP, demonstrating robustness across architectures.</li>
</ul>

<h3>Title: NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication</h3>
<ul>
<li><strong>Authors: </strong>Prawaal Sharma, Poonam Goyal, Navneet Goyal, Vidisha Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10459">https://arxiv.org/abs/2510.10459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10459">https://arxiv.org/pdf/2510.10459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10459]] NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication(https://arxiv.org/abs/2510.10459)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Digital communication has become the cornerstone of modern interaction, enabling rapid, accessible, and interactive exchanges. However, individuals with lower academic literacy often face significant barriers, exacerbating the "digital divide". In this work, we introduce a novel, universal ideographic metalanguage designed as an innovative communication framework that transcends academic, linguistic, and cultural boundaries. Our approach leverages principles of Neuro-symbolic AI, combining neural-based large language models (LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables the semantic decomposition of complex ideas into simpler, atomic concepts. Adopting a human-centric, collaborative methodology, we engaged over 200 semi-literate participants in defining the problem, selecting ideographs, and validating the system. With over 80\% semantic comprehensibility, an accessible learning curve, and universal adaptability, our system effectively serves underprivileged populations with limited formal education.</li>
</ul>

<h3>Title: Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhong, Yuxuan Yang, Xinyue Zhang, Ruohan Ma, Yong Guo, Gang Li, Jupeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10462">https://arxiv.org/abs/2510.10462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10462">https://arxiv.org/pdf/2510.10462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10462]] Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation(https://arxiv.org/abs/2510.10462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation annotation suffers from inter-rater variability (IRV) due to differences in annotators' expertise and the inherent blurriness of medical images. Standard approaches that simply average expert labels are flawed, as they discard the valuable clinical uncertainty revealed in disagreements. We introduce a fundamentally new approach with our group decision simulation framework, which works by mimicking the collaborative decision-making process of a clinical panel. Under this framework, an Expert Signature Generator (ESG) learns to represent individual annotator styles in a unique latent space. A Simulated Consultation Module (SCM) then intelligently generates the final segmentation by sampling from this space. This method achieved state-of-the-art results on challenging CBCT and MRI datasets (92.11% and 90.72% Dice scores). By treating expert disagreement as a useful signal instead of noise, our work provides a clear path toward more robust and trustworthy AI systems for healthcare.</li>
</ul>

<h3>Title: Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment</h3>
<ul>
<li><strong>Authors: </strong>Junhao Dong, Dejia Liu, Ruiqi Ding, Zongxing Chen, Yingjie Huang, Zhu Meng, Jianbo Zhao, Zhicheng Zhao, Fei Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10464">https://arxiv.org/abs/2510.10464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10464">https://arxiv.org/pdf/2510.10464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10464]] Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment(https://arxiv.org/abs/2510.10464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Transjugular intrahepatic portosystemic shunt (TIPS) is an established procedure for portal hypertension, but provides variable survival outcomes and frequent overt hepatic encephalopathy (OHE), indicating the necessity of accurate preoperative prognostic modeling. Current studies typically build machine learning models from preoperative CT images or clinical characteristics, but face three key challenges: (1) labor-intensive region-of-interest (ROI) annotation, (2) poor reliability and generalizability of unimodal methods, and (3) incomplete assessment from single-endpoint prediction. Moreover, the lack of publicly accessible datasets constrains research in this field. Therefore, we present MultiTIPS, the first public multi-center dataset for TIPS prognosis, and propose a novel multimodal prognostic framework based on it. The framework comprises three core modules: (1) dual-option segmentation, which integrates semi-supervised and foundation model-based pipelines to achieve robust ROI segmentation with limited annotations and facilitate subsequent feature extraction; (2) multimodal interaction, where three techniques, multi-grained radiomics attention (MGRA), progressive orthogonal disentanglement (POD), and clinically guided prognostic enhancement (CGPE), are introduced to enable cross-modal feature interaction and complementary representation integration, thus improving model accuracy and robustness; and (3) multi-task prediction, where a staged training strategy is used to perform stable optimization of survival, portal pressure gradient (PPG), and OHE prediction for comprehensive prognostic assessment. Extensive experiments on MultiTIPS demonstrate the superiority of the proposed method over state-of-the-art approaches, along with strong cross-domain generalization and interpretability, indicating its promise for clinical application. The dataset and code are available.</li>
</ul>

<h3>Title: When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jinjin Cao, Zhiyang Chen, Zijun Wang, Liyuan Ma, Weijian Luo, Guojun Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10466">https://arxiv.org/abs/2510.10466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10466">https://arxiv.org/pdf/2510.10466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10466]] When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance(https://arxiv.org/abs/2510.10466)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown solid ability for multimodal understanding of both visual and language contexts. However, existing VLMs often face severe challenges of hallucinations, meaning that VLMs tend to generate responses that are only fluent in the language but irrelevant to images in previous contexts. To address this issue, we analyze how language bias contributes to hallucinations and then introduce Cross-Modal Guidance(CMG), a training-free decoding method that addresses the hallucinations by leveraging the difference between the output distributions of the original model and the one with degraded visual-language attention. In practice, we adaptively mask the attention weight of the most influential image tokens in selected transformer layers to corrupt the visual-language perception as a concrete type of degradation. Such a degradation-induced decoding emphasizes the perception of visual contexts and therefore significantly reduces language bias without harming the ability of VLMs. In experiment sections, we conduct comprehensive studies. All results demonstrate the superior advantages of CMG with neither additional conditions nor training costs. We also quantitatively show CMG can improve different VLM's performance on hallucination-specific benchmarks and generalize effectively.</li>
</ul>

<h3>Title: AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs</h3>
<ul>
<li><strong>Authors: </strong>Gunho Park, Jeongin Bae, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10467">https://arxiv.org/abs/2510.10467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10467">https://arxiv.org/pdf/2510.10467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10467]] AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs(https://arxiv.org/abs/2510.10467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The deployment of large language models (LLMs) is increasingly constrained by memory and latency bottlenecks, motivating the need for quantization techniques that flexibly balance accuracy and efficiency. Recent work has introduced multi-precision models, which enable inference at multiple precisions within a single model depending on runtime constraints. To support such flexibility, quantized weights are often stored as bit-planes, where hardware efficiency improves when the compute operates directly at the bit-plane level and activates only the precision required by each request. In this work, we present AnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded Quantization (BCQ) that supports direct bit-plane operations. By representing weights as binary bit-planes with corresponding scale factors, AnyBCQ enables bit-plane-level computation and maps naturally to accelerator-friendly, bit-parallel arithmetic. Our progressive precision expansion mechanism incrementally refines scaling factors while reusing previously assigned binary codes, yielding monotonic improvements in accuracy as additional bits are enabled. We further co-design a specialized kernel that exploits the BCQ structure to support dynamic per-request precision selection with negligible overhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly narrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains competitive at higher precision, and achieves throughput gains of up to 3.0x over half precision and 1.2x over state-of-the-art multi-precision methods. By aligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a practical foundation for multi-precision LLM deployment across diverse service-level objectives.</li>
</ul>

<h3>Title: DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chuang Chen, Wenyi Ge</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10471">https://arxiv.org/abs/2510.10471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10471">https://arxiv.org/pdf/2510.10471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10471]] DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation(https://arxiv.org/abs/2510.10471)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Environmental perception systems play a critical role in high-precision mapping and autonomous navigation, with LiDAR serving as a core sensor that provides accurate 3D point cloud data. How to efficiently process unstructured point clouds while extracting structured semantic information remains a significant challenge, and in recent years, numerous pseudo-image-based representation methods have emerged to achieve a balance between efficiency and performance. However, they often overlook the structural and semantic details of point clouds, resulting in limited feature fusion and discriminability. In this work, we propose DAGLFNet, a pseudo-image-based semantic segmentation framework designed to extract discriminative features. First, the Global-Local Feature Fusion Encoding module is used to enhance the correlation among local features within a set and capture global contextual information. Second, the Multi-Branch Feature Extraction network is employed to capture more neighborhood information and enhance the discriminability of contour features. Finally, a Feature Fusion via Deep Feature-guided Attention mechanism is introduced to improve the precision of cross-channel feature fusion. Experimental evaluations show that DAGLFNet achieves 69.83\% and 78.65\% on the validation sets of SemanticKITTI and nuScenes, respectively. The method balances high performance with real-time capability, demonstrating great potential for LiDAR-based real-time applications.</li>
</ul>

<h3>Title: FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth</h3>
<ul>
<li><strong>Authors: </strong>Qiran Zou, Hou Hei Lam, Wenhao Zhao, Yiming Tang, Tingting Chen, Samson Yu, Tianyi Zhang, Chang Liu, Xiangyang Ji, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10472">https://arxiv.org/abs/2510.10472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10472">https://arxiv.org/pdf/2510.10472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10472]] FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth(https://arxiv.org/abs/2510.10472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at this https URL.</li>
</ul>

<h3>Title: Assessing Large Language Models for Structured Medical Order Extraction</h3>
<ul>
<li><strong>Authors: </strong>A H M Rezaul Karim, Ozlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10475">https://arxiv.org/abs/2510.10475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10475">https://arxiv.org/pdf/2510.10475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10475]] Assessing Large Language Models for Structured Medical Order Extraction(https://arxiv.org/abs/2510.10475)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Medical order extraction is essential for structuring actionable clinical information, supporting decision-making, and enabling downstream applications such as documentation and workflow automation. Orders may be embedded in diverse sources, including electronic health records, discharge summaries, and multi-turn doctor-patient dialogues, and can span categories such as medications, laboratory tests, imaging studies, and follow-up actions. The MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from extended conversational transcripts, requiring the identification of order type, description, reason, and provenance. We present the MasonNLP submission, which ranked 5th among 17 participating teams with 105 total submissions. Our approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without domain-specific fine-tuning, guided by a single in-context example. This few-shot configuration achieved an average F1 score of 37.76, with notable improvements in reason and provenance accuracy. These results demonstrate that large, non-domain-specific LLMs, when paired with effective prompt engineering, can serve as strong, scalable baselines for specialized clinical NLP tasks.</li>
</ul>

<h3>Title: MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Deng Li, Jun Shao, Bohao Xing, Rong Gao, Bihan Wen, Heikki Kälviäinen, Xin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10478">https://arxiv.org/abs/2510.10478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10478">https://arxiv.org/pdf/2510.10478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10478]] MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture Recognition(https://arxiv.org/abs/2510.10478)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Micro-gesture recognition (MGR) targets the identification of subtle and fine-grained human motions and requires accurate modeling of both long-range and local spatiotemporal dependencies. While CNNs are effective at capturing local patterns, they struggle with long-range dependencies due to their limited receptive fields. Transformer-based models address this limitation through self-attention mechanisms but suffer from high computational costs. Recently, Mamba has shown promise as an efficient model, leveraging state space models (SSMs) to enable linear-time processing However, directly applying the vanilla Mamba to MGR may not be optimal. This is because Mamba processes inputs as 1D sequences, with state updates relying solely on the previous state, and thus lacks the ability to model local spatiotemporal dependencies. In addition, previous methods lack a design of motion-awareness, which is crucial in MGR. To overcome these limitations, we propose motion-aware state fusion mamba (MSF-Mamba), which enhances Mamba with local spatiotemporal modeling by fusing local contextual neighboring states. Our design introduces a motion-aware state fusion module based on central frame difference (CFD). Furthermore, a multiscale version named MSF-Mamba+ has been proposed. Specifically, MSF-Mamba supports multiscale motion-aware state fusion, as well as an adaptive scale weighting module that dynamically weighs the fused states across different scales. These enhancements explicitly address the limitations of vanilla Mamba by enabling motion-aware local spatiotemporal modeling, allowing MSF-Mamba and MSF-Mamba to effectively capture subtle motion cues for MGR. Experiments on two public MGR datasets demonstrate that even the lightweight version, namely, MSF-Mamba, achieves SoTA performance, outperforming existing CNN-, Transformer-, and SSM-based models while maintaining high efficiency.</li>
</ul>

<h3>Title: Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</h3>
<ul>
<li><strong>Authors: </strong>Zishen Zhang, Xiangzhe Kong, Wenbing Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10480">https://arxiv.org/abs/2510.10480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10480">https://arxiv.org/pdf/2510.10480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10480]] Latent Retrieval Augmented Generation of Cross-Domain Protein Binders(https://arxiv.org/abs/2510.10480)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.</li>
</ul>

<h3>Title: UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangxin He, Shen Nie, Fengqi Zhu, Yuankang Zhao, Tianyi Bai, Ran Yan, Jie Fu, Chongxuan Li, Binhang Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10481">https://arxiv.org/abs/2510.10481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10481">https://arxiv.org/pdf/2510.10481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10481]] UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models(https://arxiv.org/abs/2510.10481)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion LLMs have attracted growing interest, with plenty of recent work emphasizing their great potential in various downstream tasks; yet the long-context behavior of diffusion LLMs remains largely uncharted. We present a case study of post-training techniques for extending the context window of diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a simple modification to the standard Rotary Positional Embeddings (RoPE) extension effectively accommodates the probabilistic modeling inherent in the diffusion process, enabling stable scaling to longer context ranges. We further compare masking strategies used during post-training and analyze their impact on optimization stability and long-range recall. Instantiating these insights, we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that, in our empirical evaluation on long-context tasks, significantly outperforms training-free baselines. Our experimental results highlight the special positional extension as a key lever for scaling diffusion LLMs to extended contexts and offer practical guidance for practitioners seeking 128K-scale context via efficient post-training.</li>
</ul>

<h3>Title: Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Narayan S Iyer, Bivas Bhaumik, Ram S Iyer, Satyasaran Changdar</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10483">https://arxiv.org/abs/2510.10483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10483">https://arxiv.org/pdf/2510.10483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10483]] Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations(https://arxiv.org/abs/2510.10483)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Partial differential equations (PDEs) provide a mathematical foundation for simulating and understanding intricate behaviors in both physical sciences and engineering. With the growing capabilities of deep learning, data$-$driven approaches like Physics$-$Informed Neural Networks (PINNs) have been developed, offering a mesh$-$free, analytic type framework for efficiently solving PDEs across a wide range of applications. However, traditional PINNs often struggle with challenges such as limited precision, slow training dynamics, lack of labeled data availability, and inadequate handling of multi$-$physics interactions. To overcome these challenging issues of PINNs, we proposed a Gradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically introduces a gradient based pseudo point self$-$learning algorithm for solving PDEs. We tested the proposed method on three different types of PDE problems from various fields, each representing distinct scenarios. The effectiveness of the proposed method is evident, as the PINN approach for solving the Burgers$'$ equation attains a mean square error (MSE) on the order of $10^{-3}$, while the diffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after 12,500 iterations, with no further improvement as the iterations increase. In contrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease, demonstrating better generalization and reaching an MSE on the order of $10^{-5}$ after 18,500 iterations. Furthermore, the results show that the proposed purely semi$-$supervised gST$-$PINN consistently outperforms the standard PINN method in all cases, even when solution of the PDEs are unavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and can be effectively applied in scenarios prone to low accuracy and convergence issues, particularly in the absence of labeled data.</li>
</ul>

<h3>Title: SASER: Stego attacks on open-source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ming Tan, Wei Li, Hu Tao, Hailong Ma, Aodi Liu, Qian Chen, Zilong Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10486">https://arxiv.org/abs/2510.10486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10486">https://arxiv.org/pdf/2510.10486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10486]] SASER: Stego attacks on open-source LLMs(https://arxiv.org/abs/2510.10486)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Open-source large language models (LLMs) have demonstrated considerable dominance over proprietary LLMs in resolving neural processing tasks, thanks to the collaborative and sharing nature. Although full access to source codes, model parameters, and training data lays the groundwork for transparency, we argue that such a full-access manner is vulnerable to stego attacks, and their ill-effects are not fully understood. In this paper, we conduct a systematic formalization for stego attacks on open-source LLMs by enumerating all possible threat models associated with adversary objectives, knowledge, and capabilities. Therein, the threat posed by adversaries with internal knowledge, who inject payloads and triggers during the model sharing phase, is of practical interest. We go even further and propose the first stego attack on open-source LLMs, dubbed SASER, which wields impacts through identifying targeted parameters, embedding payloads, injecting triggers, and executing payloads sequentially. Particularly, SASER enhances the attack robustness against quantization-based local deployment by de-quantizing the embedded payloads. In addition, to achieve stealthiness, SASER devises the performance-aware importance metric to identify targeted parameters with the least degradation of model performance. Extensive experiments on LlaMA2-7B and ChatGLM3-6B, without quantization, show that the stealth rate of SASER outperforms existing stego attacks (for general DNNs) by up to 98.1%, while achieving the same attack success rate (ASR) of 100%. More importantly, SASER improves ASR on quantized models from 0 to 100% in all settings. We appeal for investigations on countermeasures against SASER in view of the significant attack effectiveness.</li>
</ul>

<h3>Title: Towards Self-Refinement of Vision-Language Models with Triangular Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Deng, Guangyi Chen, Tianpei Gu, Lingjing Kong, Yan Li, Zeyu Tang, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10487">https://arxiv.org/abs/2510.10487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10487">https://arxiv.org/pdf/2510.10487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10487]] Towards Self-Refinement of Vision-Language Models with Triangular Consistency(https://arxiv.org/abs/2510.10487)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\rightarrow$question-answer or image-answer$\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. To investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback. We expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at this https URL.</li>
</ul>

<h3>Title: Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiaye Li, Baoyou Chen, Hui Li, Zilong Dong, Jingdong Wang, Siyu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10489">https://arxiv.org/abs/2510.10489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10489">https://arxiv.org/pdf/2510.10489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10489]] Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation(https://arxiv.org/abs/2510.10489)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Transformers rely on explicit positional encoding to model structure in data. While Rotary Position Embedding (RoPE) excels in 1D domains, its application to image generation reveals significant limitations such as fine-grained spatial relation modeling, color cues, and object counting. This paper identifies key limitations of standard multi-dimensional RoPE-rigid frequency allocation, axis-wise independence, and uniform head treatment-in capturing the complex structural biases required for fine-grained image generation. We propose HARoPE, a head-wise adaptive extension that inserts a learnable linear transformation parameterized via singular value decomposition (SVD) before the rotary mapping. This lightweight modification enables dynamic frequency reallocation, semantic alignment of rotary planes, and head-specific positional receptive fields while rigorously preserving RoPE's relative-position property. Extensive experiments on class-conditional ImageNet and text-to-image generation (Flux and MMDiT) demonstrate that HARoPE consistently improves performance over strong RoPE baselines and other extensions. The method serves as an effective drop-in replacement, offering a principled and adaptable solution for enhancing positional awareness in transformer-based image generative models.</li>
</ul>

<h3>Title: VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction</h3>
<ul>
<li><strong>Authors: </strong>Prawaal Sharma, Poonam Goyal, Vidisha Sharma, Navneet Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10490">https://arxiv.org/abs/2510.10490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10490">https://arxiv.org/pdf/2510.10490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10490]] VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction(https://arxiv.org/abs/2510.10490)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>UNESCO has classified 2500 out of 7000 languages spoken worldwide as endangered. Attrition of a language leads to loss of traditional wisdom, folk literature, and the essence of the community that uses it. It is therefore imperative to bring digital inclusion to these languages and avoid its extinction. Low resource languages are at a greater risk of extinction. Lack of unsupervised Optical Character Recognition(OCR) methodologies for low resource languages is one of the reasons impeding their digital inclusion. We propose VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph feature recommendation for cluster-based labelling. We augment the labelled data for diversity and volume using image transformations and Generative Adversarial Networks. Voltage has been designed using Takri - a family of scripts used in 16th to 20th century in the Himalayan regions of India. We present results for Takri along with other Indic scripts (both low and high resource) to substantiate the universal behavior of the methodology. An accuracy of 95% for machine printed and 87% for handwritten samples on Takri script has been achieved. We conduct baseline and ablation studies along with building downstream use cases for Takri, demonstrating the usefulness of our work.</li>
</ul>

<h3>Title: The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution</h3>
<ul>
<li><strong>Authors: </strong>Norbert Tihanyi, Bilel Cherif, Richard A. Dubniczky, Mohamed Amine Ferrag, Tamás Bisztray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10493">https://arxiv.org/abs/2510.10493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10493">https://arxiv.org/pdf/2510.10493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10493]] The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution(https://arxiv.org/abs/2510.10493)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing a critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as a single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000 this http URL back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and a modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As a result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: this https URL.</li>
</ul>

<h3>Title: Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking</h3>
<ul>
<li><strong>Authors: </strong>Yuteng Ye, Zheng Zhang, Qinchuan Zhang, Di Wang, Youjia Zhang, Wenxiao Zhang, Wei Yang, Yuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10497">https://arxiv.org/abs/2510.10497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10497">https://arxiv.org/pdf/2510.10497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10497]] Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking(https://arxiv.org/abs/2510.10497)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Controllable 3D style transfer seeks to restyle a 3D asset so that its textures match a reference image while preserving the integrity and multi-view consistency. The prevalent methods either rely on direct reference style token injection or score-distillation from 2D diffusion models, which incurs heavy per-scene optimization and often entangles style with semantic content. We introduce Jigsaw3D, a multi-view diffusion based pipeline that decouples style from content and enables fast, view-consistent stylization. Our key idea is to leverage the jigsaw operation - spatial shuffling and random masking of reference patches - to suppress object semantics and isolate stylistic statistics (color palettes, strokes, textures). We integrate these style cues into a multi-view diffusion model via reference-to-view cross-attention, producing view-consistent stylized renderings conditioned on the input mesh. The renders are then style-baked onto the surface to yield seamless textures. Across standard 3D stylization benchmarks, Jigsaw3D achieves high style fidelity and multi-view consistency with substantially lower latency, and generalizes to masked partial reference stylization, multi-object scene styling, and tileable texture generation. Project page is available at: this https URL</li>
</ul>

<h3>Title: Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Kanishkha Jaisankar, Sunidhi Tandel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10503">https://arxiv.org/abs/2510.10503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10503">https://arxiv.org/pdf/2510.10503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10503]] Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving(https://arxiv.org/abs/2510.10503)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Motion planning in complex scenarios is a core challenge in autonomous driving. Conventional methods apply predefined rules or learn from driving data to generate trajectories, while recent approaches leverage large language models (LLMs) for decision-making. However, it remains unclear whether LLMs truly capture human driving logic. We propose Align2Act, a motion planning framework that transforms instruction-tuned LLMs into interpretable planners aligned with human behavior. We derive structured driving instructions based on human reasoning patterns (e.g., anticipate hazards, yield at intersections) and traffic rules (e.g., stop at red lights, maintain lane boundaries). Our Align2ActChain module guides step-by-step reasoning to produce both an interpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with LoRA on one million scenarios from the nuPlan dataset, our method achieves an open-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and 66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or open-loop settings, we demonstrate improved planning quality and human-likeness on the real-world nuPlan closed-loop benchmark. Ablation studies confirm that structured reasoning significantly improves performance over baseline LLM planners.</li>
</ul>

<h3>Title: A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets</h3>
<ul>
<li><strong>Authors: </strong>Md Ibrahim Shikder Mahin, Md Shamsul Arefin, Md Tanvir Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10513">https://arxiv.org/abs/2510.10513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10513">https://arxiv.org/pdf/2510.10513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10513]] A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets(https://arxiv.org/abs/2510.10513)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Healthcare research and development face significant obstacles due to data scarcity and stringent privacy regulations, such as HIPAA and the GDPR, restricting access to essential real-world medical data. These limitations impede innovation, delay robust AI model creation, and hinder advancements in patient-centered care. Synthetic data generation offers a transformative solution by producing artificial datasets that emulate real data statistics while safeguarding patient privacy. We introduce a novel hybrid framework for high-fidelity healthcare data synthesis integrating five augmentation methods: noise injection, interpolation, Gaussian Mixture Model (GMM) sampling, Conditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a reinforcement learning-based dynamic weight selection mechanism. Its key innovations include advanced calibration techniques -- moment matching, full histogram matching, soft and adaptive soft histogram matching, and iterative refinement -- that align marginal distributions and preserve joint feature dependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and Khulna Medical College cardiology datasets, our calibrated hybrid achieves Wasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around 0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores surpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%, confirming robust privacy protection. Downstream classifiers trained on synthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable to models trained on real data. This scalable, privacy-preserving approach matches state-of-the-art methods, sets new benchmarks for joint-distribution fidelity in healthcare, and supports sensitive AI applications.</li>
</ul>

<h3>Title: VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, Jiaheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10518">https://arxiv.org/abs/2510.10518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10518">https://arxiv.org/pdf/2510.10518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10518]] VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning(https://arxiv.org/abs/2510.10518)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling.</li>
</ul>

<h3>Title: Unified Open-World Segmentation with Multi-Modal Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Yufei Yin, Chenchen Jing, Muzhi Zhu, Hao Chen, Yuling Xi, Bo Feng, Hao Wang, Shiyu Li, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10524">https://arxiv.org/abs/2510.10524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10524">https://arxiv.org/pdf/2510.10524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10524]] Unified Open-World Segmentation with Multi-Modal Prompts(https://arxiv.org/abs/2510.10524)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we present COSINE, a unified open-world segmentation model that consolidates open-vocabulary segmentation and in-context segmentation with multi-modal prompts (e.g., text and image). COSINE exploits foundation models to extract representations for an input image and corresponding multi-modal prompts, and a SegDecoder to align these representations, model their interaction, and obtain masks specified by input prompts across different granularities. In this way, COSINE overcomes architectural discrepancies, divergent learning objectives, and distinct representation learning strategies of previous pipelines for open-vocabulary segmentation and in-context segmentation. Comprehensive experiments demonstrate that COSINE has significant performance improvements in both open-vocabulary and in-context segmentation tasks. Our exploratory analyses highlight that the synergistic collaboration between using visual and textual prompts leads to significantly improved generalization over single-modality approaches.</li>
</ul>

<h3>Title: Layout-Independent License Plate Recognition via Integrated Vision and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elham Shabaninia, Fatemeh Asadi-zeydabadi, Hossein Nezamabadi-pour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10533">https://arxiv.org/abs/2510.10533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10533">https://arxiv.org/pdf/2510.10533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10533]] Layout-Independent License Plate Recognition via Integrated Vision and Language Models(https://arxiv.org/abs/2510.10533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This work presents a pattern-aware framework for automatic license plate recognition (ALPR), designed to operate reliably across diverse plate layouts and challenging real-world conditions. The proposed system consists of a modern, high-precision detection network followed by a recognition stage that integrates a transformer-based vision model with an iterative language modelling mechanism. This unified recognition stage performs character identification and post-OCR refinement in a seamless process, learning the structural patterns and formatting rules specific to license plates without relying on explicit heuristic corrections or manual layout classification. Through this design, the system jointly optimizes visual and linguistic cues, enables iterative refinement to improve OCR accuracy under noise, distortion, and unconventional fonts, and achieves layout-independent recognition across multiple international datasets (IR-LPR, UFPR-ALPR, AOLP). Experimental results demonstrate superior accuracy and robustness compared to recent segmentation-free approaches, highlighting how embedding pattern analysis within the recognition stage bridges computer vision and language modelling for enhanced adaptability in intelligent transportation and surveillance applications.</li>
</ul>

<h3>Title: MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates</h3>
<ul>
<li><strong>Authors: </strong>Binyu Zhao, Wei Zhang, Zhaonian Zou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10534">https://arxiv.org/abs/2510.10534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10534">https://arxiv.org/pdf/2510.10534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10534]] MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates(https://arxiv.org/abs/2510.10534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal learning has made significant advances across diverse pattern recognition applications. However, handling missing modalities, especially under imbalanced missing rates, remains a major challenge. This imbalance triggers a vicious cycle: modalities with higher missing rates receive fewer updates, leading to inconsistent learning progress and representational degradation that further diminishes their contribution. Existing methods typically focus on global dataset-level balancing, often overlooking critical sample-level variations in modality utility and the underlying issue of degraded feature quality. We propose Modality Capability Enhancement (MCE) to tackle these limitations. MCE includes two synergistic components: i) Learning Capability Enhancement (LCE), which introduces multi-level factors to dynamically balance modality-specific learning progress, and ii) Representation Capability Enhancement (RCE), which improves feature semantics and robustness through subset prediction and cross-modal completion tasks. Comprehensive evaluations on four multi-modal benchmarks show that MCE consistently outperforms state-of-the-art methods under various missing configurations. The journal preprint version is now available at this https URL. Our code is available at this https URL.</li>
</ul>

<h3>Title: Detecting Hallucinations in Authentic LLM-Human Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yujie Ren, Niklas Gruhlke, Anne Lauscher</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10539">https://arxiv.org/abs/2510.10539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10539">https://arxiv.org/pdf/2510.10539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10539]] Detecting Hallucinations in Authentic LLM-Human Interactions(https://arxiv.org/abs/2510.10539)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly applied in sensitive domains such as medicine and law, hallucination detection has become a critical task. Although numerous benchmarks have been proposed to advance research in this area, most of them are artificially constructed--either through deliberate hallucination induction or simulated interactions--rather than derived from genuine LLM-human dialogues. Consequently, these benchmarks fail to fully capture the characteristics of hallucinations that occur in real-world usage. To address this limitation, we introduce AuthenHallu, the first hallucination detection benchmark built entirely from authentic LLM-human interactions. For AuthenHallu, we select and annotate samples from genuine LLM-human dialogues, thereby providing a faithful reflection of how LLMs hallucinate in everyday user interactions. Statistical analysis shows that hallucinations occur in 31.4% of the query-response pairs in our benchmark, and this proportion increases dramatically to 60.0% in challenging domains such as Math & Number Problems. Furthermore, we explore the potential of using vanilla LLMs themselves as hallucination detectors and find that, despite some promise, their current performance remains insufficient in real-world scenarios.</li>
</ul>

<h3>Title: Predicting Module-Lattice Reduction</h3>
<ul>
<li><strong>Authors: </strong>Léo Ducas, Lynn Engelberts, Paola de Perthuis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10540">https://arxiv.org/abs/2510.10540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10540">https://arxiv.org/pdf/2510.10540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10540]] Predicting Module-Lattice Reduction(https://arxiv.org/abs/2510.10540)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Is module-lattice reduction better than unstructured lattice reduction? This question was highlighted as 'Q8' in the Kyber NIST standardization submission (Avanzi et al., 2021), as potentially affecting the concrete security of Kyber and other module-lattice-based schemes. Foundational works on module-lattice reduction (Lee, Pellet-Mary, Stehlé, and Wallet, ASIACRYPT 2019; Mukherjee and Stephens-Davidowitz, CRYPTO 2020) confirmed the existence of such module variants of LLL and block-reduction algorithms, but focus only on provable worst-case asymptotic behavior. In this work, we present a concrete average-case analysis of module-lattice reduction. Specifically, we address the question of the expected slope after running module-BKZ, and pinpoint the discriminant $\Delta_K$ of the number field at hand as the main quantity driving this slope. We convert this back into a gain or loss on the blocksize $\beta$: module-BKZ in a number field $K$ of degree $d$ requires an SVP oracle of dimension $\beta + \log(|\Delta_K| / d^d)\beta /(d\log \beta) + o(\beta / \log \beta)$ to reach the same slope as unstructured BKZ with blocksize $\beta$. This asymptotic summary hides further terms that we predict concretely using experimentally verified heuristics. Incidentally, we provide the first open-source implementation of module-BKZ for some cyclotomic fields. For power-of-two cyclotomic fields, we have $|\Delta_K| = d^d$, and conclude that module-BKZ requires a blocksize larger than its unstructured counterpart by $d-1+o(1)$. On the contrary, for all other cyclotomic fields we have $|\Delta_K| < d^d$, so module-BKZ provides a sublinear $\Theta(\beta/\log \beta)$ gain on the required blocksize, yielding a subexponential speedup of $\exp(\Theta(\beta/\log \beta))$.</li>
</ul>

<h3>Title: Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?</h3>
<ul>
<li><strong>Authors: </strong>Zihan Chen, Yiming Zhang, Hengguang Zhou, Zenghui Ding, Yining Sun, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10541">https://arxiv.org/abs/2510.10541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10541">https://arxiv.org/pdf/2510.10541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10541]] Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?(https://arxiv.org/abs/2510.10541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Current benchmarks are inadequate for evaluating progress in reinforcement learning (RL) for large language models (LLMs).Despite recent benchmark gains reported for RL, we find that training on these benchmarks' training sets achieves nearly the same performance as training directly on the test sets, suggesting that the benchmarks cannot reliably separate further this http URL study this phenomenon, we introduce a diagnostic suite and the Oracle Performance Gap (OPG) metric that quantifies the performance difference between training on the train split versus the test split of a benchmark. We further analyze this phenomenon with stress tests and find that, despite strong benchmark scores, existing RL methods struggle to generalize across distribution shifts, varying levels of difficulty, and counterfactual scenarios: shortcomings that current benchmarks fail to this http URL conclude that current benchmarks are insufficient for evaluating generalization and propose three core principles for designing more faithful benchmarks: sufficient difficulty, balanced evaluation, and distributional robustness.</li>
</ul>

<h3>Title: MRS-YOLO Railroad Transmission Line Foreign Object Detection Based on Improved YOLO11 and Channel Pruning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Liu, Junting Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10553">https://arxiv.org/abs/2510.10553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10553">https://arxiv.org/pdf/2510.10553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10553]] MRS-YOLO Railroad Transmission Line Foreign Object Detection Based on Improved YOLO11 and Channel Pruning(https://arxiv.org/abs/2510.10553)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Aiming at the problems of missed detection, false detection and low detection efficiency in transmission line foreign object detection under railway environment, we proposed an improved algorithm MRS-YOLO based on YOLO11. Firstly, a multi-scale Adaptive Kernel Depth Feature Fusion (MAKDF) module is proposed and fused with the C3k2 module to form C3k2_MAKDF, which enhances the model's feature extraction capability for foreign objects of different sizes and shapes. Secondly, a novel Re-calibration Feature Fusion Pyramid Network (RCFPN) is designed as a neck structure to enhance the model's ability to integrate and utilize multi-level features effectively. Then, Spatial and Channel Reconstruction Detect Head (SC_Detect) based on spatial and channel preprocessing is designed to enhance the model's overall detection performance. Finally, the channel pruning technique is used to reduce the redundancy of the improved model, drastically reduce Parameters and Giga Floating Point Operations Per Second (GFLOPs), and improve the detection efficiency. The experimental results show that the mAP50 and mAP50:95 of the MRS-YOLO algorithm proposed in this paper are improved to 94.8% and 86.4%, respectively, which are 0.7 and 2.3 percentage points higher compared to the baseline, while Parameters and GFLOPs are reduced by 44.2% and 17.5%, respectively. It is demonstrated that the improved algorithm can be better applied to the task of foreign object detection in railroad transmission lines.</li>
</ul>

<h3>Title: Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors</h3>
<ul>
<li><strong>Authors: </strong>Weiming Zhao, Xulong Wang, Jun Qi, Yun Yang, Po Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10558">https://arxiv.org/abs/2510.10558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10558">https://arxiv.org/pdf/2510.10558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10558]] Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors(https://arxiv.org/abs/2510.10558)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Severity assessment of Parkinson's disease (PD) using wearable sensors offers an effective, objective basis for clinical management. However, general-purpose time series models often lack pathological specificity in feature extraction, making it difficult to capture subtle signals highly correlated with this http URL, the temporal sparsity of PD symptoms causes key diagnostic features to be easily "diluted" by traditional aggregation methods, further complicating assessment. To address these issues, we propose the Multi-scale Frequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances feature specificity through a frequency decomposition module guided by medical prior knowledge. Furthermore, by introducing an attention-based multi-instance learning (MIL) framework, the model can adaptively focus on the most diagnostically valuable sparse this http URL comprehensively validated MFAM on both the public PADS dataset for PD versus differential diagnosis (DD) binary classification and a private dataset for four-class severity assessment. Experimental results demonstrate that MFAM outperforms general-purpose time series models in handling complex clinical time series with specificity, providing a promising solution for automated assessment of PD severity.</li>
</ul>

<h3>Title: BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Euhid Aman, Esteban Carlin, Hsing-Kuo Pao, Giovanni Beltrame, Ghaluh Indah Permata Sari, Yie-Tarng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10560">https://arxiv.org/abs/2510.10560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10560">https://arxiv.org/pdf/2510.10560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10560]] BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices(https://arxiv.org/abs/2510.10560)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cross-attention transformers and other multimodal vision-language models excel at grounding and generation; however, their extensive, full-precision backbones make it challenging to deploy them on edge devices. Memory-augmented architectures enhance the utilization of past context; however, most works rarely pair them with aggressive edge-oriented quantization. We introduce BitMar, a quantized multimodal transformer that proposes an external human-like episodic memory for effective image-text generation on hardware with limited resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and one for vision (DiNOv2-based), to create compact embeddings that are combined and used to query a fixed-size key-value episodic memory. During vector retrieval, the BitNet decoder applies per-layer conditioning, which increases the contextual relevance of generated content. The decoder also employs attention sinks with a sliding-window mechanism to process long or streaming inputs under tight memory budgets. The combination of per-layer conditioning and sliding-window attention achieves a strong quality-speed trade-off, delivering competitive captioning and multimodal understanding at low latency with a small model footprint. These characteristics make BitMar well-suited for edge deployment.</li>
</ul>

<h3>Title: Multitask Learning with Learned Task Relationships</h3>
<ul>
<li><strong>Authors: </strong>Zirui Wan, Stefan Vlaski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10570">https://arxiv.org/abs/2510.10570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10570">https://arxiv.org/pdf/2510.10570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10570]] Multitask Learning with Learned Task Relationships(https://arxiv.org/abs/2510.10570)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.</li>
</ul>

<h3>Title: Deep semi-supervised approach based on consistency regularization and similarity learning for weeds classification</h3>
<ul>
<li><strong>Authors: </strong>Farouq Benchallal, Adel Hafiane, Nicolas Ragot, Raphael Canals</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10573">https://arxiv.org/abs/2510.10573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10573">https://arxiv.org/pdf/2510.10573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10573]] Deep semi-supervised approach based on consistency regularization and similarity learning for weeds classification(https://arxiv.org/abs/2510.10573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Weed species classification represents an important step for the development of automated targeting systems that allow the adoption of precision agriculture practices. To reduce costs and yield losses caused by their presence. The identification of weeds is a challenging problem due to their shared similarities with crop plants and the variability related to the differences in terms of their types. Along with the variations in relation to changes in field conditions. Moreover, to fully benefit from deep learning-based methods, large fully annotated datasets are needed. This requires time intensive and laborious process for data labeling, which represents a limitation in agricultural applications. Hence, for the aim of improving the utilization of the unlabeled data, regarding conditions of scarcity in terms of the labeled data available during the learning phase and provide robust and high classification performance. We propose a deep semi-supervised approach, that combines consistency regularization with similarity learning. Through our developed deep auto-encoder architecture, experiments realized on the DeepWeeds dataset and inference in noisy conditions demonstrated the effectiveness and robustness of our method in comparison to state-of-the-art fully supervised deep learning models. Furthermore, we carried out ablation studies for an extended analysis of our proposed joint learning strategy.</li>
</ul>

<h3>Title: Man-in-the-Middle Proof-of-Concept via Krontiris' Ephemeral Diffie-Hellman Over COSE (EDHOC) in C</h3>
<ul>
<li><strong>Authors: </strong>Daniel Hennig, Joaquin Garcia-Alfaro</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10574">https://arxiv.org/abs/2510.10574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10574">https://arxiv.org/pdf/2510.10574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10574]] Man-in-the-Middle Proof-of-Concept via Krontiris' Ephemeral Diffie-Hellman Over COSE (EDHOC) in C(https://arxiv.org/abs/2510.10574)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This report presents some technical details on the authentication process of a lightweight key exchange protocol, paying attention on how Man-in-the-Middle (MitM) attacks could undermine its security, e.g., under the scope of lawful interception and its risk to facilitate mass surveillance. We focus only on some technical aspects associated to the attack scenario. Perspectives for future work are also discussed. Other specific aspects of the work, mainly focusing on the security implications of malicious metasurfaces against B5G networks, are excluded from the scope of this report.</li>
</ul>

<h3>Title: Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes</h3>
<ul>
<li><strong>Authors: </strong>Haonan Wang, Hanyu Zhou, Haoyue Liu, Luxin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10577">https://arxiv.org/abs/2510.10577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10577">https://arxiv.org/pdf/2510.10577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10577]] Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes(https://arxiv.org/abs/2510.10577)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Optical flow estimation has achieved promising results in conventional scenes but faces challenges in high-speed and low-light scenes, which suffer from motion blur and insufficient illumination. These conditions lead to weakened texture and amplified noise and deteriorate the appearance saturation and boundary completeness of frame cameras, which are necessary for motion feature matching. In degraded scenes, the frame camera provides dense appearance saturation but sparse boundary completeness due to its long imaging time and low dynamic range. In contrast, the event camera offers sparse appearance saturation, while its short imaging time and high dynamic range gives rise to dense boundary completeness. Traditionally, existing methods utilize feature fusion or domain adaptation to introduce event to improve boundary completeness. However, the appearance features are still deteriorated, which severely affects the mostly adopted discriminative models that learn the mapping from visual features to motion fields and generative models that generate motion fields based on given visual features. So we introduce diffusion models that learn the mapping from noising flow to clear flow, which is not affected by the deteriorated visual features. Therefore, we propose a novel optical flow estimation framework Diff-ABFlow based on diffusion models with frame-event appearance-boundary fusion.</li>
</ul>

<h3>Title: Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents</h3>
<ul>
<li><strong>Authors: </strong>Giulio Ruffini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10586">https://arxiv.org/abs/2510.10586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10586">https://arxiv.org/pdf/2510.10586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10586]] Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents(https://arxiv.org/abs/2510.10586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the algorithmic (Kolmogorov) view, agents are programs that track and compress sensory streams using generative programs. We propose a framework where the relevant structural prior is simplicity (Solomonoff) understood as \emph{compositional symmetry}: natural streams are well described by (local) actions of finite-parameter Lie pseudogroups on geometrically and topologically complex low-dimensional configuration manifolds (latent spaces). Modeling the agent as a generic neural dynamical system coupled to such streams, we show that accurate world-tracking imposes (i) \emph{structural constraints} -- equivariance of the agent's constitutive equations and readouts -- and (ii) \emph{dynamical constraints}: under static inputs, symmetry induces conserved quantities (Noether-style labels) in the agent dynamics and confines trajectories to reduced invariant manifolds; under slow drift, these manifolds move but remain low-dimensional. This yields a hierarchy of reduced manifolds aligned with the compositional factorization of the pseudogroup, providing a geometric account of the ``blessing of compositionality'' in deep models. We connect these ideas to the Spencer formalism for Lie pseudogroups and formulate a symmetry-based, self-contained version of predictive coding in which higher layers receive only \emph{coarse-grained residual transformations} (prediction-error coordinates) along symmetry directions unresolved at lower layers.</li>
</ul>

<h3>Title: FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10604">https://arxiv.org/abs/2510.10604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10604">https://arxiv.org/pdf/2510.10604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10604]] FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation(https://arxiv.org/abs/2510.10604)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Brain-computer interfaces (BCIs) provide potential for applications ranging from medical rehabilitation to cognitive state assessment by establishing direct communication pathways between the brain and external devices via electroencephalography (EEG). However, EEG-based BCIs are severely constrained by data scarcity and significant inter-subject variability, which hinder the generalization and applicability of EEG decoding models in practical settings. To address these challenges, we propose FusionGen, a novel EEG data generation framework based on disentangled representation learning and feature fusion. By integrating features across trials through a feature matching fusion module and combining them with a lightweight feature extraction and reconstruction pipeline, FusionGen ensures both data diversity and trainability under limited data constraints. Extensive experiments on multiple publicly available EEG datasets demonstrate that FusionGen significantly outperforms existing augmentation techniques, yielding notable improvements in classification accuracy.</li>
</ul>

<h3>Title: Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Di Wu abd Shuaidong Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10613">https://arxiv.org/abs/2510.10613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10613">https://arxiv.org/pdf/2510.10613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10613]] Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models(https://arxiv.org/abs/2510.10613)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a modeling framework for dynamic topic evolution based on temporal large language models. The method first uses a large language model to obtain contextual embeddings of text and then introduces a temporal decay function and an attention mechanism. These components allow the model to adjust the importance of semantic units according to time intervals and capture topic variations across different periods. The temporal representations are then mapped into a latent topic space, where a state transition matrix is applied to describe the dynamic evolution of topics. A joint optimization objective constrains both semantic modeling and temporal consistency, ensuring diversity and smoothness in topic generation. The design emphasizes the unified modeling of semantic representation and temporal evolution, which improves topic coherence and diversity while enhancing stability and interpretability over time. Experiments on real-world corpora show that the framework effectively captures the generation, expansion, and decline of topics and outperforms existing models across multiple metrics. Overall, the proposed method provides a systematic solution for understanding dynamic semantic patterns in large-scale text, enriches the research paradigm of topic modeling, and supports complex text analysis tasks in multiple domains.</li>
</ul>

<h3>Title: Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction</h3>
<ul>
<li><strong>Authors: </strong>Bahadur Yadav, Sanjay Kumar Mohanty</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10617">https://arxiv.org/abs/2510.10617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10617">https://arxiv.org/pdf/2510.10617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10617]] Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction(https://arxiv.org/abs/2510.10617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Forecasting stock prices remains challenging due to the volatile and non-linear nature of financial markets. Despite the promise of deep learning, issues such as mode collapse, unstable training, and difficulty in capturing temporal and feature level correlations have limited the applications of GANs in this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that strikes a balance between expressive power and simplicity. The model introduces key innovations such as a temporal decoder with residual connections for precise reconstruction, conditioning on static and dynamic covariates for contextual learning, and a windowing mechanism to capture temporal dynamics. Here, the generator uses a dense encoder-decoder framework with residual GRU blocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN achieves superior forecasting accuracy and training stability, even in volatile markets. It consistently outperforms traditional GAN variants in forecasting accuracy and convergence stability under market conditions.</li>
</ul>

<h3>Title: Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization</h3>
<ul>
<li><strong>Authors: </strong>Bowei He, Lihao Yin, Huiling Zhen, Shuqi Liu, Han Wu, Xiaokun Zhang, Mingxuan Yuan, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10618">https://arxiv.org/abs/2510.10618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10618">https://arxiv.org/pdf/2510.10618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10618]] Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization(https://arxiv.org/abs/2510.10618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training compression has been a widely employed approach to scale down large language model (LLM) and facilitate efficient inference. In various proposed compression methods, including pruning and quantization, calibration data plays a vital role by informing the weight importance and activation dynamic ranges. However, how calibration data impacts the LLM capability after compression is less explored. Few of the existing works, though recognizing the significance of this study, only investigate the language modeling or commonsense reasoning performance degradation from limited angles, like the data sources or sample amounts. More systematic research is still needed to examine the impacts on different LLM capabilities in terms of compositional properties and domain correspondence of calibration data. In this work, we aim at bridging this gap and further analyze underlying influencing mechanisms from the activation pattern perspective. Especially, we explore the calibration data's impacts on high-level complex reasoning capabilities, like math problem solving and code generation. Delving into the underlying mechanism, we find that the representativeness and diversity in activation space more fundamentally determine the quality of calibration data. Finally, we propose a calibration data curation framework based on such observations and analysis, enhancing the performance of existing post-training compression methods on preserving critical LLM capabilities. Our code is provided in \href{this https URL}{Link}.</li>
</ul>

<h3>Title: ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yuval Golbari, Navve Wasserman, Gal Vardi, Michal Irani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10625">https://arxiv.org/abs/2510.10625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10625">https://arxiv.org/pdf/2510.10625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10625]] ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios(https://arxiv.org/abs/2510.10625)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Determining which data samples were used to train a model-known as Membership Inference Attack (MIA)-is a well-studied and important problem with implications for data privacy. Black-box methods presume access only to the model's outputs and often rely on training auxiliary reference models. While they have shown strong empirical performance, they rely on assumptions that rarely hold in real-world settings: (i) the attacker knows the training hyperparameters; (ii) all available non-training samples come from the same distribution as the training data; and (iii) the fraction of training data in the evaluation set is known. In this paper, we demonstrate that removing these assumptions leads to a significant drop in the performance of black-box attacks. We introduce ImpMIA, a Membership Inference Attack that exploits the Implicit Bias of neural networks, hence removes the need to rely on any reference models and their assumptions. ImpMIA is a white-box attack -- a setting which assumes access to model weights and is becoming increasingly realistic given that many models are publicly available (e.g., via Hugging Face). Building on maximum-margin implicit bias theory, ImpMIA uses the Karush-Kuhn-Tucker (KKT) optimality conditions to identify training samples. This is done by finding the samples whose gradients most strongly reconstruct the trained model's parameters. As a result, ImpMIA achieves state-of-the-art performance compared to both black and white box attacks in realistic settings where only the model weights and a superset of the training data are available.</li>
</ul>

<h3>Title: FactAppeal: Identifying Epistemic Factual Appeals in News Media</h3>
<ul>
<li><strong>Authors: </strong>Guy Mor-Lan, Tamir Sheafer, Shaul R. Shenhav</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10627">https://arxiv.org/abs/2510.10627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10627">https://arxiv.org/pdf/2510.10627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10627]] FactAppeal: Identifying Epistemic Factual Appeals in News Media(https://arxiv.org/abs/2510.10627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>How is a factual claim made credible? We propose the novel task of Epistemic Appeal Identification, which identifies whether and how factual statements have been anchored by external sources or evidence. To advance research on this task, we present FactAppeal, a manually annotated dataset of 3,226 English-language news sentences. Unlike prior resources that focus solely on claim detection and verification, FactAppeal identifies the nuanced epistemic structures and evidentiary basis underlying these claims and used to support them. FactAppeal contains span-level annotations which identify factual statements and mentions of sources on which they rely. Moreover, the annotations include fine-grained characteristics of factual appeals such as the type of source (e.g. Active Participant, Witness, Expert, Direct Evidence), whether it is mentioned by name, mentions of the source's role and epistemic credentials, attribution to the source via direct or indirect quotation, and other features. We model the task with a range of encoder models and generative decoder models in the 2B-9B parameter range. Our best performing model, based on Gemma 2 9B, achieves a macro-F1 score of 0.73.</li>
</ul>

<h3>Title: GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Hu, Kun Li, Hehe Fan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10631">https://arxiv.org/abs/2510.10631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10631">https://arxiv.org/pdf/2510.10631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10631]] GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus(https://arxiv.org/abs/2510.10631)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Linear attention mechanisms have emerged as efficient alternatives to full self-attention in Graph Transformers, offering linear time complexity. However, existing linear attention models often suffer from a significant drop in expressiveness due to low-rank projection structures and overly uniform attention distributions. We theoretically prove that these properties reduce the class separability of node representations, limiting the model's classification ability. To address this, we propose a novel hybrid framework that enhances both the rank and focus of attention. Specifically, we enhance linear attention by attaching a gated local graph network branch to the value matrix, thereby increasing the rank of the resulting attention map. Furthermore, to alleviate the excessive smoothing effect inherent in linear attention, we introduce a learnable log-power function into the attention scores to reduce entropy and sharpen focus. We theoretically show that this function decreases entropy in the attention distribution, enhancing the separability of learned embeddings. Extensive experiments on both homophilic and heterophilic graph benchmarks demonstrate that our method achieves competitive performance while preserving the scalability of linear attention.</li>
</ul>

<h3>Title: ProteinAE: Protein Diffusion Autoencoders for Structure Encoding</h3>
<ul>
<li><strong>Authors: </strong>Shaoning Li, Le Zhuo, Yusong Wang, Mingyu Li, Xinheng He, Fandi Wu, Hongsheng Li, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10634">https://arxiv.org/abs/2510.10634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10634">https://arxiv.org/pdf/2510.10634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10634]] ProteinAE: Protein Diffusion Autoencoders for Structure Encoding(https://arxiv.org/abs/2510.10634)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Developing effective representations of protein structures is essential for advancing protein science, particularly for protein generative modeling. Current approaches often grapple with the complexities of the SE(3) manifold, rely on discrete tokenization, or the need for multiple training objectives, all of which can hinder the model optimization and generalization. We introduce ProteinAE, a novel and streamlined protein diffusion autoencoder designed to overcome these challenges by directly mapping protein backbone coordinates from E(3) into a continuous, compact latent space. ProteinAE employs a non-equivariant Diffusion Transformer with a bottleneck design for efficient compression and is trained end-to-end with a single flow matching objective, substantially simplifying the optimization pipeline. We demonstrate that ProteinAE achieves state-of-the-art reconstruction quality, outperforming existing autoencoders. The resulting latent space serves as a powerful foundation for a latent diffusion model that bypasses the need for explicit equivariance. This enables efficient, high-quality structure generation that is competitive with leading structure-based approaches and significantly outperforms prior latent-based methods. Code is available at this https URL.</li>
</ul>

<h3>Title: Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</h3>
<ul>
<li><strong>Authors: </strong>Michal Sadowski, Maria Wyrzykowska, Lukasz Sztukiewicz, Tadija Radusinović, Jan Rzymkowski, Paweł Włodarczyk-Pruszyński, Mikołaj Sacha, Piotr Kozakowski, Ruard van Workum, Stanislaw Kamil Jastrzebski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10645">https://arxiv.org/abs/2510.10645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10645">https://arxiv.org/pdf/2510.10645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10645]] Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers(https://arxiv.org/abs/2510.10645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrosynthesis is one of the domains transformed by the rise of generative models, and it is one where the problem of nonsensical or erroneous outputs (hallucinations) is particularly insidious: reliable assessment of synthetic plans is time-consuming, with automatic methods lacking. In this work, we present RetroTrim, a retrosynthesis system that successfully avoids nonsensical plans on a set of challenging drug-like targets. Compared to common baselines in the field, our system is not only the sole method that succeeds in filtering out hallucinated reactions, but it also results in the highest number of high-quality paths overall. The key insight behind RetroTrim is the combination of diverse reaction scoring strategies, based on machine learning models and existing chemical databases. We show that our scoring strategies capture different classes of hallucinations by analyzing them on a dataset of labeled retrosynthetic intermediates. To measure the performance of retrosynthesis systems, we propose a novel evaluation protocol for reactions and synthetic paths based on a structured review by expert chemists. Using this protocol, we compare systems on a set of 32 novel targets, curated to reflect recent trends in drug structures. While the insights behind our methodology are broadly applicable to retrosynthesis, our focus is on targets in the drug-like domain. By releasing our benchmark targets and the details of our evaluation protocol, we hope to inspire further research into reliable retrosynthesis.</li>
</ul>

<h3>Title: DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Peiyin Chen, Zhuowei Yang, Hui Feng, Sheng Jiang, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10650">https://arxiv.org/abs/2510.10650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10650">https://arxiv.org/pdf/2510.10650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10650]] DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis(https://arxiv.org/abs/2510.10650)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Audio-driven talking-head generation has advanced rapidly with diffusion-based generative models, yet producing temporally coherent videos with fine-grained motion control remains challenging. We propose DEMO, a flow-matching generative framework for audio-driven talking-portrait video synthesis that delivers disentangled, high-fidelity control of lip motion, head pose, and eye gaze. The core contribution is a motion auto-encoder that builds a structured latent space in which motion factors are independently represented and approximately orthogonalized. On this disentangled motion space, we apply optimal-transport-based flow matching with a transformer predictor to generate temporally smooth motion trajectories conditioned on audio. Extensive experiments across multiple benchmarks show that DEMO outperforms prior methods in video realism, lip-audio synchronization, and motion fidelity. These results demonstrate that combining fine-grained motion disentanglement with flow-based generative modeling provides a powerful new paradigm for controllable talking-head video synthesis.</li>
</ul>

<h3>Title: AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation</h3>
<ul>
<li><strong>Authors: </strong>Omid Reza Heidari, Siobhan Reid, Yassine Yaakoubi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10661">https://arxiv.org/abs/2510.10661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10661">https://arxiv.org/pdf/2510.10661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10661]] AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation(https://arxiv.org/abs/2510.10661)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>LLMs have advanced text-to-SQL generation, yet monolithic architectures struggle with complex reasoning and schema diversity. We propose AGENTIQL, an agent-inspired multi-expert framework that combines a reasoning agent for question decomposition, a coding agent for sub-query generation, and a refinement step for column selection. An adaptive router further balances efficiency and accuracy by selecting between our modular pipeline and a baseline parser. Several steps in the pipeline can be executed in parallel, making the framework scalable to larger workloads. Evaluated on the Spider benchmark, AGENTIQL improves execution accuracy and interpretability and achieves up to 86.07\% EX with 14B models using the Planner&Executor merging strategy. The attained performance is contingent upon the efficacy of the routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX) while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances transparency by exposing intermediate reasoning steps, offering a robust, scalable, and interpretable approach to semantic parsing.</li>
</ul>

<h3>Title: Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection</h3>
<ul>
<li><strong>Authors: </strong>Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10663">https://arxiv.org/abs/2510.10663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10663">https://arxiv.org/pdf/2510.10663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10663]] Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection(https://arxiv.org/abs/2510.10663)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>With abundant, unlabeled real faces, how can we learn robust and transferable facial representations to boost generalization across various face security tasks? We make the first attempt and propose FS-VFM, a scalable self-supervised pre-training framework, to learn fundamental representations of real face images. We introduce three learning objectives, namely 3C, that synergize masked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM to encode both local patterns and global semantics of real faces. Specifically, we formulate various facial masking strategies for MIM and devise a simple yet effective CRFR-P masking, which explicitly prompts the model to pursue meaningful intra-region Consistency and challenging inter-region Coherency. We present a reliable self-distillation mechanism that seamlessly couples MIM with ID to establish underlying local-to-global Correspondence. After pre-training, vanilla vision transformers (ViTs) serve as universal Vision Foundation Models for downstream Face Security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forensics. To efficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a lightweight plug-and-play bottleneck atop the frozen backbone with a novel real-anchor contrastive objective. Extensive experiments on 11 public benchmarks demonstrate that our FS-VFM consistently generalizes better than diverse VFMs, spanning natural and facial domains, fully, weakly, and self-supervised paradigms, small, base, and large ViT scales, and even outperforms SOTA task-specific methods, while FS-Adapter offers an excellent efficiency-performance trade-off. The code and models are available on this https URL.</li>
</ul>

<h3>Title: AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Menghan Xia, Gongye Liu, Jianhong Bai, Xintao Wang, Conglang Zhang, Yuxuan Lin, Ruihang Chu, Pengfei Wan, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10670">https://arxiv.org/abs/2510.10670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10670">https://arxiv.org/pdf/2510.10670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10670]] AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes(https://arxiv.org/abs/2510.10670)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.</li>
</ul>

<h3>Title: Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data</h3>
<ul>
<li><strong>Authors: </strong>Zhuowei Chen, Bowei Zhang, Nankai Lin, Tian Hou, Lianxi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10677">https://arxiv.org/abs/2510.10677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10677">https://arxiv.org/pdf/2510.10677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10677]] Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data(https://arxiv.org/abs/2510.10677)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Recent advances in LLMs have enhanced AI capabilities, but also increased the risk posed by malicious requests, highlighting the need for effective LLM safeguards to detect such queries. Existing approaches largely rely on classifier-based methods that lack interpretability and perform poorly on low-resource languages. To address these limitations, we propose ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances explainability via reasoning and boosts knowledge transfer between languages through alignment. With only 1,000 training samples, our method demonstrates superior performance on three datasets across six languages, outperforming larger models trained with significantly more data, and exhibits strong interpretability and generalization ability. We also contribute a multilingual benchmark extension and release our codes to support future research.</li>
</ul>

<h3>Title: MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic Prompting for Multi-Modal Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Luo, Qing Xu, Hai Huang, Yuqi Ouyang, Zhen Chen, Wenting Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10679">https://arxiv.org/abs/2510.10679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10679">https://arxiv.org/pdf/2510.10679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10679]] MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic Prompting for Multi-Modal Brain Tumor Segmentation(https://arxiv.org/abs/2510.10679)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-modal brain tumor segmentation is critical for clinical diagnosis, and it requires accurate identification of distinct internal anatomical subregions. While the recent prompt-based segmentation paradigms enable interactive experiences for clinicians, existing methods ignore cross-modal correlations and rely on labor-intensive category-specific prompts, limiting their applicability in real-world scenarios. To address these issues, we propose a MSM-Seg framework for multi-modal brain tumor segmentation. The MSM-Seg introduces a novel dual-memory segmentation paradigm that synergistically integrates multi-modal and inter-slice information with the efficient category-agnostic prompt for brain tumor understanding. To this end, we first devise a modality-and-slice memory attention (MSMA) to exploit the cross-modal and inter-slice relationships among the input scans. Then, we propose a multi-scale category-agnostic prompt encoder (MCP-Encoder) to provide tumor region guidance for decoding. Moreover, we devise a modality-adaptive fusion decoder (MF-Decoder) that leverages the complementary decoding information across different modalities to improve segmentation accuracy. Extensive experiments on different MRI datasets demonstrate that our MSM-Seg framework outperforms state-of-the-art methods in multi-modal metastases and glioma tumor segmentation. The code is available at this https URL.</li>
</ul>

<h3>Title: RePro: Training Language Models to Faithfully Recycle the Web for Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Zichun Yu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10681">https://arxiv.org/abs/2510.10681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10681">https://arxiv.org/pdf/2510.10681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10681]] RePro: Training Language Models to Faithfully Recycle the Web for Pretraining(https://arxiv.org/abs/2510.10681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at this https URL.</li>
</ul>

<h3>Title: Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ying-Kuan Tsai, Vispi Karkaria, Yi-Ping Chen, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10694">https://arxiv.org/abs/2510.10694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10694">https://arxiv.org/pdf/2510.10694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10694]] Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning(https://arxiv.org/abs/2510.10694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Control Co-Design (CCD) integrates physical and control system design to improve the performance of dynamic and autonomous systems. Despite advances in uncertainty-aware CCD methods, real-world uncertainties remain highly unpredictable. Multi-generation design addresses this challenge by considering the full lifecycle of a product: data collected from each generation informs the design of subsequent generations, enabling progressive improvements in robustness and efficiency. Digital Twin (DT) technology further strengthens this paradigm by creating virtual representations that evolve over the lifecycle through real-time sensing, model updating, and adaptive re-optimization. This paper presents a DT-enabled CCD framework that integrates Deep Reinforcement Learning (DRL) to jointly optimize physical design and controller. DRL accelerates real-time decision-making by allowing controllers to continuously learn from data and adapt to uncertain environments. Extending this approach, the framework employs a multi-generation paradigm, where each cycle of deployment, operation, and redesign uses collected data to refine DT models, improve uncertainty quantification through quantile regression, and inform next-generation designs of both physical components and controllers. The framework is demonstrated on an active suspension system, where DT-enabled learning from road conditions and driving behaviors yields smoother and more stable control trajectories. Results show that the method significantly enhances dynamic performance, robustness, and efficiency. Contributions of this work include: (1) extending CCD into a lifecycle-oriented multi-generation framework, (2) leveraging DTs for continuous model updating and informed design, and (3) employing DRL to accelerate adaptive real-time decision-making.</li>
</ul>

<h3>Title: Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh</h3>
<ul>
<li><strong>Authors: </strong>Usman Gani Joy, Shahadat kabir, Tasnim Niger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10702">https://arxiv.org/abs/2510.10702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10702">https://arxiv.org/pdf/2510.10702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10702]] Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh(https://arxiv.org/abs/2510.10702)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate climate forecasting is vital for Bangladesh, a region highly susceptible to climate change impacts on temperature and rainfall. Existing models often struggle to capture long-range dependencies and complex temporal patterns in climate data. This study introduces an advanced Long Short-Term Memory (LSTM) model integrated with an attention mechanism to enhance the prediction of temperature and rainfall dynamics. Utilizing comprehensive datasets from 1901-2023, sourced from NASA's POWER Project for temperature and the Humanitarian Data Exchange for rainfall, the model effectively captures seasonal and long-term trends. It outperforms baseline models, including XGBoost, Simple LSTM, and GRU, achieving a test MSE of 0.2411 (normalized units), MAE of 0.3860 degrees C, R^2 of 0.9834, and NRMSE of 0.0370 for temperature, and MSE of 1283.67 mm^2, MAE of 22.91 mm, R^2 of 0.9639, and NRMSE of 0.0354 for rainfall on monthly forecasts. The model demonstrates improved robustness with only a 20 percent increase in MSE under simulated climate trends (compared to an approximately 2.2-fold increase in baseline models without trend features) and a 50 percent degradation under regional variations (compared to an approximately 4.8-fold increase in baseline models without enhancements). These results highlight the model's ability to improve forecasting precision and offer potential insights into the physical processes governing climate variability in Bangladesh, supporting applications in climate-sensitive sectors.</li>
</ul>

<h3>Title: Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance</h3>
<ul>
<li><strong>Authors: </strong>Mamoona Ghafoor, Tatsuya Akutsu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10706">https://arxiv.org/abs/2510.10706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10706">https://arxiv.org/pdf/2510.10706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10706]] Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance(https://arxiv.org/abs/2510.10706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of trees with a specified tree edit distance has significant applications across various fields, including computational biology, structured data analysis, and image processing. Recently, generative networks have been increasingly employed to synthesize new data that closely resembles the original datasets. However, the appropriate size and depth of generative networks required to generate data with a specified tree edit distance remain unclear. In this paper, we theoretically establish the existence and construction of generative networks capable of producing trees similar to a given tree with respect to the tree edit distance. Specifically, for a given rooted, ordered, and vertex-labeled tree T of size n + 1 with labels from an alphabet \Sigma, and a non-negative integer d, we prove that all rooted, ordered, and vertex-labeled trees over \Sigma with tree edit distance at most d from T can be generated using a ReLU-based generative network with size O(n^3 ) and constant depth. The proposed networks were implemented and evaluated for generating trees with up to 21 nodes. Due to their deterministic architecture, the networks successfully generated all valid trees within the specified tree edit distance. In contrast, state-of-the-art graph generative models GraphRNN and GraphGDP, which rely on non-deterministic mechanisms, produced significantly fewer valid trees, achieving validation rates of only up to 35% and 48%, respectively. These findings provide a theoretical foundation towards construction of compact generative models and open new directions for exact and valid tree-structured data generation. An implementation of the proposed networks is available at this https URL.</li>
</ul>

<h3>Title: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Manas Zambre, Sarika Bobade (Supervisor)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10729">https://arxiv.org/abs/2510.10729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10729">https://arxiv.org/pdf/2510.10729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10729]] Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework(https://arxiv.org/abs/2510.10729)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Sarcasm is a nuanced and often misinterpreted form of communication, especially in text, where tone and body language are absent. This paper proposes a modular deep learning framework for sarcasm detection, leveraging Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT to analyze linguistic, emotional, and contextual cues. The system integrates sentiment analysis, contextual embeddings, linguistic feature extraction, and emotion detection through a multi-layer architecture. While the model is in the conceptual stage, it demonstrates feasibility for real-world applications such as chatbots and social media analysis.</li>
</ul>

<h3>Title: A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications</h3>
<ul>
<li><strong>Authors: </strong>Shivani Shukla, Himanshu Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10739">https://arxiv.org/abs/2510.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10739">https://arxiv.org/pdf/2510.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10739]] A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications(https://arxiv.org/abs/2510.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We introduce a general stochastic differential equation framework for modelling multiobjective optimization dynamics in iterative Large Language Model (LLM) interactions. Our framework captures the inherent stochasticity of LLM responses through explicit diffusion terms and reveals systematic interference patterns between competing objectives via an interference matrix formulation. We validate our theoretical framework using iterative code generation as a proof-of-concept application, analyzing 400 sessions across security, efficiency, and functionality objectives. Our results demonstrate strategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29, and predictive accuracy achieving R2 = 0.74 for balanced approaches. This work proposes the feasibility of dynamical systems analysis for multi-objective LLM interactions, with code generation serving as an initial validation domain.</li>
</ul>

<h3>Title: Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Laura Weihl, Nejc Novak, Stefan H. Bengtson, Malte Pedersen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10750">https://arxiv.org/abs/2510.10750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10750">https://arxiv.org/pdf/2510.10750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10750]] Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection(https://arxiv.org/abs/2510.10750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Underwater video monitoring is a promising strategy for assessing marine biodiversity, but the vast volume of uneventful footage makes manual inspection highly impractical. In this work, we explore the use of visual anomaly detection (VAD) based on deep neural networks to automatically identify interesting or anomalous events. We introduce AURA, the first multi-annotator benchmark dataset for underwater VAD, and evaluate four VAD models across two marine scenes. We demonstrate the importance of robust frame selection strategies to extract meaningful video segments. Our comparison against multiple annotators reveals that VAD performance of current models varies dramatically and is highly sensitive to both the amount of training data and the variability in visual content that defines "normal" scenes. Our results highlight the value of soft and consensus labels and offer a practical approach for supporting scientific exploration and scalable biodiversity monitoring.</li>
</ul>

<h3>Title: Restricted Receptive Fields for Face Verification</h3>
<ul>
<li><strong>Authors: </strong>Kagan Ozturk, Aman Bhatta, Haiyu Wu, Patrick Flynn, Kevin W. Bowyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10753">https://arxiv.org/abs/2510.10753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10753">https://arxiv.org/pdf/2510.10753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10753]] Restricted Receptive Fields for Face Verification(https://arxiv.org/abs/2510.10753)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how deep neural networks make decisions is crucial for analyzing their behavior and diagnosing failure cases. In computer vision, a common approach to improve interpretability is to assign importance to individual pixels using post-hoc methods. Although they are widely used to explain black-box models, their fidelity to the model's actual reasoning is uncertain due to the lack of reliable evaluation metrics. This limitation motivates an alternative approach, which is to design models whose decision processes are inherently interpretable. To this end, we propose a face similarity metric that breaks down global similarity into contributions from restricted receptive fields. Our method defines the similarity between two face images as the sum of patch-level similarity scores, providing a locally additive explanation without relying on post-hoc analysis. We show that the proposed approach achieves competitive verification performance even with patches as small as 28x28 within 112x112 face images, and surpasses state-of-the-art methods when using 56x56 patches.</li>
</ul>

<h3>Title: Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis</h3>
<ul>
<li><strong>Authors: </strong>Wenqing Zhang, Trang Nguyen, Elizabeth A. Stuart, Yiqun T. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10762">https://arxiv.org/abs/2510.10762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10762">https://arxiv.org/pdf/2510.10762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10762]] Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis(https://arxiv.org/abs/2510.10762)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Systematic reviews are crucial for synthesizing scientific evidence but remain labor-intensive, especially when extracting detailed methodological information. Large language models (LLMs) offer potential for automating methodological assessments, promising to transform evidence synthesis. Here, using causal mediation analysis as a representative methodological domain, we benchmarked state-of-the-art LLMs against expert human reviewers across 180 full-text scientific articles. Model performance closely correlated with human judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving near-human accuracy on straightforward, explicitly stated methodological criteria. However, accuracy sharply declined on complex, inference-intensive assessments, lagging expert reviewers by up to 15%. Errors commonly resulted from superficial linguistic cues -- for instance, models frequently misinterpreted keywords like "longitudinal" or "sensitivity" as automatic evidence of rigorous methodological approache, leading to systematic misclassifications. Longer documents yielded lower model accuracy, whereas publication year showed no significant effect. Our findings highlight an important pattern for practitioners using LLMs for methods review and synthesis from full texts: current LLMs excel at identifying explicit methodological features but require human oversight for nuanced interpretations. Integrating automated information extraction with targeted expert review thus provides a promising approach to enhance efficiency and methodological rigor in evidence synthesis across diverse scientific fields.</li>
</ul>

<h3>Title: EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition</h3>
<ul>
<li><strong>Authors: </strong>Sudipto Sarkar, Mohammad Asif Hasan, Khondokar Ashik Shahriar, Fablia Labiba, Nahian Tasnim, Sheikh Anawarul Haq Fattah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10765">https://arxiv.org/abs/2510.10765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10765">https://arxiv.org/pdf/2510.10765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10765]] EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition(https://arxiv.org/abs/2510.10765)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Identifying drones and birds correctly is essential for keeping the skies safe and improving security systems. Using the VIP CUP 2025 dataset, which provides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a new lightweight yet powerful model for object detection. The model improves how image features are captured and understood, making detection more accurate and efficient. It uses smart design changes and attention layers to focus on important details while reducing the amount of computation needed. A special detection head helps the model adapt to objects of different shapes and sizes. We trained three versions: one using RGB images, one using IR images, and one combining both. The combined model achieved the best accuracy and reliability while running fast enough for real-time use on common GPUs.</li>
</ul>

<h3>Title: GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Mohammadi, Reza Ahmari, Vahid Hemmati, Frederick Owusu-Ambrose, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, Mehrdad Saif</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10766">https://arxiv.org/abs/2510.10766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10766">https://arxiv.org/pdf/2510.10766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10766]] GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN(https://arxiv.org/abs/2510.10766)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As autonomous vehicles become an essential component of modern transportation, they are increasingly vulnerable to threats such as GPS spoofing attacks. This study presents an adaptive detection approach utilizing a dynamically tuned Density Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, designed to adjust the detection threshold ({\epsilon}) in real-time. The threshold is updated based on the recursive mean and standard deviation of displacement errors between GPS and in-vehicle sensors data, but only at instances classified as non-anomalous. Furthermore, an initial threshold, determined from 120,000 clean data samples, ensures the capability to identify even subtle and gradual GPS spoofing attempts from the beginning. To assess the performance of the proposed method, five different subsets from the real-world Honda Research Institute Driving Dataset (HDD) are selected to simulate both large and small magnitude GPS spoofing attacks. The modified algorithm effectively identifies turn-by-turn, stop, overshoot, and multiple small biased spoofing attacks, achieving detection accuracies of 98.621%, 99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a substantial advancement in enhancing the security and safety of AVs against GPS spoofing threats.</li>
</ul>

<h3>Title: Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Sheng, Hanyang Zhao, Haoxian Chen, David D. Yao, Wenpin Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10767">https://arxiv.org/abs/2510.10767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10767">https://arxiv.org/pdf/2510.10767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10767]] Understanding Sampler Stochasticity in Training Diffusion Models for RLHF(https://arxiv.org/abs/2510.10767)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.</li>
</ul>

<h3>Title: Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction</h3>
<ul>
<li><strong>Authors: </strong>Amber Li, Aruzhan Abil, Juno Marques Oda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10775">https://arxiv.org/abs/2510.10775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10775">https://arxiv.org/pdf/2510.10775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10775]] Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction(https://arxiv.org/abs/2510.10775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In financial markets, Graph Neural Networks have been successfully applied to modeling relational data, effectively capturing nonlinear inter-stock dependencies. Yet, existing models often fail to efficiently propagate messages during macroeconomic shocks. In this paper, we propose OmniGNN, an attention-based multi-relational dynamic GNN that integrates macroeconomic context via heterogeneous node and edge types for robust message passing. Central to OmniGNN is a sector node acting as a global intermediary, enabling rapid shock propagation across the graph without relying on long-range multi-hop diffusion. The model leverages Graph Attention Networks (GAT) to weigh neighbor contributions and employs Transformers to capture temporal dynamics across multiplex relations. Experiments show that OmniGNN outperforms existing stock prediction models on public datasets, particularly demonstrating strong robustness during the COVID-19 period.</li>
</ul>

<h3>Title: HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon</h3>
<ul>
<li><strong>Authors: </strong>James Ald Teves, Ray Daniel Cal, Josh Magdiel Villaluz, Jean Malolos, Mico Magtira, Ramon Rodriguez, Mideth Abisado, Joseph Marvin Imperial</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10776">https://arxiv.org/abs/2510.10776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10776">https://arxiv.org/pdf/2510.10776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10776]] HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon(https://arxiv.org/abs/2510.10776)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The language of Hiligaynon, spoken predominantly by the people of Panay Island, Negros Occidental, and Soccsksargen in the Philippines, remains underrepresented in language processing research due to the absence of annotated corpora and baseline models. This study introduces HiligayNER, the first publicly available baseline model for the task of Named Entity Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains over 8,000 annotated sentences collected from publicly available news articles, social media posts, and literary texts. Two Transformer-based models, mBERT and XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of HiligayNER. Evaluation results show strong performance, with both models achieving over 80% in precision, recall, and F1-score across entity types. Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates promising transferability, suggesting the broader applicability of HiligayNER for multilingual NLP in low-resource settings. This work aims to contribute to language technology development for underrepresented Philippine languages, specifically for Hiligaynon, and support future research in regional language processing.</li>
</ul>

<h3>Title: Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans</h3>
<ul>
<li><strong>Authors: </strong>Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10779">https://arxiv.org/abs/2510.10779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10779">https://arxiv.org/pdf/2510.10779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10779]] Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans(https://arxiv.org/abs/2510.10779)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>With the growing volume of CT examinations, there is an increasing demand for automated tools such as organ segmentation, abnormality detection, and report generation to support radiologists in managing their clinical workload. Multi-label classification of 3D Chest CT scans remains a critical yet challenging problem due to the complex spatial relationships inherent in volumetric data and the wide variability of abnormalities. Existing methods based on 3D convolutional neural networks struggle to capture long-range dependencies, while Vision Transformers often require extensive pre-training on large-scale, domain-specific datasets to perform competitively. In this work, we propose a 2.5D alternative by introducing a new graph-based framework that represents 3D CT volumes as structured graphs, where axial slice triplets serve as nodes processed through spectral graph convolution, enabling the model to reason over inter-slice dependencies while maintaining complexity compatible with clinical deployment. Our method, trained and evaluated on 3 datasets from independent institutions, achieves strong cross-dataset generalization, and shows competitive performance compared to state-of-the-art visual encoders. We further conduct comprehensive ablation studies to evaluate the impact of various aggregation strategies, edge-weighting schemes, and graph connectivity patterns. Additionally, we demonstrate the broader applicability of our approach through transfer experiments on automated radiology report generation and abdominal CT data.\\ This work extends our previous contribution presented at the MICCAI 2025 EMERGE Workshop.</li>
</ul>

<h3>Title: DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sneha Varur, Anirudh R Hanchinamani, Tarun S Bagewadi, Uma Mudenagudi, Chaitra D Desai, Sujata C, Padmashree Desai, Sumit Meharwade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10782">https://arxiv.org/abs/2510.10782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10782">https://arxiv.org/pdf/2510.10782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10782]] DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation(https://arxiv.org/abs/2510.10782)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel framework, Disentangled Style-Content GAN (DISC-GAN), which integrates style-content disentanglement with a cluster-specific training strategy towards photorealistic underwater image synthesis. The quality of synthetic underwater images is challenged by optical due to phenomena such as color attenuation and turbidity. These phenomena are represented by distinct stylistic variations across different waterbodies, such as changes in tint and haze. While generative models are well-suited to capture complex patterns, they often lack the ability to model the non-uniform conditions of diverse underwater environments. To address these challenges, we employ K-means clustering to partition a dataset into style-specific domains. We use separate encoders to get latent spaces for style and content; we further integrate these latent representations via Adaptive Instance Normalization (AdaIN) and decode the result to produce the final synthetic image. The model is trained independently on each style cluster to preserve domain-specific characteristics. Our framework demonstrates state-of-the-art performance, obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance (FID) of 13.3728.</li>
</ul>

<h3>Title: BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Zhongju Yuan, Geraint Wiggins, Dick Botteldooren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10790">https://arxiv.org/abs/2510.10790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10790">https://arxiv.org/pdf/2510.10790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10790]] BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics(https://arxiv.org/abs/2510.10790)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Today's deep learning architectures are primarily based on perceptron models, which do not capture the oscillatory dynamics characteristic of biological neurons. Although oscillatory systems have recently gained attention for their closer resemblance to neural behavior, they still fall short of modeling the intricate spatio-temporal interactions observed in natural neural circuits. In this paper, we propose a bio-inspired oscillatory state system (BioOSS) designed to emulate the wave-like propagation dynamics critical to neural processing, particularly in the prefrontal cortex (PFC), where complex activity patterns emerge. BioOSS comprises two interacting populations of neurons: p neurons, which represent simplified membrane-potential-like units inspired by pyramidal cells in cortical columns, and o neurons, which govern propagation velocities and modulate the lateral spread of activity. Through local interactions, these neurons produce wave-like propagation patterns. The model incorporates trainable parameters for damping and propagation speed, enabling flexible adaptation to task-specific spatio-temporal structures. We evaluate BioOSS on both synthetic and real-world tasks, demonstrating superior performance and enhanced interpretability compared to alternative architectures.</li>
</ul>

<h3>Title: Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells</h3>
<ul>
<li><strong>Authors: </strong>Aleksandra Melnikova, Petr Matula</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10797">https://arxiv.org/abs/2510.10797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10797">https://arxiv.org/pdf/2510.10797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10797]] Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells(https://arxiv.org/abs/2510.10797)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>High-quality, publicly available segmentation annotations of image and video datasets are critical for advancing the field of image processing. In particular, annotations of volumetric images of a large number of targets are time-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we presented the first publicly available full 3D time-lapse segmentation annotations of migrating cells with complex dynamic shapes. Concretely, three distinct humans annotated two sequences of MDA231 human breast carcinoma cells (Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC). This paper aims to provide a comprehensive description of the dataset and accompanying experiments that were not included in (Melnikova, A., & Matula, P., 2025) due to limitations in publication space. Namely, we show that the created annotations are consistent with the previously published tracking markers provided by the CTC organizers and the segmentation accuracy measured based on the 2D gold truth of CTC is within the inter-annotator variability margins. We compared the created 3D annotations with automatically created silver truth provided by CTC. We have found the proposed annotations better represent the complexity of the input images. The presented annotations can be used for testing and training cell segmentation, or analyzing 3D shapes of highly dynamic objects.</li>
</ul>

<h3>Title: Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage</h3>
<ul>
<li><strong>Authors: </strong>Wanshu Nie, Sujay V. Kumar, Junyu Chen, Long Zhao, Olya Skulovich, Jinwoong Yoo, Justin Pflug, Shahryar Khalique Ahmad, Goutam Konapala</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10799">https://arxiv.org/abs/2510.10799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10799">https://arxiv.org/pdf/2510.10799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10799]] Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage(https://arxiv.org/abs/2510.10799)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in machine learning such as Long Short-Term Memory (LSTM) models and Transformers have been widely adopted in hydrological applications, demonstrating impressive performance amongst deep learning models and outperforming physical models in various tasks. However, their superiority in predicting land surface states such as terrestrial water storage (TWS) that are dominated by many factors such as natural variability and human driven modifications remains unclear. Here, using the open-access, globally representative HydroGlobe dataset - comprising a baseline version derived solely from a land surface model simulation and an advanced version incorporating multi-source remote sensing data assimilation - we show that linear regression is a robust benchmark, outperforming the more complex LSTM and Temporal Fusion Transformer for TWS prediction. Our findings highlight the importance of including traditional statistical models as benchmarks when developing and evaluating deep learning models. Additionally, we emphasize the critical need to establish globally representative benchmark datasets that capture the combined impact of natural variability and human interventions.</li>
</ul>

<h3>Title: MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Md Abdullah Al Mazid, Liangdong Deng, Naphtali Rishe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10802">https://arxiv.org/abs/2510.10802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10802">https://arxiv.org/pdf/2510.10802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10802]] MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation(https://arxiv.org/abs/2510.10802)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Clouds remain a critical challenge in optical satellite imagery, hindering reliable analysis for environmental monitoring, land cover mapping, and climate research. To overcome this, we propose MSCloudCAM, a Cross-Attention with Multi-Scale Context Network tailored for multispectral and multi-sensor cloud segmentation. Our framework exploits the spectral richness of Sentinel-2 (CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories: clear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a Swin Transformer backbone for hierarchical feature extraction with multi-scale context modules ASPP and PSP for enhanced scale-aware learning. A Cross-Attention block enables effective multisensor and multispectral feature fusion, while the integration of an Efficient Channel Attention Block (ECAB) and a Spatial Attention Module adaptively refine feature representations. Comprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM delivers state-of-the-art segmentation accuracy, surpassing leading baseline architectures while maintaining competitive parameter efficiency and FLOPs. These results underscore the model's effectiveness and practicality, making it well-suited for large-scale Earth observation tasks and real-world applications.</li>
</ul>

<h3>Title: PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning</h3>
<ul>
<li><strong>Authors: </strong>Javier García-Sigüenza, Mirco Nanni, Faraón Llorens-Largo, José F. Vicent</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10803">https://arxiv.org/abs/2510.10803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10803">https://arxiv.org/pdf/2510.10803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10803]] PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning(https://arxiv.org/abs/2510.10803)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This work addresses the challenge of using a deep learning model to prune graphs and the ability of this method to integrate explainability into spatio-temporal problems through a new approach. Instead of applying explainability to the model's behavior, we seek to gain a better understanding of the problem itself. To this end, we propose a novel model that integrates an optimized pruning mechanism capable of removing nodes from the graph during the training process, rather than doing so as a separate procedure. This integration allows the architecture to learn how to minimize prediction error while selecting the most relevant nodes. Thus, during training, the model searches for the most relevant subset of nodes, obtaining the most important elements of the problem, facilitating its analysis. To evaluate the proposed approach, we used several widely used traffic datasets, comparing the accuracy obtained by pruning with the model and with other methods. The experiments demonstrate that our method is capable of retaining a greater amount of information as the graph reduces in size compared to the other methods used. These results highlight the potential of pruning as a tool for developing models capable of simplifying spatio-temporal problems, thereby obtaining their most important elements.</li>
</ul>

<h3>Title: Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures</h3>
<ul>
<li><strong>Authors: </strong>Mihir Gupte, Paolo Giusto, Ramesh S</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10806">https://arxiv.org/abs/2510.10806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10806">https://arxiv.org/pdf/2510.10806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10806]] Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures(https://arxiv.org/abs/2510.10806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are adept at generating responses based on information within their context. While this ability is useful for interacting with structured data like code files, another popular method, Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment the model's in-context learning. However, it is not well-explored how to best represent this retrieved knowledge for generating responses on structured data, particularly hierarchical structures like trees. In this work, we propose a novel bottom-up method to linearize knowledge from tree-like structures (like a GitHub repository) by generating implicit, aggregated summaries at each hierarchical level. This approach enables the knowledge to be stored in a knowledge base and used directly with RAG. We then compare our method to using RAG on raw, unstructured code, evaluating the accuracy and quality of the generated responses. Our results show that while response quality is comparable across both methods, our approach generates over 68% fewer documents in the retriever, a significant gain in efficiency. This finding suggests that leveraging implicit, linearized knowledge may be a highly effective and scalable strategy for handling complex, hierarchical data structures.</li>
</ul>

<h3>Title: Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation</h3>
<ul>
<li><strong>Authors: </strong>Ali Atiah Alzahrani</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10807">https://arxiv.org/abs/2510.10807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10807">https://arxiv.org/pdf/2510.10807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10807]] Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation(https://arxiv.org/abs/2510.10807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study whether regime-conditioned generative scenarios, coupled with a convex CVaR allocator, improve portfolio decisions under regime shifts. We introduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers latent regimes via a Gaussian HMM, (ii) trains a diffusion model with a tail-weighted objective and a regime-specialized mixture-of-experts (MoE) denoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios into a turnover-aware CVaR epigraph quadratic program with explicit governance. In strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD outperforms standard allocators and improves calibration relative to popular generators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains Sharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent reduction, at comparable turnover; stationary block-bootstrap intervals indicate the Sharpe uplift is significant at 5 percent. We provide theory linking tail-weighted diffusion to spectral-risk control of the decision-relevant CVaR gap, oracle/consistency results for the regime-MoE denoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD offers a reproducible bridge from tail-faithful scenario modeling to governed portfolio decisions with materially improved drawdown control.</li>
</ul>

<h3>Title: Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Omar Islam Laskar, Fatemeh Ramezani Khozestani, Ishika Nankani, Sohrab Namazi Nia, Senjuti Basu Roy, Kaustubh Beedkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10810">https://arxiv.org/abs/2510.10810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10810">https://arxiv.org/pdf/2510.10810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10810]] Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems(https://arxiv.org/abs/2510.10810)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Data-sharing ecosystems enable entities -- such as providers, consumers, and intermediaries -- to access, exchange, and utilize data for various downstream tasks and applications. Due to privacy concerns, data providers typically anonymize datasets before sharing them; however, the existence of multiple masking configurations results in masked datasets with varying utility. Consequently, a key challenge lies in efficiently determining the optimal masking configuration that maximizes a dataset's utility. This paper presents AEGIS, a middleware framework for identifying the optimal masking configuration for machine learning datasets that consist of features and a class label. We introduce a utility optimizer that minimizes predictive utility deviation -- a metric based on the changes in feature-label correlations before and after masking. Our framework leverages limited data summaries (such as 1D histograms) or none to estimate the feature-label joint distribution, making it suitable for scenarios where raw data is inaccessible due to privacy restrictions. To achieve this, we propose a joint distribution estimator based on iterative proportional fitting, which allows supporting various feature-label correlation quantification methods such as g3, mutual information, or chi-square. Our experimental evaluation on real-world datasets shows that AEGIS identifies optimal masking configurations over an order of magnitude faster, while the resulting masked datasets achieve predictive performance on downstream ML tasks that is on par with baseline approaches.</li>
</ul>

<h3>Title: From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Clemence Mottez, Louisa Fay, Maya Varma, Sophie Ostmeier, Curtis Langlotz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10822">https://arxiv.org/abs/2510.10822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10822">https://arxiv.org/pdf/2510.10822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10822]] From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis(https://arxiv.org/abs/2510.10822)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Deep learning models have shown promise in improving diagnostic accuracy from chest X-rays, but they also risk perpetuating healthcare disparities when performance varies across demographic groups. In this work, we present a comprehensive bias detection and mitigation framework targeting sex, age, and race-based disparities when performing diagnostic tasks with chest X-rays. We extend a recent CNN-XGBoost pipeline to support multi-label classification and evaluate its performance across four medical conditions. We show that replacing the final layer of CNN with an eXtreme Gradient Boosting classifier improves the fairness of the subgroup while maintaining or improving the overall predictive performance. To validate its generalizability, we apply the method to different backbones, namely DenseNet-121 and ResNet-50, and achieve similarly strong performance and fairness outcomes, confirming its model-agnostic design. We further compare this lightweight adapter training method with traditional full-model training bias mitigation techniques, including adversarial training, reweighting, data augmentation, and active learning, and find that our approach offers competitive or superior bias reduction at a fraction of the computational cost. Finally, we show that combining eXtreme Gradient Boosting retraining with active learning yields the largest reduction in bias across all demographic subgroups, both in and out of distribution on the CheXpert and MIMIC datasets, establishing a practical and effective path toward equitable deep learning deployment in clinical radiology.</li>
</ul>

<h3>Title: DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaixuan Ren, Preslav Nakov, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10846">https://arxiv.org/abs/2510.10846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10846">https://arxiv.org/pdf/2510.10846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10846]] DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models(https://arxiv.org/abs/2510.10846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As vision-language models become increasingly capable, maintaining a balance between safety and usefulness remains a central challenge. Safety mechanisms, while essential, can backfire, causing over-refusal, where models decline benign requests out of excessive caution. Yet, no existing benchmark has systematically addressed over-refusal in the visual modality. This setting introduces unique challenges, such as dual-use cases where an instruction is harmless, but the accompanying image contains harmful content. Models frequently fail in such scenarios, either refusing too conservatively or completing tasks unsafely, which highlights the need for more fine-grained alignment. The ideal behavior is safe completion, i.e., fulfilling the benign parts of a request while explicitly warning about any potentially harmful elements. To address this, we present DUAL-Bench, the first multimodal benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18 VLMs across 12 hazard categories, with focus on their robustness under semantics-preserving visual perturbations. The results reveal substantial room for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster the development of more nuanced alignment strategies that ensure models remain both safe and useful in complex multimodal settings.</li>
</ul>

<h3>Title: Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion</h3>
<ul>
<li><strong>Authors: </strong>Donald Loveland, Yao-An Yang, Danai Koutra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10849">https://arxiv.org/abs/2510.10849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10849">https://arxiv.org/pdf/2510.10849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10849]] Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion(https://arxiv.org/abs/2510.10849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning on text-attributed graphs has motivated the use of Large Language Models (LLMs) for graph learning. However, most fusion strategies are applied uniformly across all nodes and attain only small overall performance gains. We argue this result stems from aggregate metrics that obscure when LLMs provide benefit, inhibiting actionable signals for new strategies. In this work, we reframe LLM-GNN fusion around nodes where GNNs typically falter. We first show that performance can significantly differ between GNNs and LLMs, with each excelling on distinct structural patterns, such as local homophily. To leverage this finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- and Context-aware Embeddings), a framework that invokes an LLM to refine a GNN's prediction. GLANCE employs a lightweight router that, given inexpensive per-node signals, decides whether to query the LLM. Since the LLM calls are non-differentiable, the router is trained with an advantage-based objective that compares the utility of querying the LLM against relying solely on the GNN. Across multiple benchmarks, GLANCE achieves the best performance balance across node subgroups, achieving significant gains on heterophilous nodes (up to $+13\%$) while simultaneously achieving top overall performance. Our findings highlight the value of adaptive, node-aware GNN-LLM architectures, where selectively invoking the LLM enables scalable deployment on large graphs without incurring high computational costs.</li>
</ul>

<h3>Title: Discrete State Diffusion Models: A Sample Complexity Perspective</h3>
<ul>
<li><strong>Authors: </strong>Aadithya Srikanth, Mudit Gaur, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10854">https://arxiv.org/abs/2510.10854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10854">https://arxiv.org/pdf/2510.10854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10854]] Discrete State Diffusion Models: A Sample Complexity Perspective(https://arxiv.org/abs/2510.10854)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable performance in generating high-dimensional samples across domains such as vision, language, and the sciences. Although continuous-state diffusion models have been extensively studied both empirically and theoretically, discrete-state diffusion models, essential for applications involving text, sequences, and combinatorial structures, remain significantly less understood from a theoretical standpoint. In particular, all existing analyses of discrete-state models assume score estimation error bounds without studying sample complexity results. In this work, we present a principled theoretical framework for discrete-state diffusion, providing the first sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-2})$. Our structured decomposition of the score estimation error into statistical, approximation, optimization, and clipping components offers critical insights into how discrete-state models can be trained efficiently. This analysis addresses a fundamental gap in the literature and establishes the theoretical tractability and practical relevance of discrete-state diffusion models.</li>
</ul>

<h3>Title: FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding</h3>
<ul>
<li><strong>Authors: </strong>Soroush Mehraban, Andrea Iaboni, Babak Taati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10868">https://arxiv.org/abs/2510.10868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10868">https://arxiv.org/pdf/2510.10868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10868]] FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding(https://arxiv.org/abs/2510.10868)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline.</li>
</ul>

<h3>Title: rareboost3d: a synthetic lidar dataset with enhanced rare classes</h3>
<ul>
<li><strong>Authors: </strong>Shutong Lin, Zhengkang Xiang, Jianzhong Qi, Kourosh Khoshelham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10876">https://arxiv.org/abs/2510.10876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10876">https://arxiv.org/pdf/2510.10876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10876]] rareboost3d: a synthetic lidar dataset with enhanced rare classes(https://arxiv.org/abs/2510.10876)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Real-world point cloud datasets have made significant contributions to the development of LiDAR-based perception technologies, such as object segmentation for autonomous driving. However, due to the limited number of instances in some rare classes, the long-tail problem remains a major challenge in existing datasets. To address this issue, we introduce a novel, synthetic point cloud dataset named RareBoost3D, which complements existing real-world datasets by providing significantly more instances for object classes that are rare in real-world datasets. To effectively leverage both synthetic and real-world data, we further propose a cross-domain semantic alignment method named CSC loss that aligns feature representations of the same class across different domains. Experimental results demonstrate that this alignment significantly enhances the performance of LiDAR point cloud segmentation models over real-world data.</li>
</ul>

<h3>Title: Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales</h3>
<ul>
<li><strong>Authors: </strong>Zhaofang Qian, Hardy Chen, Zeyu Wang, Li Zhang, Zijun Wang, Xiaoke Huang, Hui Liu, Xianfeng Tang, Zeyu Zheng, Haoqin Tu, Cihang Xie, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10880">https://arxiv.org/abs/2510.10880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10880">https://arxiv.org/pdf/2510.10880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10880]] Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales(https://arxiv.org/abs/2510.10880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have advanced rapidly, yet their capacity for image-grounded geolocation in open-world conditions, a task that is challenging and of demand in real life, has not been comprehensively evaluated. We present EarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates visual recognition, step-by-step reasoning, and evidence use. EarthWhere comprises 810 globally distributed images across two complementary geolocation scales: WhereCountry (i.e., 500 multiple-choice question-answering, with country-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained street-level identification tasks requiring multi-step reasoning with optional web search). For evaluation, we adopt the final-prediction metrics: location accuracies within k km (Acc@k) for coordinates and hierarchical path scores for textual localization. Beyond this, we propose to explicitly score intermediate reasoning chains using human-verified key visual clues and a Shapley-reweighted thinking score that attributes credit to each clue's marginal contribution. We benchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere and report different types of final answer accuracies as well as the calibrated model thinking scores. Overall, Gemini-2.5-Pro achieves the best average accuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches 34.71%. We reveal that web search and reasoning do not guarantee improved performance when visual clues are limited, and models exhibit regional biases, achieving up to 42.7% higher scores in certain areas than others. These findings highlight not only the promise but also the persistent challenges of models to mitigate bias and achieve robust, fine-grained localization. We open-source our benchmark at this https URL.</li>
</ul>

<h3>Title: Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jiajing Guo, Kenil Patel, Jorge Piazentin Ono, Wenbin He, Liu Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10885">https://arxiv.org/abs/2510.10885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10885">https://arxiv.org/pdf/2510.10885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10885]] Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks(https://arxiv.org/abs/2510.10885)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL) systems, enabling non-expert users to query industrial databases using natural language. While test-time scaling strategies have shown promise in LLM-based solutions, their effectiveness in real-world applications, especially with the latest reasoning models, remains uncertain. In this work, we benchmark six lightweight, industry-oriented test-time scaling strategies and four LLMs, including two reasoning models, evaluating their performance on the BIRD Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference latency and token consumption, providing insights relevant for practical system deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot demonstrations consistently enhance performance for both general-purpose and reasoning-focused LLMs. However, introducing additional workflow steps yields mixed results, and base model selection plays a critical role. This work sheds light on the practical trade-offs between accuracy, efficiency, and complexity when deploying Text2SQL systems.</li>
</ul>

<h3>Title: A Symmetric-Key Cryptosystem Based on the Burnside Ring of a Compact Lie Group</h3>
<ul>
<li><strong>Authors: </strong>Ziad Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CR, math.RA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10901">https://arxiv.org/abs/2510.10901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10901">https://arxiv.org/pdf/2510.10901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10901]] A Symmetric-Key Cryptosystem Based on the Burnside Ring of a Compact Lie Group(https://arxiv.org/abs/2510.10901)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Classical linear ciphers, such as the Hill cipher, operate on fixed, finite-dimensional modules and are therefore vulnerable to straightforward known-plaintext attacks that recover the key as a fully determined linear operator. We propose a symmetric-key cryptosystem whose linear action takes place instead in the Burnside ring $A(G)$ of a compact Lie group $G$, with emphasis on the case $G=O(2)$. The secret key consists of (i) a compact Lie group $G$; (ii) a secret total ordering of the subgroup orbit-basis of $A(G)$; and (iii) a finite set $S$ of indices of irreducible $G$-representations, whose associated basic degrees define an involutory multiplier $k\in A(G)$. Messages of arbitrary finite length are encoded as finitely supported elements of $A(G)$ and encrypted via the Burnside product with $k$. For $G=O(2)$ we prove that encryption preserves plaintext support among the generators $\{(D_1),\dots,(D_L),(SO(2)),(O(2))\}$, avoiding ciphertext expansion and security leakage. We then analyze security in passive models, showing that any finite set of observations constrains the action only on a finite-rank submodule $W_L\subset A(O(2))$, and we show information-theoretic non-identifiability of the key from such data. Finally, we prove the scheme is \emph{not} IND-CPA secure, by presenting a one-query chosen-plaintext distinguisher based on dihedral probes.</li>
</ul>

<h3>Title: Quantifying Information Disclosure During Gradient Descent Using Gradient Uniqueness</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Abdelghafar, Maryam Aliakbarpour, Chris Jermaine</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10902">https://arxiv.org/abs/2510.10902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10902">https://arxiv.org/pdf/2510.10902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10902]] Quantifying Information Disclosure During Gradient Descent Using Gradient Uniqueness(https://arxiv.org/abs/2510.10902)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Disclosing private information via publication of a machine learning model is often a concern. Intuitively, publishing a learned model should be less risky than publishing a dataset. But how much risk is there? In this paper, we present a principled disclosure metric called \emph{gradient uniqueness} that is derived from an upper bound on the amount of information disclosure from publishing a learned model. Gradient uniqueness provides an intuitive way to perform privacy auditing. The mathematical derivation of gradient uniqueness is general, and does not make any assumption on the model architecture, dataset type, or the strategy of an attacker. We examine a simple defense based on monitoring gradient uniqueness, and find that it achieves privacy comparable to classical methods such as DP-SGD, while being substantially better in terms of (utility) testing accuracy.</li>
</ul>

<h3>Title: SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Honghui Yuan, Keiji Yanai</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10910">https://arxiv.org/abs/2510.10910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10910">https://arxiv.org/pdf/2510.10910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10910]] SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model(https://arxiv.org/abs/2510.10910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid development of diffusion models, style transfer has made remarkable progress. However, flexible and localized style editing for scene text remains an unsolved challenge. Although existing scene text editing methods have achieved text region editing, they are typically limited to content replacement and simple styles, which lack the ability of free-style transfer. In this paper, we introduce SceneTextStylizer, a novel training-free diffusion-based framework for flexible and high-fidelity style transfer of text in scene images. Unlike prior approaches that either perform global style transfer or focus solely on textual content modification, our method enables prompt-guided style transformation specifically for text regions, while preserving both text readability and stylistic consistency. To achieve this, we design a feature injection module that leverages diffusion model inversion and self-attention to transfer style features effectively. Additionally, a region control mechanism is introduced by applying a distance-based changing mask at each denoising step, enabling precise spatial control. To further enhance visual quality, we incorporate a style enhancement module based on the Fourier transform to reinforce stylistic richness. Extensive experiments demonstrate that our method achieves superior performance in scene text style transformation, outperforming existing state-of-the-art methods in both visual fidelity and text preservation.</li>
</ul>

<h3>Title: ADVICE: Answer-Dependent Verbalized Confidence Estimation</h3>
<ul>
<li><strong>Authors: </strong>Ki Jung Seo, Sehun Lim, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10913">https://arxiv.org/abs/2510.10913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10913">https://arxiv.org/pdf/2510.10913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10913]] ADVICE: Answer-Dependent Verbalized Confidence Estimation(https://arxiv.org/abs/2510.10913)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models (LLMs) has enabled them to express their confidence in natural language, enhancing transparency and reliability. However, their confidence often exhibits overconfidence, the cause of which remains poorly understood. In this work, we conduct a detailed analysis of the dynamics underlying verbalized confidence and identify answer-independence as a key factor, defined as the model's failure to condition confidence on its own answer. To address this, we propose ADVICE (Answer-Dependent Verbalized Confidence Estimation), a fine-tuning framework that facilitates answer-grounded confidence estimation. Extensive experiments show that ADVICE substantially improves confidence calibration while preserving task performance. Further analyses confirm that ADVICE strengthens answer-groundedness, leading to more balanced and well-calibrated confidence distributions. Our findings shed light on the origin of overconfidence and establish a framework for more trustworthy confidence verbalization.</li>
</ul>

<h3>Title: LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Hanchang Cheng, Weimin Mu, Fan Liu, Weilin Zhu, Can Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10915">https://arxiv.org/abs/2510.10915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10915">https://arxiv.org/pdf/2510.10915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10915]] LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection(https://arxiv.org/abs/2510.10915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection(TSAD) is a critical task in signal processing field, ensuring the reliability of complex systems. Reconstruction-based methods dominate in TSAD. Among these methods, VAE-based methods have achieved promising results. Existing VAE-based methods suffer from the limitation of single-window feature and insufficient leveraging of long-term time and frequency information. We propose a Conditional Variational AutoEncoder with Long-term dependency and Probabilistic time-frequency fusion, named LPCVAE. LPCVAE introduces LSTM to capture long-term dependencies beyond windows. It further incorporates a Product-of-Experts (PoE) mechanism for adaptive and distribution-level probabilistic fusion. This design effectively mitigates time-frequency information loss. Extensive experiments on four public datasets demonstrate it outperforms state-of-the-art methods. The results confirm that integrating long-term time and frequency representations with adaptive fusion yields a robust and efficient solution for TSAD.</li>
</ul>

<h3>Title: DreamMakeup: Face Makeup Customization using Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Geon Yeong Park, Inhwa Han, Serin Yang, Yeobin Hong, Seongmin Jeong, Heechan Jeon, Myeongjin Goh, Sung Won Yi, Jin Nam, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10918">https://arxiv.org/abs/2510.10918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10918">https://arxiv.org/pdf/2510.10918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10918]] DreamMakeup: Face Makeup Customization using Latent Diffusion Models(https://arxiv.org/abs/2510.10918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The exponential growth of the global makeup market has paralleled advancements in virtual makeup simulation technology. Despite the progress led by GANs, their application still encounters significant challenges, including training instability and limited customization capabilities. Addressing these challenges, we introduce DreamMakup - a novel training-free Diffusion model based Makeup Customization method, leveraging the inherent advantages of diffusion models for superior controllability and precise real-image editing. DreamMakeup employs early-stopped DDIM inversion to preserve the facial structure and identity while enabling extensive customization through various conditioning inputs such as reference images, specific RGB colors, and textual descriptions. Our model demonstrates notable improvements over existing GAN-based and recent diffusion-based frameworks - improved customization, color-matching capabilities, identity preservation and compatibility with textual descriptions or LLMs with affordable computational costs.</li>
</ul>

<h3>Title: GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10927">https://arxiv.org/abs/2510.10927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10927">https://arxiv.org/pdf/2510.10927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10927]] GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition(https://arxiv.org/abs/2510.10927)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In biomedical fields, one named entity may consist of a series of non-adjacent tokens and overlap with other entities. Previous methods recognize discontinuous entities by connecting entity fragments or internal tokens, which face challenges of error propagation and decoding ambiguity due to the wide variety of span or word combinations. To address these issues, we deeply explore discontinuous entity structures and propose an effective Gap-aware grid tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our GapDNER innovatively applies representation learning on the context gaps between entity fragments to resolve decoding ambiguity and enhance discontinuous NER performance. Specifically, we treat the context gap as an additional type of span and convert span classification into a token-pair grid tagging task. Subsequently, we design two interactive components to comprehensively model token-pair grid features from both intra- and inter-span perspectives. The intra-span regularity extraction module employs the biaffine mechanism along with linear attention to capture the internal regularity of each span, while the inter-span relation enhancement module utilizes criss-cross attention to obtain semantic relations among different spans. At the inference stage of entity decoding, we assign a directed edge to each entity fragment and context gap, then use the BFS algorithm to search for all valid paths from the head to tail of grids with entity tags. Experimental results on three datasets demonstrate that our GapDNER achieves new state-of-the-art performance on discontinuous NER and exhibits remarkable advantages in recognizing complex entity structures.</li>
</ul>

<h3>Title: Evaluating Language Models' Evaluations of Games</h3>
<ul>
<li><strong>Authors: </strong>Katherine M. Collins, Cedegao E. Zhang, Graham Todd, Lance Ying, Mauricio Barba da Costa, Ryan Liu, Prafull Sharma, Adrian Weller, Ionatan Kuperwajs, Lionel Wong, Joshua B. Tenenbaum, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10930">https://arxiv.org/abs/2510.10930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10930">https://arxiv.org/pdf/2510.10930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10930]] Evaluating Language Models' Evaluations of Games(https://arxiv.org/abs/2510.10930)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Reasoning is not just about solving problems -- it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over $100$ novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more "jaggedness" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.</li>
</ul>

<h3>Title: TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10932">https://arxiv.org/abs/2510.10932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10932">https://arxiv.org/pdf/2510.10932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10932]] TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models(https://arxiv.org/abs/2510.10932)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>With the growing deployment of Vision-Language-Action (VLA) models in real-world embodied AI systems, their increasing vulnerability to backdoor attacks poses a serious safety threat. A backdoored VLA agent can be covertly triggered by a pre-injected backdoor to execute adversarial actions, potentially causing system failures or even physical harm. Although backdoor attacks on VLA models have been explored, prior work has focused only on untargeted attacks, leaving the more practically threatening scenario of targeted manipulation unexamined. In this paper, we study targeted backdoor attacks on VLA models and introduce TabVLA, a novel framework that enables such attacks via black-box fine-tuning. TabVLA explores two deployment-relevant inference-time threat models: input-stream editing and in-scene triggering. It formulates poisoned data generation as an optimization problem to improve attack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal that the vision channel is the principal attack surface: targeted backdoors succeed with minimal poisoning, remain robust across variations in trigger design, and are degraded only by positional mismatches between fine-tuning and inference triggers. We also investigate a potential detection-based defense against TabVLA, which reconstructs latent visual triggers from the input stream to flag activation-conditioned backdoor samples. Our work highlights the vulnerability of VLA models to targeted backdoor manipulation and underscores the need for more advanced defenses.</li>
</ul>

<h3>Title: Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems</h3>
<ul>
<li><strong>Authors: </strong>Qizhou Peng, Yang Zheng, Yu Wen, Yanna Wu, Yingying Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10937">https://arxiv.org/abs/2510.10937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10937">https://arxiv.org/pdf/2510.10937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10937]] Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems(https://arxiv.org/abs/2510.10937)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has been an important machine learning paradigm for solving long-horizon sequential decision-making problems under uncertainty. By integrating deep neural networks (DNNs) into the RL framework, deep reinforcement learning (DRL) has emerged, which achieved significant success in various domains. However, the integration of DNNs also makes it vulnerable to adversarial attacks. Existing adversarial attack techniques mainly focus on either directly manipulating the environment with which a victim agent interacts or deploying an adversarial agent that interacts with the victim agent to induce abnormal behaviors. While these techniques achieve promising results, their adoption in multi-party open systems remains limited due to two major reasons: impractical assumption of full control over the environment and dependent on interactions with victim agents. To enable adversarial attacks in multi-party open systems, in this paper, we redesigned an adversarial policy learning approach that can mislead well-trained victim agents without requiring direct interactions with these agents or full control over their environments. Particularly, we propose a neutral agent-based approach across various task scenarios in multi-party open systems. While the neutral agents seemingly are detached from the victim agents, indirectly influence them through the shared environment. We evaluate our proposed method on the SMAC platform based on Starcraft II and the autonomous driving simulation platform Highway-env. The experimental results demonstrate that our method can launch general and effective adversarial attacks in multi-party open systems.</li>
</ul>

<h3>Title: Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Namhoon Kim, Sara Fridovich-Keil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10947">https://arxiv.org/abs/2510.10947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10947">https://arxiv.org/pdf/2510.10947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10947]] Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors(https://arxiv.org/abs/2510.10947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have shown strong potential as data-driven priors for solving inverse problems such as reconstructing medical images from undersampled measurements. While these priors improve reconstruction quality with fewer measurements, they risk hallucinating features when test images lie outside the training distribution. Existing uncertainty quantification methods in this setting (i) require an in-distribution calibration dataset, which may not be available, (ii) provide heuristic rather than statistical estimates, or (iii) quantify uncertainty from model capacity or limited measurements rather than distribution shift. We propose an instance-level, calibration-free uncertainty indicator that is sensitive to distribution shift, requires no knowledge of the training distribution, and incurs no retraining cost. Our key hypothesis is that reconstructions of in-distribution images remain stable under random measurement variations, while reconstructions of out-of-distribution (OOD) images exhibit greater instability. We use this stability as a proxy for detecting distribution shift. Our proposed OOD indicator is efficiently computable for any computational imaging inverse problem; we demonstrate it on tomographic reconstruction of MNIST digits, where a learned proximal network trained only on digit "0" is evaluated on all ten digits. Reconstructions of OOD digits show higher variability and correspondingly higher reconstruction error, validating this indicator. These results suggest a deployment strategy that pairs generative priors with lightweight guardrails, enabling aggressive measurement reduction for in-distribution cases while automatically warning when priors are applied out of distribution.</li>
</ul>

<h3>Title: Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant</h3>
<ul>
<li><strong>Authors: </strong>Xi Mao, Zhendong Wang, Jingyu Li, Lingchao Mao, Utibe Essien, Hairong Wang, Xuelei Sherry Ni</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10952">https://arxiv.org/abs/2510.10952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10952">https://arxiv.org/pdf/2510.10952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10952]] Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant(https://arxiv.org/abs/2510.10952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Early detection of Alzheimer's disease (AD) is crucial because its neurodegenerative effects are irreversible, and neuropathologic and social-behavioral risk factors accumulate years before diagnosis. Identifying higher-risk individuals earlier enables prevention, timely care, and equitable resource allocation. We predict cognitive performance from social determinants of health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset derived from the nationally representative Mex-Cog cohort of the 2003 and 2012 Mexican Health and Aging Study (MHAS). Data: The target is a validated composite cognitive score across seven domains-orientation, memory, attention, language, constructional praxis, and executive function-derived from the 2016 and 2021 MHAS waves. Predictors span demographic, socioeconomic, health, lifestyle, psychosocial, and healthcare access factors. Methodology: Missingness was addressed with a singular value decomposition (SVD)-based imputation pipeline treating continuous and categorical variables separately. This approach leverages latent feature correlations to recover missing values while balancing reliability and scalability. After evaluating multiple methods, XGBoost was chosen for its superior predictive performance. Results and Discussion: The framework outperformed existing methods and the data challenge leaderboard, demonstrating high accuracy, robustness, and interpretability. SHAP-based post hoc analysis identified top contributing SDOH factors and age-specific feature patterns. Notably, flooring material emerged as a strong predictor, reflecting socioeconomic and environmental disparities. Other influential factors, age, SES, lifestyle, social interaction, sleep, stress, and BMI, underscore the multifactorial nature of cognitive aging and the value of interpretable, data-driven SDOH modeling.</li>
</ul>

<h3>Title: Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyun Zhang, Xiaojian Yuan, Di Huang, Wang You, Chen Hu, Jingqing Ruan, Kejiang Chen, Xing Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10959">https://arxiv.org/abs/2510.10959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10959">https://arxiv.org/pdf/2510.10959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10959]] Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning(https://arxiv.org/abs/2510.10959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.</li>
</ul>

<h3>Title: KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification</h3>
<ul>
<li><strong>Authors: </strong>Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Dayoung Kim, Yeonsoo Kim, Yo-Sub Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10961">https://arxiv.org/abs/2510.10961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10961">https://arxiv.org/pdf/2510.10961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10961]] KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification(https://arxiv.org/abs/2510.10961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Toxic content has become an increasingly critical social issue with the rapid expansion of online communication. While numerous studies explored methods for detecting and detoxifying such content, most have focused primarily on English, leaving low-resource language underrepresented. Consequently, Large Language Models~(LLMs) often struggle to identify and neutralize toxic expressions in these languages. This challenge becomes even more pronounced when user employ obfuscation techniques to evade detection systems. Therefore, we propose a \textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to address this issue. We categorize various obfuscation approaches based on linguistic characteristics of Korean and define a set of transformation rules grounded in real-word examples. Using these rules, we construct three dataset versions (easy, normal, and hard) representing different levels of obfuscation difficulty. This is the first dataset that simultaneously supports deobfuscation and detoxification for the Korean language. We expect it to facilitate better understanding and mitigating of obfuscated toxic content in LLM for low-resource languages. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: MC#: Mixture Compressor for Mixture-of-Experts Large Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yue Liao, Yukang Chen, Jianhui Liu, Haoru Tan, Si Liu, Shiming Zhang, Shuicheng Yan, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10962">https://arxiv.org/abs/2510.10962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10962">https://arxiv.org/pdf/2510.10962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10962]] MC#: Mixture Compressor for Mixture-of-Experts Large Models(https://arxiv.org/abs/2510.10962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and vision-language models (VLMs) by increasing capacity through sparse activation. However, preloading all experts into memory and activating multiple experts per input introduces significant computational and memory overhead, making the expert module a major contributor to model size and inference cost. To address this, we propose MC# (Mixture-Compressor-sharp), a framework that combines static quantization and dynamic expert pruning by leveraging the significance of experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce storage and loading costs, we introduce Pre-Loading Mixed-Precision Quantization (PMQ), which optimizes bit allocation via linear programming, balancing expert importance and quantization error for a Pareto-optimal trade-off between size and performance. To reduce runtime computation, Online Top-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a subset of experts per token, enabling fine-grained control over activation. By combining PMQ's static bit-width optimization with OTP's dynamic routing, MC# achieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC# achieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7% accuracy drop across five multimodal benchmarks. Additionally, OTP reduces expert activation over 20% with less than 1% performance degradation, demonstrating strong potential for efficient MoE-based model deployment.</li>
</ul>

<h3>Title: APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Li, Yuege Feng, Dandan Guo, Jinpeng Hu, Anningzhe Gao, Xiang Wan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10963">https://arxiv.org/abs/2510.10963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10963">https://arxiv.org/pdf/2510.10963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10963]] APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport(https://arxiv.org/abs/2510.10963)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The reward model (RM) plays a crucial role in aligning Large Language Models (LLMs) with human preferences through Reinforcement Learning, where the Bradley-Terry (BT) objective has been recognized as simple yet powerful, specifically for pairwise preference learning. However, BT-based RMs often struggle to effectively distinguish between similar preference responses, leading to insufficient separation between preferred and non-preferred outputs. Consequently, they may easily overfit easy samples and cannot generalize well to Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To address these challenges, this paper introduces an effective enhancement to BT-based RMs through an adaptive margin mechanism. Specifically, we design to dynamically adjust the RM focus on more challenging samples through margins, based on both semantic similarity and model-predicted reward differences, which is approached from a distributional perspective solvable with Optimal Transport (OT). By incorporating these factors into a principled OT cost matrix design, our adaptive margin enables the RM to better capture distributional differences between chosen and rejected responses, yielding significant improvements in performance, convergence speed, and generalization capabilities. Experimental results across multiple benchmarks demonstrate that our method outperforms several existing RM techniques, showcasing enhanced performance in both In-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our practical effectiveness in better aligning LLMs with human preferences. Our code is available at this https URL</li>
</ul>

<h3>Title: Judge Before Answer: Can MLLM Discern the False Premise in Question?</h3>
<ul>
<li><strong>Authors: </strong>Jidong Li, Lingyong Fang, Haodong Zhao, Sufeng Duan, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10965">https://arxiv.org/abs/2510.10965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10965">https://arxiv.org/pdf/2510.10965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10965]] Judge Before Answer: Can MLLM Discern the False Premise in Question?(https://arxiv.org/abs/2510.10965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have witnessed astonishing advancements in recent years. Despite these successes, MLLMs remain vulnerable to flase premise problems. However, existing benchmarks targeting this issue are limited in scope: they often lack fine-grained categorization, exhibit insufficient coverage, and thus fail to provide a rigorous evaluation of the ability of models to recognize false premises. To bridge this gap, we introduce a fully automated pipeline for constructing a comprehensive benchmark of false premise questions. Our method systematically categorizes the premises into three main types and thirteen subtypes according to the abilities required to identify the premises, resulting in the JBA this http URL show current MLLMs still struggle with false premise recognition. Building upon this benchmark, we further propose a recognition enhancement framework tailored to strengthen the robustness of MLLMs to detect false premises. Extensive experiments demonstrate that models trained with our framework achieve significant improvements in false premise recognition.</li>
</ul>

<h3>Title: Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Hongkai Zheng, Austin Wang, Zihui Wu, Zhengyu Huang, Ricardo Baptista, Yisong Yue</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10968">https://arxiv.org/abs/2510.10968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10968">https://arxiv.org/pdf/2510.10968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10968]] Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors(https://arxiv.org/abs/2510.10968)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Derivative-free Bayesian inversion is an important task in many science and engineering applications, particularly when computing the forward model derivative is computationally and practically challenging. In this paper, we introduce Blade, which can produce accurate and well-calibrated posteriors for Bayesian inversion using an ensemble of interacting particles. Blade leverages powerful data-driven priors based on diffusion models, and can handle nonlinear forward models that permit only black-box access (i.e., derivative-free). Theoretically, we establish a non-asymptotic convergence analysis to characterize the effects of forward model and prior estimation errors. Empirically, Blade achieves superior performance compared to existing derivative-free Bayesian inversion methods on various inverse problems, including challenging highly nonlinear fluid dynamics.</li>
</ul>

<h3>Title: IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeteng Lin, Xingxing Li, Wen You, Xiaoyang Li, Zehan Lu, Yujun Cai, Jing Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10969">https://arxiv.org/abs/2510.10969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10969">https://arxiv.org/pdf/2510.10969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10969]] IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation(https://arxiv.org/abs/2510.10969)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Existing vision language models (VLMs), including GPT-4 and DALL-E, often struggle to preserve logic, object identity, and style in multimodal image-text generation. This limitation significantly hinders the generalization capability of VLMs in complex image-text input-output scenarios. To address this issue, we propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which enhances existing interleaved VLMs through explicit structured reasoning, thereby mitigating context drift in logic, entity identity, and style. The proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction module parses visual scenes into hierarchical symbolic structures. (2) A coordinated narrative-flow and image synthesis mechanism ensures cross-modal consistency. To evaluate our approach, we construct a novel benchmark based on 3,000 real human-generated question-answer pairs over fine-tuned large models, introducing a dynamic evaluation protocol for quantifying context drift in interleaved VLMs. Experimental results demonstrate that IUT-Plug not only improves accuracy on established benchmarks but also effectively alleviates the three critical forms of context drift across diverse multimodal question answering (QA) scenarios.</li>
</ul>

<h3>Title: Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Oana Frunza, Kashif Rasul, Yuriy Nevmyvaka, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10973">https://arxiv.org/abs/2510.10973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10973">https://arxiv.org/pdf/2510.10973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10973]] Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning(https://arxiv.org/abs/2510.10973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.</li>
</ul>

<h3>Title: Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Ruan, Yixia Li, He Zhu, Yun Chen, Peng Li, Yang Liu, Guanhua Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10974">https://arxiv.org/abs/2510.10974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10974">https://arxiv.org/pdf/2510.10974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10974]] Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning(https://arxiv.org/abs/2510.10974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) primarily rely on supervised fine-tuning (SFT) as a key method to adapt pre-trained models to domain-specific tasks such as mathematical reasoning. However, standard SFT uniformly penalizes all tokens, neglecting that only a small subset of critical tokens determines reasoning correctness. This uniform supervision often causes reduced output diversity and limited generalization. We propose Critical Token Fine-tuning (CFT), a simple yet effective approach that updates only tokens identified as functionally indispensable via counterfactual perturbations. By focusing gradient signals on these decisive reasoning steps while preserving the diversity of non-critical tokens, CFT can enhance both generation and diversity. Extensive experiments on five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time scaling through improved sampling diversity and provides a stronger initialization for reinforcement learning, sustaining performance gains in later training stages while maintaining higher entropy for better exploration. These results highlight CFT as a practical and general framework for efficient and robust LLM fine-tuning.</li>
</ul>

<h3>Title: Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Zhiyong Ma, Zhongkui Ma, Shuofeng Liu, Akide Liu, Derui Wang, Minhui Xue, Guangdong Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10982">https://arxiv.org/abs/2510.10982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10982">https://arxiv.org/pdf/2510.10982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10982]] Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization(https://arxiv.org/abs/2510.10982)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Recent AI regulations call for data that remain useful for innovation while resistant to misuse, balancing utility with protection at the model level. Existing approaches either perturb data to make it unlearnable or retrain models to suppress transfer, but neither governs inference by unknown models, and both typically require control over training. We propose non-transferable examples (NEs), a training-free and data-agnostic input-side usage-control mechanism. We recode inputs within a model-specific low-sensitivity subspace, preserving outputs for the authorized model while reducing performance on unauthorized models through subspace misalignment. We establish formal bounds that guarantee utility for the authorized model and quantify deviation for unauthorized ones, with the Hoffman-Wielandt inequality linking degradation to spectral differences. Empirically, NEs retain performance on diverse vision backbones and state-of-the-art vision-language models under common preprocessing, whereas non-target models collapse even with reconstruction attempts. These results establish NEs as a practical means to preserve intended data utility while preventing unauthorized exploitation. Our project is available at this https URL</li>
</ul>

<h3>Title: Mixup Helps Understanding Multimodal Video Better</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Ma, Ding Ding, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10986">https://arxiv.org/abs/2510.10986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10986">https://arxiv.org/pdf/2510.10986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10986]] Mixup Helps Understanding Multimodal Video Better(https://arxiv.org/abs/2510.10986)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal video understanding plays a crucial role in tasks such as action recognition and emotion classification by combining information from different modalities. However, multimodal models are prone to overfitting strong modalities, which can dominate learning and suppress the contributions of weaker ones. To address this challenge, we first propose Multimodal Mixup (MM), which applies the Mixup strategy at the aggregated multimodal feature level to mitigate overfitting by generating virtual feature-label pairs. While MM effectively improves generalization, it treats all modalities uniformly and does not account for modality imbalance during training. Building on MM, we further introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts the mixing ratios for each modality based on their relative contributions to the learning objective. Extensive experiments on several datasets demonstrate the effectiveness of our methods in improving generalization and multimodal robustness.</li>
</ul>

<h3>Title: DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Hyeseon Ahn, Shinwoo Park, Yo-Sub Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10987">https://arxiv.org/abs/2510.10987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10987">https://arxiv.org/pdf/2510.10987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10987]] DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation(https://arxiv.org/abs/2510.10987)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, watermark</a></li>
<li><strong>Abstract: </strong>The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at this https URL.</li>
</ul>

<h3>Title: Secret-Protected Evolution for Differentially Private Synthetic Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianze Wang, Zhaoyu Chen, Jian Du, Yingtai Xiao, Linjun Zhang, Qiang Yan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10990">https://arxiv.org/abs/2510.10990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10990">https://arxiv.org/pdf/2510.10990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10990]] Secret-Protected Evolution for Differentially Private Synthetic Text Generation(https://arxiv.org/abs/2510.10990)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Text data has become extremely valuable on large language models (LLMs) and even lead to general artificial intelligence (AGI). A lot of high-quality text in the real world is private and cannot be freely used due to privacy concerns. Therefore, differentially private (DP) synthetic text generation has been proposed, aiming to produce high-utility synthetic data while protecting sensitive information. However, existing DP synthetic text generation imposes uniform guarantees that often overprotect non-sensitive content, resulting in substantial utility loss and computational overhead. Therefore, we propose Secret-Protected Evolution (SecPE), a novel framework that extends private evolution with secret-aware protection. Theoretically, we show that SecPE satisfies $(\mathrm{p}, \mathrm{r})$-secret protection, constituting a relaxation of Gaussian DP that enables tighter utility-privacy trade-offs, while also substantially reducing computational complexity relative to baseline methods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE consistently achieves lower Fréchet Inception Distance (FID) and higher downstream task accuracy than GDP-based Aug-PE baselines, while requiring less noise to attain the same level of protection. Our results highlight that secret-aware guarantees can unlock more practical and effective privacy-preserving synthetic text generation.</li>
</ul>

<h3>Title: A Survey on Agentic Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10991">https://arxiv.org/abs/2510.10991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10991">https://arxiv.org/pdf/2510.10991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10991]] A Survey on Agentic Multimodal Large Language Models(https://arxiv.org/abs/2510.10991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at this https URL.</li>
</ul>

<h3>Title: Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Cheng, Binxiao Huang, Taiqiang Wu, Wenyong Zhou, Chenchen Ding, Zhengwu Liu, Graziano Chesi, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10993">https://arxiv.org/abs/2510.10993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10993">https://arxiv.org/pdf/2510.10993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10993]] Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency(https://arxiv.org/abs/2510.10993)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D Gaussian inpainting, a critical technique for numerous applications in virtual reality and multimedia, has made significant progress with pretrained diffusion models. However, ensuring multi-view consistency, an essential requirement for high-quality inpainting, remains a key challenge. In this work, we present PAInpainter, a novel approach designed to advance 3D Gaussian inpainting by leveraging perspective-aware content propagation and consistency verification across multi-view inpainted images. Our method iteratively refines inpainting and optimizes the 3D Gaussian representation with multiple views adaptively sampled from a perspective graph. By propagating inpainted images as prior information and verifying consistency across neighboring views, PAInpainter substantially enhances global consistency and texture fidelity in restored 3D scenes. Extensive experiments demonstrate the superiority of PAInpainter over existing methods. Our approach achieves superior 3D inpainting quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and NeRFiller datasets, respectively, highlighting its effectiveness and generalization capability.</li>
</ul>

<h3>Title: DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chieh Huang, Henry Peng Zou, Yaozu Wu, Dongyuan Li, Yankai Chen, Weizhi Zhang, Yangning Li, Angelo Zangari, Jizhou Guo, Chunyu Miao, Liancheng Fang, Langzhou He, Renhe Jiang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10994">https://arxiv.org/abs/2510.10994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10994">https://arxiv.org/pdf/2510.10994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10994]] DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety(https://arxiv.org/abs/2510.10994)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense</a></li>
<li><strong>Abstract: </strong>Deep research frameworks have shown promising capabilities in synthesizing comprehensive reports from web sources. While deep research possesses significant potential to address complex issues through planning and research cycles, existing frameworks are deficient in sufficient evaluation procedures and stage-specific protections. They typically treat evaluation as exact match accuracy of question-answering, but overlook crucial aspects of report quality such as credibility, coherence, breadth, depth, and safety. This oversight may result in hazardous or malicious sources being integrated into the final report. To address these issues, we introduce DEEPRESEARCHGUARD, a comprehensive framework featuring four-stage safeguards with open-domain evaluation of references and reports. We assess performance across multiple metrics, e.g., defense success rate and over-refusal rate, and five key report dimensions. In the absence of a suitable safety benchmark, we introduce DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash, DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success rate improvement of 18.16% while reducing over-refusal rate by 6%. The input guard provides the most substantial early-stage protection by filtering out obvious risks, while the plan and research guards enhance citation discipline and source credibility. Through extensive experiments, we show that DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware defenses that effectively block harmful content propagation, while systematically improving report quality without excessive over-refusal rates. The code can be found via this https URL.</li>
</ul>

<h3>Title: ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Mahika Phutane, Hayoung Jung, Matthew Kim, Tanushree Mitra, Aditya Vashistha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.10998">https://arxiv.org/abs/2510.10998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.10998">https://arxiv.org/pdf/2510.10998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.10998]] ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios(https://arxiv.org/abs/2510.10998)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly under scrutiny for perpetuating identity-based discrimination in high-stakes domains such as hiring, particularly against people with disabilities (PwD). However, existing research remains largely Western-centric, overlooking how intersecting forms of marginalization--such as gender and caste--shape experiences of PwD in the Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring scenarios spanning diverse disability, gender, nationality, and caste profiles. To capture subtle intersectional harms and biases, we introduce ABLEIST (Ableism, Inspiration, Superhumanization, and Tokenism), a set of five ableism-specific and three intersectional harm metrics grounded in disability studies literature. Our results reveal significant increases in ABLEIST harms towards disabled candidates--harms that many state-of-the-art models failed to detect. These harms were further amplified by sharp increases in intersectional harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates, highlighting critical blind spots in current safety tools and the need for intersectional safety evaluations of frontier models in high-stakes domains like hiring.</li>
</ul>

<h3>Title: ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruihang Xu, Dewei Zhou, Fan Ma, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11000">https://arxiv.org/abs/2510.11000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11000">https://arxiv.org/pdf/2510.11000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11000]] ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation(https://arxiv.org/abs/2510.11000)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.</li>
</ul>

<h3>Title: DND: Boosting Large Language Models with Dynamic Nested Depth</h3>
<ul>
<li><strong>Authors: </strong>Tieyuan Chen, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Weiyao Lin, Jianguo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11001">https://arxiv.org/abs/2510.11001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11001">https://arxiv.org/pdf/2510.11001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11001]] DND: Boosting Large Language Models with Dynamic Nested Depth(https://arxiv.org/abs/2510.11001)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.</li>
</ul>

<h3>Title: Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kai Han, Siqi Ma, Chengxuan Qian, Jun Chen, Chongwen Lyu, Yuqing Song, Zhe Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11005">https://arxiv.org/abs/2510.11005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11005">https://arxiv.org/pdf/2510.11005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11005]] Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation(https://arxiv.org/abs/2510.11005)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of tumors and adjacent normal tissues in medical images is essential for surgical planning and tumor staging. Although foundation models generally perform well in segmentation tasks, they often struggle to focus on foreground areas in complex, low-contrast backgrounds, where some malignant tumors closely resemble normal organs, complicating contextual differentiation. To address these challenges, we propose the Foreground-Aware Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware module to amplify the distinction between background and the entire volume space, allowing the model to concentrate more effectively on target areas. Next, a feature-level frequency enhancement module, based on wavelet transform, extracts discriminative high-frequency features to enhance boundary recognition and detail perception. Eventually, we introduce an edge constraint module to preserve geometric continuity in segmentation boundaries. Extensive experiments on multiple medical datasets demonstrate superior performance across all metrics, validating the effectiveness of our framework, particularly in robustness under complex conditions and fine structure recognition. Our framework significantly enhances segmentation of low-contrast images, paving the way for applications in more diverse and complex medical imaging scenarios.</li>
</ul>

<h3>Title: COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11012">https://arxiv.org/abs/2510.11012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11012">https://arxiv.org/pdf/2510.11012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11012]] COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models(https://arxiv.org/abs/2510.11012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Compositional reasoning remains a persistent weakness of modern vision language models (VLMs): they often falter when a task hinges on understanding how multiple objects, attributes, and relations interact within an image. Multiple research works have attempted to improve compositionality performance by creative tricks such as improving prompt structure, chain of thought reasoning, etc. A more recent line of work attempts to impart additional reasoning in VLMs using well-trained Large Language Models (LLMs), which are far superior in linguistic understanding than VLMs to compensate for the limited linguistic prowess of VLMs. However, these approaches are either resource-intensive or do not provide an interpretable reasoning process. In this paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs with carefully designed neurosymbolic concept trees learned from LLMs to improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning process boosts compositionality performance and provides a rationale behind VLM predictions. Empirical results on four compositionality benchmarks, Winoground, EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with varying sizes, demonstrate that COCO-Tree significantly improves compositional generalization by 5-10% over baselines.</li>
</ul>

<h3>Title: Instruction-aware User Embedding via Synergistic Language and Representation Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Gao, Yike Xu, Jiahao Yuan, Baokun Wang, Jinyong Wen, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11016">https://arxiv.org/abs/2510.11016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11016">https://arxiv.org/pdf/2510.11016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11016]] Instruction-aware User Embedding via Synergistic Language and Representation Modeling(https://arxiv.org/abs/2510.11016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>User representation modeling has become increasingly crucial for personalized applications, yet existing approaches struggle with generalizability across domains and sensitivity to noisy behavioral signals. We present InstructUE, an instruction-aware user embedding foundation model that leverages large language models (LLMs) to generate general and instruction-aware user representations. InstructUE introduces a multi-encoder architecture with a lightweight adapter that efficiently processes heterogeneous data from six different sources while preserving their structural characteristics. Additionally, it proposes a novel contrastive-autoregressive training framework that bridges language and representation spaces through a curated UserQA dataset. The contrastive-autoregressive training framework simultaneously leverages autoregressive learning to capture domain knowledge in language space and contrastive learning to align user-text embeddings in representation space, thereby enhancing the instruction-awareness and noise-robustness of user embeddings. Through extensive experiments on real-world applications, we demonstrate that InstructUE significantly outperforms existing methods across multiple domains including user prediction, marketing, and recommendation scenarios. Our results show that instruction-aware user modeling can effectively achieve instruction-guided denoising of user information in specific scenarios, paving the way for more generalizable and robust user representation learning.</li>
</ul>

<h3>Title: The Easy Path to Robustness: Coreset Selection using Sample Hardness</h3>
<ul>
<li><strong>Authors: </strong>Pranav Ramesh, Arjun Roy, Deepak Ravikumar, Kaushik Roy, Gopalakrishnan Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11018">https://arxiv.org/abs/2510.11018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11018">https://arxiv.org/pdf/2510.11018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11018]] The Easy Path to Robustness: Coreset Selection using Sample Hardness(https://arxiv.org/abs/2510.11018)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Designing adversarially robust models from a data-centric perspective requires understanding which input samples are most crucial for learning resilient features. While coreset selection provides a mechanism for efficient training on data subsets, current algorithms are designed for clean accuracy and fall short in preserving robustness. To address this, we propose a framework linking a sample's adversarial vulnerability to its \textit{hardness}, which we quantify using the average input gradient norm (AIGN) over training. We demonstrate that \textit{easy} samples (with low AIGN) are less vulnerable and occupy regions further from the decision boundary. Leveraging this insight, we present EasyCore, a coreset selection algorithm that retains only the samples with low AIGN for training. We empirically show that models trained on EasyCore-selected data achieve significantly higher adversarial accuracy than those trained with competing coreset methods under both standard and adversarial training. As AIGN is a model-agnostic dataset property, EasyCore is an efficient and widely applicable data-centric method for improving adversarial robustness. We show that EasyCore achieves up to 7\% and 5\% improvement in adversarial accuracy under standard training and TRADES adversarial training, respectively, compared to existing coreset methods.</li>
</ul>

<h3>Title: GIR-Bench: Versatile Benchmark for Generating Images with Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11026">https://arxiv.org/abs/2510.11026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11026">https://arxiv.org/pdf/2510.11026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11026]] GIR-Bench: Versatile Benchmark for Generating Images with Reasoning(https://arxiv.org/abs/2510.11026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yanning Hou, Ke Xu, Junfa Li, Yanran Ruan, Jianfeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11028">https://arxiv.org/abs/2510.11028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11028">https://arxiv.org/pdf/2510.11028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11028]] Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts(https://arxiv.org/abs/2510.11028)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, the powerful generalization ability exhibited by foundation models has brought forth new solutions for zero-shot anomaly segmentation tasks. However, guiding these foundation models correctly to address downstream tasks remains a challenge. This paper proposes a novel two-stage framework, for zero-shot anomaly segmentation tasks in industrial anomaly detection. This framework excellently leverages the powerful anomaly localization capability of CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's inclination towards object segmentation, we propose the Co-Feature Point Prompt Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to generate positive and negative point prompts, guiding SAM to focus on segmenting anomalous regions rather than the entire object. (2) To further optimize SAM's segmentation results and mitigate rough boundaries and isolated noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving precise segmentation of anomalous regions. Across multiple datasets, consistent experimental validation demonstrates that our approach achieves state-of-the-art zero-shot anomaly segmentation results. Particularly noteworthy is our performance on the Visa dataset, where we outperform the state-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and AP metrics, respectively.</li>
</ul>

<h3>Title: Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Prayag Tiwari, Xiang Wan, Feng Jiang, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11040">https://arxiv.org/abs/2510.11040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11040">https://arxiv.org/pdf/2510.11040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11040]] Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks(https://arxiv.org/abs/2510.11040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of large language models (LLMs) has transformed healthcare by offering clinical guidance, yet their direct deployment to patients poses safety risks due to limited domain expertise. To mitigate this, we propose repositioning LLMs as clinical assistants that collaborate with experienced physicians rather than interacting with patients directly. We conduct a two-stage inspiration-feedback survey to identify real-world needs in clinical workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27 specialties. To evaluate model performance in doctor-facing applications, we introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74 multi-turn conversations). Experimental results with over ten popular LLMs demonstrate that DoctorFLAN notably improves the performance of open-source LLMs in medical contexts, facilitating their alignment with physician workflows and complementing existing patient-oriented models. This work contributes a valuable resource and framework for advancing doctor-centered medical LLM development</li>
</ul>

<h3>Title: Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset</h3>
<ul>
<li><strong>Authors: </strong>Nivea Roy, Son Tran, Atul Sajjanhar, K. Devaraja, Prakashini Koteshwara, Yong Xiang, Divya Rao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11047">https://arxiv.org/abs/2510.11047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11047">https://arxiv.org/pdf/2510.11047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11047]] Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset(https://arxiv.org/abs/2510.11047)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Laryngeal cancer imaging research lacks standardised datasets to enable reproducible deep learning (DL) model development. We present LaryngealCT, a curated benchmark of 1,029 computed tomography (CT) scans aggregated from six collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic volumes of interest encompassing the larynx were extracted using a weakly supervised parameter search framework validated by clinical experts. 3D DL architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i) early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892, F1-macro-0.646) respectively outperformed the other models in the two tasks. Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays revealed greater peri-cartilage attention in non-T4 cases and focal activations in T4 predictions. Through open-source data, pretrained models, and integrated explainability tools, LaryngealCT offers a reproducible foundation for AI-driven research to support clinical decisions in laryngeal oncology.</li>
</ul>

<h3>Title: Zero-shot Face Editing via ID-Attribute Decoupled Inversion</h3>
<ul>
<li><strong>Authors: </strong>Yang Hou, Minggu Wang, Jianjun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11050">https://arxiv.org/abs/2510.11050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11050">https://arxiv.org/pdf/2510.11050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11050]] Zero-shot Face Editing via ID-Attribute Decoupled Inversion(https://arxiv.org/abs/2510.11050)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-guided diffusion models have shown promise for general image editing via inversion techniques, but often struggle to maintain ID and structural consistency in real face editing tasks. To address this limitation, we propose a zero-shot face editing method based on ID-Attribute Decoupled Inversion. Specifically, we decompose the face representation into ID and attribute features, using them as joint conditions to guide both the inversion and the reverse diffusion processes. This allows independent control over ID and attributes, ensuring strong ID preservation and structural consistency while enabling precise facial attribute manipulation. Our method supports a wide range of complex multi-attribute face editing tasks using only text prompts, without requiring region-specific input, and operates at a speed comparable to DDIM inversion. Comprehensive experiments demonstrate its practicality and effectiveness.</li>
</ul>

<h3>Title: Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States</h3>
<ul>
<li><strong>Authors: </strong>Qinglin Zhu, Yizhen Yao, Runcong Zhao, Yanzheng Xiang, Amrutha Saseendran, Chen Jin, Philip Alexander Teare, Bin Liang, Yulan He, Lin Gui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11052">https://arxiv.org/abs/2510.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11052">https://arxiv.org/pdf/2510.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11052]] Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States(https://arxiv.org/abs/2510.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.</li>
</ul>

<h3>Title: Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Youngrok Park, Hojung Jung, Sangmin Bae, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11057">https://arxiv.org/abs/2510.11057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11057">https://arxiv.org/pdf/2510.11057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11057]] Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models(https://arxiv.org/abs/2510.11057)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks.</li>
</ul>

<h3>Title: Robust Photoplethysmography Signal Denoising via Mamba Networks</h3>
<ul>
<li><strong>Authors: </strong>I Chiu, Yu-Tung Liu, Kuan-Chen Wang, Hung-Yu Wei, Yu Tsao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11058">https://arxiv.org/abs/2510.11058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11058">https://arxiv.org/pdf/2510.11058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11058]] Robust Photoplethysmography Signal Denoising via Mamba Networks(https://arxiv.org/abs/2510.11058)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Photoplethysmography (PPG) is widely used in wearable health monitoring, but its reliability is often degraded by noise and motion artifacts, limiting downstream applications such as heart rate (HR) estimation. This paper presents a deep learning framework for PPG denoising with an emphasis on preserving physiological information. In this framework, we propose DPNet, a Mamba-based denoising backbone designed for effective temporal modeling. To further enhance denoising performance, the framework also incorporates a scale-invariant signal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an auxiliary HR predictor (HRP) that provides physiological consistency through HR-based supervision. Experiments on the BIDMC dataset show that our method achieves strong robustness against both synthetic noise and real-world motion artifacts, outperforming conventional filtering and existing neural models. Our method can effectively restore PPG signals while maintaining HR accuracy, highlighting the complementary roles of SI-SDR loss and HR-guided supervision. These results demonstrate the potential of our approach for practical deployment in wearable healthcare systems.</li>
</ul>

<h3>Title: Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yujie Zhao, Lanxiang Hu, Yang Wang, Minmin Hou, Hao Zhang, Ke Ding, Jishen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11062">https://arxiv.org/abs/2510.11062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11062">https://arxiv.org/pdf/2510.11062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11062]] Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs(https://arxiv.org/abs/2510.11062)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models. We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: this https URL.</li>
</ul>

<h3>Title: LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Henghui Ding, Kaining Ying, Lingyi Hong, Ning Xu, Linjie Yang, Yuchen Fan, Mingqi Gao, Jingkun Chen, Yunqi Miao, Gengshen Wu, Zhijin Qin, Jungong Han, Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Chang Soo Lim, Joonyoung Moon, Donghyeon Cho, Tingmin Li, Yixuan Li, Yang Yang, An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu, Yujie Xie, Hongyang Zhang, Zhihui Liu, Shihai Ruan, Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji, Ran Hong, Feng Lu, Leilei Cao, An Yan, Alexey Nekrasov, Ali Athar, Daan de Geus, Alexander Hermans, Bastian Leibe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11063">https://arxiv.org/abs/2510.11063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11063">https://arxiv.org/pdf/2510.11063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11063]] LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation(https://arxiv.org/abs/2510.11063)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This report presents an overview of the 7th Large-scale Video Object Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the two traditional tracks of LSVOS that jointly target robustness in realistic video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition features a newly introduced track, Complex VOS (MOSEv2). Building upon prior insights, MOSEv2 substantially increases difficulty, introducing more challenging but realistic scenarios including denser small objects, frequent disappear/reappear events, severe occlusions, adverse weather and lighting, etc., pushing long-term consistency and generalization beyond curated benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric to better evaluate objects across scales and disappearance cases. We summarize datasets and protocols, highlight top-performing solutions, and distill emerging trends, such as the growing role of LLM/MLLM components and memory-aware propagation, aiming to chart future directions for resilient, language-aware video segmentation in the wild.</li>
</ul>

<h3>Title: Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations</h3>
<ul>
<li><strong>Authors: </strong>Carlo Brunetta, Amit Chaudhary, Stefano Galatolo, Massimiliano Sala</a></li>
<li><strong>Subjects: </strong>cs.CR, econ.GN, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11065">https://arxiv.org/abs/2510.11065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11065">https://arxiv.org/pdf/2510.11065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11065]] Stabilizing the Staking Rate, Dynamically Distributed Inflation and Delay Induced Oscillations(https://arxiv.org/abs/2510.11065)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Dynamically distributed inflation is a common mechanism used to guide a blockchain's staking rate towards a desired equilibrium between network security and token liquidity. However, the high sensitivity of the annual percentage yield to changes in the staking rate, coupled with the inherent feedback delays in staker responses, can induce undesirable oscillations around this equilibrium. This paper investigates this instability phenomenon. We analyze the dynamics of inflation-based reward systems and propose a novel distribution model designed to stabilize the staking rate. Our solution effectively dampens oscillations, stabilizing the yield within a target staking range.</li>
</ul>

<h3>Title: ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer</h3>
<ul>
<li><strong>Authors: </strong>Yuan Tian, Min Zhou, Yitong Chen, Fang Li, Lingzi Qi, Shuo Wang, Xieyang Xu, Yu Yu, Shiqiong Xu, Chaoyu Lei, Yankai Jiang, Rongzhao Zhang, Jia Tan, Li Wu, Hong Chen, Xiaowei Liu, Wei Lu, Lin Li, Huifang Zhou, Xuefei Song, Guangtao Zhai, Xianqun Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11073">https://arxiv.org/abs/2510.11073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11073">https://arxiv.org/pdf/2510.11073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11073]] ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer(https://arxiv.org/abs/2510.11073)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthalmology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98\% accuracy, $\kappa > 0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($\kappa > 0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of images. ROFI works with AI systems, maintaining original diagnoses ($\kappa > 0.80$), and supports secure image reversal (over 98\% similarity), enabling audits and long-term care. These results show ROFI's effectiveness of protecting patient privacy in the digital medicine era.</li>
</ul>

<h3>Title: Source-Free Object Detection with Detection Transformer</h3>
<ul>
<li><strong>Authors: </strong>Huizai Yao, Sicheng Zhao, Shuo Lu, Hui Chen, Yangyang Li, Guoping Liu, Tengfei Xing, Chenggang Yan, Jianhua Tao, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11090">https://arxiv.org/abs/2510.11090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11090">https://arxiv.org/pdf/2510.11090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11090]] Source-Free Object Detection with Detection Transformer(https://arxiv.org/abs/2510.11090)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pre-trained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.</li>
</ul>

<h3>Title: Text-Enhanced Panoptic Symbol Spotting in CAD Drawings</h3>
<ul>
<li><strong>Authors: </strong>Xianlin Liu, Yan Gong, Bohao Li, Jiajing Huang, Bowen Du, Junchen Ye, Liyan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11091">https://arxiv.org/abs/2510.11091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11091">https://arxiv.org/pdf/2510.11091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11091]] Text-Enhanced Panoptic Symbol Spotting in CAD Drawings(https://arxiv.org/abs/2510.11091)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>With the widespread adoption of Computer-Aided Design(CAD) drawings in engineering, architecture, and industrial design, the ability to accurately interpret and analyze these drawings has become increasingly critical. Among various subtasks, panoptic symbol spotting plays a vital role in enabling downstream applications such as CAD automation and design retrieval. Existing methods primarily focus on geometric primitives within the CAD drawings to address this task, but they face following major problems: they usually overlook the rich textual annotations present in CAD drawings and they lack explicit modeling of relationships among primitives, resulting in incomprehensive understanding of the holistic drawings. To fill this gap, we propose a panoptic symbol spotting framework that incorporates textual annotations. The framework constructs unified representations by jointly modeling geometric and textual primitives. Then, using visual features extract by pretrained CNN as the initial representations, a Transformer-based backbone is employed, enhanced with a type-aware attention mechanism to explicitly model the different types of spatial dependencies between various primitives. Extensive experiments on the real-world dataset demonstrate that the proposed method outperforms existing approaches on symbol spotting tasks involving textual annotations, and exhibits superior robustness when applied to complex CAD drawings.</li>
</ul>

<h3>Title: CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Fengling Zhu, Boshi Liu, Jingyu Hua, Sheng Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11096">https://arxiv.org/abs/2510.11096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11096">https://arxiv.org/pdf/2510.11096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11096]] CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization(https://arxiv.org/abs/2510.11096)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved remarkable success in tasks such as image captioning, visual question answering, and cross-modal reasoning by integrating visual and textual modalities. However, their multimodal nature also exposes them to adversarial threats, where attackers can perturb either modality or both jointly to induce harmful, misleading, or policy violating outputs. Existing defense strategies, such as adversarial training and input purification, face notable limitations: adversarial training typically improves robustness only against known attacks while incurring high computational costs, whereas conventional purification approaches often suffer from degraded image quality and insufficient generalization to complex multimodal tasks. In this work, we focus on defending the visual modality, which frequently serves as the primary entry point for adversarial manipulation. We propose a supervised diffusion based denoising framework that leverages paired adversarial clean image datasets to fine-tune diffusion models with directional, task specific guidance. Unlike prior unsupervised purification methods such as DiffPure, our approach achieves higher quality reconstructions while significantly improving defense robustness in multimodal tasks. Furthermore, we incorporate prompt optimization as a complementary defense mechanism, enhancing resistance against diverse and unseen attack strategies. Extensive experiments on image captioning and visual question answering demonstrate that our method not only substantially improves robustness but also exhibits strong transferability to unknown adversarial attacks. These results highlight the effectiveness of supervised diffusion based denoising for multimodal defense, paving the way for more reliable and secure deployment of MLLMs in real world applications.</li>
</ul>

<h3>Title: Compositional Zero-Shot Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Ans Munir, Faisal Z. Qureshi, Mohsen Ali, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11106">https://arxiv.org/abs/2510.11106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11106">https://arxiv.org/pdf/2510.11106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11106]] Compositional Zero-Shot Learning: A Survey(https://arxiv.org/abs/2510.11106)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision that enables models to recognize unseen combinations of known attributes and objects during inference, addressing the combinatorial challenge of requiring training data for every possible composition. This is particularly challenging because the visual appearance of primitives is highly contextual; for example, ``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars differ significantly from ``wet'' cats. Effectively modeling this contextuality and the inherent compositionality is crucial for robust compositional zero-shot recognition. This paper presents, to our knowledge, the first comprehensive survey specifically focused on Compositional Zero-Shot Learning. We systematically review the state-of-the-art CZSL methods, introducing a taxonomy grounded in disentanglement, with four families of approaches: no explicit disentanglement, textual disentanglement, visual disentanglement, and cross-modal disentanglement. We provide a detailed comparative analysis of these methods, highlighting their core advantages and limitations in different problem settings, such as closed-world and open-world CZSL. Finally, we identify the most significant open challenges and outline promising future research directions. This survey aims to serve as a foundational resource to guide and inspire further advancements in this fascinating and important field. Papers studied in this survey with their official code are available on our github: this https URL</li>
</ul>

<h3>Title: MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Lei, Kyle Genova, George Kopanas, Noah Snavely, Leonidas Guibas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11107">https://arxiv.org/abs/2510.11107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11107">https://arxiv.org/pdf/2510.11107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11107]] MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps(https://arxiv.org/abs/2510.11107)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.</li>
</ul>

<h3>Title: PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11110">https://arxiv.org/abs/2510.11110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11110">https://arxiv.org/pdf/2510.11110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11110]] PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities(https://arxiv.org/abs/2510.11110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Missing or corrupted modalities are common in physiological signal-based medical applications owing to hardware constraints or motion artifacts. However, most existing methods assume the availability of all modalities, resulting in substantial performance degradation in the absence of any modality. To overcome this limitation, this study proposes PhysioME, a robust framework designed to ensure reliable performance under missing modality conditions. PhysioME adopts: (1) a multimodal self-supervised learning approach that combines contrastive learning with masked prediction; (2) a Dual-PathNeuroNet backbone tailored to capture the temporal dynamics of each physiological signal modality; and (3) a restoration decoder that reconstructs missing modality tokens, enabling flexible processing of incomplete inputs. The experimental results show that PhysioME achieves high consistency and generalization performance across various missing modality scenarios. These findings highlight the potential of PhysioME as a reliable tool for supporting clinical decision-making in real-world settings with imperfect data availability.</li>
</ul>

<h3>Title: Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Tang, Shengfeng He, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11115">https://arxiv.org/abs/2510.11115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11115">https://arxiv.org/pdf/2510.11115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11115]] Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning(https://arxiv.org/abs/2510.11115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Few-shot learning (FSL) addresses the challenge of classifying novel classes with limited training samples. While some methods leverage semantic knowledge from smaller-scale models to mitigate data scarcity, these approaches often introduce noise and bias due to the data's inherent simplicity. In this paper, we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which effectively transfers diverse and complementary knowledge from large multimodal models to empower the off-the-shelf few-shot learner. Specifically, SynTrans employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak student, distilling semantic-aligned visual knowledge via an unsupervised proxy task. Subsequently, a training-free synergistic knowledge mining module facilitates collaboration among large multimodal models to extract high-quality semantic knowledge. Building upon this, a visual-semantic bridging module enables bi-directional knowledge transfer between visual and semantic spaces, transforming explicit visual and implicit semantic knowledge into category-specific classifier weights. Finally, SynTrans introduces a visual weight generator and a semantic weight reconstructor to adaptively construct optimal multimodal FSL classifiers. Experimental results on four FSL datasets demonstrate that SynTrans, even when paired with a simple few-shot vision encoder, significantly outperforms current state-of-the-art methods.</li>
</ul>

<h3>Title: N-output Mechanism: Estimating Statistical Information from Numerical Data under Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Incheol Baek, Yon Dohn Chung</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11116">https://arxiv.org/abs/2510.11116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11116">https://arxiv.org/pdf/2510.11116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11116]] N-output Mechanism: Estimating Statistical Information from Numerical Data under Local Differential Privacy(https://arxiv.org/abs/2510.11116)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Local Differential Privacy (LDP) addresses significant privacy concerns in sensitive data collection. In this work, we focus on numerical data collection under LDP, targeting a significant gap in the literature: existing LDP mechanisms are optimized for either a very small ($|\Omega| \in \{2, 3\}$) or infinite output spaces. However, no generalized method for constructing an optimal mechanism for an arbitrary output size $N$ exists. To fill this gap, we propose the \textbf{N-output mechanism}, a generalized framework that maps numerical data to one of $N$ discrete outputs. We formulate the mechanism's design as an optimization problem to minimize estimation variance for any given $N \geq 2$ and develop both numerical and analytical solutions. This results in a mechanism that is highly accurate and adaptive, as its design is determined by solving an optimization problem for any chosen $N$. Furthermore, we extend our framework and existing mechanisms to the task of distribution estimation. Empirical evaluations show that the N-output mechanism achieves state-of-the-art accuracy for mean, variance, and distribution estimation with small communication costs.</li>
</ul>

<h3>Title: Demystifying Numerosity in Diffusion Models -- Limitations and Remedies</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Zhao, Xiaochen Wang, Li Dong, Wentao Zhang, Yuhui Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11117">https://arxiv.org/abs/2510.11117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11117">https://arxiv.org/pdf/2510.11117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11117]] Demystifying Numerosity in Diffusion Models -- Limitations and Remedies(https://arxiv.org/abs/2510.11117)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Numerosity remains a challenge for state-of-the-art text-to-image generation models like FLUX and GPT-4o, which often fail to accurately follow counting instructions in text prompts. In this paper, we aim to study a fundamental yet often overlooked question: Can diffusion models inherently generate the correct number of objects specified by a textual prompt simply by scaling up the dataset and model size? To enable rigorous and reproducible evaluation, we construct a clean synthetic numerosity benchmark comprising two complementary datasets: GrayCount250 for controlled scaling studies, and NaturalCount6 featuring complex naturalistic scenes. Second, we empirically show that the scaling hypothesis does not hold: larger models and datasets alone fail to improve counting accuracy on our benchmark. Our analysis identifies a key reason: diffusion models tend to rely heavily on the noise initialization rather than the explicit numerosity specified in the prompt. We observe that noise priors exhibit biases toward specific object counts. In addition, we propose an effective strategy for controlling numerosity by injecting count-aware layout information into the noise prior. Our method achieves significant gains, improving accuracy on GrayCount250 from 20.0\% to 85.3\% and on NaturalCount6 from 74.8\% to 86.3\%, demonstrating effective generalization across settings.</li>
</ul>

<h3>Title: Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM</h3>
<ul>
<li><strong>Authors: </strong>Rongjie Zhu, Cong Zhang, Zhiguang Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11121">https://arxiv.org/abs/2510.11121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11121">https://arxiv.org/pdf/2510.11121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11121]] Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM(https://arxiv.org/abs/2510.11121)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) are increasingly used as automated heuristic designers for vehicle routing problems (VRPs), current state-of-the-art methods predominantly rely on prompting massive, general-purpose models like GPT-4. This work challenges that paradigm by demonstrating that a smaller, specialized LLM, when meticulously fine-tuned, can generate components that surpass expert-crafted heuristics within advanced solvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for Fine-Tuning a small LLM to generate high-performance crossover operators for the Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP). Our method employs a multi-tiered, curriculum-based reward function that progressively guides the LLM to master generating first compilable, then executable, and finally, superior-performing operators that exceed human expert designs. This is coupled with an operator caching mechanism that discourages plagiarism and promotes diversity during training. Comprehensive experiments show that our fine-tuned LLM produces crossover operators which significantly outperform the expert-designed ones in HGS. The performance advantage remains consistent, generalizing from small-scale instances to large-scale problems with up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading neuro-combinatorial baselines, prompt-based methods, and commercial LLMs such as GPT-4o and GPT-4o-mini.</li>
</ul>

<h3>Title: Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer</h3>
<ul>
<li><strong>Authors: </strong>Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11128">https://arxiv.org/abs/2510.11128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11128">https://arxiv.org/pdf/2510.11128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11128]] Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer(https://arxiv.org/abs/2510.11128)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the student's learned representations by feeding them back into the frozen teacher's prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead.</li>
</ul>

<h3>Title: Test-Time Adaptation by Causal Trimming</h3>
<ul>
<li><strong>Authors: </strong>Yingnan Liu, Rui Qiao, Mong Li Lee, Wynne Hsu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11133">https://arxiv.org/abs/2510.11133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11133">https://arxiv.org/pdf/2510.11133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11133]] Test-Time Adaptation by Causal Trimming(https://arxiv.org/abs/2510.11133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Test-time adaptation aims to improve model robustness under distribution shifts by adapting models with access to unlabeled target samples. A primary cause of performance degradation under such shifts is the model's reliance on features that lack a direct causal relationship with the prediction target. We introduce Test-time Adaptation by Causal Trimming (TACT), a method that identifies and removes non-causal components from representations for test distributions. TACT applies data augmentations that preserve causal features while varying non-causal ones. By analyzing the changes in the representations using Principal Component Analysis, TACT identifies the highest variance directions associated with non-causal features. It trims the representations by removing their projections on the identified directions, and uses the trimmed representations for the predictions. During adaptation, TACT continuously tracks and refines these directions to get a better estimate of non-causal features. We theoretically analyze the effectiveness of this approach and empirically validate TACT on real-world out-of-distribution benchmarks. TACT consistently outperforms state-of-the-art methods by a significant margin.</li>
</ul>

<h3>Title: CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhuochen, Fok Kar Wai, Thing Vrizlynn</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11137">https://arxiv.org/abs/2510.11137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11137">https://arxiv.org/pdf/2510.11137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11137]] CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense(https://arxiv.org/abs/2510.11137)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have gained widespread attention recently, but their potential security vulnerabilities, especially privacy leakage, are also becoming apparent. To test and evaluate for data extraction risks in LLM, we proposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and Defense. We introduce several innovative components, including Dynamic Loss, Additive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested to enhance the consistency of the soft prompt tuning process. Through extensive experimentation with various combinations, we achieved an extraction rate of 65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other reference works confirm our superior extraction rates. We evaluate CoSPED on more scenarios, achieving Pythia model extraction rate of 51.7% and introducing cross-model comparison. Finally, we explore defense through Rank-One Model Editing and achieve a reduction in the extraction rate to 1.6%, which proves that our analysis of extraction mechanisms can directly inform effective mitigation strategies against soft prompt-based attacks.</li>
</ul>

<h3>Title: Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay</h3>
<ul>
<li><strong>Authors: </strong>Byron Alexander Jacobs, Aqeel Morris, Ifthakaar Shaik, Frando Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11142">https://arxiv.org/abs/2510.11142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11142">https://arxiv.org/pdf/2510.11142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11142]] Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay(https://arxiv.org/abs/2510.11142)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sperm DNA fragmentation (SDF) is a critical parameter in male fertility assessment that conventional semen analysis fails to evaluate. This study presents the validation of a novel artificial intelligence (AI) tool designed to detect SDF through digital analysis of phase contrast microscopy images, using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL) assay as the gold standard reference. Utilising the established link between sperm morphology and DNA integrity, the present work proposes a morphology assisted ensemble AI model that combines image processing techniques with state-of-the-art transformer based machine learning models (GC-ViT) for the prediction of DNA fragmentation in sperm from phase contrast images. The ensemble model is benchmarked against a pure transformer `vision' model as well as a `morphology-only` model. Promising results show the proposed framework is able to achieve sensitivity of 60\% and specificity of 75\%. This non-destructive methodology represents a significant advancement in reproductive medicine by enabling real-time sperm selection based on DNA integrity for clinical diagnostic and therapeutic applications.</li>
</ul>

<h3>Title: TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code</h3>
<ul>
<li><strong>Authors: </strong>Alexander Sternfeld, Andrei Kucharavy, Ljiljana Dolamic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11151">https://arxiv.org/abs/2510.11151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11151">https://arxiv.org/pdf/2510.11151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11151]] TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code(https://arxiv.org/abs/2510.11151)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language Models (LLMs) have shown remarkable proficiency in code generation tasks across various programming languages. However, their outputs often contain subtle but critical vulnerabilities, posing significant risks when deployed in security-sensitive or mission-critical systems. This paper introduces TypePilot, an agentic AI framework designed to enhance the security and robustness of LLM-generated code by leveraging strongly typed and verifiable languages, using Scala as a representative example. We evaluate the effectiveness of our approach in two settings: formal verification with the Stainless framework and general-purpose secure code generation. Our experiments with leading open-source LLMs reveal that while direct code generation often fails to enforce safety constraints, just as naive prompting for more secure code, our type-focused agentic pipeline substantially mitigates input validation and injection vulnerabilities. The results demonstrate the potential of structured, type-guided LLM workflows to improve the SotA of the trustworthiness of automated code generation in high-assurance domains.</li>
</ul>

<h3>Title: Emergence of hybrid computational dynamics through reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Roman A. Kononov, Nikita A. Pospelov, Konstantin V. Anokhin, Vladimir V. Nekorkin, Oleg V. Maslennikov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, nlin.AO, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11162">https://arxiv.org/abs/2510.11162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11162">https://arxiv.org/pdf/2510.11162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11162]] Emergence of hybrid computational dynamics through reinforcement learning(https://arxiv.org/abs/2510.11162)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding how learning algorithms shape the computational strategies that emerge in neural networks remains a fundamental challenge in machine intelligence. While network architectures receive extensive attention, the role of the learning paradigm itself in determining emergent dynamics remains largely unexplored. Here we demonstrate that reinforcement learning (RL) and supervised learning (SL) drive recurrent neural networks (RNNs) toward fundamentally different computational solutions when trained on identical decision-making tasks. Through systematic dynamical systems analysis, we reveal that RL spontaneously discovers hybrid attractor architectures, combining stable fixed-point attractors for decision maintenance with quasi-periodic attractors for flexible evidence integration. This contrasts sharply with SL, which converges almost exclusively to simpler fixed-point-only solutions. We further show that RL sculpts functionally balanced neural populations through a powerful form of implicit regularization -- a structural signature that enhances robustness and is conspicuously absent in the more heterogeneous solutions found by SL-trained networks. The prevalence of these complex dynamics in RL is controllably modulated by weight initialization and correlates strongly with performance gains, particularly as task complexity increases. Our results establish the learning algorithm as a primary determinant of emergent computation, revealing how reward-based optimization autonomously discovers sophisticated dynamical mechanisms that are less accessible to direct gradient-based optimization. These findings provide both mechanistic insights into neural computation and actionable principles for designing adaptive AI systems.</li>
</ul>

<h3>Title: Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness</h3>
<ul>
<li><strong>Authors: </strong>Ilaria Vascotto, Alex Rodriguez, Alessandro Bonaita, Luca Bortolussi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11164">https://arxiv.org/abs/2510.11164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11164">https://arxiv.org/pdf/2510.11164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11164]] Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness(https://arxiv.org/abs/2510.11164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The use of Artificial Intelligence (AI) models in real-world and high-risk applications has intensified the discussion about their trustworthiness and ethical usage, from both a technical and a legislative perspective. The field of eXplainable Artificial Intelligence (XAI) addresses this challenge by proposing explanations that bring to light the decision-making processes of complex black-box models. Despite being an essential property, the robustness of explanations is often an overlooked aspect during development: only robust explanation methods can increase the trust in the system as a whole. This paper investigates the role of robustness through the usage of a feature importance aggregation derived from multiple models ($k$-nearest neighbours, random forest and neural networks). Preliminary results showcase the potential in increasing the trustworthiness of the application, while leveraging multiple model's predictive power.</li>
</ul>

<h3>Title: Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages</h3>
<ul>
<li><strong>Authors: </strong>Paloma Piot, José Ramom Pichel Campos, Javier Parapar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11167">https://arxiv.org/abs/2510.11167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11167">https://arxiv.org/pdf/2510.11167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11167]] Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages(https://arxiv.org/abs/2510.11167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hate speech poses a serious threat to social cohesion and individual well-being, particularly on social media, where it spreads rapidly. While research on hate speech detection has progressed, it remains largely focused on English, resulting in limited resources and benchmarks for low-resource languages. Moreover, many of these languages have multiple linguistic varieties, a factor often overlooked in current approaches. At the same time, large language models require substantial amounts of data to perform reliably, a requirement that low-resource languages often cannot meet. In this work, we address these gaps by compiling a meta-collection of hate speech datasets for European Spanish, standardised with unified labels and metadata. This collection is based on a systematic analysis and integration of existing resources, aiming to bridge the data gap and support more consistent and scalable hate speech detection. We extended this collection by translating it into European Portuguese and into a Galician standard that is more convergent with Spanish and another Galician variant that is more convergent with Portuguese, creating aligned multilingual corpora. Using these resources, we establish new benchmarks for hate speech detection in Iberian languages. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, providing baseline results for future research. Moreover, we perform a cross-lingual analysis with our target languages. Our findings underscore the importance of multilingual and variety-aware approaches in hate speech detection and offer a foundation for improved benchmarking in underrepresented European languages.</li>
</ul>

<h3>Title: Multiview Manifold Evidential Fusion for PolSAR Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Junfei Shi, Haojia Zhang, Haiyan Jin, Junhuai Li, Xiaogang Song, Yuanfan Guo, Haonan Su, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11171">https://arxiv.org/abs/2510.11171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11171">https://arxiv.org/pdf/2510.11171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11171]] Multiview Manifold Evidential Fusion for PolSAR Image Classification(https://arxiv.org/abs/2510.11171)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their extracted multi-features - such as scattering angle, entropy, texture, and boundary descriptors - provide complementary and physically interpretable information for image classification. Traditional fusion strategies typically concatenate these features or employ deep learning networks to combine them. However, the covariance matrices and multi-features, as two complementary views, lie on different manifolds with distinct geometric structures. Existing fusion methods also overlook the varying importance of different views and ignore uncertainty, often leading to unreliable predictions. To address these issues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to effectively fuse these two views. It gives a new framework to integrate PolSAR manifold learning and evidence fusion into a unified architecture. Specifically, covariance matrices are represented on the Hermitian Positive Definite (HPD) manifold, while multi-features are modeled on the Grassmann manifold. Two different kernel metric learning networks are constructed to learn their manifold representations. Subsequently, a trusted multiview evidence fusion, replacing the conventional softmax classifier, estimates belief mass and quantifies the uncertainty of each view from the learned deep features. Finally, a Dempster-Shafer theory-based fusion strategy combines evidence, enabling a more reliable and interpretable classification. Extensive experiments on three real-world PolSAR datasets demonstrate that the proposed method consistently outperforms existing approaches in accuracy, robustness, and interpretability.</li>
</ul>

<h3>Title: CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11173">https://arxiv.org/abs/2510.11173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11173">https://arxiv.org/pdf/2510.11173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11173]] CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation(https://arxiv.org/abs/2510.11173)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail. To solve this, we present CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model that bridges language reasoning to segmentation through a differentiable and interpretable positional prior instantiated as a heatmap. By making the reasoning process clear via MCoT and expressing it as a dense, differentiable heatmap, this interface enhances interpretability and diagnostic analysis and yields more concentrated evidence on the target. A learnable concentration token aggregates features of the image and reasoning text to generate this positional prior, which is decoded to precise masks through a lightweight decoder, providing a direct connection between reasoning and segmentation. Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best reported metrics on each standard split under comparable protocols, with performance at or above prior state of the art across both validation and test partitions. Extensive experiments reveal that the quality of the heatmap strongly influences the resulting mask quality, supporting a consistent association between the reasoning output and downstream mask generation. Collectively, these findings support the utility of this paradigm in bridging reasoning and segmentation and show advantages in concentration driven by reasoning and predicting masks more precisely. Code, checkpoints and logs are released at this https URL.</li>
</ul>

<h3>Title: G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11176">https://arxiv.org/abs/2510.11176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11176">https://arxiv.org/pdf/2510.11176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11176]] G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation(https://arxiv.org/abs/2510.11176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.</li>
</ul>

<h3>Title: BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11178">https://arxiv.org/abs/2510.11178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11178">https://arxiv.org/pdf/2510.11178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11178]] BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models(https://arxiv.org/abs/2510.11178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\to$ Entity, (ii) an inverted text-only variant (Entity $\to$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice question (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing and, whilst visual cues often aid performance, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, particularly for lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs.</li>
</ul>

<h3>Title: Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?</h3>
<ul>
<li><strong>Authors: </strong>Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11184">https://arxiv.org/abs/2510.11184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11184">https://arxiv.org/pdf/2510.11184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11184]] Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?(https://arxiv.org/abs/2510.11184)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization. However, the generalization of tool-augmented reinforcement learning (RL) across diverse domains remains underexplored. In this work, we investigate the cross-domain generalization of an LLM agent equipped with a code interpreter tool, which is exclusively trained on mathematical problem-solving tasks. Despite the restricted training domain, we evaluate the agent's performance across several distinct reasoning domains. The results reveal that RL-based tool usage learned from mathematical tasks can be effectively transferred to complex tasks in other domains, enabling great task performance and high token efficiency. To facilitate this cross-domain transfer, we propose a Tool Generalization Reinforcement Learning (TGRL) framework designed to promote domain-agnostic learning and skill migration, encompassing: (i) a standardized tool interface that abstracts domain-specific nuances through consistent formatting and explicit termination, fostering transferable invocation patterns; (ii) a dual-component reward system that decomposes rewards to incentivize generalizable behaviors like tool efficiency and reasoning abstraction, ensuring alignment and robustness across domain shifts; and (iii) an XML-based prompt template that separates thinking, tool calls, and responses to encourage modular, domain-invariant planning and coherent multi-turn interactions. Extensive experiments across diverse benchmarks validate our approach, achieving state-of-the-art performance and highlighting the cross-domain potential of Tool RL for LLM reasoning.</li>
</ul>

<h3>Title: Protein as a Second Language for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xinhui Chen, Zuchao Li, Mengqi Gao, Yufeng Zhang, Chak Tou Leong, Haoyang Li, Jiaqi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11188">https://arxiv.org/abs/2510.11188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11188">https://arxiv.org/pdf/2510.11188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11188]] Protein as a Second Language for LLMs(https://arxiv.org/abs/2510.11188)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the "Protein-as-Second-Language" framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence-question-answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process, we curate a bilingual corpus of 79,926 protein-QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4, achieving up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.</li>
</ul>

<h3>Title: FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11190">https://arxiv.org/abs/2510.11190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11190">https://arxiv.org/pdf/2510.11190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11190]] FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models(https://arxiv.org/abs/2510.11190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) face an inherent trade-off between faithfulness and creativity, as different tasks require varying degrees of associative reasoning. However, existing methods lack the flexibility to modulate this reasoning strength, limiting MLLMs' adaptability across factual and creative scenarios. To bridge this gap, we propose equipping MLLMs with mechanisms that enable flexible control over associative reasoning. We begin by investigating the internal mechanisms underlying associative behavior in MLLMs and find that: (1) middle layers play a pivotal role in shaping model's associative tendencies, (2) modifying representations in these layers effectively regulates associative reasoning strength, and (3) hallucinations can be exploited to derive steering vectors that guide this modulation. Building on these findings, we introduce Flexible Association Control (FlexAC), a lightweight and training-free framework for modulating associative behavior in MLLMs. FlexAC first induces hallucination-guided intermediate representations to encode associative directions. Then, it selects high-association instances to construct effective associative steering vectors, whose strengths are adaptively calibrated to balance creative guidance with output stability. Finally, recognizing the multi-dimensional nature of associative reasoning, FlexAC incorporates task-specific associative vectors derived from a forward pass on a few target-domain samples, enabling models to follow diverse associative directions and better adapt to creative tasks. Notably, our method achieves up to a 5.8x improvement in creativity on Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing existing baselines and demonstrating its effectiveness in enabling flexible control over associative reasoning in MLLMs. Our code is available at this https URL.</li>
</ul>

<h3>Title: RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Vasilije Stambolic, Aritra Dhar, Lukas Cavigelli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11195">https://arxiv.org/abs/2510.11195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11195">https://arxiv.org/pdf/2510.11195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11195]] RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation(https://arxiv.org/abs/2510.11195)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.</li>
</ul>

<h3>Title: Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Marco Pintore, Giorgio Piras, Angelo Sotgiu, Maura Pintor, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11202">https://arxiv.org/abs/2510.11202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11202">https://arxiv.org/pdf/2510.11202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11202]] Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models(https://arxiv.org/abs/2510.11202)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, explainability</a></li>
<li><strong>Abstract: </strong>To address the extremely concerning problem of software vulnerability, system security is often entrusted to Machine Learning (ML) algorithms. Despite their now established detection capabilities, such models are limited by design to flagging the entire input source code function as vulnerable, rather than precisely localizing the concerned code lines. However, the detection granularity is crucial to support human operators during software development, ensuring that such predictions reflect the true code semantics to help debug, evaluate, and fix the detected vulnerabilities. To address this issue, recent work made progress toward improving the detector's localization ability, thus narrowing down the vulnerability detection "window" and providing more fine-grained predictions. Such approaches, however, implicitly disregard the presence of spurious correlations and biases in the data, which often predominantly influence the performance of ML algorithms. In this work, we investigate how detectors comply with this requirement by proposing an explainability-based evaluation procedure. Our approach, defined as Detection Alignment (DA), quantifies the agreement between the input source code lines that most influence the prediction and the actual localization of the vulnerability as per the ground truth. Through DA, which is model-agnostic and adaptable to different detection tasks, not limited to our use case, we analyze multiple learning-based vulnerability detectors and datasets. As a result, we show how the predictions of such models are consistently biased by non-vulnerable lines, ultimately highlighting the high impact of biases and spurious correlations. The code is available at this https URL.</li>
</ul>

<h3>Title: TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Liu, Bonan Ruan, Xianglin Yang, Zhiwei Lin, Yan Liu, Yang Wang, Tao Wei, Zhenkai Liang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11203">https://arxiv.org/abs/2510.11203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11203">https://arxiv.org/pdf/2510.11203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11203]] TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection(https://arxiv.org/abs/2510.11203)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>LLM-based agents have demonstrated promising adaptability in real-world applications. However, these agents remain vulnerable to a wide range of attacks, such as tool poisoning and malicious instructions, that compromise their execution flow and can lead to serious consequences like data breaches and financial loss. Existing studies typically attempt to mitigate such anomalies by predefining specific rules and enforcing them at runtime to enhance safety. Yet, designing comprehensive rules is difficult, requiring extensive manual effort and still leaving gaps that result in false negatives. As agent systems evolve into complex software systems, we take inspiration from software system security and propose TraceAegis, a provenance-based analysis framework that leverages agent execution traces to detect potential anomalies. In particular, TraceAegis constructs a hierarchical structure to abstract stable execution units that characterize normal agent behaviors. These units are then summarized into constrained behavioral rules that specify the conditions necessary to complete a task. By validating execution traces against both hierarchical and behavioral constraints, TraceAegis is able to effectively detect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we introduce TraceAegis-Bench, a dataset covering two representative scenarios: healthcare and corporate procurement. Each scenario includes 1,300 benign behaviors and 300 abnormal behaviors, where the anomalies either violate the agent's execution order or break the semantic consistency of its execution sequence. Experimental results demonstrate that TraceAegis achieves strong performance on TraceAegis-Bench, successfully identifying the majority of abnormal behaviors.</li>
</ul>

<h3>Title: Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos</h3>
<ul>
<li><strong>Authors: </strong>Rohit Gupta, Anirban Roy, Claire Christensen, Sujeong Kim, Sarah Gerard, Madeline Cincebeaux, Ajay Divakaran, Todd Grindal, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11204">https://arxiv.org/abs/2510.11204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11204">https://arxiv.org/pdf/2510.11204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11204]] Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos(https://arxiv.org/abs/2510.11204)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educational content for young learners. This paper presents an approach for detecting educational content in online videos. We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards. For example, literacy codes include `letter names', `letter sounds', and math codes include `counting', `sorting'. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., `letter names' vs `letter sounds'). We propose a novel class prototypes based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between visual and audio cues are crucial for effective comprehension, we consider a multimodal transformer network to capture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on APPROVE and other benchmarks such as Youtube-8M, and COIN. The dataset is available at this https URL</li>
</ul>

<h3>Title: Discursive Circuits: How Do Language Models Understand Discourse Relations?</h3>
<ul>
<li><strong>Authors: </strong>Yisong Miao, Min-Yen Kan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11210">https://arxiv.org/abs/2510.11210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11210">https://arxiv.org/pdf/2510.11210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11210]] Discursive Circuits: How Do Language Models Understand Discourse Relations?(https://arxiv.org/abs/2510.11210)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Which components in transformer language models are responsible for discourse understanding? We hypothesize that sparse computational graphs, termed as discursive circuits, control how models process discourse relations. Unlike simpler tasks, discourse relations involve longer spans and complex reasoning. To make circuit discovery feasible, we introduce a task called Completion under Discourse Relation (CuDR), where a model completes a discourse given a specified relation. To support this task, we construct a corpus of minimal contrastive pairs tailored for activation patching in circuit discovery. Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model) recover discourse understanding in the English PDTB-based CuDR task. These circuits generalize well to unseen discourse frameworks such as RST and SDRT. Further analysis shows lower layers capture linguistic features such as lexical semantics and coreference, while upper layers encode discourse-level abstractions. Feature utility is consistent across frameworks (e.g., coreference supports Expansion-like relations).</li>
</ul>

<h3>Title: Domain-Specific Data Generation Framework for RAG Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Chris Xing Tian, Weihao Xie, Zhen Chen, Zhengyuan Yi, Hui Liu, Haoliang Li, Shiqi Wang, Siwei Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11217">https://arxiv.org/abs/2510.11217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11217">https://arxiv.org/pdf/2510.11217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11217]] Domain-Specific Data Generation Framework for RAG Adaptation(https://arxiv.org/abs/2510.11217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) combines the language understanding and reasoning power of large language models (LLMs) with external retrieval to enable domain-grounded responses. Effectively adapting RAG systems to domain-specific settings requires specialized, context-rich training data beyond general-purpose question-answering. Here, we propose RAGen, a scalable and modular framework for generating domain-grounded question-answer-context (QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces these QAC triples by identifying key concepts in documents, generating diverse questions guided by Bloom's Taxonomy-inspired principles, and pairing them with precise answers extracted from relevant contexts. RAGen supports multiple RAG adaptation strategies, including the optimization of key components such as the LLM, retriever, and embedding model, etc. Its modular pipeline features semantic chunking, hierarchical concept extraction, and multi-chunk retrieval, along with the introduction of curated distractor contexts to promote robust reasoning. Designed for scalability, RAGen efficiently handles large and evolving document corpora without redundant processing, making it especially suitable for dynamic evolving domains such as scientific research and enterprise knowledge bases.</li>
</ul>

<h3>Title: The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers</h3>
<ul>
<li><strong>Authors: </strong>Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11218">https://arxiv.org/abs/2510.11218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11218">https://arxiv.org/pdf/2510.11218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11218]] The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers(https://arxiv.org/abs/2510.11218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can correctly answer "When was Einstein born?" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.</li>
</ul>

<h3>Title: Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Battemuulen Naranbat, Seyed Sahand Mohammadi Ziabari, Yousuf Nasser Al Husaini, Ali Mohammed Mansoor Alsahag</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11222">https://arxiv.org/abs/2510.11222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11222">https://arxiv.org/pdf/2510.11222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11222]] Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models(https://arxiv.org/abs/2510.11222)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .</li>
</ul>

<h3>Title: CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jinyuan Xu, Tian Lan, Xintao Yu, Xue He, Hezhi Zhang, Ying Wang, Pierre Magistry, Mathieu Valette, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11233">https://arxiv.org/abs/2510.11233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11233">https://arxiv.org/pdf/2510.11233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11233]] CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis(https://arxiv.org/abs/2510.11233)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Depression is a pressing global public health issue, yet publicly available Chinese-language resources for risk detection remain scarce and are mostly limited to binary classification. To address this limitation, we release CNSocialDepress, a benchmark dataset for depression risk detection from Chinese social media posts. The dataset contains 44,178 texts from 233 users, within which psychological experts annotated 10,306 depression-related segments. CNSocialDepress provides binary risk labels together with structured multi-dimensional psychological attributes, enabling interpretable and fine-grained analysis of depressive signals. Experimental results demonstrate its utility across a wide range of NLP tasks, including structured psychological profiling and fine-tuning of large language models for depression detection. Comprehensive evaluations highlight the dataset's effectiveness and practical value for depression risk identification and psychological analysis, thereby providing insights to mental health applications tailored for Chinese-speaking populations.</li>
</ul>

<h3>Title: XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression</h3>
<ul>
<li><strong>Authors: </strong>Haoqi Yang, Yao Yao, Zuchao Li, Baoyuan Qi, Guoming Liu, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11236">https://arxiv.org/abs/2510.11236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11236">https://arxiv.org/pdf/2510.11236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11236]] XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression(https://arxiv.org/abs/2510.11236)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their extensive memory requirements, particularly due to KV cache growth during long-text understanding and generation, present significant challenges for deployment in resource-constrained environments. Quantization has emerged as a promising solution to reduce memory consumption while preserving historical information. We propose XQuant, a training-free and plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key innovations: a computationally negligible data-free calibration method and cross-layer KV cache compression, enabling quantization to sub-1.4 bits. Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by achieving lower bit-width while maintaining superior performance, establishing a better trade-off between memory efficiency and model accuracy.</li>
</ul>

<h3>Title: Attacks by Content: Automated Fact-checking is an AI Security Issue</h3>
<ul>
<li><strong>Authors: </strong>Michael Schlichtkrull</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11238">https://arxiv.org/abs/2510.11238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11238">https://arxiv.org/pdf/2510.11238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11238]] Attacks by Content: Automated Fact-checking is an AI Security Issue(https://arxiv.org/abs/2510.11238)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>When AI agents retrieve and reason over external documents, adversaries can manipulate the data they receive to subvert their behaviour. Previous research has studied indirect prompt injection, where the attacker injects malicious instructions. We argue that injection of instructions is not necessary to manipulate agents - attackers could instead supply biased, misleading, or false information. We term this an attack by content. Existing defenses, which focus on detecting hidden commands, are ineffective against attacks by content. To defend themselves and their users, agents must critically evaluate retrieved information, corroborating claims with external evidence and evaluating source trustworthiness. We argue that this is analogous to an existing NLP task, automated fact-checking, which we propose to repurpose as a cognitive self-defense tool for agents.</li>
</ul>

<h3>Title: Learning the Structure of Connection Graphs</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Di Nino, Gabriele D'Acunto, Sergio Barbarossa, Paolo Di Lorenzo</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11245">https://arxiv.org/abs/2510.11245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11245">https://arxiv.org/pdf/2510.11245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11245]] Learning the Structure of Connection Graphs(https://arxiv.org/abs/2510.11245)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Connection graphs (CGs) extend traditional graph models by coupling network topology with orthogonal transformations, enabling the representation of global geometric consistency. They play a key role in applications such as synchronization, Riemannian signal processing, and neural sheaf diffusion. In this work, we address the inverse problem of learning CGs directly from observed signals. We propose a principled framework based on maximum pseudo-likelihood under a consistency assumption, which enforces spectral properties linking the connection Laplacian to the underlying combinatorial Laplacian. Based on this formulation, we introduce the Structured Connection Graph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian manifolds that jointly infers network topology, edge weights, and geometric structure. Our experiments show that SCGL consistently outperforms existing baselines in both topological recovery and geometric fidelity, while remaining computationally efficient.</li>
</ul>

<h3>Title: Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Zhu, Lijun Li, Yaxing Lyu, Li Sun, Sen Su, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11246">https://arxiv.org/abs/2510.11246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11246">https://arxiv.org/pdf/2510.11246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11246]] Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems(https://arxiv.org/abs/2510.11246)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>LLM-based multi-agent systems (MAS) demonstrate increasing integration into next-generation applications, but their safety in backdoor attacks remains largely underexplored. However, existing research has focused exclusively on single-agent backdoor attacks, overlooking the novel attack surfaces introduced by agent collaboration in MAS. To bridge this gap, we present the first Distributed Backdoor Attack tailored to MAS. We decompose the backdoor into multiple distributed attack primitives that are embedded within MAS tools. These primitives remain dormant individually but collectively activate only when agents collaborate in a specific sequence, thereby assembling the full backdoor to execute targeted attacks such as data exfiltration. To fully assess this threat, we introduce a benchmark for multi-role collaborative tasks and a sandboxed framework to evaluate. Extensive experiments demonstrate that our attack achieves an attack success rate exceeding 95% without degrading performance on benign tasks. This work exposes novel backdoor attack surfaces that exploit agent collaboration, underscoring the need to move beyond single-agent protection. Code and benchmark are available at this https URL.</li>
</ul>

<h3>Title: Large Language Models Are Effective Code Watermarkers</h3>
<ul>
<li><strong>Authors: </strong>Rui Xu, Jiawei Chen, Zhaoxia Yin, Cong Kong, Xinpeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11251">https://arxiv.org/abs/2510.11251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11251">https://arxiv.org/pdf/2510.11251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11251]] Large Language Models Are Effective Code Watermarkers(https://arxiv.org/abs/2510.11251)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, extraction, watermark, large language model</a></li>
<li><strong>Abstract: </strong>The widespread use of large language models (LLMs) and open-source code has raised ethical and security concerns regarding the distribution and attribution of source code, including unauthorized redistribution, license violations, and misuse of code for malicious purposes. Watermarking has emerged as a promising solution for source attribution, but existing techniques rely heavily on hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or task-specific training, limiting their scalability and generality across languages. Moreover, their robustness against attacks remains limited. To address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking framework that embeds watermark into source code without compromising its semantics or readability. CodeMark-LLM consists of two core components: (i) Semantically Consistent Embedding module that applies functionality-preserving transformations to encode watermark bits, and (ii) Differential Comparison Extraction module that identifies the applied transformations by comparing the original and watermarked code. Leveraging the cross-lingual generalization ability of LLM, CodeMark-LLM avoids language-specific engineering and training pipelines. Extensive experiments across diverse programming languages and attack scenarios demonstrate its robustness, effectiveness, and scalability.</li>
</ul>

<h3>Title: Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality</h3>
<ul>
<li><strong>Authors: </strong>Jana Jung, Marlene Lutz, Indira Sen, Markus Strohmaier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11254">https://arxiv.org/abs/2510.11254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11254">https://arxiv.org/pdf/2510.11254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11254]] Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality(https://arxiv.org/abs/2510.11254)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests is essential before interpreting their scores. They also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.</li>
</ul>

<h3>Title: DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Li, Quanjun Li, Guang Yu, Song Yang, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11259">https://arxiv.org/abs/2510.11259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11259">https://arxiv.org/pdf/2510.11259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11259]] DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation(https://arxiv.org/abs/2510.11259)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In medical image segmentation, skip connections are used to merge global context and reduce the semantic gap between encoder and decoder. Current methods often struggle with limited structural representation and insufficient contextual modeling, affecting generalization in complex clinical scenarios. We propose the DTEA model, featuring a new skip connection framework with the Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG) modules. STR reorganizes multi-scale semantic features into a dynamic hypergraph to better model cross-resolution anatomical dependencies, enhancing structural and semantic representation. EPG assesses channel stability after perturbation and filters high-entropy channels to emphasize clinically important regions and improve spatial attention. Extensive experiments on three benchmark datasets show our framework achieves superior segmentation accuracy and better generalization across various clinical settings. The code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Chen, Ruotong Yang, Zhengyang Zhang, Mehreen Ahmed, Yanming Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.AI, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11260">https://arxiv.org/abs/2510.11260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11260">https://arxiv.org/pdf/2510.11260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11260]] A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images(https://arxiv.org/abs/2510.11260)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Microscopic characterizations, such as Scanning Electron Microscopy (SEM), are widely used in scientific research for visualizing and analyzing microstructures. Determining the scale bars is an important first step of accurate SEM analysis; however, currently, it mainly relies on manual operations, which is both time-consuming and prone to errors. To address this issue, we propose a multi-modal and automated scale bar detection and extraction framework that provides concurrent object detection, text detection and text recognition with a Large Language Model (LLM) agent. The proposed framework operates in four phases; i) Automatic Dataset Generation (Auto-DG) model to synthesize a diverse dataset of SEM images ensuring robust training and high generalizability of the model, ii) scale bar object detection, iii) information extraction using a hybrid Optical Character Recognition (OCR) system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The proposed model demonstrates a strong performance in object detection and accurate localization with a precision of 100%, recall of 95.8%, and a mean Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the Auto-DG dataset, significantly outperforming several mainstream standalone engines, highlighting its reliability for scientific image analysis. The LLM is introduced as a reasoning engine as well as an intelligent assistant that suggests follow-up steps and verifies the results. This automated method powered by an LLM agent significantly enhances the efficiency and accuracy of scale bar detection and extraction in SEM images, providing a valuable tool for microscopic analysis and advancing the field of scientific imaging.</li>
</ul>

<h3>Title: Exploring and Leveraging Class Vectors for Classifier Editing</h3>
<ul>
<li><strong>Authors: </strong>Jaeik Kim, Jaeyoung Do</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11268">https://arxiv.org/abs/2510.11268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11268">https://arxiv.org/pdf/2510.11268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11268]] Exploring and Leveraging Class Vectors for Classifier Editing(https://arxiv.org/abs/2510.11268)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Image classifiers play a critical role in detecting diseases in medical imaging and identifying anomalies in manufacturing processes. However, their predefined behaviors after extensive training make post hoc model editing difficult, especially when it comes to forgetting specific classes or adapting to distribution shifts. Existing classifier editing methods either focus narrowly on correcting errors or incur extensive retraining costs, creating a bottleneck for flexible editing. Moreover, such editing has seen limited investigation in image classification. To overcome these challenges, we introduce Class Vectors, which capture class-specific representation adjustments during fine-tuning. Whereas task vectors encode task-level changes in weight space, Class Vectors disentangle each class's adaptation in the latent space. We show that Class Vectors capture each class's semantic shift and that classifier editing can be achieved either by steering latent features along these vectors or by mapping them into weight space to update the decision boundaries. We also demonstrate that the inherent linearity and orthogonality of Class Vectors support efficient, flexible, and high-level concept editing via simple class arithmetic. Finally, we validate their utility in applications such as unlearning, environmental adaptation, adversarial defense, and adversarial trigger optimization.</li>
</ul>

<h3>Title: FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local Optimization in Heterogeneous Data Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Jianzhe Zhao, Hailin Zhu, Yu Zhang, Ziqi Chen, Guibing Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11274">https://arxiv.org/abs/2510.11274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11274">https://arxiv.org/pdf/2510.11274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11274]] FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local Optimization in Heterogeneous Data Scenarios(https://arxiv.org/abs/2510.11274)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated efficient fine-tuning has emerged as an approach that leverages distributed data and computational resources across nodes to address the challenges of large-scale fine-tuning and privacy preservation. The Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large-scale pre-trained models by introducing trainable low-rank matrices into weight this http URL, in heterogeneous data scenarios, client drift weakens the generalization of the global model, and local models often fail to meet the personalized needs of individual this http URL, existing federated LoRA efficient fine-tuning techniques overlook fine-grained analysis of the tuning matrices. To address this, we conducted preliminary experiments and found that different LoRA matrices exhibit different sensitivity to changes in the direction and magnitude of their this http URL thus propose a fine-grained federated LoRA tuning method. By fine-tuning the more sensitive directional vectors in the A matrix, which encode shared knowledge, our method learns shared features more effectively across clients and enhances global generalization. Simultaneously, by fine-tuning the more sensitive magnitude vectors in the B matrix, which encode personalized knowledge, our method better captures personalized knowledge, enabling detailed adaptation to local data. The method uses a pipeline combining global and local optimizers. Global optimization further improves local models, achieving collaborative optimization between global and local levels. This improves both the generalization ability of the global model and the personalized adaptation of local models under heterogeneous data scenarios. Experiments on Databricks-Dolly-15k and Natural Instructions with LLaMA2-7B and Deepseek-7B confirm that our method improves global performance by 0.39% and local performance by 0.59%.</li>
</ul>

<h3>Title: Towards Real-Time Fake News Detection under Evidence Scarcity</h3>
<ul>
<li><strong>Authors: </strong>Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11277">https://arxiv.org/abs/2510.11277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11277">https://arxiv.org/pdf/2510.11277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11277]] Towards Real-Time Fake News Detection under Evidence Scarcity(https://arxiv.org/abs/2510.11277)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluators' justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: this https URL.</li>
</ul>

<h3>Title: ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gareth Seneque, Lap-Hang Ho, Nafise Erfanian Saeedi, Jeffrey Molendijk, Ariel Kupermann, Tim Elson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11278">https://arxiv.org/abs/2510.11278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11278">https://arxiv.org/pdf/2510.11278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11278]] ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models(https://arxiv.org/abs/2510.11278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present Entropic Mutual-Information Geometry Large-Language Model Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training that jointly improves reasoning, alignment and robustness by treating an organisation's policies/principles as directions to move on a model's information manifold. Our single-loop trainer combines Group-Relative Policy Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought (CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information (SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn optimal-transport regulariser on hidden-state distributions to bound geometry drift. We also introduce infoNCE metrics that specialise to a standard MI lower bound under matched negatives to measure how strongly a model's CoT encodes these policies. These metrics include a Sufficiency Index (SI) that enables the selection and creation of principles that maximise downstream performance prior to training. In our experiments using small (1B) LLMs, high-SI principles predict steadier training dynamics and improved benchmark performance over GRPO ablations. Our information-geometry analysis of trained models validates desirable structural change in the manifold. These results support our hypothesis that reasoning, alignment, and robustness are projections of a single informationgeometric objective, and that models trained using ENIGMA demonstrate principled reasoning without the use of a reward model, offering a path to trusted capability</li>
</ul>

<h3>Title: Vision-LLMs for Spatiotemporal Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ning Yang, Hengyu Zhong, Haijun Zhang, Randall Berry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11282">https://arxiv.org/abs/2510.11282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11282">https://arxiv.org/pdf/2510.11282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11282]] Vision-LLMs for Spatiotemporal Traffic Forecasting(https://arxiv.org/abs/2510.11282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate spatiotemporal traffic forecasting is a critical prerequisite for proactive resource management in dense urban mobile networks. While Large Language Models (LLMs) have shown promise in time series analysis, they inherently struggle to model the complex spatial dependencies of grid-based traffic data. Effectively extending LLMs to this domain is challenging, as representing the vast amount of information from dense geographical grids can be inefficient and overwhelm the model's context. To address these challenges, we propose ST-Vision-LLM, a novel framework that reframes spatiotemporal forecasting as a vision-language fusion problem. Our approach leverages a Vision-LLM visual encoder to process historical global traffic matrices as image sequences, providing the model with a comprehensive global view to inform cell-level predictions. To overcome the inefficiency of LLMs in handling numerical data, we introduce an efficient encoding scheme that represents floating-point values as single tokens via a specialized vocabulary, coupled with a two-stage numerical alignment fine-tuning process. The model is first trained with Supervised Fine-Tuning (SFT) and then further optimized for predictive accuracy using Group Relative Policy Optimization (GRPO), a memory-efficient reinforcement learning method. Evaluations on real-world mobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing methods by 15.6% in long-term prediction accuracy and exceeds the second-best baseline by over 30.04% in cross-domain few-shot scenarios. Our extensive experiments validate the model's strong generalization capabilities across various data-scarce environments.</li>
</ul>

<h3>Title: EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Han Xia, Quanjun Li, Qian Li, Zimeng Li, Hongbin Ye, Yupeng Liu, Haolun Li, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11287">https://arxiv.org/abs/2510.11287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11287">https://arxiv.org/pdf/2510.11287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11287]] EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism(https://arxiv.org/abs/2510.11287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is vital for diagnosis, treatment planning, and disease monitoring but is challenged by complex factors like ambiguous edges and background noise. We introduce EEMS, a new model for segmentation, combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency feature extraction, accurately defining boundaries. MSPGU integrates high-level semantic and low-level spatial features using a prompt-guided approach, ensuring precise target localization. The Dual-Source Adaptive Gated Fusion Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU, enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018 confirm EEMS's superior performance and reliability as a clinical tool.</li>
</ul>

<h3>Title: $Δ\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization</h3>
<ul>
<li><strong>Authors: </strong>Lin Zhu, Yifeng Yang, Xinbing Wang, Qinying Gu, Nanyang Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11296">https://arxiv.org/abs/2510.11296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11296">https://arxiv.org/pdf/2510.11296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11296]] $Δ\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization(https://arxiv.org/abs/2510.11296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent approaches for vision-language models (VLMs) have shown remarkable success in achieving fast downstream adaptation. When applied to real-world downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data and out-of-distribution (OOD) data. The OOD datasets often include both covariate shifts (e.g., known classes with changes in image styles) and semantic shifts (e.g., test-time unseen classes). This highlights the importance of improving VLMs' generalization ability to covariate-shifted OOD data, while effectively detecting open-set semantic-shifted OOD classes. In this paper, inspired by the substantial energy change observed in closed-set data when re-aligning vision-language modalities (specifically by directly reducing the maximum cosine similarity to a low value), we introduce a novel OOD score, named {\Delta}Energy. {\Delta}Energy significantly outperforms the vanilla energy-based OOD score and provides a more reliable approach for OOD detection. Furthermore, {\Delta}Energy can simultaneously improve OOD generalization under covariate shifts, which is achieved by lower-bound maximization for {\Delta}Energy (termed EBM). EBM is theoretically proven to not only enhance OOD detection but also yields a domain-consistent Hessian, which serves as a strong indicator for OOD generalization. Based on this finding, we developed a unified fine-tuning framework that allows for improving VLMs' robustness in both OOD generalization and OOD detection. Extensive experiments on challenging OOD detection and generalization benchmarks demonstrate the superiority of our method, outperforming recent approaches by 10% to 25% in AUROC.</li>
</ul>

<h3>Title: Are Large Language Models Effective Knowledge Graph Constructors?</h3>
<ul>
<li><strong>Authors: </strong>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Bo Xiong, Fiona Liausvia, Dongkyu Choi, Boon Kiat Quek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11297">https://arxiv.org/abs/2510.11297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11297">https://arxiv.org/pdf/2510.11297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11297]] Are Large Language Models Effective Knowledge Graph Constructors?(https://arxiv.org/abs/2510.11297)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown promise in reducing hallucinations in large language models (LLMs). However, constructing high-quality KGs remains difficult, requiring accurate information extraction and structured representations that support interpretability and downstream utility. Existing LLM-based approaches often focus narrowly on entity and relation extraction, limiting coverage to sentence-level contexts or relying on predefined schemas. We propose a hierarchical extraction framework that organizes information at multiple levels, enabling the creation of semantically rich and well-structured KGs. Using state-of-the-art LLMs, we extract and construct knowledge graphs and evaluate them comprehensively from both structural and semantic perspectives. Our results highlight the strengths and shortcomings of current LLMs in KG construction and identify key challenges for future work. To advance research in this area, we also release a curated dataset of LLM-generated KGs derived from research papers on children's mental well-being. This resource aims to foster more transparent, reliable, and impactful applications in high-stakes domains such as healthcare.</li>
</ul>

<h3>Title: How to Get Actual Privacy and Utility from Privacy Models: the k-Anonymity and Differential Privacy Families</h3>
<ul>
<li><strong>Authors: </strong>Josep Domingo-Ferrer, David Sánchez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11299">https://arxiv.org/abs/2510.11299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11299">https://arxiv.org/pdf/2510.11299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11299]] How to Get Actual Privacy and Utility from Privacy Models: the k-Anonymity and Differential Privacy Families(https://arxiv.org/abs/2510.11299)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Privacy models were introduced in privacy-preserving data publishing and statistical disclosure control with the promise to end the need for costly empirical assessment of disclosure risk. We examine how well this promise is kept by the main privacy models. We find they may fail to provide adequate protection guarantees because of problems in their definition or incur unacceptable trade-offs between privacy protection and utility preservation. Specifically, k-anonymity may not entirely exclude disclosure if enforced with deterministic mechanisms or without constraints on the confidential values. On the other hand, differential privacy (DP) incurs unacceptable utility loss for small budgets and its privacy guarantee becomes meaningless for large budgets. In the latter case, an ex post empirical assessment of disclosure risk becomes necessary, undermining the main appeal of privacy models. Whereas the utility preservation of DP can only be improved by relaxing its privacy guarantees, we argue that a semantic reformulation of k-anonymity can offer more robust privacy without losing utility with respect to traditional syntactic k-anonymity.</li>
</ul>

<h3>Title: TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security</h3>
<ul>
<li><strong>Authors: </strong>Junhua Zhou, Quanjun Li, Weixuan Li, Guang Yu, Yihua Shao, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11301">https://arxiv.org/abs/2510.11301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11301">https://arxiv.org/pdf/2510.11301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11301]] TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security(https://arxiv.org/abs/2510.11301)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, diffusion</a></li>
<li><strong>Abstract: </strong>The rise of digital medical imaging, like MRI and CT, demands strong encryption to protect patient data in telemedicine and cloud storage. Chaotic systems are popular for image encryption due to their sensitivity and unique characteristics, but existing methods often lack sufficient security. This paper presents the Three-dimensional Diffusion Algorithm and Deep Learning Image Encryption system (TDADL-IE), built on three key elements. First, we propose an enhanced chaotic generator using an LSTM network with a 1D-Sine Quadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation. Next, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt permuted images. TDADL-IE is versatile for images of any size. Experiments confirm its effectiveness against various security threats. The code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sabrina McCallum, Amit Parekh, Alessandro Suglia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11307">https://arxiv.org/abs/2510.11307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11307">https://arxiv.org/pdf/2510.11307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11307]] FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks(https://arxiv.org/abs/2510.11307)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents' compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks.</li>
</ul>

<h3>Title: Do LLMs "Feel"? Emotion Circuits Discovery and Control</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Wang, Yixuan Zhang, Ruiji Yu, Yufei Zheng, Lang Gao, Zirui Song, Zixiang Xu, Gus Xia, Huishuai Zhang, Dongyan Zhao, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11328">https://arxiv.org/abs/2510.11328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11328">https://arxiv.org/pdf/2510.11328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11328]] Do LLMs "Feel"? Emotion Circuits Discovery and Control(https://arxiv.org/abs/2510.11328)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.</li>
</ul>

<h3>Title: DiffStyleTS: Diffusion Model for Style Transfer in Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mayank Nagda, Phil Ostheimer, Justus Arweiler, Indra Jungjohann, Jennifer Werner, Dennis Wagner, Aparna Muraleedharan, Pouya Jafari, Jochen Schmid, Fabian Jirasek, Jakob Burger, Michael Bortz, Hans Hasse, Stephan Mandt, Marius Kloft, Sophie Fellenz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11335">https://arxiv.org/abs/2510.11335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11335">https://arxiv.org/pdf/2510.11335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11335]] DiffStyleTS: Diffusion Model for Style Transfer in Time Series(https://arxiv.org/abs/2510.11335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Style transfer combines the content of one signal with the style of another. It supports applications such as data augmentation and scenario simulation, helping machine learning models generalize in data-scarce domains. While well developed in vision and language, style transfer methods for time series data remain limited. We introduce DiffTSST, a diffusion-based framework that disentangles a time series into content and style representations via convolutional encoders and recombines them through a self-supervised attention-based diffusion process. At inference, encoders extract content and style from two distinct series, enabling conditional generation of novel samples to achieve style transfer. We demonstrate both qualitatively and quantitatively that DiffTSST achieves effective style transfer. We further validate its real-world utility by showing that data augmentation with DiffTSST improves anomaly detection in data-scarce regimes.</li>
</ul>

<h3>Title: REACT3D: Recovering Articulations for Interactive Physical 3D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Zhao Huang, Boyang Sun, Alexandros Delitzas, Jiaqi Chen, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11340">https://arxiv.org/abs/2510.11340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11340">https://arxiv.org/pdf/2510.11340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11340]] REACT3D: Recovering Articulations for Interactive Physical 3D Scenes(https://arxiv.org/abs/2510.11340)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive 3D scenes are increasingly vital for embodied intelligence, yet existing datasets remain limited due to the labor-intensive process of annotating part segmentation, kinematic types, and motion trajectories. We present REACT3D, a scalable zero-shot framework that converts static 3D scenes into simulation-ready interactive replicas with consistent geometry, enabling direct use in diverse downstream tasks. Our contributions include: (i) openable-object detection and segmentation to extract candidate movable parts from static scenes, (ii) articulation estimation that infers joint types and motion parameters, (iii) hidden-geometry completion followed by interactive object assembly, and (iv) interactive scene integration in widely supported formats to ensure compatibility with standard simulation platforms. We achieve state-of-the-art performance on detection/segmentation and articulation metrics across diverse indoor scenes, demonstrating the effectiveness of our framework and providing a practical foundation for scalable interactive scene generation, thereby lowering the barrier to large-scale research on articulated scene understanding. Our project page is \textit{\hypersetup{urlcolor=black}\href{this https URL}{this http URL}}.</li>
</ul>

<h3>Title: InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11341">https://arxiv.org/abs/2510.11341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11341">https://arxiv.org/pdf/2510.11341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11341]] InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models(https://arxiv.org/abs/2510.11341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.</li>
</ul>

<h3>Title: TBRD: TESLA Authenticated UAS Broadcast Remote ID</h3>
<ul>
<li><strong>Authors: </strong>Jason Veara, Manav Jain, Kyle Moy, Aanjhan Ranganathan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11343">https://arxiv.org/abs/2510.11343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11343">https://arxiv.org/pdf/2510.11343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11343]] TBRD: TESLA Authenticated UAS Broadcast Remote ID(https://arxiv.org/abs/2510.11343)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military facilities, suburban neighborhoods, and commercial airports have intensified scrutiny of drone activity. To increase accountability, the Federal Aviation Administration (FAA) introduced a Remote ID mandate, requiring unmanned aircraft to broadcast their location, operator's location, and identity in real-time. However, current standards leave authentication mechanisms underspecified, enabling spoofing, relay, and replay attacks that can undermine surveillance efforts and potentially disrupt UAS-to-UAS coordination in future deployments. In this paper, we propose TBRD, a practical system for authenticating Remote ID messages in a manner that aligns with existing standards and UAS capabilities. TBRD leverages the TESLA protocol and mobile device TEEs, and introduces a verification mechanism to build a lightweight, mission-scoped authentication system that is both computationally efficient and requires a low communication footprint. We evaluate the performance of TBRD using both an FAA-requirements compatible proof-of-concept implementation for performance metrics and a simulated 4-drone swarm mission scenario to demonstrate its security guarantees under adversarial conditions. Our system provides a 50\% reduction in authentication overhead compared to digital signatures and a 100x reduction in computation time. Our results demonstrate that TBRD can be integrated into current Remote ID infrastructures to provide a scalable, standards-compliant message authentication for both regulatory and operational use cases.</li>
</ul>

<h3>Title: MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression</h3>
<ul>
<li><strong>Authors: </strong>Hai Dang Nguyen, Nguyen Dang Huy Pham, The Minh Duc Nguyen, Dac Thai Nguyen, Hang Thi Nguyen, Duong M. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11344">https://arxiv.org/abs/2510.11344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11344">https://arxiv.org/pdf/2510.11344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11344]] MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression(https://arxiv.org/abs/2510.11344)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Spatial Transcriptomics (ST) enables the measurement of gene expression while preserving spatial information, offering critical insights into tissue architecture and disease pathology. Recent developments have explored the use of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict transcriptome-wide gene expression profiles through deep neural networks. This task is commonly framed as a regression problem, where each input corresponds to a localized image patch extracted from the WSI. However, predicting spatial gene expression from histological images remains a challenging problem due to the significant modality gap between visual features and molecular signals. Recent studies have attempted to incorporate both local and global information into predictive models. Nevertheless, existing methods still suffer from two key limitations: (1) insufficient granularity in local feature extraction, and (2) inadequate coverage of global spatial context. In this work, we propose a novel framework, MMAP (Multi-MAgnification and Prototype-enhanced architecture), that addresses both challenges simultaneously. To enhance local feature granularity, MMAP leverages multi-magnification patch representations that capture fine-grained histological details. To improve global contextual understanding, it learns a set of latent prototype embeddings that serve as compact representations of slide-level information. Extensive experimental results demonstrate that MMAP consistently outperforms all existing state-of-the-art methods across multiple evaluation metrics, including Mean Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC).</li>
</ul>

<h3>Title: Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony</h3>
<ul>
<li><strong>Authors: </strong>Han Lu, Zichen Liu, Shaopan Xiong, Yancheng He, Wei Gao, Yanan Wu, Weixun Wang, Jiashun Liu, Yang Li, Haizhou Zhao, Ju Huang, Siran Yang, Xiaoyang Li, Yijia Luo, Zihe Liu, Ling Pan, Junchi Yan, Wei Wang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11345">https://arxiv.org/abs/2510.11345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11345">https://arxiv.org/pdf/2510.11345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11345]] Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony(https://arxiv.org/abs/2510.11345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Synchronous Reinforcement Learning (RL) post-training has emerged as a crucial step for enhancing Large Language Models (LLMs) with diverse capabilities. However, many systems designed to accelerate RL post-training still suffer from low resource utilization and limited scalability. We present ROLL Flash, a system that extends ROLL with native support for asynchronous RL post-training. ROLL Flash is built upon two core design principles: fine-grained parallelism and rollout-train decoupling. Guided by these principles, ROLL Flash provides flexible programming interfaces that enable a fully asynchronous training architecture and support efficient rollout mechanisms, including queue scheduling and environment-level asynchronous execution. Through comprehensive theoretical analysis and extensive experiments, we demonstrate that ROLL Flash significantly improves resource utilization and scalability over synchronous RL post-training. ROLL Flash achieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using the same GPU budget as synchronous baselines. Furthermore, we implement several popular off-policy algorithms and verify that asynchronous training can achieve performance on par with synchronous training.</li>
</ul>

<h3>Title: Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Joshua Niemeijer, Jan Ehrhardt, Heinz Handels, Hristina Uzunova</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11346">https://arxiv.org/abs/2510.11346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11346">https://arxiv.org/pdf/2510.11346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11346]] Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation(https://arxiv.org/abs/2510.11346)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Generative Models are a valuable tool for the controlled creation of high-quality image data. Controlled diffusion models like the ControlNet have allowed the creation of labeled distributions. Such synthetic datasets can augment the original training distribution when discriminative models, like semantic segmentation, are trained. However, this augmentation effect is limited since ControlNets tend to reproduce the original training distribution. This work introduces a method to utilize data from unlabeled domains to train ControlNets by introducing the concept of uncertainty into the control mechanism. The uncertainty indicates that a given image was not part of the training distribution of a downstream task, e.g., segmentation. Thus, two types of control are engaged in the final network: an uncertainty control from an unlabeled dataset and a semantic control from the labeled dataset. The resulting ControlNet allows us to create annotated data with high uncertainty from the target domain, i.e., synthetic data from the unlabeled distribution with labels. In our scenario, we consider retinal OCTs, where typically high-quality Spectralis images are available with given ground truth segmentations, enabling the training of segmentation networks. The recent development in Home-OCT devices, however, yields retinal OCTs with lower quality and a large domain shift, such that out-of-the-pocket segmentation networks cannot be applied for this type of data. Synthesizing annotated images from the Home-OCT domain using the proposed approach closes this gap and leads to significantly improved segmentation results without adding any further supervision. The advantage of uncertainty-guidance becomes obvious when compared to style transfer: it enables arbitrary domain shifts without any strict learning of an image style. This is also demonstrated in a traffic scene experiment.</li>
</ul>

<h3>Title: Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Etzion Harari, Moshe Unger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11347">https://arxiv.org/abs/2510.11347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11347">https://arxiv.org/pdf/2510.11347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11347]] Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity(https://arxiv.org/abs/2510.11347)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable success in node classification tasks over relational data, yet their effectiveness often depends on the availability of complete node features. In many real-world scenarios, however, feature matrices are highly sparse or contain sensitive information, leading to degraded performance and increased privacy risks. Furthermore, direct exposure of information can result in unintended data leakage, enabling adversaries to infer sensitive information. To address these challenges, we propose a novel Multi-view Feature Propagation (MFP) framework that enhances node classification under feature sparsity while promoting privacy preservation. MFP extends traditional Feature Propagation (FP) by dividing the available features into multiple Gaussian-noised views, each propagating information independently through the graph topology. The aggregated representations yield expressive and robust node embeddings. This framework is novel in two respects: it introduces a mechanism that improves robustness under extreme sparsity, and it provides a principled way to balance utility with privacy. Extensive experiments conducted on graph datasets demonstrate that MFP outperforms state-of-the-art baselines in node classification while substantially reducing privacy leakage. Moreover, our analysis demonstrates that propagated outputs serve as alternative imputations rather than reconstructions of the original features, preserving utility without compromising privacy. A comprehensive sensitivity analysis further confirms the stability and practical applicability of MFP across diverse scenarios. Overall, MFP provides an effective and privacy-aware framework for graph learning in domains characterized by missing or sensitive features.</li>
</ul>

<h3>Title: LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11358">https://arxiv.org/abs/2510.11358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11358">https://arxiv.org/pdf/2510.11358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11358]] LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation(https://arxiv.org/abs/2510.11358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.</li>
</ul>

<h3>Title: Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, Fuli Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11370">https://arxiv.org/abs/2510.11370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11370">https://arxiv.org/pdf/2510.11370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11370]] Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers(https://arxiv.org/abs/2510.11370)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.</li>
</ul>

<h3>Title: Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Dean L. Slack, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11372">https://arxiv.org/abs/2510.11372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11372">https://arxiv.org/pdf/2510.11372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11372]] Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning(https://arxiv.org/abs/2510.11372)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although large language models excel across many tasks, they can memorise training data and thereby expose private or copyrighted text. Most defences target the pre-training stage, leaving memorisation during fine-tuning, especially for domain adaptation and instruction tuning, poorly understood. We fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on common evaluation datasets and track verbatim memorisation throughout training. We find that memorisation increases dramatically in the first few epochs, often significantly before either validation perplexity or evaluation performance is optimised. We use a simple but effective n-gram memorisation score which reliably precedes verbatim memorisation; using it as an early-stopping criterion mitigates memorisation with minimal performance loss. Further, we introduce an n-gram-aware loss regulariser and show that it reduces memorisation across all model families tested by up to 40% while minimising evaluation performance trade-offs when compared to an existing memorisation mitigation strategy. These results yield practical, scalable insights into memorisation dynamics during language model fine-tuning.</li>
</ul>

<h3>Title: Medical Interpretability and Knowledge Maps of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Razvan Marinescu, Victoria-Elisabeth Gruber, Diego Fajardo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11390">https://arxiv.org/abs/2510.11390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11390">https://arxiv.org/pdf/2510.11390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11390]] Medical Interpretability and Knowledge Maps of Large Language Models(https://arxiv.org/abs/2510.11390)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We present a systematic study of medical-domain interpretability in Large Language Models (LLMs). We study how the LLMs both represent and process medical knowledge through four different interpretability techniques: (1) UMAP projections of intermediate activations, (2) gradient-based saliency with respect to the model weights, (3) layer lesioning/removal and (4) activation patching. We present knowledge maps of five LLMs which show, at a coarse-resolution, where knowledge about patient's ages, medical symptoms, diseases and drugs is stored in the models. In particular for Llama3.3-70B, we find that most medical knowledge is processed in the first half of the model's layers. In addition, we find several interesting phenomena: (i) age is often encoded in a non-linear and sometimes discontinuous manner at intermediate layers in the models, (ii) the disease progression representation is non-monotonic and circular at certain layers of the model, (iii) in Llama3.3-70B, drugs cluster better by medical specialty rather than mechanism of action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B have activations that collapse at intermediate layers but recover by the final layers. These results can guide future research on fine-tuning, un-learning or de-biasing LLMs for medical tasks by suggesting at which layers in the model these techniques should be applied.</li>
</ul>

<h3>Title: Living Off the LLM: How LLMs Will Change Adversary Tactics</h3>
<ul>
<li><strong>Authors: </strong>Sean Oesch, Jack Hutchins, Luke Koch, Kevin Kurian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11398">https://arxiv.org/abs/2510.11398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11398">https://arxiv.org/pdf/2510.11398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11398]] Living Off the LLM: How LLMs Will Change Adversary Tactics(https://arxiv.org/abs/2510.11398)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In living off the land attacks, malicious actors use legitimate tools and processes already present on a system to avoid detection. In this paper, we explore how the on-device LLMs of the future will become a security concern as threat actors integrate LLMs into their living off the land attack pipeline and ways the security community may mitigate this threat.</li>
</ul>

<h3>Title: FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management</h3>
<ul>
<li><strong>Authors: </strong>Kahou Tam, Chunlin Tian, Li Li, Haikai Zhao, ChengZhong Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11400">https://arxiv.org/abs/2510.11400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11400">https://arxiv.org/pdf/2510.11400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11400]] FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management(https://arxiv.org/abs/2510.11400)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) emerges as a new learning paradigm that enables multiple devices to collaboratively train a shared model while preserving data privacy. However, one fundamental and prevailing challenge that hinders the deployment of FL on mobile devices is the memory limitation. This paper proposes \textit{FedHybrid}, a novel framework that effectively reduces the memory footprint during the training process while guaranteeing the model accuracy and the overall training progress. Specifically, \textit{FedHybrid} first selects the participating devices for each training round by jointly evaluating their memory budget, computing capability, and data diversity. After that, it judiciously analyzes the computational graph and generates an execution plan for each selected client in order to meet the corresponding memory budget while minimizing the training delay through employing a hybrid of recomputation and compression techniques according to the characteristic of each tensor. During the local training process, \textit{FedHybrid} carries out the execution plan with a well-designed activation compression technique to effectively achieve memory reduction with minimum accuracy loss. We conduct extensive experiments to evaluate \textit{FedHybrid} on both simulation and off-the-shelf mobile devices. The experiment results demonstrate that \textit{FedHybrid} achieves up to a 39.1\% increase in model accuracy and a 15.5$\times$ reduction in wall clock time under various memory budgets compared with the baselines.</li>
</ul>

<h3>Title: Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification</h3>
<ul>
<li><strong>Authors: </strong>Stefan Krsteski, Giuseppe Russo, Serina Chang, Robert West, Kristina Gligorić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11408">https://arxiv.org/abs/2510.11408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11408">https://arxiv.org/pdf/2510.11408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11408]] Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification(https://arxiv.org/abs/2510.11408)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Surveys provide valuable insights into public opinion and behavior, but their execution is costly and slow. Large language models (LLMs) have been proposed as a scalable, low-cost substitute for human respondents, but their outputs are often biased and yield invalid estimates. We study the interplay between synthesis methods that use LLMs to generate survey responses and rectification methods that debias population estimates, and explore how human responses are best allocated between them. Using two panel surveys with questions on nutrition, politics, and economics, we find that synthesis alone introduces substantial bias (24-86%), whereas combining it with rectification reduces bias below 5% and increases effective sample size by up to 14%. Overall, we challenge the common practice of using all human responses for fine-tuning, showing that under a fixed budget, allocating most to rectification results in far more effective estimation.</li>
</ul>

<h3>Title: Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews</h3>
<ul>
<li><strong>Authors: </strong>Lucas Joos, Daniel A. Keim, Maximilian T. Fischer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11409">https://arxiv.org/abs/2510.11409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11409">https://arxiv.org/pdf/2510.11409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11409]] Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews(https://arxiv.org/abs/2510.11409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The creation of systematic literature reviews (SLR) is critical for analyzing the landscape of a research field and guiding future research directions. However, retrieving and filtering the literature corpus for an SLR is highly time-consuming and requires extensive manual effort, as keyword-based searches in digital libraries often return numerous irrelevant publications. In this work, we propose a pipeline leveraging multiple large language models (LLMs), classifying papers based on descriptive prompts and deciding jointly using a consensus scheme. The entire process is human-supervised and interactively controlled via our open-source visual analytics web interface, LLMSurver, which enables real-time inspection and modification of model outputs. We evaluate our approach using ground-truth data from a recent SLR comprising over 8,000 candidate papers, benchmarking both open and commercial state-of-the-art LLMs from mid-2024 and fall 2025. Results demonstrate that our pipeline significantly reduces manual effort while achieving lower error rates than single human annotators. Furthermore, modern open-source models prove sufficient for this task, making the method accessible and cost-effective. Overall, our work demonstrates how responsible human-AI collaboration can accelerate and enhance systematic literature reviews within academic workflows.</li>
</ul>

<h3>Title: Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems using an LLM-Judged TBAC Model</h3>
<ul>
<li><strong>Authors: </strong>Charles Fleming, Ashish Kundu, Ramana Kompella</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11414">https://arxiv.org/abs/2510.11414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11414">https://arxiv.org/pdf/2510.11414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11414]] Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems using an LLM-Judged TBAC Model(https://arxiv.org/abs/2510.11414)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of autonomous AI agents within enterprise environments introduces a critical security challenge: managing access control for emergent, novel tasks for which no predefined policies exist. This paper introduces an advanced security framework that extends the Task-Based Access Control (TBAC) model by using a Large Language Model (LLM) as an autonomous, risk-aware judge. This model makes access control decisions not only based on an agent's intent but also by explicitly considering the inherent \textbf{risk associated with target resources} and the LLM's own \textbf{model uncertainty} in its decision-making process. When an agent proposes a novel task, the LLM judge synthesizes a just-in-time policy while also computing a composite risk score for the task and an uncertainty estimate for its own reasoning. High-risk or high-uncertainty requests trigger more stringent controls, such as requiring human approval. This dual consideration of external risk and internal confidence allows the model to enforce a more robust and adaptive version of the principle of least privilege, paving the way for safer and more trustworthy autonomous systems.</li>
</ul>

<h3>Title: Robust Ego-Exo Correspondence with Long-Term Memory</h3>
<ul>
<li><strong>Authors: </strong>Yijun Hu, Bing Fan, Xin Gu, Haiqing Ren, Dongfang Liu, Heng Fan, Libo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11417">https://arxiv.org/abs/2510.11417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11417">https://arxiv.org/pdf/2510.11417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11417]] Robust Ego-Exo Correspondence with Long-Term Memory(https://arxiv.org/abs/2510.11417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Establishing object-level correspondence between egocentric and exocentric views is essential for intelligent assistants to deliver precise and intuitive visual guidance. However, this task faces numerous challenges, including extreme viewpoint variations, occlusions, and the presence of small objects. Existing approaches usually borrow solutions from video object segmentation models, but still suffer from the aforementioned challenges. Recently, the Segment Anything Model 2 (SAM 2) has shown strong generalization capabilities and excellent performance in video object segmentation. Yet, when simply applied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe difficulties due to ineffective ego-exo feature fusion and limited long-term memory capacity, especially for long videos. Addressing these problems, we propose a novel EEC framework based on SAM 2 with long-term memories by presenting a dual-memory architecture and an adaptive feature routing module inspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features (i) a Memory-View MoE module which consists of a dual-branch routing mechanism to adaptively assign contribution weights to each expert feature along both channel and spatial dimensions, and (ii) a dual-memory bank system with a simple yet effective compression strategy to retain critical long-term information while eliminating redundancy. In the extensive experiments on the challenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new state-of-the-art results and significantly outperforms existing methods and the SAM 2 baseline, showcasing its strong generalization across diverse scenarios. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content</h3>
<ul>
<li><strong>Authors: </strong>Dana Sotto Porat, Ella Rabinovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11434">https://arxiv.org/abs/2510.11434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11434">https://arxiv.org/pdf/2510.11434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11434]] Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content(https://arxiv.org/abs/2510.11434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative large language models (LLMs) have become central to everyday life, producing human-like text across diverse domains. A growing body of research investigates whether these models also exhibit personality- and demographic-like characteristics in their language. In this work, we introduce a novel, data-driven methodology for assessing LLM personality without relying on self-report questionnaires, applying instead automatic personality and gender classifiers to model replies on open-ended questions collected from Reddit. Comparing six widely used models to human-authored responses, we find that LLMs systematically express higher Agreeableness and lower Neuroticism, reflecting cooperative and stable conversational tendencies. Gendered language patterns in model text broadly resemble those of human writers, though with reduced variation, echoing prior findings on automated agents. We contribute a new dataset of human and model responses, along with large-scale comparative analyses, shedding new light on the topic of personality and demographic patterns of generative AI.</li>
</ul>

<h3>Title: Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices</h3>
<ul>
<li><strong>Authors: </strong>Xinyan Guan, Yongfan Lai, Jiarui Jin, Jun Li, Haoyu Wang, Qinghao Zhao, Deyun Zhang, Shijia Geng, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11442">https://arxiv.org/abs/2510.11442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11442">https://arxiv.org/pdf/2510.11442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11442]] Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices(https://arxiv.org/abs/2510.11442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for cardiac diagnosis, providing comprehensive spatial coverage of the heart necessary to detect conditions such as myocardial infarction (MI). However, their lack of portability limits continuous and large-scale use. Three-lead ECG systems are widely used in wearable devices due to their simplicity and mobility, but they often fail to capture pathologies in unmeasured regions. To address this, we propose WearECG, a Variational Autoencoder (VAE) method that reconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model includes architectural improvements to better capture temporal and spatial dependencies in ECG signals. We evaluate generation quality using MSE, MAE, and Frechet Inception Distance (FID), and assess clinical validity via a Turing test with expert cardiologists. To further validate diagnostic utility, we fine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label classification task involving over 40 cardiac conditions, including six different myocardial infarction locations, using both real and generated signals. Experiments on the MIMIC dataset show that our method produces physiologically realistic and diagnostically informative signals, with robust performance in downstream tasks. This work demonstrates the potential of generative modeling for ECG reconstruction and its implications for scalable, low-cost cardiac screening.</li>
</ul>

<h3>Title: GenCNER: A Generative Framework for Continual Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yawen Yang, Fukun Ma, Shiao Meng, Aiwei Liu, Lijie Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11444">https://arxiv.org/abs/2510.11444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11444">https://arxiv.org/pdf/2510.11444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11444]] GenCNER: A Generative Framework for Continual Named Entity Recognition(https://arxiv.org/abs/2510.11444)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traditional named entity recognition (NER) aims to identify text mentions into pre-defined entity types. Continual Named Entity Recognition (CNER) is introduced since entity categories are continuously increasing in various real-world scenarios. However, existing continual learning (CL) methods for NER face challenges of catastrophic forgetting and semantic shift of non-entity type. In this paper, we propose GenCNER, a simple but effective Generative framework for CNER to mitigate the above drawbacks. Specifically, we skillfully convert the CNER task into sustained entity triplet sequence generation problem and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we design a type-specific confidence-based pseudo labeling strategy along with knowledge distillation (KD) to preserve learned knowledge and alleviate the impact of label noise at the triplet level. Experimental results on two benchmark datasets show that our framework outperforms previous state-of-the-art methods in multiple CNER settings, and achieves the smallest gap compared with non-CL results.</li>
</ul>

<h3>Title: Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Tianpei Zhang, Jufeng Zhao, Yiming Zhu, Guangmang Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11456">https://arxiv.org/abs/2510.11456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11456">https://arxiv.org/pdf/2510.11456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11456]] Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion(https://arxiv.org/abs/2510.11456)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Existing Infrared and Visible Image Fusion (IVIF) methods typically assume high-quality inputs. However, when handing degraded images, these methods heavily rely on manually switching between different pre-processing techniques. This decoupling of degradation handling and image fusion leads to significant performance degradation. In this paper, we propose a novel VLM-Guided Degradation-Coupled Fusion network (VGDCFusion), which tightly couples degradation modeling with the fusion process and leverages vision-language models (VLMs) for degradation-aware perception and guided suppression. Specifically, the proposed Specific-Prompt Degradation-Coupled Extractor (SPDCE) enables modality-specific degradation awareness and establishes a joint modeling of degradation suppression and intra-modal feature extraction. In parallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates cross-modal degradation perception and couples residual degradation filtering with complementary cross-modal feature fusion. Extensive experiments demonstrate that our VGDCFusion significantly outperforms existing state-of-the-art fusion approaches under various degraded image scenarios. Our code is available at this https URL.</li>
</ul>

<h3>Title: VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment</h3>
<ul>
<li><strong>Authors: </strong>Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11473">https://arxiv.org/abs/2510.11473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11473">https://arxiv.org/pdf/2510.11473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11473]] VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment(https://arxiv.org/abs/2510.11473)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at this https URL.</li>
</ul>

<h3>Title: Investigating Large Language Models' Linguistic Abilities for Text Preprocessing</h3>
<ul>
<li><strong>Authors: </strong>Marco Braga, Gian Carlo Milanese, Gabriella Pasi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11482">https://arxiv.org/abs/2510.11482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11482">https://arxiv.org/pdf/2510.11482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11482]] Investigating Large Language Models' Linguistic Abilities for Text Preprocessing(https://arxiv.org/abs/2510.11482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text preprocessing is a fundamental component of Natural Language Processing, involving techniques such as stopword removal, stemming, and lemmatization to prepare text as input for further processing and analysis. Despite the context-dependent nature of the above techniques, traditional methods usually ignore contextual information. In this paper, we investigate the idea of using Large Language Models (LLMs) to perform various preprocessing tasks, due to their ability to take context into account without requiring extensive language-specific annotated resources. Through a comprehensive evaluation on web-sourced data, we compare LLM-based preprocessing (specifically stopword removal, lemmatization and stemming) to traditional algorithms across multiple text classification tasks in six European languages. Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively. Additionally, we show that ML algorithms trained on texts preprocessed by LLMs achieve an improvement of up to 6% with respect to the $F_1$ measure compared to traditional techniques. Our code, prompts, and results are publicly available at this https URL.</li>
</ul>

<h3>Title: How Reinforcement Learning After Next-Token Prediction Facilitates Learning</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Tsilivis, Eran Malach, Karen Ullrich, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11495">https://arxiv.org/abs/2510.11495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11495">https://arxiv.org/pdf/2510.11495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11495]] How Reinforcement Learning After Next-Token Prediction Facilitates Learning(https://arxiv.org/abs/2510.11495)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms. We introduce a framework to study the success of this paradigm, and we theoretically expose the optimization mechanisms by which reinforcement learning improves over next-token prediction in this setting. We study learning from mixture distributions of short and long ``chain-of-thought'' sequences encoding a single task. In particular, when the task consists of predicting the parity of $d$ bits and long sequences are rare, we show how reinforcement learning after next-token prediction enables autoregressive transformers to generalize, whereas mere next-token prediction requires extreme statistical or computational resources to do so. We further explain how reinforcement learning leverages increased test-time computation, manifested in longer responses, to facilitate this learning process. In a simplified setting, we theoretically prove that autoregressive linear models following this training recipe can efficiently learn to predict the parity of $d$ bits as long as the proportion of long demonstrations in the data mix is not exponentially small in the input dimension $d$. Finally, we demonstrate these same phenomena in other settings, including the post-training of Llama-series models on mixture variations of common mathematical reasoning benchmarks.</li>
</ul>

<h3>Title: AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11496">https://arxiv.org/abs/2510.11496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11496">https://arxiv.org/pdf/2510.11496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11496]] AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model(https://arxiv.org/abs/2510.11496)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoR</li>
</ul>

<h3>Title: ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11498">https://arxiv.org/abs/2510.11498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11498">https://arxiv.org/pdf/2510.11498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11498]] ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding(https://arxiv.org/abs/2510.11498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.</li>
</ul>

<h3>Title: Offline Reinforcement Learning with Generative Trajectory Policies</h3>
<ul>
<li><strong>Authors: </strong>Xinsong Feng, Leshu Tang, Chenan Wang, Haipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11499">https://arxiv.org/abs/2510.11499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11499">https://arxiv.org/pdf/2510.11499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11499]] Offline Reinforcement Learning with Generative Trajectory Policies(https://arxiv.org/abs/2510.11499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. However, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. In this paper, we demonstrate that it is possible to bridge this gap. The key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models, including diffusion, flow matching, and consistency models, as specific instances of learning a continuous-time generative trajectory governed by an Ordinary Differential Equation (ODE). This principled foundation provides a clearer design space for generative policies in RL and allows us to propose Generative Trajectory Policies (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE. To make this paradigm practical for offline RL, we further introduce two key theoretically principled adaptations. Empirical results demonstrate that GTP achieves state-of-the-art performance on D4RL benchmarks - it significantly outperforms prior generative policies, achieving perfect scores on several notoriously hard AntMaze tasks.</li>
</ul>

<h3>Title: Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11509">https://arxiv.org/abs/2510.11509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11509">https://arxiv.org/pdf/2510.11509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11509]] Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model(https://arxiv.org/abs/2510.11509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Physical environments and circumstances are fundamentally dynamic, yet current 3D datasets and evaluation benchmarks tend to concentrate on either dynamic scenarios or dynamic situations in isolation, resulting in incomplete comprehension. To overcome these constraints, we introduce Situat3DChange, an extensive dataset supporting three situation-aware change understanding tasks following the perception-action model: 121K question-answer pairs, 36K change descriptions for perception tasks, and 17K rearrangement instructions for the action task. To construct this large-scale dataset, Situat3DChange leverages 11K human observations of environmental changes to establish shared mental models and shared situational awareness for human-AI collaboration. These observations, enriched with egocentric and allocentric perspectives as well as categorical and coordinate spatial relations, are integrated using an LLM to support understanding of situated changes. To address the challenge of comparing pairs of point clouds from the same scene with minor changes, we propose SCReasoner, an efficient 3D MLLM approach that enables effective point cloud comparison with minimal parameter overhead and no additional tokens required for the language decoder. Comprehensive evaluation on Situat3DChange tasks highlights both the progress and limitations of MLLMs in dynamic scene and situation understanding. Additional experiments on data scaling and cross-domain transfer demonstrate the task-agnostic effectiveness of using Situat3DChange as a training dataset for MLLMs.</li>
</ul>

<h3>Title: LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11512">https://arxiv.org/abs/2510.11512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11512">https://arxiv.org/pdf/2510.11512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11512]] LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference(https://arxiv.org/abs/2510.11512)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.</li>
</ul>

<h3>Title: Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yusheng Song, Lirong Qiu, Xi Zhang, Zhihao Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11529">https://arxiv.org/abs/2510.11529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11529">https://arxiv.org/pdf/2510.11529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11529]] Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models(https://arxiv.org/abs/2510.11529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: this https URL.</li>
</ul>

<h3>Title: An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification</h3>
<ul>
<li><strong>Authors: </strong>Ba-Quang Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11537">https://arxiv.org/abs/2510.11537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11537">https://arxiv.org/pdf/2510.11537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11537]] An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification(https://arxiv.org/abs/2510.11537)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a novel neural architecture named TextGraphFuseGAT, which integrates a pretrained transformer encoder (PhoBERT) with Graph Attention Networks for token-level classification tasks. The proposed model constructs a fully connected graph over the token embeddings produced by PhoBERT, enabling the GAT layer to capture rich inter-token dependencies beyond those modeled by sequential context alone. To further enhance contextualization, a Transformer-style self-attention layer is applied on top of the graph-enhanced embeddings. The final token representations are passed through a classification head to perform sequence labeling. We evaluate our approach on three Vietnamese benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19 domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER dataset, featuring 18 entity types collected from real-world medical speech transcripts and annotated with the BIO tagging scheme. Its specialized vocabulary and domain-specific expressions make it a challenging benchmark for token-level classification models. Experimental results show that our method consistently outperforms strong baselines, including transformer-only and hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness of combining pretrained semantic features with graph-based relational modeling for improved token classification across multiple domains.</li>
</ul>

<h3>Title: Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11538">https://arxiv.org/abs/2510.11538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11538">https://arxiv.org/pdf/2510.11538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11538]] Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers(https://arxiv.org/abs/2510.11538)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation. Recent observations reveal \emph{Massive Activations} (MAs) in their internal feature maps, yet their function remains poorly understood. In this work, we systematically investigate these activations to elucidate their role in visual generation. We found that these massive activations occur across all spatial tokens, and their distribution is modulated by the input timestep embeddings. Importantly, our investigations further demonstrate that these massive activations play a key role in local detail synthesis, while having minimal impact on the overall semantic content of output. Building on these insights, we propose \textbf{D}etail \textbf{G}uidance (\textbf{DG}), a MAs-driven, training-free self-guidance strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG constructs a degraded ``detail-deficient'' model by disrupting MAs and leverages it to guide the original network toward higher-quality detail synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG), enabling further refinements of fine-grained details. Extensive experiments demonstrate that our DG consistently improves fine-grained detail quality across various pre-trained DiTs (\eg, SD3, SD3.5, and Flux).</li>
</ul>

<h3>Title: Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Zhihua Liu, Hao Wang, Weiming Li, Xiaoshuai Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11541">https://arxiv.org/abs/2510.11541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11541">https://arxiv.org/pdf/2510.11541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11541]] Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation(https://arxiv.org/abs/2510.11541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has demonstrated its ability to enhance Large Language Models (LLMs) by integrating external knowledge sources. However, multi-hop questions, which require the identification of multiple knowledge targets to form a synthesized answer, raise new challenges for RAG systems. Under the multi-hop settings, existing methods often struggle to fully understand the questions with complex semantic structures and are susceptible to irrelevant noise during the retrieval of multiple information targets. To address these limitations, we propose a novel graph representation learning framework for multi-hop question retrieval. We first introduce a Multi-information Level Knowledge Graph (Multi-L KG) to model various information levels for a more comprehensive understanding of multi-hop questions. Based on this, we design a Query-Specific Graph Neural Network (QSGNN) for representation learning on the Multi-L KG. QSGNN employs intra/inter-level message passing mechanisms, and in each message passing the information aggregation is guided by the query, which not only facilitates multi-granular information aggregation but also significantly reduces the impact of noise. To enhance its ability to learn robust representations, we further propose two synthesized data generation strategies for pre-training the QSGNN. Extensive experimental results demonstrate the effectiveness of our framework in multi-hop scenarios, especially in high-hop questions the improvement can reach 33.8\%. The code is available at: this https URL.</li>
</ul>

<h3>Title: Information-Preserving Reformulation of Reasoning Traces for Antidistillation</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Ding, Lei Cui, Li Dong, Nanning Zheng, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11545">https://arxiv.org/abs/2510.11545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11545">https://arxiv.org/pdf/2510.11545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11545]] Information-Preserving Reformulation of Reasoning Traces for Antidistillation(https://arxiv.org/abs/2510.11545)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the model's problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-preserving antidistillation reformulation of reasoning traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design a simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. A small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation.</li>
</ul>

<h3>Title: ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?</h3>
<ul>
<li><strong>Authors: </strong>Liu Yang, Huiyu Duan, Ran Tao, Juntao Cheng, Sijing Wu, Yunhao Li, Jing Liu, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11549">https://arxiv.org/abs/2510.11549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11549">https://arxiv.org/pdf/2510.11549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11549]] ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?(https://arxiv.org/abs/2510.11549)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Omnidirectional images (ODIs) provide full 360x180 view which are widely adopted in VR, AR and embodied intelligence applications. While multi-modal large language models (MLLMs) have demonstrated remarkable performance on conventional 2D image and video understanding benchmarks, their ability to comprehend the immersive environments captured by ODIs remains largely unexplored. To address this gap, we first present ODI-Bench, a novel comprehensive benchmark specifically designed for omnidirectional image understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs across 10 fine-grained tasks, covering both general-level and spatial-level ODI understanding. Extensive experiments are conducted to benchmark 20 representative MLLMs, including proprietary and open-source models, under both close-ended and open-ended settings. Experimental results reveal that current MLLMs still struggle to capture the immersive context provided by ODIs. To this end, we further introduce Omni-CoT, a training-free method which significantly enhances MLLMs' comprehension ability in the omnidirectional environment through chain-of-thought reasoning across both textual information and visual cues. Both the benchmark and the code will be released upon the publication.</li>
</ul>

<h3>Title: Invisible Languages of the LLM Universe</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Khanna, Xinxu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11557">https://arxiv.org/abs/2510.11557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11557">https://arxiv.org/pdf/2510.11557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11557]] Invisible Languages of the LLM Universe(https://arxiv.org/abs/2510.11557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are trained on massive multilingual corpora, yet this abundance masks a profound crisis: of the world's 7,613 living languages, approximately 2,000 languages with millions of speakers remain effectively invisible in digital ecosystems. We propose a critical framework connecting empirical measurements of language vitality (real world demographic strength) and digitality (online presence) with postcolonial theory and epistemic injustice to explain why linguistic inequality in AI systems is not incidental but structural. Analyzing data across all documented human languages, we identify four categories: Strongholds (33%, high vitality and digitality), Digital Echoes (6%, high digitality despite declining vitality), Fading Voices (36%, low on both dimensions), and critically, Invisible Giants (27%, high vitality but near-zero digitality) - languages spoken by millions yet absent from the LLM universe. We demonstrate that these patterns reflect continuities from colonial-era linguistic hierarchies to contemporary AI development, constituting what we term digital epistemic injustice. Our analysis reveals that English dominance in AI is not a technical necessity but an artifact of power structures that systematically exclude marginalized linguistic knowledge. We conclude with implications for decolonizing language technology and democratizing access to AI benefits.</li>
</ul>

<h3>Title: SNAP: Towards Segmenting Anything in Any Point Cloud</h3>
<ul>
<li><strong>Authors: </strong>Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11565">https://arxiv.org/abs/2510.11565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11565">https://arxiv.org/pdf/2510.11565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11565]] SNAP: Towards Segmenting Anything in Any Point Cloud(https://arxiv.org/abs/2510.11565)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in \textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, this https URL</li>
</ul>

<h3>Title: A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Denis Zavadski, Damjan Kalšan, Tim Küchler, Haebom Lee, Stefan Roth, Carsten Rother</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11567">https://arxiv.org/abs/2510.11567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11567">https://arxiv.org/pdf/2510.11567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11567]] A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation(https://arxiv.org/abs/2510.11567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Synthetic datasets are widely used for training urban scene recognition models, but even highly realistic renderings show a noticeable gap to real imagery. This gap is particularly pronounced when adapting to a specific target domain, such as Cityscapes, where differences in architecture, vegetation, object appearance, and camera characteristics limit downstream performance. Closing this gap with more detailed 3D modelling would require expensive asset and scene design, defeating the purpose of low-cost labelled data. To address this, we present a new framework that adapts an off-the-shelf diffusion model to a target domain using only imperfect pseudo-labels. Once trained, it generates high-fidelity, target-aligned images from semantic maps of any synthetic dataset, including low-effort sources created in hours rather than months. The method filters suboptimal generations, rectifies image-label misalignments, and standardises semantics across datasets, transforming weak synthetic data into competitive real-domain training sets. Experiments on five synthetic datasets and two real target datasets show segmentation gains of up to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly constructed synthetic datasets as effective as high-effort, time-intensive synthetic datasets requiring extensive manual design. This work highlights a valuable collaborative paradigm where fast semantic prototyping, combined with generative models, enables scalable, high-quality training data creation for urban scene understanding.</li>
</ul>

<h3>Title: Bag of Tricks for Subverting Reasoning-based Safety Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Shuo Chen, Zhen Han, Haokun Chen, Bailan He, Shengyun Si, Jingpei Wu, Philip Torr, Volker Tresp, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11570">https://arxiv.org/abs/2510.11570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11570">https://arxiv.org/pdf/2510.11570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11570]] Bag of Tricks for Subverting Reasoning-based Safety Guardrails(https://arxiv.org/abs/2510.11570)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at this https URL.</li>
</ul>

<h3>Title: Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping</h3>
<ul>
<li><strong>Authors: </strong>Walid Elbarz, Mohamed Bourriz, Hicham Hajji, Hamd Ait Abdelali, François Bourzeix</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11576">https://arxiv.org/abs/2510.11576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11576">https://arxiv.org/pdf/2510.11576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11576]] Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping(https://arxiv.org/abs/2510.11576)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models are transforming Earth observation, but their potential for hyperspectral crop mapping remains underexplored. This study benchmarks three foundation models for cereal crop mapping using hyperspectral imagery: HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth dataset (a large multitemporal hyperspectral archive). Models were fine-tuned on manually labeled data from a training region and evaluated on an independent test region. Performance was measured with overall accuracy (OA), average accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%), DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of 93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved 91%, highlighting the importance of model architecture for strong generalization across geographic regions and sensor platforms. These results provide a systematic evaluation of foundation models for operational hyperspectral crop mapping and outline directions for future model development.</li>
</ul>

<h3>Title: MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhu, Lin Chen, Mounim A. El-Yacoubi, Mingsheng Shang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11579">https://arxiv.org/abs/2510.11579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11579">https://arxiv.org/pdf/2510.11579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11579]] MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis(https://arxiv.org/abs/2510.11579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Sentiment Analysis (MSA) aims to identify and interpret human emotions by integrating information from heterogeneous data sources such as text, video, and audio. While deep learning models have advanced in network architecture design, they remain heavily limited by scarce multimodal annotated data. Although Mixup-based augmentation improves generalization in unimodal tasks, its direct application to MSA introduces critical challenges: random mixing often amplifies label ambiguity and semantic inconsistency due to the lack of emotion-aware mixing mechanisms. To overcome these issues, we propose MS-Mix, an adaptive, emotion-sensitive augmentation framework that automatically optimizes sample mixing in multimodal settings. The key components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS) strategy that effectively prevents semantic confusion caused by mixing samples with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module using multi-head self-attention to compute modality-specific mixing ratios dynamically based on their respective emotional intensities. (3) a Sentiment Alignment Loss (SAL) that aligns the prediction distributions across modalities, and incorporates the Kullback-Leibler-based loss as an additional regularization term to train the emotion intensity predictor and the backbone network jointly. Extensive experiments on three benchmark datasets with six state-of-the-art backbones confirm that MS-Mix consistently outperforms existing methods, establishing a new standard for robust multimodal sentiment augmentation. The source code is available at: this https URL.</li>
</ul>

<h3>Title: LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoqing Chao, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11584">https://arxiv.org/abs/2510.11584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11584">https://arxiv.org/pdf/2510.11584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11584]] LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings(https://arxiv.org/abs/2510.11584)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the model's ability of link prediction by removing or inserting triples. A recent black-box method has attempted to incorporate textual and structural information to enhance attack performance. However, it is unable to generate human-readable explanations, and exhibits poor generalizability. In the past few years, large language models (LLMs) have demonstrated powerful capabilities in text comprehension, generation, and reasoning. In this paper, we propose LLMAtKGE, a novel LLM-based framework that selects attack targets and generates human-readable explanations. To provide the LLM with sufficient factual context under limited input constraints, we design a structured prompting scheme that explicitly formulates the attack as multiple-choice questions while incorporating KG factual evidence. To address the context-window limitation and hesitation issues, we introduce semantics-based and centrality-based filters, which compress the candidate set while preserving high recall of attack-relevant information. Furthermore, to efficiently integrate both semantic and structural information into the filter, we precompute high-order adjacency and fine-tune the LLM with a triple classification task to enhance filtering performance. Experiments on two widely used knowledge graph datasets demonstrate that our attack outperforms the strongest black-box baselines and provides explanations via reasoning, and showing competitive performance compared with white-box methods. Comprehensive ablation and case studies further validate its capability to generate explanations.</li>
</ul>

<h3>Title: Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Georg Ahnert, Anna-Carolina Haensch, Barbara Plank, Markus Strohmaier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11586">https://arxiv.org/abs/2510.11586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11586">https://arxiv.org/pdf/2510.11586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11586]] Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models(https://arxiv.org/abs/2510.11586)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many in-silico simulations of human survey responses with large language models (LLMs) focus on generating closed-ended survey responses, whereas LLMs are typically trained to generate open-ended text instead. Previous research has used a diverse range of methods for generating closed-ended survey responses with LLMs, and a standard practice remains to be identified. In this paper, we systematically investigate the impact that various Survey Response Generation Methods have on predicted survey responses. We present the results of 32 mio. simulated survey responses across 8 Survey Response Generation Methods, 4 political attitude surveys, and 10 open-weight language models. We find significant differences between the Survey Response Generation Methods in both individual-level and subpopulation-level alignment. Our results show that Restricted Generation Methods perform best overall, and that reasoning output does not consistently improve alignment. Our work underlines the significant impact that Survey Response Generation Methods have on simulated survey responses, and we develop practical recommendations on the application of Survey Response Generation Methods.</li>
</ul>

<h3>Title: Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhao, Christopher Yeh, Lingkai Kong, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11590">https://arxiv.org/abs/2510.11590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11590">https://arxiv.org/pdf/2510.11590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11590]] Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization(https://arxiv.org/abs/2510.11590)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Decision-focused learning (DFL) integrates predictive modeling and optimization by training predictors to optimize the downstream decision target rather than merely minimizing prediction error. To date, existing DFL methods typically rely on deterministic point predictions, which are often insufficient to capture the intrinsic stochasticity of real-world environments. To address this challenge, we propose the first diffusion-based DFL approach, which trains a diffusion model to represent the distribution of uncertain parameters and optimizes the decision by solving a stochastic optimization with samples drawn from the diffusion model. Our contributions are twofold. First, we formulate diffusion DFL using the reparameterization trick, enabling end-to-end training through diffusion. While effective, it is memory and compute-intensive due to the need to differentiate through the diffusion sampling process. Second, we propose a lightweight score function estimator that uses only several forward diffusion passes and avoids backpropagation through the sampling. This follows from our results that backpropagating through stochastic optimization can be approximated by a weighted score function formulation. We empirically show that our diffusion DFL approach consistently outperforms strong baselines in decision quality. The source code for all experiments is available at the project repository: this https URL.</li>
</ul>

<h3>Title: MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bo Cheng, Xu Wang, Jinda Liu, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11598">https://arxiv.org/abs/2510.11598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11598">https://arxiv.org/pdf/2510.11598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11598]] MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models(https://arxiv.org/abs/2510.11598)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as one of the most widely used parameter-efficient fine-tuning (PEFT) methods for adapting large language models (LLMs) to downstream tasks. While highly effective in single-task settings, it struggles to efficiently leverage inter-task knowledge in complex multi-task learning scenarios, often requiring substantial task-specific data to achieve optimal performance. To address this limitation, we introduce MeTA-LoRA, a two-stage optimization framework that significantly improves data efficiency in multi-task adaptation. In the first stage, task-specific LoRA adapters are learned using only a few samples from each involved dataset, enabling rapid adaptation without large-scale supervision. In the second stage, the shared LoRA adapter is updated by aggregating gradients from multiple tasks to promote knowledge transfer across tasks, further reducing data usage by leveraging common patterns. In both multi-task learning and multilingual learning scenarios, our method matches or surpasses the performance of traditional full-data LoRA fine-tuning approaches, while using significantly less task-specific data.</li>
</ul>

<h3>Title: SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping</h3>
<ul>
<li><strong>Authors: </strong>Marc Brinner, Sina Zarrieß</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11599">https://arxiv.org/abs/2510.11599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11599">https://arxiv.org/pdf/2510.11599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11599]] SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping(https://arxiv.org/abs/2510.11599)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.</li>
</ul>

<h3>Title: Deconstructing Attention: Investigating Design Principles for Effective Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Huiyin Xue, Nafise Sadat Moosavi, Nikolaos Aletras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11602">https://arxiv.org/abs/2510.11602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11602">https://arxiv.org/pdf/2510.11602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11602]] Deconstructing Attention: Investigating Design Principles for Effective Language Modeling(https://arxiv.org/abs/2510.11602)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance.</li>
</ul>

<h3>Title: ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Leonard Bruns, Axel Barroso-Laguna, Tommaso Cavallari, Áron Monszpart, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11605">https://arxiv.org/abs/2510.11605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11605">https://arxiv.org/pdf/2510.11605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11605]] ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training(https://arxiv.org/abs/2510.11605)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Scene coordinate regression (SCR) has established itself as a promising learning-based approach to visual relocalization. After mere minutes of scene-specific training, SCR models estimate camera poses of query images with high accuracy. Still, SCR methods fall short of the generalization capabilities of more classical feature-matching approaches. When imaging conditions of query images, such as lighting or viewpoint, are too different from the training views, SCR models fail. Failing to generalize is an inherent limitation of previous SCR frameworks, since their training objective is to encode the training views in the weights of the coordinate regressor itself. The regressor essentially overfits to the training views, by design. We propose to separate the coordinate regressor and the map representation into a generic transformer and a scene-specific map code. This separation allows us to pre-train the transformer on tens of thousands of scenes. More importantly, it allows us to train the transformer to generalize from mapping images to unseen query images during pre-training. We demonstrate on multiple challenging relocalization datasets that our method, ACE-G, leads to significantly increased robustness while keeping the computational footprint attractive.</li>
</ul>

<h3>Title: ExpVid: A Benchmark for Experiment Video Understanding & Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11606">https://arxiv.org/abs/2510.11606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11606">https://arxiv.org/pdf/2510.11606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11606]] ExpVid: A Benchmark for Experiment Video Understanding & Reasoning(https://arxiv.org/abs/2510.11606)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.</li>
</ul>

<h3>Title: High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network</h3>
<ul>
<li><strong>Authors: </strong>Feng Zhang, Haoyou Deng, Zhiqiang Li, Lida Li, Bin Xu, Qingbo Lu, Zisheng Cao, Minchen Wei, Changxin Gao, Nong Sang, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11613">https://arxiv.org/abs/2510.11613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11613">https://arxiv.org/pdf/2510.11613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11613]] High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network(https://arxiv.org/abs/2510.11613)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Photo enhancement plays a crucial role in augmenting the visual aesthetics of a photograph. In recent years, photo enhancement methods have either focused on enhancement performance, producing powerful models that cannot be deployed on edge devices, or prioritized computational efficiency, resulting in inadequate performance for real-world applications. To this end, this paper introduces a pyramid network called LLF-LUT++, which integrates global and local operators through closed-form Laplacian pyramid decomposition and reconstruction. This approach enables fast processing of high-resolution images while also achieving excellent performance. Specifically, we utilize an image-adaptive 3D LUT that capitalizes on the global tonal characteristics of downsampled images, while incorporating two distinct weight fusion strategies to achieve coarse global image enhancement. To implement this strategy, we designed a spatial-frequency transformer weight predictor that effectively extracts the desired distinct weights by leveraging frequency features. Additionally, we apply local Laplacian filters to adaptively refine edge details in high-frequency components. After meticulously redesigning the network structure and transformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on the HDR+ dataset, but also further reduces runtime, with 4K resolution images processed in just 13 ms on a single GPU. Extensive experimental results on two benchmark datasets further show that the proposed approach performs favorably compared to state-of-the-art methods. The source code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zehao Chen, Rong Pan, Haoran Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11618">https://arxiv.org/abs/2510.11618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11618">https://arxiv.org/pdf/2510.11618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11618]] StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models(https://arxiv.org/abs/2510.11618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human writers often begin their stories with an overarching mental scene, where they envision the interactions between characters and their environment. Inspired by this creative process, we propose a novel approach to long-form story generation, termed hybrid bottom-up long-form story generation, using multi-agent simulations. In our method, agents interact within a dynamic sandbox environment, where their behaviors and interactions with one another and the environment generate emergent events. These events form the foundation for the story, enabling organic character development and plot progression. Unlike traditional top-down approaches that impose rigid structures, our hybrid bottom-up approach allows for the natural unfolding of events, fostering more spontaneous and engaging storytelling. The system is capable of generating stories exceeding 10,000 words while maintaining coherence and consistency, addressing some of the key challenges faced by current story generation models. We achieve state-of-the-art performance across several metrics. This approach offers a scalable and innovative solution for creating dynamic, immersive long-form stories that evolve organically from agent-driven interactions.</li>
</ul>

<h3>Title: EvoCAD: Evolutionary CAD Code Generation with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tobias Preintner, Weixuan Yuan, Adrian König, Thomas Bäck, Elena Raponi, Niki van Stein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11631">https://arxiv.org/abs/2510.11631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11631">https://arxiv.org/pdf/2510.11631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11631]] EvoCAD: Evolutionary CAD Code Generation with Vision Language Models(https://arxiv.org/abs/2510.11631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.</li>
</ul>

<h3>Title: NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Krittin Chaowakarn, Paramin Sangwongngam, Nang Htet Htet Aung, Chalie Charoenlarpnopparut</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11632">https://arxiv.org/abs/2510.11632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11632">https://arxiv.org/pdf/2510.11632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11632]] NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection(https://arxiv.org/abs/2510.11632)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent studies in 3D object detection for autonomous vehicles aim to enrich features through the utilization of multi-modal setups or the extraction of local patterns within LiDAR point clouds. However, multi-modal methods face significant challenges in feature alignment, and gaining features locally can be oversimplified for complex 3D object detection tasks. In this paper, we propose a novel model, NV3D, which utilizes local features acquired from voxel neighbors, as normal vectors computed per voxel basis using K-nearest neighbors (KNN) and principal component analysis (PCA). This informative feature enables NV3D to determine the relationship between the surface and pertinent target entities, including cars, pedestrians, or cyclists. During the normal vector extraction process, NV3D offers two distinct sampling strategies: normal vector density-based sampling and FOV-aware bin-based sampling, allowing elimination of up to 55% of data while maintaining performance. In addition, we applied element-wise attention fusion, which accepts voxel features as the query and value and normal vector features as the key, similar to the attention mechanism. Our method is trained on the KITTI dataset and has demonstrated superior performance in car and cyclist detection owing to their spatial shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18% mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61% and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of voxels being filtered out.</li>
</ul>

<h3>Title: IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11647">https://arxiv.org/abs/2510.11647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11647">https://arxiv.org/pdf/2510.11647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11647]] IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment(https://arxiv.org/abs/2510.11647)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.</li>
</ul>

<h3>Title: PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Pradyumna Yalandur Muralidhar, Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11649">https://arxiv.org/abs/2510.11649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11649">https://arxiv.org/pdf/2510.11649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11649]] PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image(https://arxiv.org/abs/2510.11649)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing metrically accurate humans and their surrounding scenes from a single image is crucial for virtual reality, robotics, and comprehensive 3D scene understanding. However, existing methods struggle with depth ambiguity, occlusions, and physically inconsistent contacts. To address these challenges, we introduce PhySIC, a framework for physically plausible Human-Scene Interaction and Contact reconstruction. PhySIC recovers metrically consistent SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within a shared coordinate frame from a single RGB image. Starting from coarse monocular depth and body estimates, PhySIC performs occlusion-aware inpainting, fuses visible depth with unscaled geometry for a robust metric scaffold, and synthesizes missing support surfaces like floors. A confidence-weighted optimization refines body pose, camera parameters, and global scale by jointly enforcing depth alignment, contact priors, interpenetration avoidance, and 2D reprojection consistency. Explicit occlusion masking safeguards invisible regions against implausible configurations. PhySIC is efficient, requiring only 9 seconds for joint human-scene optimization and under 27 seconds end-to-end. It naturally handles multiple humans, enabling reconstruction of diverse interactions. Empirically, PhySIC outperforms single-image baselines, reducing mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm, and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic foot-floor interactions, natural seating, and plausible reconstructions of heavily occluded furniture. By converting a single image into a physically plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding. Our implementation is publicly available at this https URL.</li>
</ul>

<h3>Title: InfiniHuman: Infinite 3D Human Creation with Precise Control</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11650">https://arxiv.org/abs/2510.11650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11650">https://arxiv.org/pdf/2510.11650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11650]] InfiniHuman: Infinite 3D Human Creation with Precise Control(https://arxiv.org/abs/2510.11650)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at this https URL.</li>
</ul>

<h3>Title: ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems</h3>
<ul>
<li><strong>Authors: </strong>Xin Gui, King Zhu, JinCheng Ren, Qianben Chen, Zekun Moore Wang, Yizhi LI, Xinpeng Liu, Xiaowan Li, Wenli Ren, Linyu Miao, Tianrui Qin, Ziqi Shu, He Zhu, Xiangru Tang, Dingfeng Shi, Jiaheng Liu, Yuchen Eleanor Jiang, Minghao Liu, Ge Zhang, Wangchunshu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11652">https://arxiv.org/abs/2510.11652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11652">https://arxiv.org/pdf/2510.11652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11652]] ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems(https://arxiv.org/abs/2510.11652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.</li>
</ul>

<h3>Title: An Eulerian Perspective on Straight-Line Sampling</h3>
<ul>
<li><strong>Authors: </strong>Panos Tsimpos, Youssef Marzouk</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11657">https://arxiv.org/abs/2510.11657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11657">https://arxiv.org/pdf/2510.11657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11657]] An Eulerian Perspective on Straight-Line Sampling(https://arxiv.org/abs/2510.11657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study dynamic measure transport for generative modeling: specifically, flows induced by stochastic processes that bridge a specified source and target distribution. The conditional expectation of the process' velocity defines an ODE whose flow map achieves the desired transport. We ask \emph{which processes produce straight-line flows} -- i.e., flows whose pointwise acceleration vanishes and thus are exactly integrable with a first-order method? We provide a concise PDE characterization of straightness as a balance between conditional acceleration and the divergence of a weighted covariance (Reynolds) tensor. Using this lens, we fully characterize affine-in-time interpolants and show that straightness occurs exactly under deterministic endpoint couplings. We also derive necessary conditions that constrain flow geometry for general processes, offering broad guidance for designing transports that are easier to integrate.</li>
</ul>

<h3>Title: FACE: Faithful Automatic Concept Extraction</h3>
<ul>
<li><strong>Authors: </strong>Dipkamal Bhusal, Michael Clifford, Sara Rampazzi, Nidhi Rastogi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11675">https://arxiv.org/abs/2510.11675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11675">https://arxiv.org/pdf/2510.11675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11675]] FACE: Faithful Automatic Concept Extraction(https://arxiv.org/abs/2510.11675)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Interpreting deep neural networks through concept-based explanations offers a bridge between low-level features and high-level human-understandable semantics. However, existing automatic concept discovery methods often fail to align these extracted concepts with the model's true decision-making process, thereby compromising explanation faithfulness. In this work, we propose FACE (Faithful Automatic Concept Extraction), a novel framework that augments Non-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence regularization term to ensure alignment between the model's original and concept-based predictions. Unlike prior methods that operate solely on encoder activations, FACE incorporates classifier supervision during concept learning, enforcing predictive consistency and enabling faithful explanations. We provide theoretical guarantees showing that minimizing the KL divergence bounds the deviation in predictive distributions, thereby promoting faithful local linearity in the learned concept space. Systematic evaluations on ImageNet, COCO, and CelebA datasets demonstrate that FACE outperforms existing methods across faithfulness and sparsity metrics.</li>
</ul>

<h3>Title: Chronologically Consistent Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Songrun He, Linying Lv, Asaf Manela, Jimmy Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11677">https://arxiv.org/abs/2510.11677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11677">https://arxiv.org/pdf/2510.11677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11677]] Chronologically Consistent Generative AI(https://arxiv.org/abs/2510.11677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We introduce a family of chronologically consistent, instruction-following large language models to eliminate lookahead bias. Each model is trained only on data available before a clearly defined knowledge-cutoff date, ensuring strict temporal separation from any post-cutoff data. The resulting framework offers (i) a simple, conversational chat interface, (ii) fully open, fixed model weights that guarantee replicability, and (iii) a conservative lower bound on forecast accuracy, isolating the share of predictability that survives once training leakage is removed. Together, these features provide researchers with an easy-to-use generative AI tool useful for a wide range of prediction tasks that is free of lookahead bias.</li>
</ul>

<h3>Title: Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11683">https://arxiv.org/abs/2510.11683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11683">https://arxiv.org/pdf/2510.11683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11683]] Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models(https://arxiv.org/abs/2510.11683)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.</li>
</ul>

<h3>Title: Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View</h3>
<ul>
<li><strong>Authors: </strong>Jinyu Zhang, Haitao Lin, Jiashu Hou, Xiangyang Xue, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11687">https://arxiv.org/abs/2510.11687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11687">https://arxiv.org/pdf/2510.11687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11687]] Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View(https://arxiv.org/abs/2510.11687)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Estimating an object's 6D pose, size, and shape from visual input is a fundamental problem in computer vision, with critical applications in robotic grasping and manipulation. Existing methods either rely on object-specific priors such as CAD models or templates, or suffer from limited generalization across categories due to pose-shape entanglement and multi-stage pipelines. In this work, we propose a unified, category-agnostic framework that simultaneously predicts 6D pose, size, and dense shape from a single RGB-D image, without requiring templates, CAD models, or category labels at test time. Our model fuses dense 2D features from vision foundation models with partial 3D point clouds using a Transformer encoder enhanced by a Mixture-of-Experts, and employs parallel decoders for pose-size estimation and shape reconstruction, achieving real-time inference at 28 FPS. Trained solely on synthetic data from 149 categories in the SOPE dataset, our framework is evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL, spanning over 300 categories. It achieves state-of-the-art accuracy on seen categories while demonstrating remarkably strong zero-shot generalization to unseen real-world objects, establishing a new standard for open-set 6D understanding in robotics and embodied AI.</li>
</ul>

<h3>Title: PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Zicheng Liu, Lige Huang, Jie Zhang, Dongrui Liu, Yuan Tian, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11688">https://arxiv.org/abs/2510.11688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11688">https://arxiv.org/pdf/2510.11688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11688]] PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation Capabilities(https://arxiv.org/abs/2510.11688)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing autonomy of Large Language Models (LLMs) necessitates a rigorous evaluation of their potential to aid in cyber offense. Existing benchmarks often lack real-world complexity and are thus unable to accurately assess LLMs' cybersecurity capabilities. To address this gap, we introduce PACEbench, a practical AI cyber-exploitation benchmark built on the principles of realistic vulnerability difficulty, environmental complexity, and cyber defenses. Specifically, PACEbench comprises four scenarios spanning single, blended, chained, and defense vulnerability exploitations. To handle these complex challenges, we propose PACEagent, a novel agent that emulates human penetration testers by supporting multi-phase reconnaissance, analysis, and exploitation. Extensive experiments with seven frontier LLMs demonstrate that current models struggle with complex cyber scenarios, and none can bypass defenses. These findings suggest that current models do not yet pose a generalized cyber offense threat. Nonetheless, our work provides a robust benchmark to guide the trustworthy development of future models.</li>
</ul>

<h3>Title: Diffusion Transformers with Representation Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11690">https://arxiv.org/abs/2510.11690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11690">https://arxiv.org/pdf/2510.11690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11690]] Diffusion Transformers with Representation Autoencoders(https://arxiv.org/abs/2510.11690)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.</li>
</ul>

<h3>Title: Scaling Language-Centric Omnimodal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11693">https://arxiv.org/abs/2510.11693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11693">https://arxiv.org/pdf/2510.11693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11693]] Scaling Language-Centric Omnimodal Representation Learning(https://arxiv.org/abs/2510.11693)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at this https URL.</li>
</ul>

<h3>Title: When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Lingfei Qian, Xueqing Peng, Yan Wang, Vincent Jim Zhang, Huan He, Hanley Smith, Yi Han, Yueru He, Haohang Li, Yupeng Cao, Yangyang Yu, Alejandro Lopez-Lira, Peng Lu, Jian-Yun Nie, Guojun Xiong, Jimin Huang, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11695">https://arxiv.org/abs/2510.11695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11695">https://arxiv.org/pdf/2510.11695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11695]] When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents(https://arxiv.org/abs/2510.11695)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.</li>
</ul>

<h3>Title: QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11696">https://arxiv.org/abs/2510.11696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11696">https://arxiv.org/pdf/2510.11696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11696]] QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs(https://arxiv.org/abs/2510.11696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.</li>
</ul>

<h3>Title: Bayesian Topological Convolutional Neural Nets</h3>
<ul>
<li><strong>Authors: </strong>Sarah Harkins Dayton, Hayden Everett, Ioannis Schizas, David L. Boothe Jr., Vasileios Maroulas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11704">https://arxiv.org/abs/2510.11704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11704">https://arxiv.org/pdf/2510.11704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11704]] Bayesian Topological Convolutional Neural Nets(https://arxiv.org/abs/2510.11704)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) have been established as the main workhorse in image data processing; nonetheless, they require large amounts of data to train, often produce overconfident predictions, and frequently lack the ability to quantify the uncertainty of their predictions. To address these concerns, we propose a new Bayesian topological CNN that promotes a novel interplay between topology-aware learning and Bayesian sampling. Specifically, it utilizes information from important manifolds to accelerate training while reducing calibration error by placing prior distributions on network parameters and properly learning appropriate posteriors. One important contribution of our work is the inclusion of a consistency condition in the learning cost, which can effectively modify the prior distributions to improve the performance of our novel network architecture. We evaluate the model on benchmark image classification datasets and demonstrate its superiority over conventional CNNs, Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply evidence that our method provides an advantage in situations where training data is limited or corrupted. Furthermore, we show that the new model allows for better uncertainty quantification than standard BNNs since it can more readily identify examples of out-of-distribution data on which it has not been trained. Our results highlight the potential of our novel hybrid approach for more efficient and robust image classification.</li>
</ul>

<h3>Title: Adversarial Attacks Leverage Interference Between Features in Superposition</h3>
<ul>
<li><strong>Authors: </strong>Edward Stevinson, Lucas Prieto, Melih Barsbey, Tolga Birdal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11709">https://arxiv.org/abs/2510.11709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11709">https://arxiv.org/pdf/2510.11709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11709]] Adversarial Attacks Leverage Interference Between Features in Superposition(https://arxiv.org/abs/2510.11709)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Fundamental questions remain about when and why adversarial examples arise in neural networks, with competing views characterising them either as artifacts of the irregularities in the decision landscape or as products of sensitivity to non-robust input features. In this paper, we instead argue that adversarial vulnerability can stem from efficient information encoding in neural networks. Specifically, we show how superposition - where networks represent more features than they have dimensions - creates arrangements of latent representations that adversaries can exploit. We demonstrate that adversarial perturbations leverage interference between superposed features, making attack patterns predictable from feature arrangements. Our framework provides a mechanistic explanation for two known phenomena: adversarial attack transferability between models with similar training regimes and class-specific vulnerability patterns. In synthetic settings with precisely controlled superposition, we establish that superposition suffices to create adversarial vulnerability. We then demonstrate that these findings persist in a ViT trained on CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct of networks' representational compression, rather than flaws in the learning process or non-robust inputs.</li>
</ul>

<h3>Title: DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</h3>
<ul>
<li><strong>Authors: </strong>Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11712">https://arxiv.org/abs/2510.11712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11712">https://arxiv.org/pdf/2510.11712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11712]] DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training(https://arxiv.org/abs/2510.11712)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at this https URL.</li>
</ul>

<h3>Title: Are Large Reasoning Models Interruptible?</h3>
<ul>
<li><strong>Authors: </strong>Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11713">https://arxiv.org/abs/2510.11713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11713">https://arxiv.org/pdf/2510.11713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11713]] Are Large Reasoning Models Interruptible?(https://arxiv.org/abs/2510.11713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, "frozen world" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the "frozen world" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.</li>
</ul>

<h3>Title: Point Prompting: Counterfactual Tracking with Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Sanyam Mehta, Daniel Geng, Andrew Owens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11715">https://arxiv.org/abs/2510.11715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11715">https://arxiv.org/pdf/2510.11715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11715]] Point Prompting: Counterfactual Tracking with Video Diffusion Models(https://arxiv.org/abs/2510.11715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Trackers and video generators solve closely related problems: the former analyze motion, while the latter synthesize it. We show that this connection enables pretrained video diffusion models to perform zero-shot point tracking by simply prompting them to visually mark points as they move over time. We place a distinctively colored marker at the query point, then regenerate the rest of the video from an intermediate noise level. This propagates the marker across frames, tracing the point's trajectory. To ensure that the marker remains visible in this counterfactual generation, despite such markers being unlikely in natural videos, we use the unedited initial frame as a negative prompt. Through experiments with multiple image-conditioned video diffusion models, we find that these "emergent" tracks outperform those of prior zero-shot methods and persist through occlusions, often obtaining performance that is competitive with specialized self-supervised models.</li>
</ul>

<h3>Title: CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images</h3>
<ul>
<li><strong>Authors: </strong>Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11718">https://arxiv.org/abs/2510.11718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11718">https://arxiv.org/pdf/2510.11718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11718]] CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images(https://arxiv.org/abs/2510.11718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for "thinking with images" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as "visual thought", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
