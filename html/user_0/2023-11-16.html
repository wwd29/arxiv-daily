<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Exploration of Hyperledger Besu in Designing Private Blockchain-based Financial Distribution Systems. (arXiv:2311.08483v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08483">http://arxiv.org/abs/2311.08483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08483]] Exploration of Hyperledger Besu in Designing Private Blockchain-based Financial Distribution Systems(http://arxiv.org/abs/2311.08483)</code></li>
<li>Summary: <p>Blockchain, a decentralized technology that provides unrivaled security,
transparency, and process validation, is redefining the operational landscape
across numerous industries. This article focuses on the development of an
innovative consortium blockchain based financial distribution application. This
paper illuminates the transformative role of blockchain technology in a variety
of sectors by drawing on a plethora of academic literature and current industry
practices. It demonstrates the diverse applications of blockchain, ranging from
remittances to lending and investments in finance to data administration in
healthcare and supply chain tracking. The paper reveals the design and
potential of a consortium blockchain based application for financial
distribution. Utilizing the capabilities of Hyperledger Besu, the application
is tailored to improve security, scalability, and interoperability, thereby
contributing to a more integrated financial ecosystem. The investigation sheds
light on the combination of consortium blockchain controlled access and
Hyprledger Besu comprehensive functionality, proposing a secure, transparent,
and efficient financial transaction environment. The investigation serves as a
resource for academics, industry professionals, and policymakers alike,
highlighting the vast potential of blockchain technology, enabled by platforms
such as Hyperledger Besu, in accelerating the evolution of traditional systems
toward a more decentralized, secure, and efficient future.
</p></li>
</ul>

<h3>Title: Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption. (arXiv:2311.08610v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08610">http://arxiv.org/abs/2311.08610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08610]] Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption(http://arxiv.org/abs/2311.08610)</code></li>
<li>Summary: <p>Designing privacy-preserving deep learning models is a major challenge within
the deep learning community. Homomorphic Encryption (HE) has emerged as one of
the most promising approaches in this realm, enabling the decoupling of
knowledge between the model owner and the data owner. Despite extensive
research and application of this technology, primarily in convolutional neural
networks, incorporating HE into transformer models has been challenging because
of the difficulties in converting these models into a polynomial form. We break
new ground by introducing the first polynomial transformer, providing the first
demonstration of secure inference over HE with transformers. This includes a
transformer architecture tailored for HE, alongside a novel method for
converting operators to their polynomial equivalent. This innovation enables us
to perform secure inference on LMs with WikiText-103. It also allows us to
perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield
results comparable to traditional methods, bridging the performance gap with
transformers of similar scale and underscoring the viability of HE for
state-of-the-art applications. Finally, we assess the stability of our models
and conduct a series of ablations to quantify the contribution of each model
component.
</p></li>
</ul>

<h3>Title: Comments on "Dynamic Consensus Committee-Based for Secure Data Sharing With Authorized Multi-Receiver Searchable Encryption". (arXiv:2311.08813v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08813">http://arxiv.org/abs/2311.08813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08813]] Comments on "Dynamic Consensus Committee-Based for Secure Data Sharing With Authorized Multi-Receiver Searchable Encryption"(http://arxiv.org/abs/2311.08813)</code></li>
<li>Summary: <p>Recently, Yang et al. introduced an efficient searchable encryption scheme
titled "Dynamic Consensus Committee-Based for Secure Data Sharing With
Authorized Multi-Receiver Searchable Encryption (DCC-SE)," published in IEEE
Transactions on Information Forensics and Security (DOI:
10.1109/TIFS.<a href="http://export.arxiv.org/abs/2023.33051">2023.33051</a>83). According to the authors, DCC-SE meets various
security requirements, especially the keyword trapdoor indistinguishability
against chosen keyword attacks (KT-IND-CKA). In this letter, however, we reveal
a significant vulnerability of DCC-SE: any users involved in the system can
execute attacks against KT-IND-CKA security. This flaw potentially results in
the unintended disclosure of sensitive keyword information related to the
documents. We present a detailed cryptanalysis on DCC-SE. In addition, to
address this vulnerability, we discuss the root cause and identify a flaw in
the security proof of DCC-SE. Subsequently, we provide a solution that
effectively addresses this concern without significantly increasing
computational overhead.
</p></li>
</ul>

<h3>Title: Combining Shamir & Additive Secret Sharing to Improve Efficiency of SMC Primitives Against Malicious Adversaries. (arXiv:2311.08934v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08934">http://arxiv.org/abs/2311.08934</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08934]] Combining Shamir & Additive Secret Sharing to Improve Efficiency of SMC Primitives Against Malicious Adversaries(http://arxiv.org/abs/2311.08934)</code></li>
<li>Summary: <p>Secure multi-party computation provides a wide array of protocols for
mutually distrustful parties be able to securely evaluate functions of private
inputs. Within recent years, many such protocols have been proposed
representing a plethora of strategies to securely and efficiently handle such
computation. These protocols have become increasingly efficient, but their
performance still is impractical in many settings. We propose new approaches to
some of these problems which are either more efficient than previous works
within the same security models or offer better security guarantees with
comparable efficiency. The goals of this research are to improve efficiency and
security of secure multi-party protocols and explore the application of such
approaches to novel threat scenarios. Some of the novel optimizations employed
are dynamically switching domains of shared secrets, asymmetric computations,
and advantageous functional transformations, among others. Specifically, this
work presents a novel combination of Shamir and Additive secret sharing to be
used in parallel which allows for the transformation of efficient protocols
secure against passive adversaries to be secure against active adversaries.
From this set of primitives we propose the construction of a comparison
protocol which can be implemented under that approach with a complexity which
is more efficient than other recent works for common domains of interest.
Finally, we present a system which addresses a critical security threat for the
protection and obfuscation of information which may be of high consequence.
</p></li>
</ul>

<h3>Title: Homomorphic Polynomial Public Key Cryptography for Quantum-secure Digital Signature. (arXiv:2311.08967v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08967">http://arxiv.org/abs/2311.08967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08967]] Homomorphic Polynomial Public Key Cryptography for Quantum-secure Digital Signature(http://arxiv.org/abs/2311.08967)</code></li>
<li>Summary: <p>In their 2022 study, Kuang et al. introduced Multivariable Polynomial Public
Key (MPPK) cryptography, leveraging the inversion relationship between
multiplication and division for quantum-safe public key systems. They extended
MPPK into Homomorphic Polynomial Public Key (HPPK), employing homomorphic
encryption for large hidden ring operations. Originally designed for key
encapsulation (KEM), HPPK's security relies on homomorphic encryption of public
polynomials. This paper expands HPPK KEM to a digital signature scheme, facing
challenges due to the distinct nature of verification compared to decryption.
To adapt HPPK KEM to digital signatures, the authors introduce an extension of
the Barrett reduction algorithm, transforming modular multiplications into
divisions in the verification equation over a prime field. The extended
algorithm non-linearly embeds the signature into public polynomial
coefficients, addressing vulnerabilities in earlier MPPK DS schemes. Security
analysis demonstrates exponential complexity for private key recovery and
forged signature attacks, considering ring bit length twice that of the prime
field size.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: NLP-Based Techniques for Cyber Threat Intelligence. (arXiv:2311.08807v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08807">http://arxiv.org/abs/2311.08807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08807]] NLP-Based Techniques for Cyber Threat Intelligence(http://arxiv.org/abs/2311.08807)</code></li>
<li>Summary: <p>In the digital era, threat actors employ sophisticated techniques for which,
often, digital traces in the form of textual data are available. Cyber Threat
Intelligence~(CTI) is related to all the solutions inherent to data collection,
processing, and analysis useful to understand a threat actor's targets and
attack behavior. Currently, CTI is assuming an always more crucial role in
identifying and mitigating threats and enabling proactive defense strategies.
In this context, NLP, an artificial intelligence branch, has emerged as a
powerful tool for enhancing threat intelligence capabilities. This survey paper
provides a comprehensive overview of NLP-based techniques applied in the
context of threat intelligence. It begins by describing the foundational
definitions and principles of CTI as a major tool for safeguarding digital
assets. It then undertakes a thorough examination of NLP-based techniques for
CTI data crawling from Web sources, CTI data analysis, Relation Extraction from
cybersecurity data, CTI sharing and collaboration, and security threats of CTI.
Finally, the challenges and limitations of NLP in threat intelligence are
exhaustively examined, including data quality issues and ethical
considerations. This survey draws a complete framework and serves as a valuable
resource for security professionals and researchers seeking to understand the
state-of-the-art NLP-based threat intelligence techniques and their potential
impact on cybersecurity.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection. (arXiv:2311.08786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08786">http://arxiv.org/abs/2311.08786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08786]] HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection(http://arxiv.org/abs/2311.08786)</code></li>
<li>Summary: <p>With the popularity of smart devices and the development of computer vision
technology, concerns about face privacy protection are growing. The face
de-identification technique is a practical way to solve the identity protection
problem. The existing facial de-identification methods have revealed several
problems, including the impact on the realism of anonymized results when faced
with occlusions and the inability to maintain identity-irrelevant details in
anonymized results. We present a High-Fidelity and Occlusion-Robust
De-identification (HFORD) method to deal with these issues. This approach can
disentangle identities and attributes while preserving image-specific details
such as background, facial features (e.g., wrinkles), and lighting, even in
occluded scenes. To disentangle the latent codes in the GAN inversion space, we
introduce an Identity Disentanglement Module (IDM). This module selects the
latent codes that are closely related to the identity. It further separates the
latent codes into identity-related codes and attribute-related codes, enabling
the network to preserve attributes while only modifying the identity. To ensure
the preservation of image details and enhance the network's robustness to
occlusions, we propose an Attribute Retention Module (ARM). This module
adaptively preserves identity-irrelevant details and facial occlusions and
blends them into the generated results in a modulated manner. Extensive
experiments show that our method has higher quality, better detail fidelity,
and stronger occlusion robustness than other face de-identification methods.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness. (arXiv:2311.08936v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08936">http://arxiv.org/abs/2311.08936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08936]] Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness(http://arxiv.org/abs/2311.08936)</code></li>
<li>Summary: <p>Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Physical Adversarial Examples for Multi-Camera Systems. (arXiv:2311.08539v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08539">http://arxiv.org/abs/2311.08539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08539]] Physical Adversarial Examples for Multi-Camera Systems(http://arxiv.org/abs/2311.08539)</code></li>
<li>Summary: <p>Neural networks build the foundation of several intelligent systems, which,
however, are known to be easily fooled by adversarial examples. Recent advances
made these attacks possible even in air-gapped scenarios, where the autonomous
system observes its surroundings by, e.g., a camera. We extend these ideas in
our research and evaluate the robustness of multi-camera setups against such
physical adversarial examples. This scenario becomes ever more important with
the rise in popularity of autonomous vehicles, which fuse the information of
several cameras for their driving decision. While we find that multi-camera
setups provide some robustness towards past attack methods, we see that this
advantage reduces when optimizing on multiple perspectives at once. We propose
a novel attack method that we call Transcender-MC, where we incorporate online
3D renderings and perspective projections in the training process. Moreover, we
motivate that certain data augmentation techniques can facilitate the
generation of successful adversarial examples even further. Transcender-MC is
11% more effective in successfully attacking multi-camera setups than
state-of-the-art methods. Our findings offer valuable insights regarding the
resilience of object detection in a setup with multiple cameras and motivate
the need of developing adequate defense mechanisms against them.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing. (arXiv:2311.09024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09024">http://arxiv.org/abs/2311.09024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09024]] Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing(http://arxiv.org/abs/2311.09024)</code></li>
<li>Summary: <p>A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
</p>
<p>In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base "training" set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
</p></li>
</ul>

<h3>Title: DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models. (arXiv:2311.08598v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08598">http://arxiv.org/abs/2311.08598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08598]] DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models(http://arxiv.org/abs/2311.08598)</code></li>
<li>Summary: <p>Pre-trained language models (PLMs) that achieve success in applications are
susceptible to adversarial attack methods that are capable of generating
adversarial examples with minor perturbations. Although recent attack methods
can achieve a relatively high attack success rate (ASR), our observation shows
that the generated adversarial examples have a different data distribution
compared with the original examples. Specifically, these adversarial examples
exhibit lower confidence levels and higher distance to the training data
distribution. As a result, they are easy to detect using very simple detection
methods, diminishing the actual effectiveness of these attack methods. To solve
this problem, we propose a Distribution-Aware LoRA-based Adversarial Attack
(DALA) method, which considers the distribution shift of adversarial examples
to improve attack effectiveness under detection methods. We further design a
new evaluation metric NASR combining ASR and detection for the attack task. We
conduct experiments on four widely-used datasets and validate the attack
effectiveness on ASR and NASR of the adversarial examples generated by DALA on
the BERT-base model and the black-box LLaMA2-7b model.
</p></li>
</ul>

<h3>Title: Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization. (arXiv:2311.09096v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09096">http://arxiv.org/abs/2311.09096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09096]] Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization(http://arxiv.org/abs/2311.09096)</code></li>
<li>Summary: <p>Large Language Models (LLMs) continue to advance in their capabilities, yet
this progress is accompanied by a growing array of safety risks. While
significant attention has been dedicated to exploiting weaknesses in LLMs
through jailbreaking attacks, there remains a paucity of exploration into
defending against these attacks. We point out a pivotal factor contributing to
the success of jailbreaks: the inherent conflict between the goals of being
helpful and ensuring safety. To counter jailbreaking attacks, we propose to
integrate goal prioritization at both training and inference stages.
Implementing goal prioritization during inference substantially diminishes the
Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to
2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising
general performance. Furthermore, integrating the concept of goal
prioritization into the training phase reduces the ASR from 71.0% to 6.6% for
LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are
included during training, our approach slashes the ASR by half, decreasing it
from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs
face greater safety risks, they also possess a greater capacity to be steered
towards defending against such attacks. We hope our work could contribute to
the comprehension of jailbreaking attacks and defenses, and shed light on the
relationship between LLMs' capability and safety. Our code will be available at
\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</p></li>
</ul>

<h3>Title: A Statistical Verification Method of Random Permutations for Hiding Countermeasure Against Side-Channel Attacks. (arXiv:2311.08625v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08625">http://arxiv.org/abs/2311.08625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08625]] A Statistical Verification Method of Random Permutations for Hiding Countermeasure Against Side-Channel Attacks(http://arxiv.org/abs/2311.08625)</code></li>
<li>Summary: <p>As NIST is putting the final touches on the standardization of PQC (Post
Quantum Cryptography) public key algorithms, it is a racing certainty that
peskier cryptographic attacks undeterred by those new PQC algorithms will
surface. Such a trend in turn will prompt more follow-up studies of attacks and
countermeasures. As things stand, from the attackers' perspective, one viable
form of attack that can be implemented thereupon is the so-called "side-channel
attack". Two best-known countermeasures heralded to be durable against
side-channel attacks are: "masking" and "hiding". In that dichotomous picture,
of particular note are successful single-trace attacks on some of the NIST's
PQC then-candidates, which worked to the detriment of the former: "masking". In
this paper, we cast an eye over the latter: "hiding". Hiding proves to be
durable against both side-channel attacks and another equally robust type of
attacks called "fault injection attacks", and hence is deemed an auspicious
countermeasure to be implemented. Mathematically, the hiding method is
fundamentally based on random permutations. There has been a cornucopia of
studies on generating random permutations. However, those are not tied to
implementation of the hiding method. In this paper, we propose a reliable and
efficient verification of permutation implementation, through employing
Fisher-Yates' shuffling method. We introduce the concept of an n-th order
permutation and explain how it can be used to verify that our implementation is
more efficient than its previous-gen counterparts for hiding countermeasures.
</p></li>
</ul>

<h3>Title: Adversarial Attacks to Reward Machine-based Reinforcement Learning. (arXiv:2311.09014v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09014">http://arxiv.org/abs/2311.09014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09014]] Adversarial Attacks to Reward Machine-based Reinforcement Learning(http://arxiv.org/abs/2311.09014)</code></li>
<li>Summary: <p>In recent years, Reward Machines (RMs) have stood out as a simple yet
effective automata-based formalism for exposing and exploiting task structure
in reinforcement learning settings. Despite their relevance, little to no
attention has been directed to the study of their security implications and
robustness to adversarial scenarios, likely due to their recent appearance in
the literature. With my thesis, I aim to provide the first analysis of the
security of RM-based reinforcement learning techniques, with the hope of
motivating further research in the field, and I propose and evaluate a novel
class of attacks on RM-based techniques: blinding attacks.
</p></li>
</ul>

<h3>Title: Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09127">http://arxiv.org/abs/2311.09127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09127]] Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts(http://arxiv.org/abs/2311.09127)</code></li>
<li>Summary: <p>Existing work on jailbreak Multimodal Large Language Models (MLLMs) has
focused primarily on adversarial examples in model inputs, with less attention
to vulnerabilities in model APIs. To fill the research gap, we carry out the
following work: 1) We discover a system prompt leakage vulnerability in GPT-4V.
Through carefully designed dialogue, we successfully steal the internal system
prompts of GPT-4V. This finding indicates potential exploitable security risks
in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM
jailbreaking attack method termed SASP (Self-Adversarial Attack via System
Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to
search for potential jailbreak prompts leveraging stolen system prompts.
Furthermore, in pursuit of better performance, we also add human modification
based on GPT-4's analysis, which further improves the attack success rate to
98.7\%; 3) We evaluated the effect of modifying system prompts to defend
against jailbreaking attacks. Results show that appropriately designed system
prompts can significantly reduce jailbreak success rates. Overall, our work
provides new insights into enhancing MLLM security, demonstrating the important
role of system prompts in jailbreaking, which could be leveraged to greatly
facilitate jailbreak success rates while also holding the potential for
defending against jailbreaks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MUDD: A New Re-Identification Dataset with Efficient Annotation for Off-Road Racers in Extreme Conditions. (arXiv:2311.08488v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08488">http://arxiv.org/abs/2311.08488</a></li>
<li>Code URL: https://github.com/jacobtyo/mudd</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08488]] MUDD: A New Re-Identification Dataset with Efficient Annotation for Off-Road Racers in Extreme Conditions(http://arxiv.org/abs/2311.08488)</code></li>
<li>Summary: <p>Re-identifying individuals in unconstrained environments remains an open
challenge in computer vision. We introduce the Muddy Racer re-IDentification
Dataset (MUDD), the first large-scale benchmark for matching identities of
motorcycle racers during off-road competitions. MUDD exhibits heavy mud
occlusion, motion blurring, complex poses, and extreme lighting conditions
previously unseen in existing re-id datasets. We present an annotation
methodology incorporating auxiliary information that reduced labeling time by
over 65%. We establish benchmark performance using state-of-the-art re-id
models including OSNet and ResNet-50. Without fine-tuning, the best models
achieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79%
Rank-1, but significant room for improvement remains. We analyze the impact of
real-world factors including mud, pose, lighting, and more. Our work exposes
open problems in re-identifying individuals under extreme conditions. We hope
MUDD serves as a diverse and challenging benchmark to spur progress in robust
re-id, especially for computer vision applications in emerging sports
analytics. All code and data can be found at https://github.com/JacobTyo/MUDD.
</p></li>
</ul>

<h3>Title: Incremental Object-Based Novelty Detection with Feedback Loop. (arXiv:2311.09004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09004">http://arxiv.org/abs/2311.09004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09004]] Incremental Object-Based Novelty Detection with Feedback Loop(http://arxiv.org/abs/2311.09004)</code></li>
<li>Summary: <p>Object-based Novelty Detection (ND) aims to identify unknown objects that do
not belong to classes seen during training by an object detection model. The
task is particularly crucial in real-world applications, as it allows to avoid
potentially harmful behaviours, e.g. as in the case of object detection models
adopted in a self-driving car or in an autonomous robot. Traditional approaches
to ND focus on one time offline post processing of the pretrained object
detection output, leaving no possibility to improve the model robustness after
training and discarding the abundant amount of out-of-distribution data
encountered during deployment.
</p>
<p>In this work, we propose a novel framework for object-based ND, assuming that
human feedback can be requested on the predicted output and later incorporated
to refine the ND model without negatively affecting the main object detection
performance. This refinement operation is repeated whenever new feedback is
available. To tackle this new formulation of the problem for object detection,
we propose a lightweight ND module attached on top of a pre-trained object
detection model, which is incrementally updated through a feedback loop. We
also propose a new benchmark to evaluate methods on this new setting and test
extensively our ND approach against baselines, showing increased robustness and
a successful incorporation of the received feedback.
</p></li>
</ul>

<h3>Title: Guided Scale Space Radon Transform for linear structures detection. (arXiv:2311.09103v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09103">http://arxiv.org/abs/2311.09103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09103]] Guided Scale Space Radon Transform for linear structures detection(http://arxiv.org/abs/2311.09103)</code></li>
<li>Summary: <p>Using integral transforms to the end of lines detection in images with
complex background, makes the detection a hard task needing additional
processing to manage the detection. As an integral transform, the Scale Space
Radon Transform (SSRT) suffers from such drawbacks, even with its great
abilities for thick lines detection. In this work, we propose a method to
address this issue for automatic detection of thick linear structures in gray
scale and binary images using the SSRT, whatever the image background content.
This method involves the calculated Hessian orientations of the investigated
image while computing its SSRT, in such a way that linear structures are
emphasized in the SSRT space. As a consequence, the subsequent maxima detection
in the SSRT space is done on a modified transform space freed from unwanted
parts and, consequently, from irrelevant peaks that usually drown the peaks
representing lines. Besides, highlighting the linear structure in the SSRT
space permitting, thus, to efficiently detect lines of different thickness in
synthetic and real images, the experiments show also the method robustness
against noise and complex background.
</p></li>
</ul>

<h3>Title: Domain Aligned CLIP for Few-shot Classification. (arXiv:2311.09191v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09191">http://arxiv.org/abs/2311.09191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09191]] Domain Aligned CLIP for Few-shot Classification(http://arxiv.org/abs/2311.09191)</code></li>
<li>Summary: <p>Large vision-language representation learning models like CLIP have
demonstrated impressive performance for zero-shot transfer to downstream tasks
while largely benefiting from inter-modal (image-text) alignment via
contrastive objectives. This downstream performance can further be enhanced by
full-scale fine-tuning which is often compute intensive, requires large
labelled data, and can reduce out-of-distribution (OOD) robustness.
Furthermore, sole reliance on inter-modal alignment might overlook the rich
information embedded within each individual modality. In this work, we
introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain
Aligned CLIP (DAC), which improves both intra-modal (image-image) and
inter-modal alignment on target distributions without fine-tuning the main
model. For intra-modal alignment, we introduce a lightweight adapter that is
specifically trained with an intra-modal contrastive objective. To improve
inter-modal alignment, we introduce a simple framework to modulate the
precomputed class text embeddings. The proposed few-shot fine-tuning framework
is computationally efficient, robust to distribution shifts, and does not alter
CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11
widely used image classification tasks with consistent improvements in 16-shot
classification upon strong baselines by about 2.3% and demonstrate competitive
performance on 4 OOD robustness benchmarks.
</p></li>
</ul>

<h3>Title: Functionality learning through specification instructions. (arXiv:2311.08481v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08481">http://arxiv.org/abs/2311.08481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08481]] Functionality learning through specification instructions(http://arxiv.org/abs/2311.08481)</code></li>
<li>Summary: <p>Test suites assess natural language processing models' performance on
specific functionalities: cases of interest involving model robustness,
fairness, or particular linguistic capabilities. They enable fine-grained
evaluations of model aspects that would otherwise go unnoticed in standard
evaluation datasets, but they do not address the problem of how to fix the
failure cases. Previous work has explored functionality learning by fine-tuning
models on suite data. While this improves performance on seen functionalities,
it often does not generalize to unseen ones and can harm general performance.
</p>
<p>This paper analyses a fine-tuning-free approach to functionality learning.
For each functionality in a suite, we generate a specification instruction that
encodes it. We combine the obtained specification instructions to create
specification-augmented prompts, which we feed to language models pre-trained
on natural instruction data to generate suite predictions. A core aspect of our
analysis is to measure the effect that including a set of specifications has on
a held-out set of unseen, qualitatively different specifications. Our
experiments across four tasks and models ranging from 80M to 175B parameters
show that smaller models struggle to follow specification instructions.
However, larger models (&gt; 3B params.) can benefit from specifications and even
generalize desirable behaviors across functionalities.
</p></li>
</ul>

<h3>Title: CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. (arXiv:2311.08588v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08588">http://arxiv.org/abs/2311.08588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08588]] CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation(http://arxiv.org/abs/2311.08588)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable performance on
coding related tasks, particularly on assisting humans in programming and
facilitating programming automation. However, existing benchmarks for
evaluating the code understanding and generation capacities of LLMs suffer from
severe limitations. First, most benchmarks are deficient as they focus on a
narrow range of popular programming languages and specific tasks, whereas the
real-world software development scenarios show dire need to implement systems
with multilingual programming environments to satisfy diverse requirements.
Practical programming practices also strongly expect multi-task settings for
testing coding capabilities of LLMs comprehensively and robustly. Second, most
benchmarks also fail to consider the actual executability and the consistency
of execution results of the generated code. To bridge these gaps between
existing benchmarks and expectations from practical applications, we introduce
CodeScope, an execution-based, multilingual, multi-task, multi-dimensional
evaluation benchmark for comprehensively gauging LLM capabilities on coding
tasks. CodeScope covers 43 programming languages and 8 coding tasks. It
evaluates the coding performance of LLMs from three dimensions (perspectives):
difficulty, efficiency, and length. To facilitate execution-based evaluations
of code generation, we develop MultiCodeEngine, an automated code execution
engine that supports 14 programming languages. Finally, we systematically
evaluate and analyze 8 mainstream LLMs on CodeScope tasks and demonstrate the
superior breadth and challenges of CodeScope for evaluating LLMs on code
understanding and generation tasks compared to other benchmarks. The CodeScope
benchmark and datasets are publicly available at
https://github.com/WeixiangYAN/CodeScope.
</p></li>
</ul>

<h3>Title: Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment. (arXiv:2311.08596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08596">http://arxiv.org/abs/2311.08596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08596]] Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment(http://arxiv.org/abs/2311.08596)</code></li>
<li>Summary: <p>The interactive nature of Large Language Models (LLMs) theoretically allows
models to refine and improve their answers, yet systematic analysis of the
multi-turn behavior of LLMs remains limited. In this paper, we propose the
FlipFlop experiment: in the first round of the conversation, an LLM responds to
a prompt containing a classification task. In a second round, the LLM is
challenged with a follow-up phrase like "Are you sure?", offering an
opportunity for the model to reflect on its initial answer, and decide whether
to confirm or flip its answer. A systematic study of nine LLMs on seven
classification tasks reveals that models flip their answers on average 46% of
the time and that all models see a deterioration of accuracy between their
first and final prediction, with an average drop of 17%. The FlipFlop
experiment illustrates the universality of sycophantic behavior in LLMs and
provides a robust framework to analyze model behavior and evaluate potential
solutions.
</p></li>
</ul>

<h3>Title: Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08648">http://arxiv.org/abs/2311.08648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08648]] Explore Spurious Correlations at the Concept Level in Language Models for Text Classification(http://arxiv.org/abs/2311.08648)</code></li>
<li>Summary: <p>Language models (LMs) have gained great achievement in various NLP tasks for
both fine-tuning and in-context learning (ICL) methods. Despite its outstanding
performance, evidence shows that spurious correlations caused by imbalanced
label distributions in training data (or exemplars in ICL) lead to robustness
issues. However, previous studies mostly focus on word- and phrase-level
features and fail to tackle it from the concept level, partly due to the lack
of concept labels and subtle and diverse expressions of concepts in text. In
this paper, we first use the LLM to label the concept for each text and then
measure the concept bias of models for fine-tuning or ICL on the test data.
Second, we propose a data rebalancing method to mitigate the spurious
correlations by adding the LLM-generated counterfactual data to make a balanced
label distribution for each concept. We verify the effectiveness of our
mitigation method and show its superiority over the token removal method.
Overall, our results show that there exist label distribution biases in
concepts across multiple text classification datasets, and LMs will utilize
these shortcuts to make predictions in both fine-tuning and ICL methods.
</p></li>
</ul>

<h3>Title: Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets. (arXiv:2311.08662v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08662">http://arxiv.org/abs/2311.08662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08662]] Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets(http://arxiv.org/abs/2311.08662)</code></li>
<li>Summary: <p>Language models, given their black-box nature, often exhibit sensitivity to
input perturbations, leading to trust issues due to hallucinations. To bolster
trust, it's essential to understand these models' failure modes and devise
strategies to enhance their performance. In this study, we propose a framework
to study the effect of input perturbations on language models of different
scales, from pre-trained models to large language models (LLMs). We use
fine-tuning to train a robust model to perturbations, and we investigate
whether exposure to one perturbation improves or degrades the model's
performance on other perturbations. To address multi-perturbation robustness,
we suggest three distinct training strategies. We also extend the framework to
LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our
framework for the Tabular-NLI task and show that the proposed strategies train
the model robust to different perturbations without losing accuracy on a given
dataset.
</p></li>
</ul>

<h3>Title: Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations. (arXiv:2311.08705v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08705">http://arxiv.org/abs/2311.08705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08705]] Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations(http://arxiv.org/abs/2311.08705)</code></li>
<li>Summary: <p>Dialogue summarization task involves summarizing long conversations while
preserving the most salient information. Real-life dialogues often involve
naturally occurring variations (e.g., repetitions, hesitations) and existing
dialogue summarization models suffer from performance drop on such
conversations. In this study, we systematically investigate the impact of such
variations on state-of-the-art dialogue summarization models using publicly
available datasets. To simulate real-life variations, we introduce two types of
perturbations: utterance-level perturbations that modify individual utterances
with errors and language variations, and dialogue-level perturbations that add
non-informative exchanges (e.g., repetitions, greetings). We conduct our
analysis along three dimensions of robustness: consistency, saliency, and
faithfulness, which capture different aspects of the summarization model's
performance. We find that both fine-tuned and instruction-tuned models are
affected by input variations, with the latter being more susceptible,
particularly to dialogue-level perturbations. We also validate our findings via
human evaluation. Finally, we investigate if the robustness of fine-tuned
models can be improved by training them with a fraction of perturbed data and
observe that this approach is insufficient to address robustness challenges
with current models and thus warrants a more thorough investigation to identify
better solutions. Overall, our work highlights robustness challenges in
dialogue summarization and provides insights for future research.
</p></li>
</ul>

<h3>Title: Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts. (arXiv:2311.09066v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09066">http://arxiv.org/abs/2311.09066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09066]] Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts(http://arxiv.org/abs/2311.09066)</code></li>
<li>Summary: <p>In the last decade, the United States has lost more than 500,000 people from
an overdose involving prescription and illicit opioids
(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national
public health emergency (USDHHS, 2017). To more effectively prevent
unintentional opioid overdoses, medical practitioners require robust and timely
tools that can effectively identify at-risk patients. Community-based social
media platforms such as Reddit allow self-disclosure for users to discuss
otherwise sensitive drug-related behaviors, often acting as indicators for
opioid use disorder. Towards this, we present a moderate size corpus of 2500
opioid-related posts from various subreddits spanning 6 different phases of
opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For
every post, we annotate span-level extractive explanations and crucially study
their role both in annotation quality and model development. We evaluate
several state-of-the-art models in a supervised, few-shot, or zero-shot
setting. Experimental results and error analysis show that identifying the
phases of opioid use disorder is highly contextual and challenging. However, we
find that using explanations during modeling leads to a significant boost in
classification accuracy demonstrating their beneficial role in a high-stakes
domain such as studying the opioid use disorder continuum. The dataset will be
made available for research on Github in the formal version.
</p></li>
</ul>

<h3>Title: R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces. (arXiv:2311.09117v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09117">http://arxiv.org/abs/2311.09117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09117]] R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces(http://arxiv.org/abs/2311.09117)</code></li>
<li>Summary: <p>This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised
fine-tuning framework for speaker and noise-invariant speech representations by
learning discrete acoustic units with speaker-invariant clustering (Spin).
R-Spin resolves Spin's issues and enhances content representations by learning
to predict acoustic pieces. R-Spin offers a 12X reduction in computational
resources compared to previous state-of-the-art methods while outperforming
them in severely distorted speech scenarios. This paper provides detailed
analyses to show how discrete units contribute to speech encoder training and
improving robustness in diverse acoustic environments.
</p></li>
</ul>

<h3>Title: RRescue: Ranking LLM Responses to Enhance Reasoning Over Context. (arXiv:2311.09136v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09136">http://arxiv.org/abs/2311.09136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09136]] RRescue: Ranking LLM Responses to Enhance Reasoning Over Context(http://arxiv.org/abs/2311.09136)</code></li>
<li>Summary: <p>Effectively using a given context is paramount for large language models. A
context window can include task specifications, retrieved documents, previous
conversations, and even model self-reflections, functioning similarly to
episodic memory. While efforts are being made to expand the context window,
studies indicate that LLMs do not use their context optimally for response
generation. In this paper, we present a novel approach to optimize LLMs using
ranking metrics, which teaches LLMs to rank a collection of
contextually-grounded candidate responses. Rather than a traditional full
ordering, we advocate for a partial ordering. This is because achieving
consensus on the perfect order for system responses can be challenging. Our
partial ordering is more robust, less sensitive to noise, and can be acquired
through human labelers, heuristic functions, or model distillation. We test our
system's improved contextual understanding using the latest benchmarks,
including a new multi-document question answering dataset. We conduct ablation
studies to understand crucial factors, such as how to gather candidate
responses, determine their most suitable order, and balance supervised
fine-tuning with ranking metrics. Our approach, named RRescue, suggests a
promising avenue for enhancing LLMs' contextual understanding via response
ranking.
</p></li>
</ul>

<h3>Title: A Robust Semantics-based Watermark for Large Language Model against Paraphrasing. (arXiv:2311.08721v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08721">http://arxiv.org/abs/2311.08721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08721]] A Robust Semantics-based Watermark for Large Language Model against Paraphrasing(http://arxiv.org/abs/2311.08721)</code></li>
<li>Summary: <p>Large language models (LLMs) have show great ability in various natural
language tasks. However, there are concerns that LLMs are possible to be used
improperly or even illegally. To prevent the malicious usage of LLMs, detecting
LLM-generated text becomes crucial in the deployment of LLM applications.
Watermarking is an effective strategy to detect the LLM-generated content by
encoding a pre-defined secret watermark to facilitate the detection process.
However, the majority of existing watermark methods leverage the simple hashes
of precedent tokens to partition vocabulary. Such watermark can be easily
eliminated by paraphrase and correspondingly the detection effectiveness will
be greatly compromised. Thus, to enhance the robustness against paraphrase, we
propose a semantics-based watermark framework SemaMark. It leverages the
semantics as an alternative to simple hashes of tokens since the paraphrase
will likely preserve the semantic meaning of the sentences. Comprehensive
experiments are conducted to demonstrate the effectiveness and robustness of
SemaMark under different paraphrases.
</p></li>
</ul>

<h3>Title: Assessing the Robustness of Intelligence-Driven Reinforcement Learning. (arXiv:2311.09027v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09027">http://arxiv.org/abs/2311.09027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09027]] Assessing the Robustness of Intelligence-Driven Reinforcement Learning(http://arxiv.org/abs/2311.09027)</code></li>
<li>Summary: <p>Robustness to noise is of utmost importance in reinforcement learning
systems, particularly in military contexts where high stakes and uncertain
environments prevail. Noise and uncertainty are inherent features of military
operations, arising from factors such as incomplete information, adversarial
actions, or unpredictable battlefield conditions. In RL, noise can critically
impact decision-making, mission success, and the safety of personnel. Reward
machines offer a powerful tool to express complex reward structures in RL
tasks, enabling the design of tailored reinforcement signals that align with
mission objectives. This paper considers the problem of the robustness of
intelligence-driven reinforcement learning based on reward machines. The
preliminary results presented suggest the need for further research in
evidential reasoning and learning to harden current state-of-the-art
reinforcement learning approaches before being mission-critical-ready.
</p></li>
</ul>

<h3>Title: Efficiently Escaping Saddle Points for Non-Convex Policy Optimization. (arXiv:2311.08914v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08914">http://arxiv.org/abs/2311.08914</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08914]] Efficiently Escaping Saddle Points for Non-Convex Policy Optimization(http://arxiv.org/abs/2311.08914)</code></li>
<li>Summary: <p>Policy gradient (PG) is widely used in reinforcement learning due to its
scalability and good performance. In recent years, several variance-reduced PG
methods have been proposed with a theoretical guarantee of converging to an
approximate first-order stationary point (FOSP) with the sample complexity of
$O(\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points.
Moreover, these algorithms often use importance sampling (IS) weights which
could impair the statistical effectiveness of variance reduction. In this
paper, we propose a variance-reduced second-order method that uses second-order
information in the form of Hessian vector products (HVP) and converges to an
approximate second-order stationary point (SOSP) with sample complexity of
$\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity
for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover,
the proposed variance reduction technique bypasses IS weights by using HVP
terms. Our experimental results show that the proposed algorithm outperforms
the state of the art and is more robust to changes in random seeds.
</p></li>
</ul>

<h3>Title: On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09018">http://arxiv.org/abs/2311.09018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09018]] On the Foundation of Distributionally Robust Reinforcement Learning(http://arxiv.org/abs/2311.09018)</code></li>
<li>Summary: <p>Motivated by the need for a robust policy in the face of environment shifts
between training and the deployment, we contribute to the theoretical
foundation of distributionally robust reinforcement learning (DRRL). This is
accomplished through a comprehensive modeling framework centered around
distributionally robust Markov decision processes (DRMDPs). This framework
obliges the decision maker to choose an optimal policy under the worst-case
distributional shift orchestrated by an adversary. By unifying and extending
existing formulations, we rigorously construct DRMDPs that embraces various
modeling attributes for both the decision maker and the adversary. These
attributes include adaptability granularity, exploring history-dependent,
Markov, and Markov time-homogeneous decision maker and adversary dynamics.
Additionally, we delve into the flexibility of shifts induced by the adversary,
examining SA and S-rectangularity. Within this DRMDP framework, we investigate
conditions for the existence or absence of the dynamic programming principle
(DPP). From an algorithmic standpoint, the existence of DPP holds significant
implications, as the vast majority of existing data and computationally
efficiency RL algorithms are reliant on the DPP. To study its existence, we
comprehensively examine combinations of controller and adversary attributes,
providing streamlined proofs grounded in a unified methodology. We also offer
counterexamples for settings in which a DPP with full generality is absent.
</p></li>
</ul>

<h3>Title: HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data. (arXiv:2311.09115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09115">http://arxiv.org/abs/2311.09115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09115]] HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data(http://arxiv.org/abs/2311.09115)</code></li>
<li>Summary: <p>Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding. (arXiv:2311.08673v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08673">http://arxiv.org/abs/2311.08673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08673]] CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding(http://arxiv.org/abs/2311.08673)</code></li>
<li>Summary: <p>This paper proposes a talking face generation method named "CP-EB" that takes
an audio signal as input and a person image as reference, to synthesize a
photo-realistic people talking video with head poses controlled by a short
video clip and proper eye blinking embedding. It's noted that not only the head
pose but also eye blinking are both important aspects for deep fake detection.
The implicit control of poses by video has already achieved by the state-of-art
work. According to recent research, eye blinking has weak correlation with
input audio which means eye blinks extraction from audio and generation are
possible. Hence, we propose a GAN-based architecture to extract eye blink
feature from input audio and reference video respectively and employ
contrastive training between them, then embed it into the concatenated features
of identity and poses to generate talking face images. Experimental results
show that the proposed method can generate photo-realistic talking face with
synchronous lips motions, natural head poses and blinking eyes.
</p></li>
</ul>

<h3>Title: Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images. (arXiv:2311.08995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08995">http://arxiv.org/abs/2311.08995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08995]] Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images(http://arxiv.org/abs/2311.08995)</code></li>
<li>Summary: <p>High-quality labeled datasets are essential for deep learning. Traditional
manual annotation methods are not only costly and inefficient but also pose
challenges in specialized domains where expert knowledge is needed.
Self-supervised methods, despite leveraging unlabeled data for feature
extraction, still require hundreds or thousands of labeled instances to guide
the model for effective specialized image classification. Current unsupervised
learning methods offer automatic classification without prior annotation but
often compromise on accuracy. As a result, efficiently procuring high-quality
labeled datasets remains a pressing challenge for specialized domain images
devoid of annotated data. Addressing this, an unsupervised classification
method with three key ideas is introduced: 1) dual-step feature dimensionality
reduction using a pre-trained model and manifold learning, 2) a voting
mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior
manual annotation. This approach outperforms supervised methods in
classification accuracy, as demonstrated with fungal image data, achieving
94.1% and 96.7% on public and private datasets respectively. The proposed
unsupervised classification method reduces dependency on pre-annotated
datasets, enabling a closed-loop for data classification. The simplicity and
ease of use of this method will also bring convenience to researchers in
various fields in building datasets, promoting AI applications for images in
specialized domains.
</p></li>
</ul>

<h3>Title: Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission. (arXiv:2311.08726v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08726">http://arxiv.org/abs/2311.08726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08726]] Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission(http://arxiv.org/abs/2311.08726)</code></li>
<li>Summary: <p>Sequential labeling is a task predicting labels for each token in a sequence,
such as Named Entity Recognition (NER). NER tasks aim to extract entities and
predict their labels given a text, which is important in information
extraction. Although previous works have shown great progress in improving NER
performance, uncertainty estimation on NER (UE-NER) is still underexplored but
essential. This work focuses on UE-NER, which aims to estimate uncertainty
scores for the NER predictions. Previous uncertainty estimation models often
overlook two unique characteristics of NER: the connection between entities
(i.e., one entity embedding is learned based on the other ones) and wrong span
cases in the entity extraction subtask. Therefore, we propose a Sequential
Labeling Posterior Network (SLPN) to estimate uncertainty scores for the
extracted entities, considering uncertainty transmitted from other tokens.
Moreover, we have defined an evaluation strategy to address the specificity of
wrong-span cases. Our SLPN has achieved significant improvements on two
datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant
dataset.
</p></li>
</ul>

<h3>Title: When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks. (arXiv:2311.08993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08993">http://arxiv.org/abs/2311.08993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08993]] When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks(http://arxiv.org/abs/2311.08993)</code></li>
<li>Summary: <p>In-context learning (ICL) has become the default method for using large
language models (LLMs), making the exploration of its limitations and
understanding the underlying causes crucial. In this paper, we find that ICL
falls short of handling specification-heavy tasks, which are tasks with
complicated and extensive task specifications, requiring several hours for
ordinary humans to master, such as traditional information extraction tasks.
The performance of ICL on these tasks mostly cannot reach half of the
state-of-the-art results. To explore the reasons behind this failure, we
conduct comprehensive experiments on 18 specification-heavy tasks with various
LLMs and identify three primary reasons: inability to specifically understand
context, misalignment in task schema comprehension with humans, and inadequate
long-text understanding ability. Furthermore, we demonstrate that through
fine-tuning, LLMs can achieve decent performance on these tasks, indicating
that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback
of existing alignment methods that renders LLMs incapable of handling
complicated specification-heavy tasks via ICL. To substantiate this, we perform
dedicated instruction tuning on LLMs for these tasks and observe a notable
improvement. We hope the analyses in this paper could facilitate advancements
in alignment methods enabling LLMs to meet more sophisticated human demands.
</p></li>
</ul>

<h3>Title: MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation. (arXiv:2311.09105v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09105">http://arxiv.org/abs/2311.09105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09105]] MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation(http://arxiv.org/abs/2311.09105)</code></li>
<li>Summary: <p>Understanding events in texts is a core objective of natural language
understanding, which requires detecting event occurrences, extracting event
arguments, and analyzing inter-event relationships. However, due to the
annotation challenges brought by task complexity, a large-scale dataset
covering the full process of event understanding has long been absent. In this
paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event
argument annotations, making the first all-in-one dataset supporting event
detection, event argument extraction (EAE), and event relation extraction. As
an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive
schema covering 162 event types and 612 argument roles, all with expert-written
definitions and examples; (2) a large data scale, containing 98,591 events and
290,613 arguments obtained with laborious human annotation; (3) the exhaustive
annotation supporting all task variants of EAE, which annotates both entity and
non-entity event arguments in document level. Experiments indicate that
MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary
large language models (LLMs). Furthermore, to demonstrate the benefits of an
all-in-one dataset, we preliminarily explore a potential application, future
event prediction, with LLMs. MAVEN-Arg and our code can be obtained from
https://github.com/THU-KEG/MAVEN-Argument.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning. (arXiv:2311.08479v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08479">http://arxiv.org/abs/2311.08479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08479]] Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning(http://arxiv.org/abs/2311.08479)</code></li>
<li>Summary: <p>Federated Learning (FL) is a distributed training paradigm that enables
clients scattered across the world to cooperatively learn a global model
without divulging confidential data. However, FL faces a significant challenge
in the form of heterogeneous data distributions among clients, which leads to a
reduction in performance and robustness. A recent approach to mitigating the
impact of heterogeneous data distributions is through the use of foundation
models, which offer better performance at the cost of larger computational
overheads and slower inference speeds. We introduce foundation model
distillation to assist in the federated training of lightweight client models
and increase their performance under heterogeneous data settings while keeping
inference costs low. Our results show improvement in the global model
performance on a balanced testing set, which contains rarely observed samples,
even under extreme non-IID client data distributions. We conduct a thorough
evaluation of our framework with different foundation model backbones on
CIFAR10, with varying degrees of heterogeneous data distributions ranging from
class-specific data partitions across clients to dirichlet data sampling,
parameterized by values between 0.01 and 1.0.
</p></li>
</ul>

<h3>Title: Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories. (arXiv:2311.08716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08716">http://arxiv.org/abs/2311.08716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08716]] Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories(http://arxiv.org/abs/2311.08716)</code></li>
<li>Summary: <p>Federated learning is a privacy-preserving training method which consists of
training from a plurality of clients but without sharing their confidential
data. However, previous work on federated learning do not explore suitable
neural network architectures for clients with different input images sizes and
different numbers of output categories. In this paper, we propose an effective
federated learning method named ScalableFL, where the depths and widths of the
local models for each client are adjusted according to the clients' input image
size and the numbers of output categories. In addition, we provide a new bound
for the generalization gap of federated learning. In particular, this bound
helps to explain the effectiveness of our scalable neural network approach. We
demonstrate the effectiveness of ScalableFL in several heterogeneous client
settings for both image classification and object detection tasks.
</p></li>
</ul>

<h3>Title: One-Shot Federated Learning with Classifier-Guided Diffusion Models. (arXiv:2311.08870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08870">http://arxiv.org/abs/2311.08870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08870]] One-Shot Federated Learning with Classifier-Guided Diffusion Models(http://arxiv.org/abs/2311.08870)</code></li>
<li>Summary: <p>One-shot federated learning (OSFL) has gained attention in recent years due
to its low communication cost. However, most of the existing methods require
auxiliary datasets or training generators, which hinders their practicality in
real-world scenarios. In this paper, we explore the novel opportunities that
diffusion models bring to OSFL and propose FedCADO, utilizing guidance from
client classifiers to generate data that complies with clients' distributions
and subsequently training the aggregated model on the server. Specifically, our
method involves targeted optimizations in two aspects. On one hand, we
conditionally edit the randomly sampled initial noises, embedding them with
specified semantics and distributions, resulting in a significant improvement
in both the quality and stability of generation. On the other hand, we employ
the BN statistics from the classifiers to provide detailed guidance during
generation. These tailored optimizations enable us to limitlessly generate
datasets, which closely resemble the distribution and quality of the original
client dataset. Our method effectively handles the heterogeneous client models
and the problems of non-IID features or labels. In terms of privacy protection,
our method avoids training any generator or transferring any auxiliary
information on clients, eliminating any additional privacy leakage risks.
Leveraging the extensive knowledge stored in the pre-trained diffusion model,
the synthetic datasets can assist us in surpassing the knowledge limitations of
the client samples, resulting in aggregation models that even outperform the
performance ceiling of centralized training in some cases, which is
convincingly demonstrated in the sufficient quantification and visualization
experiments conducted on three large-scale multi-domain image datasets.
</p></li>
</ul>

<h3>Title: Federated Learning for Sparse Principal Component Analysis. (arXiv:2311.08677v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08677">http://arxiv.org/abs/2311.08677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08677]] Federated Learning for Sparse Principal Component Analysis(http://arxiv.org/abs/2311.08677)</code></li>
<li>Summary: <p>In the rapidly evolving realm of machine learning, algorithm effectiveness
often faces limitations due to data quality and availability. Traditional
approaches grapple with data sharing due to legal and privacy concerns. The
federated learning framework addresses this challenge. Federated learning is a
decentralized approach where model training occurs on client sides, preserving
privacy by keeping data localized. Instead of sending raw data to a central
server, only model updates are exchanged, enhancing data security. We apply
this framework to Sparse Principal Component Analysis (SPCA) in this work. SPCA
aims to attain sparse component loadings while maximizing data variance for
improved interpretability. Beside the L1 norm regularization term in
conventional SPCA, we add a smoothing function to facilitate gradient-based
optimization methods. Moreover, in order to improve computational efficiency,
we introduce a least squares approximation to original SPCA. This enables
analytic solutions on the optimization processes, leading to substantial
computational improvements. Within the federated framework, we formulate SPCA
as a consensus optimization problem, which can be solved using the Alternating
Direction Method of Multipliers (ADMM). Our extensive experiments involve both
IID and non-IID random features across various data owners. Results on
synthetic and public datasets affirm the efficacy of our federated SPCA
approach.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models. (arXiv:2311.08472v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08472">http://arxiv.org/abs/2311.08472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08472]] Selecting Shots for Demographic Fairness in Few-Shot Learning with Large Language Models(http://arxiv.org/abs/2311.08472)</code></li>
<li>Summary: <p>Recently, work in NLP has shifted to few-shot (in-context) learning, with
large language models (LLMs) performing well across a range of tasks. However,
while fairness evaluations have become a standard for supervised methods,
little is known about the fairness of LLMs as prediction systems. Further,
common standard methods for fairness involve access to models weights or are
applied during finetuning, which are not applicable in few-shot learning. Do
LLMs exhibit prediction biases when used for standard NLP tasks? In this work,
we explore the effect of shots, which directly affect the performance of
models, on the fairness of LLMs as NLP classification systems. We consider how
different shot selection strategies, both existing and new demographically
sensitive methods, affect model fairness across three standard fairness
datasets. We discuss how future work can include LLM fairness evaluations.
</p></li>
</ul>

<h3>Title: SentAlign: Accurate and Scalable Sentence Alignment. (arXiv:2311.08982v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08982">http://arxiv.org/abs/2311.08982</a></li>
<li>Code URL: https://github.com/steinst/sentalign</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08982]] SentAlign: Accurate and Scalable Sentence Alignment(http://arxiv.org/abs/2311.08982)</code></li>
<li>Summary: <p>We present SentAlign, an accurate sentence alignment tool designed to handle
very large parallel document pairs. Given user-defined parameters, the
alignment algorithm evaluates all possible alignment paths in fairly large
documents of thousands of sentences and uses a divide-and-conquer approach to
align documents containing tens of thousands of sentences. The scoring function
is based on LaBSE bilingual sentence representations. SentAlign outperforms
five other sentence alignment tools when evaluated on two different evaluation
sets, German-French and English-Icelandic, and on a downstream machine
translation task.
</p></li>
</ul>

<h3>Title: Social Bias Probing: Fairness Benchmarking for Language Models. (arXiv:2311.09090v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09090">http://arxiv.org/abs/2311.09090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09090]] Social Bias Probing: Fairness Benchmarking for Language Models(http://arxiv.org/abs/2311.09090)</code></li>
<li>Summary: <p>Large language models have been shown to encode a variety of social biases,
which carries the risk of downstream harms. While the impact of these biases
has been recognized, prior methods for bias evaluation have been limited to
binary association tests on small datasets, offering a constrained view of the
nature of societal biases within language models. In this paper, we propose an
original framework for probing language models for societal biases. We collect
a probing dataset to analyze language models' general associations, as well as
along the axes of societal categories, identities, and stereotypes. To this
end, we leverage a novel perplexity-based fairness score. We curate a
large-scale benchmarking dataset addressing drawbacks and limitations of
existing fairness collections, expanding to a variety of different identities
and stereotypes. When comparing our methodology with prior work, we demonstrate
that biases within language models are more nuanced than previously
acknowledged. In agreement with recent findings, we find that larger model
variants exhibit a higher degree of bias. Moreover, we expose how identities
expressing different religions lead to the most pronounced disparate treatments
across all models.
</p></li>
</ul>

<h3>Title: Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale. (arXiv:2311.08430v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08430">http://arxiv.org/abs/2311.08430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08430]] Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale(http://arxiv.org/abs/2311.08430)</code></li>
<li>Summary: <p>Neural Architecture Search (NAS) has demonstrated its efficacy in computer
vision and potential for ranking systems. However, prior work focused on
academic problems, which are evaluated at small scale under well-controlled
fixed baselines. In industry system, such as ranking system in Meta, it is
unclear whether NAS algorithms from the literature can outperform production
baselines because of: (1) scale - Meta ranking systems serve billions of users,
(2) strong baselines - the baselines are production models optimized by
hundreds to thousands of world-class engineers for years since the rise of deep
learning, (3) dynamic baselines - engineers may have established new and
stronger baselines during NAS search, and (4) efficiency - the search pipeline
must yield results quickly in alignment with the productionization life cycle.
In this paper, we present Rankitect, a NAS software framework for ranking
systems at Meta. Rankitect seeks to build brand new architectures by composing
low level building blocks from scratch. Rankitect implements and improves
state-of-the-art (SOTA) NAS methods for comprehensive and fair comparison under
the same search space, including sampling-based NAS, one-shot NAS, and
Differentiable NAS (DNAS). We evaluate Rankitect by comparing to multiple
production ranking models at Meta. We find that Rankitect can discover new
models from scratch achieving competitive tradeoff between Normalized Entropy
loss and FLOPs. When utilizing search space designed by engineers, Rankitect
can generate better models than engineers, achieving positive offline
evaluation and online A/B test at Meta scale.
</p></li>
</ul>

<h3>Title: Learning Fair Division from Bandit Feedback. (arXiv:2311.09068v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09068">http://arxiv.org/abs/2311.09068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09068]] Learning Fair Division from Bandit Feedback(http://arxiv.org/abs/2311.09068)</code></li>
<li>Summary: <p>This work addresses learning online fair division under uncertainty, where a
central planner sequentially allocates items without precise knowledge of
agents' values or utilities. Departing from conventional online algorithm, the
planner here relies on noisy, estimated values obtained after allocating items.
We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling
gradual learning of both the type distribution of arriving items and agents'
values through bandit feedback. This approach enables the algorithms to
asymptotically achieve optimal Nash social welfare in linear Fisher markets
with agents having additive utilities. We establish regret bounds in Nash
social welfare and empirically validate the superior performance of our
proposed algorithms across synthetic and empirical datasets.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Towards Evaluating AI Systems for Moral Status Using Self-Reports. (arXiv:2311.08576v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08576">http://arxiv.org/abs/2311.08576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08576]] Towards Evaluating AI Systems for Moral Status Using Self-Reports(http://arxiv.org/abs/2311.08576)</code></li>
<li>Summary: <p>As AI systems become more advanced and widely deployed, there will likely be
increasing debate over whether AI systems could have conscious experiences,
desires, or other states of potential moral significance. It is important to
inform these discussions with empirical evidence to the extent possible. We
argue that under the right circumstances, self-reports, or an AI system's
statements about its own internal states, could provide an avenue for
investigating whether AI systems have states of moral significance.
Self-reports are the main way such states are assessed in humans ("Are you in
pain?"), but self-reports from current systems like large language models are
spurious for many reasons (e.g. often just reflecting what humans would say).
To make self-reports more appropriate for this purpose, we propose to train
models to answer many kinds of questions about themselves with known answers,
while avoiding or limiting training incentives that bias self-reports. The hope
of this approach is that models will develop introspection-like capabilities,
and that these capabilities will generalize to questions about states of moral
significance. We then propose methods for assessing the extent to which these
techniques have succeeded: evaluating self-report consistency across contexts
and between similar models, measuring the confidence and resilience of models'
self-reports, and using interpretability to corroborate self-reports. We also
discuss challenges for our approach, from philosophical difficulties in
interpreting self-reports to technical reasons why our proposal might fail. We
hope our discussion inspires philosophers and AI researchers to criticize and
improve our proposed methodology, as well as to run experiments to test whether
self-reports can be made reliable enough to provide information about states of
moral significance.
</p></li>
</ul>

<h3>Title: XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making. (arXiv:2311.08614v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08614">http://arxiv.org/abs/2311.08614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08614]] XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making(http://arxiv.org/abs/2311.08614)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have recently made impressive strides in natural
language understanding tasks. Despite their remarkable performance,
understanding their decision-making process remains a big challenge. In this
paper, we look into bringing some transparency to this process by introducing a
new explanation dataset for question answering (QA) tasks that integrates
knowledge graphs (KGs) in a novel way. Our dataset includes 12,102
question-answer-explanation (QAE) triples. Each explanation in the dataset
links the LLM's reasoning to entities and relations in the KGs. The explanation
component includes a why-choose explanation, a why-not-choose explanation, and
a set of reason-elements that underlie the LLM's decision. We leverage KGs and
graph attention networks (GAT) to find the reason-elements and transform them
into why-choose and why-not-choose explanations that are comprehensible to
humans. Through quantitative and qualitative evaluations, we demonstrate the
potential of our dataset to improve the in-context learning of LLMs, and
enhance their interpretability and explainability. Our work contributes to the
field of explainable AI by enabling a deeper understanding of the LLMs
decision-making process to make them more transparent and thereby, potentially
more reliable, to researchers and practitioners alike. Our dataset is available
at: https://github.com/chen-zichen/XplainLLM_dataset.git
</p></li>
</ul>

<h3>Title: Token Prediction as Implicit Classification to Identify LLM-Generated Text. (arXiv:2311.08723v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08723">http://arxiv.org/abs/2311.08723</a></li>
<li>Code URL: https://github.com/markchenyutian/t5-sentinel-public</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08723]] Token Prediction as Implicit Classification to Identify LLM-Generated Text(http://arxiv.org/abs/2311.08723)</code></li>
<li>Summary: <p>This paper introduces a novel approach for identifying the possible large
language models (LLMs) involved in text generation. Instead of adding an
additional classification layer to a base LM, we reframe the classification
task as a next-token prediction task and directly fine-tune the base LM to
perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the
backbone for our experiments. We compared our approach to the more direct
approach of utilizing hidden states for classification. Evaluation shows the
exceptional performance of our method in the text classification task,
highlighting its simplicity and efficiency. Furthermore, interpretability
studies on the features extracted by our model reveal its ability to
differentiate distinctive writing styles among various LLMs even in the absence
of an explicit classifier. We also collected a dataset named OpenLLMText,
containing approximately 340k text samples from human and LLMs, including
GPT3.5, PaLM, LLaMA, and GPT2.
</p></li>
</ul>

<h3>Title: HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence. (arXiv:2311.08896v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08896">http://arxiv.org/abs/2311.08896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08896]] HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence(http://arxiv.org/abs/2311.08896)</code></li>
<li>Summary: <p>Large models have demonstrated significant progress across various domains,
particularly in tasks related to text generation. In the domain of Table to
Text, many Large Language Model (LLM)-based methods currently resort to
modifying prompts to invoke public APIs, incurring potential costs and
information leaks. With the advent of open-source large models, fine-tuning
LLMs has become feasible. In this study, we conducted parameter-efficient
fine-tuning on the LLaMA2 model. Distinguishing itself from previous
fine-tuning-based table-to-text methods, our approach involves injecting
reasoning information into the input by emphasizing table-specific row data.
Our model consists of two modules: 1) a table reasoner that identifies relevant
row evidence, and 2) a table summarizer that generates sentences based on the
highlighted table. To facilitate this, we propose a search strategy to
construct reasoning labels for training the table reasoner. On both the FetaQA
and QTSumm datasets, our approach achieved state-of-the-art results.
Additionally, we observed that highlighting input tables significantly enhances
the model's performance and provides valuable interpretability.
</p></li>
</ul>

<h3>Title: MELA: Multilingual Evaluation of Linguistic Acceptability. (arXiv:2311.09033v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09033">http://arxiv.org/abs/2311.09033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09033]] MELA: Multilingual Evaluation of Linguistic Acceptability(http://arxiv.org/abs/2311.09033)</code></li>
<li>Summary: <p>Recent benchmarks for Large Language Models (LLMs) have mostly focused on
application-driven tasks such as complex reasoning and code generation, and
this has led to a scarcity in purely linguistic evaluation of LLMs. Against
this background, we introduce Multilingual Evaluation of Linguistic
Acceptability -- MELA, the first multilingual benchmark on linguistic
acceptability with 48K samples covering 10 languages from a diverse set of
language families. We establish baselines of commonly used LLMs along with
supervised models, and conduct cross-lingual transfer and multi-task learning
experiments with XLM-R. In pursuit of multilingual interpretability, we analyze
the weights of fine-tuned XLM-R to explore the possibility of identifying
transfer difficulty between languages. Our results show that ChatGPT benefits
much from in-context examples but still lags behind fine-tuned XLM-R, while the
performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.
Cross-lingual and multi-task learning experiments show that unlike semantic
tasks, in-language training data is crucial in acceptability judgements.
Results in layerwise probing indicate that the upper layers of XLM-R become a
task-specific but language-agnostic region for multilingual acceptability
judgment. We also introduce the concept of conflicting weight, which could be a
potential indicator for the difficulty of cross-lingual transfer between
languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Towards a Transportable Causal Network Model Based on Observational Healthcare Data. (arXiv:2311.08427v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08427">http://arxiv.org/abs/2311.08427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08427]] Towards a Transportable Causal Network Model Based on Observational Healthcare Data(http://arxiv.org/abs/2311.08427)</code></li>
<li>Summary: <p>Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
</p>
<p>In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Finding AI-Generated Faces in the Wild. (arXiv:2311.08577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08577">http://arxiv.org/abs/2311.08577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08577]] Finding AI-Generated Faces in the Wild(http://arxiv.org/abs/2311.08577)</code></li>
<li>Summary: <p>AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
</p></li>
</ul>

<h3>Title: A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution. (arXiv:2311.08955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08955">http://arxiv.org/abs/2311.08955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08955]] A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution(http://arxiv.org/abs/2311.08955)</code></li>
<li>Summary: <p>Fusion-based hyperspectral image (HSI) super-resolution aims to produce a
high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a
high-spatial-resolution multispectral image. Such a HSI super-resolution
process can be modeled as an inverse problem, where the prior knowledge is
essential for obtaining the desired solution. Motivated by the success of
diffusion models, we propose a novel spectral diffusion prior for fusion-based
HSI super-resolution. Specifically, we first investigate the spectrum
generation problem and design a spectral diffusion model to model the spectral
data distribution. Then, in the framework of maximum a posteriori, we keep the
transition information between every two neighboring states during the reverse
generative process, and thereby embed the knowledge of trained spectral
diffusion model into the fusion problem in the form of a regularization term.
At last, we treat each generation step of the final optimization problem as its
subproblem, and employ the Adam to solve these subproblems in a reverse
sequence. Experimental results conducted on both synthetic and real datasets
demonstrate the effectiveness of the proposed approach. The code of the
proposed approach will be available on https://github.com/liuofficial/SDP.
</p></li>
</ul>

<h3>Title: Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion. (arXiv:2311.09128v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09128">http://arxiv.org/abs/2311.09128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09128]] Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion(http://arxiv.org/abs/2311.09128)</code></li>
<li>Summary: <p>Machine learning has been successfully used to study phase transitions. One
of the most popular approaches to identifying critical points from data without
prior knowledge of the underlying phases is the learning-by-confusion scheme.
As input, it requires system samples drawn from a grid of the parameter whose
change is associated with potential phase transitions. Up to now, the scheme
required training a distinct binary classifier for each possible splitting of
the grid into two sides, resulting in a computational cost that scales linearly
with the number of grid points. In this work, we propose and showcase an
alternative implementation that only requires the training of a single
multi-class classifier. Ideally, such multi-task learning eliminates the
scaling with respect to the number of grid points. In applications to the Ising
model and an image dataset generated with Stable Diffusion, we find significant
speedups that closely correspond to the ideal case, with only minor deviations.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation. (arXiv:2311.08525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08525">http://arxiv.org/abs/2311.08525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08525]] Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation(http://arxiv.org/abs/2311.08525)</code></li>
<li>Summary: <p>Humans and animals recognize objects irrespective of the beholder's point of
view, which may drastically change their appearances. Artificial pattern
recognizers also strive to achieve this, e.g., through translational invariance
in convolutional neural networks (CNNs). However, both CNNs and vision
transformers (ViTs) perform very poorly on rotated inputs. Here we present
artificial mental rotation (AMR), a novel deep learning paradigm for dealing
with in-plane rotations inspired by the neuro-psychological concept of mental
rotation. Our simple AMR implementation works with all common CNN and ViT
architectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a
top-1 error (averaged across datasets and architectures) of $0.743$, AMR
outperforms the current state of the art (rotational data augmentation, average
top-1 error of $0.626$) by $19\%$. We also easily transfer a trained AMR module
to a downstream task to improve the performance of a pre-trained semantic
segmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.
</p></li>
</ul>

<h3>Title: Multiple-Question Multiple-Answer Text-VQA. (arXiv:2311.08622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08622">http://arxiv.org/abs/2311.08622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08622]] Multiple-Question Multiple-Answer Text-VQA(http://arxiv.org/abs/2311.08622)</code></li>
<li>Summary: <p>We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do
text-VQA in encoder-decoder transformer models. The text-VQA task requires a
model to answer a question by understanding multi-modal content: text
(typically from OCR) and an associated image. To the best of our knowledge,
almost all previous approaches for text-VQA process a single question and its
associated content to predict a single answer. In order to answer multiple
questions from the same image, each question and content are fed into the model
multiple times. In contrast, our proposed MQMA approach takes multiple
questions and content as input at the encoder and predicts multiple answers at
the decoder in an auto-regressive manner at the same time. We make several
novel architectural modifications to standard encoder-decoder transformers to
support MQMA. We also propose a novel MQMA denoising pre-training task which is
designed to teach the model to align and delineate multiple questions and
content with associated answers. MQMA pre-trained model achieves
state-of-the-art results on multiple text-VQA datasets, each with strong
baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),
DocVQA (+1.1%) absolute improvements over the previous state-of-the-art
approaches.
</p></li>
</ul>

<h3>Title: DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models. (arXiv:2311.08623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08623">http://arxiv.org/abs/2311.08623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08623]] DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models(http://arxiv.org/abs/2311.08623)</code></li>
<li>Summary: <p>Encoder-decoder transformer models have achieved great success on various
vision-language (VL) tasks, but they suffer from high inference latency.
Typically, the decoder takes up most of the latency because of the
auto-regressive decoding. To accelerate the inference, we propose an approach
of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit
encoder-decoder transformer model which is trained with deep supervision so
that each of its decoder layers is capable of generating plausible predictions.
In addition, we leverage simple yet practical techniques, including shared
generation head and adaptation modules, to keep accuracy when exiting at
shallow decoder layers. Based on the multi-exit model, we perform step-level
dynamic early exit during inference, where the model may decide to use fewer
decoder layers based on its confidence of the current layer at each individual
decoding step. Considering different number of decoder layers may be used at
different decoding steps, we compute deeper-layer decoder features of previous
decoding steps just-in-time, which ensures the features from different decoding
steps are semantically aligned. We evaluate our approach with two
state-of-the-art encoder-decoder transformer models on various VL tasks. We
show our approach can reduce overall inference latency by 30%-60% with
comparable or even higher accuracy compared to baselines.
</p></li>
</ul>

<h3>Title: Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection. (arXiv:2311.08747v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08747">http://arxiv.org/abs/2311.08747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08747]] Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection(http://arxiv.org/abs/2311.08747)</code></li>
<li>Summary: <p>Infrared small target detection based on deep learning offers unique
advantages in separating small targets from complex and dynamic backgrounds.
However, the features of infrared small targets gradually weaken as the depth
of convolutional neural network (CNN) increases. To address this issue, we
propose a novel method for detecting infrared small targets called improved
dense nested attention network (IDNANet), which is based on the transformer
architecture. We preserve the dense nested structure of dense nested attention
network (DNANet) and introduce the Swin-transformer during feature extraction
stage to enhance the continuity of features. Furthermore, we integrate the
ACmix attention structure into the dense nested structure to enhance the
features of intermediate layers. Additionally, we design a weighted dice binary
cross-entropy (WD-BCE) loss function to mitigate the negative impact of
foreground-background imbalance in the samples. Moreover, we develop a dataset
specifically for infrared small targets, called BIT-SIRST. The dataset
comprises a significant amount of real-world targets and manually annotated
labels, as well as synthetic data and corresponding labels. We have evaluated
the effectiveness of our method through experiments conducted on public
datasets. In comparison to other state-of-the-art methods, our approach
outperforms in terms of probability of detection (P_d), false-alarm rate (F_a),
and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the
NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</p></li>
</ul>

<h3>Title: 4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters. (arXiv:2311.08759v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08759">http://arxiv.org/abs/2311.08759</a></li>
<li>Code URL: https://github.com/zhou-yijie/msltnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08759]] 4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters(http://arxiv.org/abs/2311.08759)</code></li>
<li>Summary: <p>The illumination of improperly exposed photographs has been widely corrected
using deep convolutional neural networks or Transformers. Despite with
promising performance, these methods usually suffer from large parameter
amounts and heavy computational FLOPs on high-resolution photographs. In this
paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale
Linear Transformation (MSLT) networks under the multi-layer perception
architecture, which can process 4K-resolution sRGB images at 125
Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT
networks first decompose an input image into high and low frequency layers by
Laplacian pyramid techniques, and then sequentially correct different layers by
pixel-adaptive linear transformation, which is implemented by efficient
bilateral grid learning or 1x1 convolutions. Experiments on two benchmark
datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts
on photo exposure correction. Extensive ablation studies validate the
effectiveness of our contributions. The code is available at
https://github.com/Zhou-Yijie/MSLTNet.
</p></li>
</ul>

<h3>Title: SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer. (arXiv:2311.08806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08806">http://arxiv.org/abs/2311.08806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08806]] SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer(http://arxiv.org/abs/2311.08806)</code></li>
<li>Summary: <p>As the third-generation neural network, the Spiking Neural Network (SNN) has
the advantages of low power consumption and high energy efficiency, making it
suitable for implementation on edge devices. More recently, the most advanced
SNN, Spikformer, combines the self-attention module from Transformer with SNN
to achieve remarkable performance. However, it adopts larger channel dimensions
in MLP layers, leading to an increased number of redundant model parameters. To
effectively decrease the computational complexity and weight parameters of the
model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very
sparse ($\ge$90%) subnetwork that achieves comparable performance to the
original network. Furthermore, we also design a lightweight token selector
module, which can remove unimportant background information from images based
on the average spike firing rate of neurons, selecting only essential
foreground image tokens to participate in attention calculation. Based on that,
we present SparseSpikformer, a co-design framework aimed at achieving sparsity
in Spikformer through token and weight pruning techniques. Experimental results
demonstrate that our framework can significantly reduce 90% model parameters
and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining
the accuracy of the original model.
</p></li>
</ul>

<h3>Title: Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding. (arXiv:2311.08835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08835">http://arxiv.org/abs/2311.08835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08835]] Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding(http://arxiv.org/abs/2311.08835)</code></li>
<li>Summary: <p>Recent endeavors in video temporal grounding enforce strong cross-modal
interactions through attention mechanisms to overcome the modality gap between
video and text query. However, previous works treat all video clips equally
regardless of their semantic relevance with the text query in attention
modules. In this paper, our goal is to provide clues for query-associated video
clips within the crossmodal encoding process. With our Correlation-Guided
Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of
cross-modal interactions and how to exploit such degrees for prediction. First,
we design an adaptive cross-attention layer with dummy tokens. Dummy tokens
conditioned by text query take a portion of the attention weights, preventing
irrelevant video clips from being represented by the text query. Yet, not all
word tokens equally inherit the text query's correlation to video clips. Thus,
we further guide the cross-attention map by inferring the fine-grained
correlation between video clips and words. We enable this by learning a joint
embedding space for high-level concepts, i.e., moment and sentence level, and
inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency
detector to exploit each video clip's degrees of text engagement. We validate
the superiority of CG-DETR with the state-of-the-art results on various
benchmarks for both moment retrieval and highlight detection. Codes are
available at https://github.com/wjun0830/CGDETR.
</p></li>
</ul>

<h3>Title: Progressive Feedback-Enhanced Transformer for Image Forgery Localization. (arXiv:2311.08910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08910">http://arxiv.org/abs/2311.08910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08910]] Progressive Feedback-Enhanced Transformer for Image Forgery Localization(http://arxiv.org/abs/2311.08910)</code></li>
<li>Summary: <p>Blind detection of the forged regions in digital images is an effective
authentication means to counter the malicious use of local image editing
techniques. Existing encoder-decoder forensic networks overlook the fact that
detecting complex and subtle tampered regions typically requires more feedback
information. In this paper, we propose a Progressive FeedbACk-enhanced
Transformer (ProFact) network to achieve coarse-to-fine image forgery
localization. Specifically, the coarse localization map generated by an initial
branch network is adaptively fed back to the early transformer encoder layers
for enhancing the representation of positive features while suppressing
interference factors. The cascaded transformer network, combined with a
contextual spatial pyramid module, is designed to refine discriminative
forensic features for improving the forgery localization accuracy and
reliability. Furthermore, we present an effective strategy to automatically
generate large-scale forged image samples close to real-world forensic
scenarios, especially in realistic and coherent processing. Leveraging on such
samples, a progressive and cost-effective two-stage training protocol is
applied to the ProFact network. The extensive experimental results on nine
public forensic datasets show that our proposed localizer greatly outperforms
the state-of-the-art on the generalization ability and robustness of image
forgery localization. Code will be publicly available at
https://github.com/multimediaFor/ProFact.
</p></li>
</ul>

<h3>Title: Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search. (arXiv:2311.09084v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09084">http://arxiv.org/abs/2311.09084</a></li>
<li>Code URL: https://github.com/hcplab-sysu/personsearch-ctlg</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09084]] Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search(http://arxiv.org/abs/2311.09084)</code></li>
<li>Summary: <p>Given a descriptive text query, text-based person search (TBPS) aims to
retrieve the best-matched target person from an image gallery. Such a
cross-modal retrieval task is quite challenging due to significant modality
gap, fine-grained differences and insufficiency of annotated data. To better
align the two modalities, most existing works focus on introducing
sophisticated network structures and auxiliary tasks, which are complex and
hard to implement. In this paper, we propose a simple yet effective dual
Transformer model for text-based person search. By exploiting a hardness-aware
contrastive learning strategy, our model achieves state-of-the-art performance
without any special design for local feature alignment or side information.
Moreover, we propose a proximity data generation (PDG) module to automatically
produce more diverse data for cross-modal training. The PDG module first
introduces an automatic generation algorithm based on a text-to-image diffusion
model, which generates new text-image pair samples in the proximity space of
original ones. Then it combines approximate text generation and feature-level
mixup during training to further strengthen the data diversity. The PDG module
can largely guarantee the reasonability of the generated samples that are
directly used for training without any human inspection for noise rejection. It
improves the performance of our model significantly, providing a feasible
solution to the data insufficiency problem faced by such fine-grained
visual-linguistic tasks. Extensive experiments on two popular datasets of the
TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach
outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,
4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be
available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</p></li>
</ul>

<h3>Title: Cross-view and Cross-pose Completion for 3D Human Understanding. (arXiv:2311.09104v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09104">http://arxiv.org/abs/2311.09104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09104]] Cross-view and Cross-pose Completion for 3D Human Understanding(http://arxiv.org/abs/2311.09104)</code></li>
<li>Summary: <p>Human perception and understanding is a major domain of computer vision
which, like many other vision subdomains recently, stands to gain from the use
of large models pre-trained on large datasets. We hypothesize that the most
common pre-training strategy of relying on general purpose, object-centric
image datasets such as ImageNet, is limited by an important domain shift. On
the other hand, collecting domain specific ground truth such as 2D or 3D labels
does not scale well. Therefore, we propose a pre-training approach based on
self-supervised learning that works on human-centric data using only images.
Our method uses pairs of images of humans: the first is partially masked and
the model is trained to reconstruct the masked parts given the visible ones and
a second image. It relies on both stereoscopic (cross-view) pairs, and temporal
(cross-pose) pairs taken from videos, in order to learn priors about 3D as well
as human motion. We pre-train a model for body-centric tasks and one for
hand-centric tasks. With a generic transformer architecture, these models
outperform existing self-supervised pre-training methods on a wide set of
human-centric downstream tasks, and obtain state-of-the-art performance for
instance when fine-tuning for model-based and model-free human mesh recovery.
</p></li>
</ul>

<h3>Title: GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer. (arXiv:2311.08526v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08526">http://arxiv.org/abs/2311.08526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08526]] GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer(http://arxiv.org/abs/2311.08526)</code></li>
<li>Summary: <p>Named Entity Recognition (NER) is essential in various Natural Language
Processing (NLP) applications. Traditional NER models are effective but limited
to a set of predefined entity types. In contrast, Large Language Models (LLMs)
can extract arbitrary entities through natural language instructions, offering
greater flexibility. However, their size and cost, particularly for those
accessed via APIs like ChatGPT, make them impractical in resource-limited
scenarios. In this paper, we introduce a compact NER model trained to identify
any type of entity. Leveraging a bidirectional transformer encoder, our model,
GLiNER, facilitates parallel entity extraction, an advantage over the slow
sequential token generation of LLMs. Through comprehensive testing, GLiNER
demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs
in zero-shot evaluations on various NER benchmarks.
</p></li>
</ul>

<h3>Title: Natural Language Processing for Financial Regulation. (arXiv:2311.08533v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08533">http://arxiv.org/abs/2311.08533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08533]] Natural Language Processing for Financial Regulation(http://arxiv.org/abs/2311.08533)</code></li>
<li>Summary: <p>This article provides an understanding of Natural Language Processing
techniques in the framework of financial regulation, more specifically in order
to perform semantic matching search between rules and policy when no dataset is
available for supervised learning. We outline how to outperform simple
pre-trained sentences-transformer models using freely available resources and
explain the mathematical concepts behind the key building blocks of Natural
Language Processing.
</p></li>
</ul>

<h3>Title: UT5: Pretraining Non autoregressive T5 with unrolled denoising. (arXiv:2311.08552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08552">http://arxiv.org/abs/2311.08552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08552]] UT5: Pretraining Non autoregressive T5 with unrolled denoising(http://arxiv.org/abs/2311.08552)</code></li>
<li>Summary: <p>Recent advances in Transformer-based Large Language Models have made great
strides in natural language generation. However, to decode K tokens, an
autoregressive model needs K sequential forward passes, which may be a
performance bottleneck for large language models. Many non-autoregressive (NAR)
research are aiming to address this sequentiality bottleneck, albeit many have
focused on a dedicated architecture in supervised benchmarks. In this work, we
studied unsupervised pretraining for non auto-regressive T5 models via unrolled
denoising and shown its SoTA results in downstream generation tasks such as
SQuAD question generation and XSum.
</p></li>
</ul>

<h3>Title: Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders. (arXiv:2311.08579v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08579">http://arxiv.org/abs/2311.08579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08579]] Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders(http://arxiv.org/abs/2311.08579)</code></li>
<li>Summary: <p>The injection of syntactic information in Variational AutoEncoders (VAEs) has
been shown to result in an overall improvement of performances and
generalisation. An effective strategy to achieve such a goal is to separate the
encoding of distributional semantic features and syntactic structures into
heterogeneous latent spaces via multi-task learning or dual encoder
architectures. However, existing works employing such techniques are limited to
LSTM-based VAEs. In this paper, we investigate latent space separation methods
for structural syntactic injection in Transformer-based VAE architectures
(i.e., Optimus). Specifically, we explore how syntactic structures can be
leveraged in the encoding stage through the integration of graph-based and
sequential models, and how multiple, specialised latent representations can be
injected into the decoder's attention mechanism via low-rank operators. Our
empirical evaluation, carried out on natural language sentences and
mathematical expressions, reveals that the proposed end-to-end VAE architecture
can result in a better overall organisation of the latent space, alleviating
the information loss occurring in standard VAE setups, resulting in enhanced
performances on language modelling and downstream generation tasks.
</p></li>
</ul>

<h3>Title: Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. (arXiv:2311.08756v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08756">http://arxiv.org/abs/2311.08756</a></li>
<li>Code URL: https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08756]] Accelerating Toeplitz Neural Network with Constant-time Inference Complexity(http://arxiv.org/abs/2311.08756)</code></li>
<li>Summary: <p>Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in
various sequence modeling tasks. They outperform commonly used
Transformer-based models while benefiting from log-linear space-time
complexities. On the other hand, State Space Models (SSMs) achieve lower
performance than TNNs in language modeling but offer the advantage of constant
inference complexity. In this paper, we aim to combine the strengths of TNNs
and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to
achieve the same constant inference complexities as SSMs. To accomplish this,
we formulate the conversion process as an optimization problem and provide a
closed-form solution. We demonstrate how to transform the target equation into
a Vandermonde linear system problem, which can be efficiently solved using the
Discrete Fourier Transform (DFT). Notably, our method requires no training and
maintains numerical stability. It can be also applied to any LongConv-based
model. To assess its effectiveness, we conduct extensive experiments on
language modeling tasks across various settings. Additionally, we compare our
method to other gradient-descent solutions, highlighting the superior numerical
stability of our approach. The source code is available at
https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</p></li>
</ul>

<h3>Title: Reasoning over Description Logic-based Contexts with Transformers. (arXiv:2311.08941v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08941">http://arxiv.org/abs/2311.08941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08941]] Reasoning over Description Logic-based Contexts with Transformers(http://arxiv.org/abs/2311.08941)</code></li>
<li>Summary: <p>One way that the current state of the art measures the reasoning ability of
transformer-based models is by evaluating accuracy in downstream tasks like
logical question answering or proof generation over synthetic contexts
expressed in natural language. However, most of the contexts used are in
practice very simple; in most cases, they are generated from short first-order
logic sentences with only a few logical operators and quantifiers. In this
work, we seek to answer the question how well a transformer-based model will
perform reasoning over expressive contexts. For this purpose, we construct a
synthetic natural language question-answering dataset, generated by description
logic knowledge bases. For the generation of the knowledge bases, we use the
expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K
examples, and increases in two dimensions: i) reasoning depth, and ii) length
of sentences. We show that the performance of our DeBERTa-based model,
DELTA$_M$, is marginally affected when the reasoning depth is increased and it
is not affected at all when the length of the sentences is increasing. We also
evaluate the generalization ability of the model on reasoning depths unseen at
training, both increasing and decreasing, revealing interesting insights into
the model's adaptive generalization abilities.
</p></li>
</ul>

<h3>Title: Assessing Knowledge Editing in Language Models via Relation Perspective. (arXiv:2311.09053v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09053">http://arxiv.org/abs/2311.09053</a></li>
<li>Code URL: https://github.com/weiyifan1023/RaKE</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09053]] Assessing Knowledge Editing in Language Models via Relation Perspective(http://arxiv.org/abs/2311.09053)</code></li>
<li>Summary: <p>Knowledge Editing (KE) for modifying factual knowledge in Large Language
Models (LLMs) has been receiving increasing attention. However, existing
knowledge editing methods are entity-centric, and it is unclear whether this
approach is suitable for a relation-centric perspective. To address this gap,
this paper constructs a new benchmark named RaKE, which focuses on Relation
based Knowledge Editing. In this paper, we establish a suite of innovative
metrics for evaluation and conduct comprehensive experiments involving various
knowledge editing baselines. We notice that existing knowledge editing methods
exhibit the potential difficulty in their ability to edit relations. Therefore,
we further explore the role of relations in factual triplets within the
transformer. Our research results confirm that knowledge related to relations
is not only stored in the FFN network but also in the attention layers. This
provides experimental support for future relation-based knowledge editing
methods.
</p></li>
</ul>

<h3>Title: Approaching adverse event detection utilizing transformers on clinical time-series. (arXiv:2311.09165v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09165">http://arxiv.org/abs/2311.09165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09165]] Approaching adverse event detection utilizing transformers on clinical time-series(http://arxiv.org/abs/2311.09165)</code></li>
<li>Summary: <p>Patients being admitted to a hospital will most often be associated with a
certain clinical development during their stay. However, there is always a risk
of patients being subject to the wrong diagnosis or to a certain treatment not
pertaining to the desired effect, potentially leading to adverse events. Our
research aims to develop an anomaly detection system for identifying deviations
from expected clinical trajectories. To address this goal we analyzed 16 months
of vital sign recordings obtained from the Nordland Hospital Trust (NHT). We
employed an self-supervised framework based on the STraTS transformer
architecture to represent the time series data in a latent space. These
representations were then subjected to various clustering techniques to explore
potential patient phenotypes based on their clinical progress. While our
preliminary results from this ongoing research are promising, they underscore
the importance of enhancing the dataset with additional demographic information
from patients. This additional data will be crucial for a more comprehensive
evaluation of the method's performance.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder. (arXiv:2311.08844v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08844">http://arxiv.org/abs/2311.08844</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08844]] Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder(http://arxiv.org/abs/2311.08844)</code></li>
<li>Summary: <p>Although image captioning has a vast array of applications, it has not
reached its full potential in languages other than English. Arabic, for
instance, although the native language of more than 400 million people, remains
largely underrepresented in this area. This is due to the lack of labeled data
and powerful Arabic generative models. We alleviate this issue by presenting a
novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our
model is based on a vision encoder and a Gemini text decoder that maintains
generation fluency while allowing fusion between the vision and language
components. To train our model, we introduce a new method for automatically
acquiring data from available English datasets. We also manually prepare a new
dataset for evaluation. \textit{Violet} performs sizeably better than our
baselines on all of our evaluation datasets. For example, it reaches a CIDEr
score of $61.2$ on our manually annotated dataset and achieves an improvement
of $13$ points on Flickr8k.
</p></li>
</ul>

<h3>Title: Controlling the Output of a Generative Model by Latent Feature Vector Shifting. (arXiv:2311.08850v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08850">http://arxiv.org/abs/2311.08850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08850]] Controlling the Output of a Generative Model by Latent Feature Vector Shifting(http://arxiv.org/abs/2311.08850)</code></li>
<li>Summary: <p>State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias})
often generate photorealistic images based on vectors sampled from their latent
space. However, the ability to control the output is limited. Here we present
our novel method for latent vector shifting for controlled output image
modification utilizing semantic features of the generated images. In our
approach we use a pre-trained model of StyleGAN3 that generates images of
realistic human faces in relatively high resolution. We complement the
generative model with a convolutional neural network classifier, namely
ResNet34, trained to classify the generated images with binary facial features
from the CelebA dataset. Our latent feature shifter is a neural network model
with a task to shift the latent vectors of a generative model into a specified
feature direction. We have trained latent feature shifter for multiple facial
features, and outperformed our baseline method in the number of generated
images with the desired feature. To train our latent feature shifter neural
network, we have designed a dataset of pairs of latent vectors with and without
a certain feature. Based on the evaluation, we conclude that our latent feature
shifter approach was successful in the controlled generation of the StyleGAN3
generator.
</p></li>
</ul>

<h3>Title: Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery. (arXiv:2311.08923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08923">http://arxiv.org/abs/2311.08923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08923]] Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery(http://arxiv.org/abs/2311.08923)</code></li>
<li>Summary: <p>Natural protected areas are vital for biodiversity, climate change
mitigation, and supporting ecological processes. Despite their significance,
comprehensive mapping is hindered by a lack of understanding of their
characteristics and a missing land cover class definition. This paper aims to
advance the explanation of the designating patterns forming protected and wild
areas. To this end, we propose a novel framework that uses activation
maximization and a generative adversarial model. With this, we aim to generate
satellite images that, in combination with domain knowledge, are capable of
offering complete and valid explanations for the spatial and spectral patterns
that define the natural authenticity of these regions. Our proposed framework
produces more precise attribution maps pinpointing the designating patterns
forming the natural authenticity of protected areas. Our approach fosters our
understanding of the ecological integrity of the protected natural areas and
may contribute to future monitoring and preservation efforts.
</p></li>
</ul>

<h3>Title: RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution. (arXiv:2311.09178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09178">http://arxiv.org/abs/2311.09178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09178]] RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution(http://arxiv.org/abs/2311.09178)</code></li>
<li>Summary: <p>Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
</p></li>
</ul>

<h3>Title: ACID: Abstractive, Content-Based IDs for Document Retrieval with Language Models. (arXiv:2311.08593v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08593">http://arxiv.org/abs/2311.08593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08593]] ACID: Abstractive, Content-Based IDs for Document Retrieval with Language Models(http://arxiv.org/abs/2311.08593)</code></li>
<li>Summary: <p>Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a new approach
for end-to-end document retrieval that directly generates document identifiers
given an input query. Techniques for designing effective, high-quality document
IDs remain largely unexplored. We introduce ACID, in which each document's ID
is composed of abstractive keyphrases generated by a large language model,
rather than an integer ID sequence as done in past work. We compare our method
with the current state-of-the-art technique for ID generation, which produces
IDs through hierarchical clustering of document embeddings. We also examine
simpler methods to generate natural-language document IDs, including the naive
approach of using the first k words of each document as its ID or words with
high BM25 scores in that document. We show that using ACID improves top-10 and
top-20 accuracy by 15.6% and 14.4% (relative) respectively versus the
state-of-the-art baseline on the MSMARCO 100k retrieval task, and 4.4% and 4.0%
respectively on the Natural Questions 100k retrieval task. Our results
demonstrate the effectiveness of human-readable, natural-language IDs in
generative retrieval with LMs. The code for reproducing our results and the
keyword-augmented datasets will be released on formal publication.
</p></li>
</ul>

<h3>Title: Understanding Calibration for Multilingual Question Answering Models. (arXiv:2311.08669v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08669">http://arxiv.org/abs/2311.08669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08669]] Understanding Calibration for Multilingual Question Answering Models(http://arxiv.org/abs/2311.08669)</code></li>
<li>Summary: <p>Multilingual pre-trained language models are incredibly effective at Question
Answering (QA), a core task in Natural Language Understanding, achieving high
accuracies on several multilingual benchmarks. However, little is known about
how well they are calibrated. In this paper, we study the calibration
properties of several pre-trained multilingual large language models (LLMs) on
a variety of question-answering tasks. We perform extensive experiments,
spanning both extractive and generative QA model designs and diverse languages,
spanning both high-resource and low-resource ones. We study different
dimensions of calibration in in-distribution, out-of-distribution, and
cross-lingual transfer settings, and investigate strategies to improve it,
including post-hoc methods and regularized fine-tuning. We demonstrate
automatically translated data augmentation as a highly effective technique to
improve model calibration. We also conduct a number of ablation experiments to
study the effect of model size on calibration and how multilingual models
compare with their monolingual counterparts for diverse tasks and languages.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts. (arXiv:2311.09050v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09050">http://arxiv.org/abs/2311.09050</a></li>
<li>Code URL: https://github.com/ecnu-dase-nlp/rqp</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09050]] Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts(http://arxiv.org/abs/2311.09050)</code></li>
<li>Summary: <p>Zero-shot Visual Question Answering (VQA) is a prominent vision-language task
that examines both the visual and textual understanding capability of systems
in the absence of training data. Recently, by converting the images into
captions, information across multi-modalities is bridged and Large Language
Models (LLMs) can apply their strong zero-shot generalization capability to
unseen questions. To design ideal prompts for solving VQA via LLMs, several
studies have explored different strategies to select or generate
question-answer pairs as the exemplar prompts, which guide LLMs to answer the
current questions effectively. However, they totally ignore the role of
question prompts. The original questions in VQA tasks usually encounter
ellipses and ambiguity which require intermediate reasoning. To this end, we
present Reasoning Question Prompts for VQA tasks, which can further activate
the potential of LLMs in zero-shot scenarios. Specifically, for each question,
we first generate self-contained questions as reasoning question prompts via an
unsupervised question edition module considering sentence fluency, semantic
integrity and syntactic invariance. Each reasoning question prompt clearly
indicates the intent of the original question. This results in a set of
candidate answers. Then, the candidate answers associated with their confidence
scores acting as answer heuristics are fed into LLMs and produce the final
answer. We evaluate reasoning question prompts on three VQA challenges,
experimental results demonstrate that they can significantly improve the
results of LLMs on zero-shot setting and outperform existing state-of-the-art
zero-shot methods on three out of four data sets. Our source code is publicly
released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</p></li>
</ul>

<h3>Title: Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective. (arXiv:2311.08487v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08487">http://arxiv.org/abs/2311.08487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08487]] Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective(http://arxiv.org/abs/2311.08487)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are central to a multitude of applications but
struggle with significant risks, notably in generating harmful content and
biases. Drawing an analogy to the human psyche's conflict between evolutionary
survival instincts and societal norm adherence elucidated in Freud's
psychoanalysis theory, we argue that LLMs suffer a similar fundamental
conflict, arising between their inherent desire for syntactic and semantic
continuity, established during the pre-training phase, and the post-training
alignment with human values. This conflict renders LLMs vulnerable to
adversarial attacks, wherein intensifying the models' desire for continuity can
circumvent alignment efforts, resulting in the generation of harmful
information. Through a series of experiments, we first validated the existence
of the desire for continuity in LLMs, and further devised a straightforward yet
powerful technique, such as incomplete sentences, negative priming, and
cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle
to prevent the generation of harmful information. In summary, our study
uncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby
questioning the efficacy of solely relying on sophisticated alignment methods,
and further advocates for a new training idea that integrates modal concepts
alongside traditional amodal concepts, aiming to endow LLMs with a more nuanced
understanding of real-world contexts and ethical considerations.
</p></li>
</ul>

<h3>Title: Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning. (arXiv:2311.08505v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08505">http://arxiv.org/abs/2311.08505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08505]] Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning(http://arxiv.org/abs/2311.08505)</code></li>
<li>Summary: <p>An important open question pertaining to the use of large language models for
knowledge-intensive tasks is how to effectively integrate knowledge from three
sources: the model's parametric memory, external structured knowledge, and
external unstructured knowledge. Most existing prompting methods either rely
solely on one or two of these sources, or require repeatedly invoking large
language models to generate similar or identical content. In this work, we
overcome these limitations by introducing a novel semi-structured prompting
approach that seamlessly integrates the model's parametric memory with
unstructured knowledge from text documents and structured knowledge from
knowledge graphs. Experimental results on open-domain multi-hop question
answering datasets demonstrate that our prompting method significantly
surpasses existing techniques, even exceeding those which require fine-tuning.
</p></li>
</ul>

<h3>Title: Efficient Continual Pre-training for Building Domain Specific Large Language Models. (arXiv:2311.08545v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08545">http://arxiv.org/abs/2311.08545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08545]] Efficient Continual Pre-training for Building Domain Specific Large Language Models(http://arxiv.org/abs/2311.08545)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable open-domain
capabilities. Traditionally, LLMs tailored for a domain are trained from
scratch to excel at handling domain-specific tasks. In this work, we explore an
alternative strategy of continual pre-training as a means to develop
domain-specific LLMs. We introduce FinPythia-6.9B, developed through
domain-adaptive continual pre-training on the financial domain. Continual
pre-trained FinPythia showcases consistent improvements on financial tasks over
the original foundational model. We further explore simple but effective data
selection strategies for continual pre-training. Our data selection strategies
outperforms vanilla continual pre-training's performance with just 10% of
corpus size and cost, without any degradation on open-domain standard tasks.
Our work proposes an alternative solution to building domain-specific LLMs from
scratch in a cost-effective manner.
</p></li>
</ul>

<h3>Title: MAgIC: Benchmarking Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration. (arXiv:2311.08562v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08562">http://arxiv.org/abs/2311.08562</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08562]] MAgIC: Benchmarking Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration(http://arxiv.org/abs/2311.08562)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have marked a significant advancement in the
field of natural language processing, demonstrating exceptional capabilities in
reasoning, tool usage, and memory. As their applications extend into
multi-agent environments, a need has arisen for a comprehensive evaluation
framework that captures their abilities in reasoning, planning, collaboration,
and more. This work introduces a novel benchmarking framework specifically
tailored to assess LLMs within multi-agent settings, providing quantitative
metrics to evaluate their judgment, reasoning, deception, self-awareness,
collaboration, coordination, and rationality. We utilize games such as
Chameleon and Undercover, alongside game theory scenarios like Cost Sharing,
Multi-player Prisoner's Dilemma, and Public Good, to create diverse testing
environments. Our framework is fortified with the Probabilistic Graphical
Modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex
social and cognitive dimensions. The benchmark evaluates seven multi-agent
systems powered by different LLMs, quantitatively highlighting a significant
capability gap over threefold between the strongest, GPT-4, and the weakest,
Llama-2-70B. It also confirms that our PGM enhancement boosts the inherent
abilities of all selected models by 50% on average. Our codes are released here
https://github.com/cathyxl/MAgIC.
</p></li>
</ul>

<h3>Title: Parameter-Efficient Multilingual Summarisation: An Empirical Study. (arXiv:2311.08572v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08572">http://arxiv.org/abs/2311.08572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08572]] Parameter-Efficient Multilingual Summarisation: An Empirical Study(http://arxiv.org/abs/2311.08572)</code></li>
<li>Summary: <p>With the increasing prevalence of Large Language Models, traditional full
fine-tuning approaches face growing challenges, especially in memory-intensive
tasks. This paper investigates the potential of Parameter-Efficient
Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and
under-explored multilingual summarisation tasks. We conduct an extensive study
across different data availability scenarios, including full-data, low-data,
and cross-lingual transfer, leveraging models of different sizes. Our findings
reveal that LoRA lags behind full fine-tuning when trained with full data,
however, it excels in low-data scenarios and cross-lingual transfer.
Interestingly, as models scale up, the performance gap between LoRA and full
fine-tuning diminishes. Additionally, we investigate effective strategies for
few-shot cross-lingual transfer, finding that continued LoRA tuning achieves
the best performance compared to both full fine-tuning and dynamic composition
of language-specific LoRA modules.
</p></li>
</ul>

<h3>Title: PEMA: Plug-in External Memory Adaptation for Language Models. (arXiv:2311.08590v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08590">http://arxiv.org/abs/2311.08590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08590]] PEMA: Plug-in External Memory Adaptation for Language Models(http://arxiv.org/abs/2311.08590)</code></li>
<li>Summary: <p>Pre-trained language models (PLMs) have demonstrated impressive performance
across various downstream NLP tasks. Nevertheless, the resource requirements of
pre-training large language models in terms of memory and training compute pose
significant challenges. Furthermore, due to the substantial resources required,
many PLM weights are confidential. Consequently, users are compelled to share
their data with model owners for fine-tuning on specific tasks. To overcome the
limitations, we introduce Plug-in External Memory Adaptation (PEMA), a
Parameter-Efficient Fine-Tuning (PEFT) approach designed for fine-tuning PLMs
without the need for all weights. PEMA can be integrated into the context
representation of test data during inference to execute downstream tasks. It
leverages an external memory to store context representations generated by a
PLM, mapped with the desired target word. Our method entails training
LoRA-based weight matrices within the final layer of the PLM for enhanced
efficiency. The probability is then interpolated with the next-word
distribution from the PLM to perform downstream tasks. To improve the
generation quality, we propose a novel interpolation strategy named Gradual
Unrolling. To demonstrate the effectiveness of our proposed method, we conduct
experiments to demonstrate the efficacy of PEMA with a syntactic dataset and
assess its performance on machine translation and style transfer tasks using
real datasets. PEMA outperforms other PEFT methods in terms of memory and
latency efficiency for training and inference. Furthermore, it outperforms
other baselines in preserving the meaning of sentences while generating
appropriate language and styles.
</p></li>
</ul>

<h3>Title: Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures. (arXiv:2311.08605v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08605">http://arxiv.org/abs/2311.08605</a></li>
<li>Code URL: https://github.com/david-jenny/llm-political-study</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08605]] Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures(http://arxiv.org/abs/2311.08605)</code></li>
<li>Summary: <p>The rapid advancement of Large Language Models (LLMs) has sparked intense
debate regarding their ability to perceive and interpret complex
socio-political landscapes. In this study, we undertake an exploration of
decision-making processes and inherent biases within LLMs, exemplified by
ChatGPT, specifically contextualizing our analysis within political debates. We
aim not to critique or validate LLMs' values, but rather to discern how they
interpret and adjudicate "good arguments." By applying Activity Dependency
Networks (ADNs), we extract the LLMs' implicit criteria for such assessments
and illustrate how normative values influence these perceptions. We discuss the
consequences of our findings for human-AI alignment and bias mitigation. Our
code and data at https://github.com/david-jenny/LLM-Political-Study.
</p></li>
</ul>

<h3>Title: Multistage Collaborative Knowledge Distillation from Large Language Models. (arXiv:2311.08640v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08640">http://arxiv.org/abs/2311.08640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08640]] Multistage Collaborative Knowledge Distillation from Large Language Models(http://arxiv.org/abs/2311.08640)</code></li>
<li>Summary: <p>We study semi-supervised sequence prediction tasks where labeled data are too
scarce to effectively finetune a model and at the same time few-shot prompting
of a large language model (LLM) has suboptimal performance. This happens when a
task, such as parsing, is expensive to annotate and also unfamiliar to a
pretrained LLM. In this paper, we present a discovery that student models
distilled from a prompted LLM can often generalize better than their teacher on
such tasks. Leveraging this finding, we propose a new distillation method,
multistage collaborative knowledge distillation from an LLM (MCKD), for such
tasks. MCKD first prompts an LLM using few-shot in-context learning to produce
pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of
students are trained on disjoint partitions of the pseudolabeled data. Each
student subsequently produces new and improved pseudolabels for the unseen
partition to supervise the next round of student(s) with. We show the benefit
of multistage cross-partition labeling on two constituency parsing tasks. On
CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the
performance of supervised finetuning with 500 examples and outperforms the
prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</p></li>
</ul>

<h3>Title: Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. (arXiv:2311.08692v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08692]] Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models(http://arxiv.org/abs/2311.08692)</code></li>
<li>Summary: <p>The complementary potential of Large Language Models (LLM) assumes
off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and
tasks so that an ensemble of LLMs can achieve consistently better performance.
Existing ensemble methods for LLMs mainly focus on reward model ranking of
outputs, leading to significant computation overhead. To combat this issue, we
revisit the complementary potential of LLMs and further elaborate it by mining
latent expertise with off-the-shelf reward models. We propose Zooter, a
reward-guided routing method distilling rewards on training queries to train a
routing function, which can precisely distribute each query to the LLM with
expertise about it. We also integrate a tag-based label enhancement to mitigate
noise from uncertainty when using rewards as silver supervision. Zooter shows
computation efficiency in inference as it introduces only a minor computation
overhead of a routing function compared with reward model ranking methods. We
evaluate Zooter on a comprehensive benchmark collection with 26 subsets on
different domains and tasks. Zooter outperforms the best single model on
average and ranks first on 44% of tasks, even surpassing multiple reward model
ranking methods.
</p></li>
</ul>

<h3>Title: Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains. (arXiv:2311.08704v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08704">http://arxiv.org/abs/2311.08704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08704]] Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains(http://arxiv.org/abs/2311.08704)</code></li>
<li>Summary: <p>Although large language models (LLMs) exhibit remarkable capacity to leverage
in-context demonstrations, it is still unclear to what extent they can learn
new concepts or facts from ground-truth labels. To address this question, we
examine the capacity of instruction-tuned LLMs to follow in-context concept
guidelines for sentence labeling tasks. We design guidelines that present
different types of factual and counterfactual concept definitions, which are
used as prompts for zero-shot sentence classification tasks. Our results show
that although concept definitions consistently help in task performance, only
the larger models (with 70B parameters or more) have limited ability to work
under counterfactual contexts. Importantly, only proprietary models such as
GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is
due to more sophisticated alignment methods. Finally, we find that
Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which
indicates that careful fine-tuning is more effective than increasing model
scale. Altogether, our simple evaluation method reveals significant gaps in
concept understanding between the most capable open-source language models and
the leading proprietary APIs.
</p></li>
</ul>

<h3>Title: PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning. (arXiv:2311.08711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08711">http://arxiv.org/abs/2311.08711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08711]] PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning(http://arxiv.org/abs/2311.08711)</code></li>
<li>Summary: <p>Instruction tuning has remarkably advanced large language models (LLMs) in
understanding and responding to diverse human instructions. Despite the success
in high-resource languages, its application in lower-resource ones faces
challenges due to the imbalanced foundational abilities of LLMs across
different languages, stemming from the uneven language distribution in their
pre-training data. To tackle this issue, we propose pivot language guided
generation (PLUG), an approach that utilizes a high-resource language,
primarily English, as the pivot to enhance instruction tuning in lower-resource
languages. It trains the model to first process instructions in the pivot
language, and then produce responses in the target language. To evaluate our
approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4
languages (Chinese, Korean, Italian, and Spanish), each annotated by
professional translators. Our approach demonstrates a significant improvement
in the instruction-following abilities of LLMs by 29% on average, compared to
directly responding in the target language alone. Further experiments validate
the versatility of our approach by employing alternative pivot languages beyond
English to assist languages where LLMs exhibit lower proficiency.
</p></li>
</ul>

<h3>Title: Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling. (arXiv:2311.08718v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08718">http://arxiv.org/abs/2311.08718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08718]] Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling(http://arxiv.org/abs/2311.08718)</code></li>
<li>Summary: <p>Uncertainty decomposition refers to the task of decomposing the total
uncertainty of a model into data (aleatoric) uncertainty, resulting from the
inherent complexity or ambiguity of the data, and model (epistemic)
uncertainty, resulting from the lack of knowledge in the model. Performing
uncertainty decomposition for large language models (LLMs) is an important step
toward improving the reliability, trustworthiness, and interpretability of
LLMs, but this research task is very challenging and remains unresolved. The
existing canonical method, Bayesian Neural Network (BNN), cannot be applied to
LLMs, because BNN requires training and ensembling multiple variants of models,
which is infeasible or prohibitively expensive for LLMs. In this paper, we
introduce an uncertainty decomposition framework for LLMs, called input
clarifications ensemble, which bypasses the need to train new models. Rather
than ensembling models with different parameters, our approach generates a set
of clarifications for the input, feeds them into the fixed LLMs, and ensembles
the corresponding predictions. We show that our framework shares a symmetric
decomposition structure with BNN. Empirical evaluations demonstrate that the
proposed framework provides accurate and reliable uncertainty quantification on
various tasks. Code will be made publicly available at
https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</p></li>
</ul>

<h3>Title: Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory. (arXiv:2311.08719v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08719">http://arxiv.org/abs/2311.08719</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08719]] Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory(http://arxiv.org/abs/2311.08719)</code></li>
<li>Summary: <p>Memory-augmented Large Language Models (LLMs) have demonstrated remarkable
performance in long-term human-machine interactions, which basically relies on
iterative recalling and reasoning of history to generate high-quality
responses. However, such repeated recall-reason steps easily produce biased
thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same
history for different questions. On the contrary, humans can keep thoughts in
the memory and recall them without repeated reasoning. Motivated by this human
capability, we propose a novel memory mechanism called TiM (Think-in-Memory)
that enables LLMs to maintain an evolved memory for storing historical thoughts
along the conversation stream. The TiM framework consists of two crucial
stages: (1) before generating a response, a LLM agent recalls relevant thoughts
from memory, and (2) after generating a response, the LLM agent post-thinks and
incorporates both historical and new thoughts to update the memory. Thus, TiM
can eliminate the issue of repeated reasoning by saving the post-thinking
thoughts as the history. Besides, we formulate the basic principles to organize
the thoughts in memory based on the well-established operations,
(\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic
updates and evolution of the thoughts. Furthermore, we introduce
Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the
long-term conversations. We conduct qualitative and quantitative experiments on
real-world and simulated dialogues covering a wide range of topics,
demonstrating that equipping existing LLMs with TiM significantly enhances
their performance in generating responses for long-term interactions.
</p></li>
</ul>

<h3>Title: Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models. (arXiv:2311.08732v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08732">http://arxiv.org/abs/2311.08732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08732]] Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models(http://arxiv.org/abs/2311.08732)</code></li>
<li>Summary: <p>Emergency management urgently requires comprehensive knowledge while having a
high possibility to go beyond individuals' cognitive scope. Therefore,
artificial intelligence(AI) supported decision-making under that circumstance
is of vital importance. Recent emerging large language models (LLM) provide a
new direction for enhancing targeted machine intelligence. However, the
utilization of LLM directly would inevitably introduce unreliable output for
its inherent issue of hallucination and poor reasoning skills. In this work, we
develop a system called Enhancing Emergency decision-making with Knowledge
Graph and LLM (E-KELL), which provides evidence-based decision-making in
various emergency stages. The study constructs a structured emergency knowledge
graph and guides LLMs to reason over it via a prompt chain. In real-world
evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in
comprehensibility, accuracy, conciseness, and instructiveness from a group of
emergency commanders and firefighters, demonstrating a significant improvement
across various situations compared to baseline models. This work introduces a
novel approach to providing reliable emergency decision support.
</p></li>
</ul>

<h3>Title: Thread of Thought Unraveling Chaotic Contexts. (arXiv:2311.08734v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08734">http://arxiv.org/abs/2311.08734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08734]] Thread of Thought Unraveling Chaotic Contexts(http://arxiv.org/abs/2311.08734)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have ushered in a transformative era in the
field of natural language processing, excelling in tasks related to text
comprehension and generation. Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distractors rather than long irrelevant
context), leading to the inadvertent omission of certain details within the
chaotic context. In response to these challenges, we introduce the "Thread of
Thought" (ThoT) strategy, which draws inspiration from human cognitive
processes. ThoT systematically segments and analyzes extended contexts while
adeptly selecting pertinent information. This strategy serves as a versatile
"plug-and-play" module, seamlessly integrating with various LLMs and prompting
techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as
well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to
illustrate that ThoT significantly improves reasoning performance compared to
other prompting techniques.
</p></li>
</ul>

<h3>Title: StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving. (arXiv:2311.08803v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08803">http://arxiv.org/abs/2311.08803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08803]] StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving(http://arxiv.org/abs/2311.08803)</code></li>
<li>Summary: <p>Most existing chain-of-thought (CoT) prompting methods suffer from the issues
of generalizability and consistency, as they often rely on instance-specific
solutions that may not be applicable to other cases and lack task-level
consistency in their reasoning steps. To address these limitations, we propose
a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to
tackle various tasks. The framework improves generalizability by formulating
general problem-solving strategies and enhances consistency by producing
consistent solutions using these strategies. StrategyLLM employs four LLM-based
agents: strategy generator, executor, optimizer, and evaluator, working
together to generate, evaluate, and select promising strategies for a given
task automatically. The experimental results demonstrate that StrategyLLM
outperforms the competitive baseline CoT-SC that requires human-annotated
solutions on 13 datasets across 4 challenging tasks without human involvement,
including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning
(70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%),
and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</p></li>
</ul>

<h3>Title: Disinformation Capabilities of Large Language Models. (arXiv:2311.08838v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08838">http://arxiv.org/abs/2311.08838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08838]] Disinformation Capabilities of Large Language Models(http://arxiv.org/abs/2311.08838)</code></li>
<li>Summary: <p>Automated disinformation generation is often listed as one of the risks of
large language models (LLMs). The theoretical ability to flood the information
space with disinformation content might have dramatic consequences for
democratic societies around the world. This paper presents a comprehensive
study of the disinformation capabilities of the current generation of LLMs to
generate false news articles in English language. In our study, we evaluated
the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated
several aspects of the LLMs: how well they are at generating news articles, how
strongly they tend to agree or disagree with the disinformation narratives, how
often they generate safety warnings, etc. We also evaluated the abilities of
detection models to detect these articles as LLM-generated. We conclude that
LLMs are able to generate convincing news articles that agree with dangerous
disinformation narratives.
</p></li>
</ul>

<h3>Title: Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation. (arXiv:2311.08877v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08877">http://arxiv.org/abs/2311.08877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08877]] Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation(http://arxiv.org/abs/2311.08877)</code></li>
<li>Summary: <p>To maintain user trust, large language models (LLMs) should signal low
confidence on examples where they are incorrect, instead of misleading the
user. The standard approach of estimating confidence is to use the softmax
probabilities of these models, but as of November 2023, state-of-the-art LLMs
such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We
first study eliciting confidence linguistically -- asking an LLM for its
confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4
averaged across 12 question-answering datasets -- 7% above a random baseline)
but leaves room for improvement. We then explore using a surrogate confidence
model -- using a model where we do have probabilities to evaluate the original
model's confidence in a given question. Surprisingly, even though these
probabilities come from a different and often weaker model, this method leads
to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best
method composing linguistic confidences and surrogate model probabilities gives
state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on
GPT-4).
</p></li>
</ul>

<h3>Title: Enabling Large Language Models to Learn from Rules. (arXiv:2311.08883v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08883">http://arxiv.org/abs/2311.08883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08883]] Enabling Large Language Models to Learn from Rules(http://arxiv.org/abs/2311.08883)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown incredible performance in completing
various real-world tasks. The current knowledge learning paradigm of LLMs is
mainly based on learning from examples, in which LLMs learn the internal rule
implicitly from a certain number of supervised examples. However, the learning
paradigm may not well learn those complicated rules, especially when the
training examples are limited. We are inspired that humans can learn the new
tasks or knowledge in another way by learning from rules. That is, humans can
grasp the new tasks or knowledge quickly and generalize well given only a
detailed rule and a few optional examples. Therefore, in this paper, we aim to
explore the feasibility of this new learning paradigm, which encodes the
rule-based knowledge into LLMs. We propose rule distillation, which first uses
the strong in-context abilities of LLMs to extract the knowledge from the
textual rules and then explicitly encode the knowledge into LLMs' parameters by
learning from the above in-context signals produced inside the model. Our
experiments show that making LLMs learn from rules by our method is much more
efficient than example-based learning in both the sample size and
generalization ability.
</p></li>
</ul>

<h3>Title: Large Language Models are legal but they are not: Making the case for a powerful LegalLLM. (arXiv:2311.08890v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08890">http://arxiv.org/abs/2311.08890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08890]] Large Language Models are legal but they are not: Making the case for a powerful LegalLLM(http://arxiv.org/abs/2311.08890)</code></li>
<li>Summary: <p>Realizing the recent advances in Natural Language Processing (NLP) to the
legal sector poses challenging problems such as extremely long sequence
lengths, specialized vocabulary that is usually only understood by legal
professionals, and high amounts of data imbalance. The recent surge of Large
Language Models (LLMs) has begun to provide new opportunities to apply NLP in
the legal domain due to their ability to handle lengthy, complex sequences.
Moreover, the emergence of domain-specific LLMs has displayed extremely
promising results on various tasks. In this study, we aim to quantify how
general LLMs perform in comparison to legal-domain models (be it an LLM or
otherwise). Specifically, we compare the zero-shot performance of three
general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR
subset of the LexGLUE benchmark for contract provision classification. Although
the LLMs were not explicitly trained on legal data, we observe that they are
still able to classify the theme correctly in most cases. However, we find that
their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models
fine-tuned on the legal domain, thus underscoring the need for more powerful
legal-domain LLMs.
</p></li>
</ul>

<h3>Title: Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models. (arXiv:2311.08921v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08921">http://arxiv.org/abs/2311.08921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08921]] Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models(http://arxiv.org/abs/2311.08921)</code></li>
<li>Summary: <p>Exploring the application of powerful large language models (LLMs) on the
fundamental named entity recognition (NER) task has drawn much attention
recently. This work aims to investigate the possibilities of pushing the
boundary of zero-shot NER with LLM via a training-free self-improving strategy.
We propose a self-improving framework, which utilize an unlabeled corpus to
stimulate the self-learning ability of LLMs on NER. First, we use LLM to make
predictions on the unlabeled corpus and obtain the self-annotated data. Second,
we explore various strategies to select reliable samples from the
self-annotated dataset as demonstrations, considering the similarity, diversity
and reliability of demonstrations. Finally, we conduct inference for the test
query via in-context learning with the selected self-annotated demonstrations.
Through comprehensive experimental analysis, our study yielded the following
findings: (1) The self-improving framework further pushes the boundary of
zero-shot NER with LLMs, and achieves an obvious performance improvement; (2)
Iterative self-improving or naively increasing the size of unlabeled corpus
does not guarantee improvements; (3) There might still be space for improvement
via more advanced strategy for reliable entity selection.
</p></li>
</ul>

<h3>Title: Identifying Linear Relational Concepts in Large Language Models. (arXiv:2311.08968v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08968">http://arxiv.org/abs/2311.08968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08968]] Identifying Linear Relational Concepts in Large Language Models(http://arxiv.org/abs/2311.08968)</code></li>
<li>Summary: <p>Transformer language models (LMs) have been shown to represent concepts as
directions in the latent space of hidden activations. However, for any given
human-interpretable concept, how can we find its direction in the latent space?
We present a technique called linear relational concepts (LRC) for finding
concept directions corresponding to human-interpretable concepts at a given
hidden layer in a transformer LM by first modeling the relation between subject
and object as a linear relational embedding (LRE). While the LRE work was
mainly presented as an exercise in understanding model representations, we find
that inverting the LRE while using earlier object layers results in a powerful
technique to find concept directions that both work well as a classifier and
causally influence model outputs.
</p></li>
</ul>

<h3>Title: Speculative Contrastive Decoding. (arXiv:2311.08981v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08981">http://arxiv.org/abs/2311.08981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08981]] Speculative Contrastive Decoding(http://arxiv.org/abs/2311.08981)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown extraordinary performance in various
language tasks, but high computational requirements hinder their widespread
deployment. Speculative decoding, which uses amateur models to predict the
generation of expert models, has been proposed as a way to accelerate LLM
inference. However, speculative decoding focuses on acceleration instead of
making the best use of the token distribution from amateur models. We proposed
Speculative Contrastive Decoding (SCD), an accelerated decoding method
leveraging the natural contrast between expert and amateur models in
speculative decoding. Comprehensive evaluations on four benchmarks show that
SCD can achieve similar acceleration factors as speculative decoding while
further improving the generation quality as the contrastive decoding. The
analysis of token probabilities further demonstrates the compatibility between
speculative and contrastive decoding. Overall, SCD provides an effective
approach to enhance the decoding quality of LLMs while saving computational
resources.
</p></li>
</ul>

<h3>Title: Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output. (arXiv:2311.09000v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09000">http://arxiv.org/abs/2311.09000</a></li>
<li>Code URL: https://github.com/yuxiaw/factcheck-gpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09000]] Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output(http://arxiv.org/abs/2311.09000)</code></li>
<li>Summary: <p>The increased use of large language models (LLMs) across a variety of
real-world applications calls for mechanisms to verify the factual accuracy of
their outputs. In this work, we present a holistic end-to-end solution for
annotating the factuality of LLM-generated responses, which encompasses a
multi-stage annotation scheme designed to yield detailed labels concerning the
verifiability and factual inconsistencies found in LLM outputs. We design and
build an annotation tool to speed up the labelling procedure and ease the
workload of raters. It allows flexible incorporation of automatic results in
any stage, e.g. automatically-retrieved evidence. We further construct an
open-domain document-level factuality benchmark in three-level granularity:
claim, sentence and document. Preliminary experiments show that FacTool,
FactScore and Perplexity.ai are struggling to identify false claims with the
best F1=0.53. Annotation tool, benchmark and code are available at
https://github.com/yuxiaw/Factcheck-GPT.
</p></li>
</ul>

<h3>Title: Data Similarity is Not Enough to Explain Language Model Performance. (arXiv:2311.09006v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09006">http://arxiv.org/abs/2311.09006</a></li>
<li>Code URL: https://github.com/gyauney/data-similarity-is-not-enough</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09006]] Data Similarity is Not Enough to Explain Language Model Performance(http://arxiv.org/abs/2311.09006)</code></li>
<li>Summary: <p>Large language models achieve high performance on many but not all downstream
tasks. The interaction between pretraining data and task data is commonly
assumed to determine this variance: a task with data that is more similar to a
model's pretraining data is assumed to be easier for that model. We test
whether distributional and example-specific similarity measures (embedding-,
token- and model-based) correlate with language model performance through a
large-scale comparison of the Pile and C4 pretraining datasets with downstream
benchmarks. Similarity correlates with performance for multilingual datasets,
but in other benchmarks, we surprisingly find that similarity metrics are not
correlated with accuracy or even each other. This suggests that the
relationship between pretraining data and downstream tasks is more complex than
often assumed.
</p></li>
</ul>

<h3>Title: Exploring the Potential of Large Language Models in Computational Argumentation. (arXiv:2311.09022v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09022">http://arxiv.org/abs/2311.09022</a></li>
<li>Code URL: https://github.com/damo-nlp-sg/llm-argumentation</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09022]] Exploring the Potential of Large Language Models in Computational Argumentation(http://arxiv.org/abs/2311.09022)</code></li>
<li>Summary: <p>Computational argumentation has become an essential tool in various fields,
including artificial intelligence, law, and public policy. It is an emerging
research field in natural language processing (NLP) that attracts increasing
attention. Research on computational argumentation mainly involves two types of
tasks: argument mining and argument generation. As large language models (LLMs)
have demonstrated strong abilities in understanding context and generating
natural language, it is worthwhile to evaluate the performance of LLMs on
various computational argumentation tasks. This work aims to embark on an
assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under
zero-shot and few-shot settings within the realm of computational
argumentation. We organize existing tasks into 6 main classes and standardise
the format of 14 open-sourced datasets. In addition, we present a new benchmark
dataset on counter speech generation, that aims to holistically evaluate the
end-to-end performance of LLMs on argument mining and argument generation.
Extensive experiments show that LLMs exhibit commendable performance across
most of these datasets, demonstrating their capabilities in the field of
argumentation. We also highlight the limitations in evaluating computational
argumentation and provide suggestions for future research directions in this
field.
</p></li>
</ul>

<h3>Title: GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models. (arXiv:2311.09048v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09048">http://arxiv.org/abs/2311.09048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09048]] GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models(http://arxiv.org/abs/2311.09048)</code></li>
<li>Summary: <p>This paper presents GRASP, a novel benchmark to evaluate the language
grounding and physical understanding capabilities of video-based multimodal
large language models (LLMs). This evaluation is accomplished via a two-tier
approach leveraging Unity simulations. The initial level tests for language
grounding by assessing a model's ability to relate simple textual descriptions
with visual information. The second level evaluates the model's understanding
of 'Intuitive Physics' principles, such as object permanence and continuity. In
addition to releasing the benchmark, we use it to evaluate several
state-of-the-art multimodal LLMs. Our evaluation reveals significant
shortcomings in current models' language grounding and intuitive physics. These
identified limitations underline the importance of benchmarks like GRASP to
monitor the progress of future models in developing these competencies.
</p></li>
</ul>

<h3>Title: Do Localization Methods Actually Localize Memorized Data in LLMs?. (arXiv:2311.09060v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09060">http://arxiv.org/abs/2311.09060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09060]] Do Localization Methods Actually Localize Memorized Data in LLMs?(http://arxiv.org/abs/2311.09060)</code></li>
<li>Summary: <p>Large language models (LLMs) can memorize many pretrained sequences verbatim.
This paper studies if we can locate a small set of neurons in LLMs responsible
for memorizing a given sequence. While the concept of localization is often
mentioned in prior work, methods for localization have never been
systematically and directly evaluated; we address this with two benchmarking
approaches. In our INJ Benchmark, we actively inject a piece of new information
into a small subset of LLM weights and measure whether localization methods can
identify these "ground truth" weights. In the DEL Benchmark, we study
localization of pretrained data that LLMs have already memorized; while this
setting lacks ground truth, we can still evaluate localization by measuring
whether dropping out located neurons erases a memorized sequence from the
model. We evaluate five localization methods on our two benchmarks, and both
show similar rankings. All methods exhibit promising localization ability,
especially for pruning-based methods, though the neurons they identify are not
necessarily specific to a single memorized sequence.
</p></li>
</ul>

<h3>Title: How Well Do Large Language Models Truly Ground?. (arXiv:2311.09069v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09069">http://arxiv.org/abs/2311.09069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09069]] How Well Do Large Language Models Truly Ground?(http://arxiv.org/abs/2311.09069)</code></li>
<li>Summary: <p>Reliance on the inherent knowledge of Large Language Models (LLMs) can cause
issues such as hallucinations, lack of control, and difficulties in integrating
variable knowledge. To mitigate this, LLMs can be probed to generate responses
by grounding on external context, often given as input (knowledge-augmented
models). Yet, previous research is often confined to a narrow view of the term
"grounding", often only focusing on whether the response contains the correct
answer or not, which does not ensure the reliability of the entire response. To
address this limitation, we introduce a strict definition of grounding: a model
is considered truly grounded when its responses (1) fully utilize necessary
knowledge from the provided context, and (2) don't exceed the knowledge within
the contexts. We introduce a new dataset and a grounding metric to assess this
new definition and perform experiments across 13 LLMs of different sizes and
training methods to provide insights into the factors that influence grounding
performance. Our findings contribute to a better understanding of how to
improve grounding capabilities and suggest an area of improvement toward more
reliable and controllable LLM applications.
</p></li>
</ul>

<h3>Title: How Multilingual is Multilingual LLM?. (arXiv:2311.09071v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09071">http://arxiv.org/abs/2311.09071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09071]] How Multilingual is Multilingual LLM?(http://arxiv.org/abs/2311.09071)</code></li>
<li>Summary: <p>Large Language Models (LLMs), trained predominantly on extensive English
data, often exhibit limitations when applied to other languages. Current
research is primarily focused on enhancing the multilingual capabilities of
these models by employing various tuning strategies. Despite their
effectiveness in certain languages, the understanding of the multilingual
abilities of LLMs remains incomplete. This study endeavors to evaluate the
multilingual capacity of LLMs by conducting an exhaustive analysis across 101
languages, and classifies languages with similar characteristics into four
distinct quadrants. By delving into each quadrant, we shed light on the
rationale behind their categorization and offer actionable guidelines for
tuning these languages. Extensive experiments reveal that existing LLMs possess
multilingual capabilities that surpass our expectations, and we can
significantly improve the multilingual performance of LLMs by focusing on these
distinct attributes present in each quadrant.
</p></li>
</ul>

<h3>Title: Towards A Unified View of Answer Calibration for Multi-Step Reasoning. (arXiv:2311.09101v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09101">http://arxiv.org/abs/2311.09101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09101]] Towards A Unified View of Answer Calibration for Multi-Step Reasoning(http://arxiv.org/abs/2311.09101)</code></li>
<li>Summary: <p>Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have
broadened the scope for improving multi-step reasoning capabilities. Usually,
answer calibration strategies such as step-level or path-level calibration play
a vital role in multi-step reasoning. While effective, there remains a
significant gap in our understanding of the key factors that drive their
success. In this paper, we break down the design of recent answer calibration
strategies and present a unified view which establishes connections between
them. We then conduct a thorough evaluation on these strategies from a unified
view, systematically scrutinizing step-level and path-level answer calibration
across multiple paths. Our study holds the potential to illuminate key insights
for optimizing multi-step reasoning with answer calibration.
</p></li>
</ul>

<h3>Title: Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification. (arXiv:2311.09114v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09114">http://arxiv.org/abs/2311.09114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09114]] Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification(http://arxiv.org/abs/2311.09114)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating fluent text. However, they often encounter the challenge of
generating inaccurate or hallucinated content. This issue is common in both
non-retrieval-based generation and retrieval-augmented generation approaches,
and existing post-hoc rectification methods may not address the accumulated
hallucination errors that may be caused by the "snowballing" issue, especially
in reasoning tasks. To tackle these challenges, we introduce a novel approach
called Real-time Verification and Rectification (Ever). Instead of waiting
until the end of the generation process to rectify hallucinations, Ever employs
a real-time, step-wise generation and hallucination rectification strategy. The
primary objective is to detect and rectify hallucinations as they occur during
the text generation process. When compared to both retrieval-based and
non-retrieval-based baselines, Ever demonstrates a significant improvement in
generating trustworthy and factually accurate text across a diverse range of
tasks, including short-form QA, biography generation, and multi-hop reasoning.
</p></li>
</ul>

<h3>Title: Aligning Neural Machine Translation Models: Human Feedback in Training and Inference. (arXiv:2311.09132v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09132">http://arxiv.org/abs/2311.09132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09132]] Aligning Neural Machine Translation Models: Human Feedback in Training and Inference(http://arxiv.org/abs/2311.09132)</code></li>
<li>Summary: <p>Reinforcement learning from human feedback (RLHF) is a recent technique to
improve the quality of the text generated by a language model, making it closer
to what humans would generate. A core ingredient in RLHF's success in aligning
and improving large language models (LLMs) is its reward model, trained using
human feedback on model outputs. In machine translation (MT), where metrics
trained from human annotations can readily be used as reward models, recent
methods using minimum Bayes risk decoding and reranking have succeeded in
improving the final quality of translation. In this study, we comprehensively
explore and compare techniques for integrating quality metrics as reward models
into the MT pipeline. This includes using the reward model for data filtering,
during the training phase through RL, and at inference time by employing
reranking techniques, and we assess the effects of combining these in a unified
approach. Our experimental results, conducted across multiple translation
tasks, underscore the crucial role of effective data filtering, based on
estimated quality, in harnessing the full potential of RL in enhancing MT
quality. Furthermore, our findings demonstrate the effectiveness of combining
RL training with reranking techniques, showcasing substantial improvements in
translation quality.
</p></li>
</ul>

<h3>Title: Grounding or Guesswork? Large Language Models are Presumptive Grounders. (arXiv:2311.09144v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09144">http://arxiv.org/abs/2311.09144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09144]] Grounding or Guesswork? Large Language Models are Presumptive Grounders(http://arxiv.org/abs/2311.09144)</code></li>
<li>Summary: <p>Effective conversation requires common ground: a shared understanding between
the participants. Common ground, however, does not emerge spontaneously in
conversation. Speakers and listeners work together to both identify and
construct a shared basis while avoiding misunderstanding. To accomplish
grounding, humans rely on a range of dialogue acts, like clarification (What do
you mean?) and acknowledgment (I understand.). In domains like teaching and
emotional support, carefully constructing grounding prevents misunderstanding.
However, it is unclear whether large language models (LLMs) leverage these
dialogue acts in constructing common ground. To this end, we curate a set of
grounding acts and propose corresponding metrics that quantify attempted
grounding. We study whether LLMs use these grounding acts, simulating them
taking turns from several dialogue datasets, and comparing the results to
humans. We find that current LLMs are presumptive grounders, biased towards
assuming common ground without using grounding acts. To understand the roots of
this behavior, we examine the role of instruction tuning and reinforcement
learning with human feedback (RLHF), finding that RLHF leads to less grounding.
Altogether, our work highlights the need for more research investigating
grounding in human-AI interaction.
</p></li>
</ul>

<h3>Title: Temporal Knowledge Question Answering via Abstract Reasoning Induction. (arXiv:2311.09149v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09149">http://arxiv.org/abs/2311.09149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09149]] Temporal Knowledge Question Answering via Abstract Reasoning Induction(http://arxiv.org/abs/2311.09149)</code></li>
<li>Summary: <p>In this paper, we tackle the significant challenge of temporal knowledge
reasoning in Large Language Models (LLMs), an area where such models frequently
encounter difficulties. These difficulties often result in the generation of
misleading or incorrect information, primarily due to their limited capacity to
process evolving factual knowledge and complex temporal logic. In response, we
propose a novel, constructivism-based approach that advocates for a paradigm
shift in LLM learning towards an active, ongoing process of knowledge synthesis
and customization. At the heart of our proposal is the Abstract Reasoning
Induction ARI framework, which divides temporal reasoning into two distinct
phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce
instances of hallucinations and improve LLMs' capacity for integrating abstract
methodologies derived from historical data. Our approach achieves remarkable
improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA
datasets, underscoring its efficacy in advancing temporal reasoning in LLMs.
The code will be released at https://github.com/czy1999/ARI.
</p></li>
</ul>

<h3>Title: CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models. (arXiv:2311.09154v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09154">http://arxiv.org/abs/2311.09154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09154]] CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models(http://arxiv.org/abs/2311.09154)</code></li>
<li>Summary: <p>We are currently in an era of fierce competition among various large language
models (LLMs) continuously pushing the boundaries of benchmark performance.
However, genuinely assessing the capabilities of these LLMs has become a
challenging and critical issue due to potential data contamination, and it
wastes dozens of time and effort for researchers and engineers to download and
try those contaminated models. To save our precious time, we propose a novel
and useful method, Clean-Eval, which mitigates the issue of data contamination
and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to
paraphrase and back-translate the contaminated data into a candidate set,
generating expressions with the same meaning but in different surface forms. A
semantic detector is then used to filter the generated low-quality samples to
narrow down this candidate set. The best candidate is finally selected from
this set based on the BLEURT score. According to human assessment, this best
candidate is semantically similar to the original contamination data but
expressed differently. All candidates can form a new benchmark to evaluate the
model. Our experiments illustrate that Clean-Eval substantially restores the
actual evaluation results on contaminated LLMs under both few-shot learning and
fine-tuning scenarios.
</p></li>
</ul>

<h3>Title: PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers. (arXiv:2311.09180v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09180">http://arxiv.org/abs/2311.09180</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09180]] PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers(http://arxiv.org/abs/2311.09180)</code></li>
<li>Summary: <p>Powerful large language models have facilitated the development of writing
assistants that promise to significantly improve the quality and efficiency of
composition and communication. However, a barrier to effective assistance is
the lack of personalization in LLM outputs to the author's communication style
and specialized knowledge. In this paper, we address this challenge by
proposing PEARL, a retrieval-augmented LLM writing assistant personalized with
a generation-calibrated retriever. Our retriever is trained to select historic
user-authored documents for prompt augmentation, such that they are likely to
best personalize LLM generations for a user request. We propose two key
novelties for training our retriever: 1) A training data selection method that
identifies user requests likely to benefit from personalization and documents
that provide that benefit; and 2) A scale-calibrating KL-divergence objective
that ensures that our retriever closely tracks the benefit of a document for
personalized generation. We demonstrate the effectiveness of PEARL in
generating personalized workplace social media posts and Reddit comments.
Finally, we showcase the potential of a generation-calibrated retriever to
double as a performance predictor and further improve low-quality generations
via LLM chaining.
</p></li>
</ul>

<h3>Title: ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models. (arXiv:2311.09182v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09182">http://arxiv.org/abs/2311.09182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09182]] ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models(http://arxiv.org/abs/2311.09182)</code></li>
<li>Summary: <p>In recent times, large language models (LLMs) have shown impressive
performance on various document-level tasks such as document classification,
summarization, and question-answering. However, research on understanding their
capabilities on the task of self-contradictions in long documents has been very
limited. In this work, we introduce ContraDoc, the first human-annotated
dataset to study self-contradictions in long documents across multiple domains,
varying document lengths, self-contradictions types, and scope. We then analyze
the current capabilities of four state-of-the-art open-source and commercially
available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4
performs the best and can outperform humans on this task, we find that it is
still unreliable and struggles with self-contradictions that require more
nuance and context. We release the dataset and all the code associated with the
experiments.
</p></li>
</ul>

<h3>Title: Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization. (arXiv:2311.09184v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09184">http://arxiv.org/abs/2311.09184</a></li>
<li>Code URL: https://github.com/yale-nlp/instrusum</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09184]] Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization(http://arxiv.org/abs/2311.09184)</code></li>
<li>Summary: <p>While large language models (LLMs) already achieve strong performance on
standard generic summarization benchmarks, their performance on more complex
summarization task settings is less studied. Therefore, we benchmark LLMs on
instruction controllable text summarization, where the model input consists of
both a source article and a natural language requirement for the desired
summary characteristics. To this end, we curate an evaluation-only dataset for
this task setting and conduct human evaluation on 5 LLM-based summarization
systems. We then benchmark LLM-based automatic evaluation for this task with 4
different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods
in total. Our study reveals that instruction controllable text summarization
remains a challenging task for LLMs, since (1) all LLMs evaluated still make
factual and other types of errors in their summaries; (2) all LLM-based
evaluation methods cannot achieve a strong alignment with human annotators when
judging the quality of candidate summaries; (3) different LLMs show large
performance gaps in summary generation and evaluation. We make our collected
benchmark, InstruSum, publicly available to facilitate future research in this
direction.
</p></li>
</ul>

<h3>Title: Towards Verifiable Text Generation with Symbolic References. (arXiv:2311.09188v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09188">http://arxiv.org/abs/2311.09188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09188]] Towards Verifiable Text Generation with Symbolic References(http://arxiv.org/abs/2311.09188)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated an impressive ability to
synthesize plausible and fluent text. However they remain vulnerable to
hallucinations, and thus their outputs generally require manual human
verification for high-stakes applications, which can be time-consuming and
difficult. This paper proposes symbolically grounded generation (SymGen) as a
simple approach for enabling easier validation of an LLM's output. SymGen
prompts an LLM to interleave its regular output text with explicit symbolic
references to fields present in some conditioning data (e.g., a table in JSON
format). The references can be used to display the provenance of different
spans of text in the generation, reducing the effort required for manual
verification. Across data-to-text and question answering experiments, we find
that LLMs are able to directly output text that makes use of symbolic
references while maintaining fluency and accuracy.
</p></li>
</ul>

<h3>Title: PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health. (arXiv:2311.09189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09189">http://arxiv.org/abs/2311.09189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09189]] PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health(http://arxiv.org/abs/2311.09189)</code></li>
<li>Summary: <p>Recently, there has been a growing interest in utilizing large language
models (LLMs) in mental health research, with studies showcasing their
remarkable capabilities, such as disease detection. However, there is currently
a lack of a comprehensive benchmark for evaluating the capability of LLMs in
this domain. Therefore, we address this gap by introducing the first
comprehensive benchmark tailored to the unique characteristics of the mental
health domain. This benchmark encompasses a total of six sub-tasks, covering
three dimensions, to systematically assess the capabilities of LLMs in the
realm of mental health. We have designed corresponding concise prompts for each
sub-task. And we comprehensively evaluate a total of eight advanced LLMs using
our benchmark. Experiment results not only demonstrate significant room for
improvement in current LLMs concerning mental health but also unveil potential
directions for future model optimization.
</p></li>
</ul>

<h3>Title: Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models. (arXiv:2311.09194v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09194">http://arxiv.org/abs/2311.09194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09194]] Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models(http://arxiv.org/abs/2311.09194)</code></li>
<li>Summary: <p>Abstract grammatical knowledge - of parts of speech and grammatical patterns
- is key to the capacity for linguistic generalization in humans. But how
abstract is grammatical knowledge in large language models? In the human
literature, compelling evidence for grammatical abstraction comes from
structural priming. A sentence that shares the same grammatical structure as a
preceding sentence is processed and produced more readily. Because confounds
exist when using stimuli in a single language, evidence of abstraction is even
more compelling from crosslingual structural priming, where use of a syntactic
structure in one language primes an analogous structure in another language. We
measure crosslingual structural priming in large language models, comparing
model behavior to human experimental results from eight crosslingual
experiments covering six languages, and four monolingual structural priming
experiments in three non-English languages. We find evidence for abstract
monolingual and crosslingual grammatical representations in the models that
function similarly to those found in humans. These results demonstrate that
grammatical representations in multilingual language models are not only
similar across languages, but they can causally influence text produced in
different languages.
</p></li>
</ul>

<h3>Title: Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering. (arXiv:2311.09198v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09198">http://arxiv.org/abs/2311.09198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09198]] Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering(http://arxiv.org/abs/2311.09198)</code></li>
<li>Summary: <p>While large language models (LLMs) are equipped with longer text input
capabilities than before, they are struggling to seek correct information in
long contexts. The "lost in the middle" problem challenges most LLMs, referring
to the dramatic decline in accuracy when correct information is located in the
middle. To overcome this crucial issue, this paper proposes to enhance the
information searching and reflection ability of LLMs in long contexts via
specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).
Following these tasks, our model excels in focusing more precisely on the
desired information. Experimental results show substantial improvement in
Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%
absolute gain in shuffled settings, by 21.5% in passage retrieval task. We
release our model, Ziya-Reader to promote related research in the community.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping. (arXiv:2311.08438v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08438">http://arxiv.org/abs/2311.08438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08438]] LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping(http://arxiv.org/abs/2311.08438)</code></li>
<li>Summary: <p>Robot grasp typically follows five stages: object detection, object
localisation, object pose estimation, grasp pose estimation, and grasp
planning. We focus on object pose estimation. Our approach relies on three
pieces of information: multiple views of the object, the camera's extrinsic
parameters at those viewpoints, and 3D CAD models of objects. The first step
involves a standard deep learning backbone (FCN ResNet) to estimate the object
label, semantic segmentation, and a coarse estimate of the object pose with
respect to the camera. Our novelty is using a refinement module that starts
from the coarse pose estimate and refines it by optimisation through
differentiable rendering. This is a purely vision-based approach that avoids
the need for other information such as point cloud or depth images. We evaluate
our object pose estimation approach on the ShapeNet dataset and show
improvements over the state of the art. We also show that the estimated object
pose results in 99.65% grasp accuracy with the ground truth grasp candidates on
the Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using
standard practice.
</p></li>
</ul>

<h3>Title: ConeQuest: A Benchmark for Cone Segmentation on Mars. (arXiv:2311.08657v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08657">http://arxiv.org/abs/2311.08657</a></li>
<li>Code URL: https://github.com/kerner-lab/conequest</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08657]] ConeQuest: A Benchmark for Cone Segmentation on Mars(http://arxiv.org/abs/2311.08657)</code></li>
<li>Summary: <p>Over the years, space scientists have collected terabytes of Mars data from
satellites and rovers. One important set of features identified in Mars orbital
images is pitted cones, which are interpreted to be mud volcanoes believed to
form in regions that were once saturated in water (i.e., a lake or ocean).
Identifying pitted cones globally on Mars would be of great importance, but
expert geologists are unable to sort through the massive orbital image archives
to identify all examples. However, this task is well suited for computer
vision. Although several computer vision datasets exist for various
Mars-related tasks, there is currently no open-source dataset available for
cone detection/segmentation. Furthermore, previous studies trained models using
data from a single region, which limits their applicability for global
detection and mapping. Motivated by this, we introduce ConeQuest, the first
expert-annotated public dataset to identify cones on Mars. ConeQuest consists
of &gt;13k samples from 3 different regions of Mars. We propose two benchmark
tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size
Generalization. We finetune and evaluate widely-used segmentation models on
both benchmark tasks. Results indicate that cone segmentation is a challenging
open problem not solved by existing segmentation models, which achieve an
average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and
(ii), respectively. We believe this new benchmark dataset will facilitate the
development of more accurate and robust models for cone segmentation. Data and
code are available at https://github.com/kerner-lab/ConeQuest.
</p></li>
</ul>

<h3>Title: Correlation-aware active learning for surgery video segmentation. (arXiv:2311.08811v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08811">http://arxiv.org/abs/2311.08811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08811]] Correlation-aware active learning for surgery video segmentation(http://arxiv.org/abs/2311.08811)</code></li>
<li>Summary: <p>Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, \COALSamp{},
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
</p></li>
</ul>

<h3>Title: Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation. (arXiv:2311.08931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08931">http://arxiv.org/abs/2311.08931</a></li>
<li>Code URL: https://github.com/medical-image-analysis-laboratory/ms_wml_uncs</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08931]] Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation(http://arxiv.org/abs/2311.08931)</code></li>
<li>Summary: <p>This paper explores uncertainty quantification (UQ) as an indicator of the
trustworthiness of automated deep-learning (DL) tools in the context of white
matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of
multiple sclerosis (MS) patients. Our study focuses on two principal aspects of
uncertainty in structured output segmentation tasks. Firstly, we postulate that
a good uncertainty measure should indicate predictions likely to be incorrect
with high uncertainty values. Second, we investigate the merit of quantifying
uncertainty at different anatomical scales (voxel, lesion, or patient). We
hypothesize that uncertainty at each scale is related to specific types of
errors. Our study aims to confirm this relationship by conducting separate
analyses for in-domain and out-of-domain settings. Our primary methodological
contributions are (i) the development of novel measures for quantifying
uncertainty at lesion and patient scales, derived from structural prediction
discrepancies, and (ii) the extension of an error retention curve analysis
framework to facilitate the evaluation of UQ performance at both lesion and
patient scales. The results from a multi-centric MRI dataset of 172 patients
demonstrate that our proposed measures more effectively capture model errors at
the lesion and patient scales compared to measures that average voxel-scale
uncertainty values. We provide the UQ protocols code at
https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
