<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Penetrating Shields: A Systematic Analysis of Memory Corruption Mitigations in the Spectre Era. (arXiv:2309.04119v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04119">http://arxiv.org/abs/2309.04119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04119]] Penetrating Shields: A Systematic Analysis of Memory Corruption Mitigations in the Spectre Era(http://arxiv.org/abs/2309.04119)</code></li>
<li>Summary: <p>This paper provides the first systematic analysis of a synergistic threat
model encompassing memory corruption vulnerabilities and microarchitectural
side-channel vulnerabilities. We study speculative shield bypass attacks that
leverage speculative execution attacks to leak secrets that are critical to the
security of memory corruption mitigations (i.e., the shields), and then use the
leaked secrets to bypass the mitigation mechanisms and successfully conduct
memory corruption exploits, such as control-flow hijacking. We start by
systematizing a taxonomy of the state-of-the-art memory corruption mitigations
focusing on hardware-software co-design solutions. The taxonomy helps us to
identify 10 likely vulnerable defense schemes out of 20 schemes that we
analyze. Next, we develop a graph-based model to analyze the 10 likely
vulnerable defenses and reason about possible countermeasures. Finally, we
present three proof-of-concept attacks targeting an already-deployed mitigation
mechanism and two state-of-the-art academic proposals.
</p></li>
</ul>

<h3>Title: Two-Dimensional Dynamic Fusion for Continuous Authentication. (arXiv:2309.04128v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04128">http://arxiv.org/abs/2309.04128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04128]] Two-Dimensional Dynamic Fusion for Continuous Authentication(http://arxiv.org/abs/2309.04128)</code></li>
<li>Summary: <p>Continuous authentication has been widely studied to provide high security
and usability for mobile devices by continuously monitoring and authenticating
users. Recent studies adopt multibiometric fusion for continuous authentication
to provide high accuracy even when some of captured biometric data are of a low
quality. However, existing continuous fusion approaches are resource-heavy as
they rely on all classifiers being activated all the time and may not be
suitable for mobile devices.
</p>
<p>In this paper, we propose a new approach to multibiometric continuous
authentication: two-dimensional dynamic fusion. Our key insight is that
multibiometric continuous authentication calculates two-dimensional matching
scores over classifiers and over time. Based on this, we dynamically select a
set of classifiers based on the context in which authentication is taking
place, and fuse matching scores by multi-classifier fusion and multi-sample
fusion. Through experimental evaluation, we show that our approach provides a
better balance between resource usage and accuracy than the existing fusion
methods. In particular, we show that our approach provides higher accuracy than
the existing methods with the same number of score calculations by adopting
multi-sample fusion.
</p></li>
</ul>

<h3>Title: Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse. (arXiv:2309.04211v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04211">http://arxiv.org/abs/2309.04211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04211]] Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse(http://arxiv.org/abs/2309.04211)</code></li>
<li>Summary: <p>Counterfactuals operationalised through algorithmic recourse have become a
powerful tool to make artificial intelligence systems explainable.
Conceptually, given an individual classified as y -- the factual -- we seek
actions such that their prediction becomes the desired class y' -- the
counterfactual. This process offers algorithmic recourse that is (1) easy to
customise and interpret, and (2) directly aligned with the goals of each
individual. However, the properties of a "good" counterfactual are still
largely debated; it remains an open challenge to effectively locate a
counterfactual along with its corresponding recourse. Some strategies use
gradient-driven methods, but these offer no guarantees on the feasibility of
the recourse and are open to adversarial attacks on carefully created
manifolds. This can lead to unfairness and lack of robustness. Other methods
are data-driven, which mostly addresses the feasibility problem at the expense
of privacy, security and secrecy as they require access to the entire training
data set. Here, we introduce LocalFACE, a model-agnostic technique that
composes feasible and actionable counterfactual explanations using
locally-acquired information at each step of the algorithmic recourse. Our
explainer preserves the privacy of users by only leveraging data that it
specifically requires to construct actionable algorithmic recourse, and
protects the model by offering transparency solely in the regions deemed
necessary for the intervention.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Blockchain-enabled Data Governance for Privacy-Preserved Sharing of Confidential Data. (arXiv:2309.04125v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04125">http://arxiv.org/abs/2309.04125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04125]] Blockchain-enabled Data Governance for Privacy-Preserved Sharing of Confidential Data(http://arxiv.org/abs/2309.04125)</code></li>
<li>Summary: <p>In a traditional cloud storage system, users benefit from the convenience it
provides but also take the risk of certain security and privacy issues. To
ensure confidentiality while maintaining data sharing capabilities, the
Ciphertext-Policy Attribute-based Encryption (CP-ABE) scheme can be used to
achieve fine-grained access control in cloud services. However, existing
approaches are impaired by three critical concerns: illegal authorization, key
disclosure, and privacy leakage.
</p>
<p>To address these, we propose a blockchain-based data governance system that
employs blockchain technology and attribute-based encryption to prevent privacy
leakage and credential misuse. First, our ABE encryption system can handle
multi-authority use cases while protecting identity privacy and hiding access
policy, which also protects data sharing against corrupt authorities. Second,
applying the Advanced Encryption Standard (AES) for data encryption makes the
whole system efficient and responsive to real-world conditions. Furthermore,
the encrypted data is stored in a decentralized storage system such as IPFS,
which does not rely on any centralized service provider and is, therefore,
resilient against single-point failures. Third, illegal authorization activity
can be readily identified through the logged on-chain data. Besides the system
design, we also provide security proofs to demonstrate the robustness of the
proposed system.
</p></li>
</ul>

<h3>Title: Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach. (arXiv:2309.04427v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04427">http://arxiv.org/abs/2309.04427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04427]] Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach(http://arxiv.org/abs/2309.04427)</code></li>
<li>Summary: <p>Several domains increasingly rely on machine learning in their applications.
The resulting heavy dependence on data has led to the emergence of various laws
and regulations around data ethics and privacy and growing awareness of the
need for privacy-preserving machine learning (ppML). Current ppML techniques
utilize methods that are either purely based on cryptography, such as
homomorphic encryption, or that introduce noise into the input, such as
differential privacy. The main criticism given to those techniques is the fact
that they either are too slow or they trade off a model s performance for
improved confidentiality. To address this performance reduction, we aim to
leverage robust representation learning as a way of encoding our data while
optimizing the privacy-utility trade-off. Our method centers on training
autoencoders in a multi-objective manner and then concatenating the latent and
learned features from the encoding part as the encoded form of our data. Such a
deep learning-powered encoding can then safely be sent to a third party for
intensive training and hyperparameter tuning. With our proposed framework, we
can share our data and use third party tools without being under the threat of
revealing its original form. We empirically validate our results on unimodal
and multimodal settings, the latter following a vertical splitting system and
show improved performance over state-of-the-art.
</p></li>
</ul>

<h3>Title: Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos. (arXiv:2309.04236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04236">http://arxiv.org/abs/2309.04236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04236]] Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos(http://arxiv.org/abs/2309.04236)</code></li>
<li>Summary: <p>Data silos, mainly caused by privacy and interoperability, significantly
constrain collaborations among different organizations with similar data for
the same purpose. Distributed learning based on divide-and-conquer provides a
promising way to settle the data silos, but it suffers from several challenges,
including autonomy, privacy guarantees, and the necessity of collaborations.
This paper focuses on developing an adaptive distributed kernel ridge
regression (AdaDKRR) by taking autonomy in parameter selection, privacy in
communicating non-sensitive information, and the necessity of collaborations in
performance improvement into account. We provide both solid theoretical
verification and comprehensive experiments for AdaDKRR to demonstrate its
feasibility and effectiveness. Theoretically, we prove that under some mild
conditions, AdaDKRR performs similarly to running the optimal learning
algorithms on the whole data, verifying the necessity of collaborations and
showing that no other distributed learning scheme can essentially beat AdaDKRR
under the same conditions. Numerically, we test AdaDKRR on both toy simulations
and two real-world applications to show that AdaDKRR is superior to other
existing distributed learning schemes. All these results show that AdaDKRR is a
feasible scheme to defend against data silos, which are highly desired in
numerous application regions such as intelligent decision-making, pricing
forecasting, and performance prediction for products.
</p></li>
</ul>

<h3>Title: Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04296">http://arxiv.org/abs/2309.04296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04296]] Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility(http://arxiv.org/abs/2309.04296)</code></li>
<li>Summary: <p>In traditional deep learning algorithms, one of the key assumptions is that
the data distribution remains constant during both training and deployment.
However, this assumption becomes problematic when faced with
Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data
distribution significantly deviates from what the model has seen during
training. This paper employs a two-fold strategy: utilizing continual learning
techniques to update models with new data and harnessing human mobility data
collected from privacy-preserving pedestrian counters located outside
buildings. In contrast to online learning, which suffers from 'catastrophic
forgetting' as newly acquired knowledge often erases prior information,
continual learning offers a holistic approach by preserving past insights while
integrating new data. This research applies FSNet, a powerful continual
learning algorithm, to real-world data from 13 building complexes in Melbourne,
Australia, a city which had the second longest total lockdown duration globally
during the pandemic. Results underscore the crucial role of continual learning
in accurate energy forecasting, particularly during Out-of-Distribution
periods. Secondary data such as mobility and temperature provided ancillary
support to the primary forecasting model. More importantly, while traditional
methods struggled to adapt during lockdowns, models featuring at least online
learning demonstrated resilience, with lockdown periods posing fewer challenges
once armed with adaptive learning techniques. This study contributes valuable
methodologies and insights to the ongoing effort to improve energy load
forecasting during future Out-of-Distribution periods.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: FIVA: Facial Image and Video Anonymization and Anonymization Defense. (arXiv:2309.04228v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04228">http://arxiv.org/abs/2309.04228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04228]] FIVA: Facial Image and Video Anonymization and Anonymization Defense(http://arxiv.org/abs/2309.04228)</code></li>
<li>Summary: <p>In this paper, we present a new approach for facial anonymization in images
and videos, abbreviated as FIVA. Our proposed method is able to maintain the
same face anonymization consistently over frames with our suggested
identity-tracking and guarantees a strong difference from the original face.
FIVA allows for 0 true positives for a false acceptance rate of 0.001. Our work
considers the important security issue of reconstruction attacks and
investigates adversarial noise, uniform noise, and parameter noise to disrupt
reconstruction attacks. In this regard, we apply different defense and
protection methods against these privacy threats to demonstrate the scalability
of FIVA. On top of this, we also show that reconstruction attack models can be
used for detection of deep fakes. Last but not least, we provide experimental
results showing how FIVA can even enable face swapping, which is purely trained
on a single target image.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning. (arXiv:2309.04036v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04036">http://arxiv.org/abs/2309.04036</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04036]] One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning(http://arxiv.org/abs/2309.04036)</code></li>
<li>Summary: <p>Image camouflage has been utilized to create clean-label poisoned images for
implanting backdoor into a DL model. But there exists a crucial limitation that
one attack/poisoned image can only fit a single input size of the DL model,
which greatly increases its attack budget when attacking multiple commonly
adopted input sizes of DL models. This work proposes to constructively craft an
attack image through camouflaging but can fit multiple DL models' input sizes
simultaneously, namely OmClic. Thus, through OmClic, we are able to always
implant a backdoor regardless of which common input size is chosen by the user
to train the DL model given the same attack budget (i.e., a fraction of the
poisoning rate). With our camouflaging algorithm formulated as a
multi-objective optimization, M=5 input sizes can be concurrently targeted with
one attack image, which artifact is retained to be almost visually
imperceptible at the same time. Extensive evaluations validate the proposed
OmClic can reliably succeed in various settings using diverse types of images.
Further experiments on OmClic based backdoor insertion to DL models show that
high backdoor performances (i.e., attack success rate and clean data accuracy)
are achievable no matter which common input size is randomly chosen by the user
to train the model. So that the OmClic based backdoor attack budget is reduced
by M$\times$ compared to the state-of-the-art camouflage based backdoor attack
as a baseline. Significantly, the same set of OmClic based poisonous attack
images is transferable to different model architectures for backdoor implant.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: BluNF: Blueprint Neural Field. (arXiv:2309.03933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03933">http://arxiv.org/abs/2309.03933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03933]] BluNF: Blueprint Neural Field(http://arxiv.org/abs/2309.03933)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRFs) have revolutionized scene novel view
synthesis, offering visually realistic, precise, and robust implicit
reconstructions. While recent approaches enable NeRF editing, such as object
removal, 3D shape modification, or material property manipulation, the manual
annotation prior to such edits makes the process tedious. Additionally,
traditional 2D interaction tools lack an accurate sense of 3D space, preventing
precise manipulation and editing of scenes. In this paper, we introduce a novel
approach, called Blueprint Neural Field (BluNF), to address these editing
issues. BluNF provides a robust and user-friendly 2D blueprint, enabling
intuitive scene editing. By leveraging implicit neural representation, BluNF
constructs a blueprint of a scene using prior semantic and depth information.
The generated blueprint allows effortless editing and manipulation of NeRF
representations. We demonstrate BluNF's editability through an intuitive
click-and-change mechanism, enabling 3D manipulations, such as masking,
appearance modification, and object removal. Our approach significantly
contributes to visual content creation, paving the way for further research in
this area.
</p></li>
</ul>

<h3>Title: REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation. (arXiv:2309.03964v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03964">http://arxiv.org/abs/2309.03964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03964]] REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation(http://arxiv.org/abs/2309.03964)</code></li>
<li>Summary: <p>Fully-test-time adaptation (F-TTA) can mitigate performance loss due to
distribution shifts between train and test data (1) without access to the
training data, and (2) without knowledge of the model training procedure. In
online F-TTA, a pre-trained model is adapted using a stream of test samples by
minimizing a self-supervised objective, such as entropy minimization. However,
models adapted with online using entropy minimization, are unstable especially
in single sample settings, leading to degenerate solutions, and limiting the
adoption of TTA inference strategies. Prior works identify noisy, or
unreliable, samples as a cause of failure in online F-TTA. One solution is to
ignore these samples, which can lead to bias in the update procedure, slow
adaptation, and poor generalization. In this work, we present a general
framework for improving robustness of F-TTA to these noisy samples, inspired by
self-paced learning and robust loss functions. Our proposed approach, Robust
Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy
than previous approaches throughout the adaptation process on corruptions of
CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.
</p></li>
</ul>

<h3>Title: Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03999">http://arxiv.org/abs/2309.03999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03999]] Adapting Self-Supervised Representations to Multi-Domain Setups(http://arxiv.org/abs/2309.03999)</code></li>
<li>Summary: <p>Current state-of-the-art self-supervised approaches, are effective when
trained on individual domains but show limited generalization on unseen
domains. We observe that these models poorly generalize even when trained on a
mixture of domains, making them unsuitable to be deployed under diverse
real-world setups. We therefore propose a general-purpose, lightweight Domain
Disentanglement Module (DDM) that can be plugged into any self-supervised
encoder to effectively perform representation learning on multiple, diverse
domains with or without shared classes. During pre-training according to a
self-supervised loss, DDM enforces a disentanglement in the representation
space by splitting it into a domain-variant and a domain-invariant portion.
When domain labels are not available, DDM uses a robust clustering approach to
discover pseudo-domains. We show that pre-training with DDM can show up to 3.5%
improvement in linear probing accuracy on state-of-the-art self-supervised
models including SimCLR, MoCo, BYOL, DINO, SimSiam and Barlow Twins on
multi-domain benchmarks including PACS, DomainNet and WILDS. Models trained
with DDM show significantly improved generalization (7.4%) to unseen domains
compared to baselines. Therefore, DDM can efficiently adapt self-supervised
encoders to provide high-quality, generalizable representations for diverse
multi-domain data.
</p></li>
</ul>

<h3>Title: Depth Completion with Multiple Balanced Bases and Confidence for Dense Monocular SLAM. (arXiv:2309.04145v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04145">http://arxiv.org/abs/2309.04145</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04145]] Depth Completion with Multiple Balanced Bases and Confidence for Dense Monocular SLAM(http://arxiv.org/abs/2309.04145)</code></li>
<li>Summary: <p>Dense SLAM based on monocular cameras does indeed have immense application
value in the field of AR/VR, especially when it is performed on a mobile
device. In this paper, we propose a novel method that integrates a light-weight
depth completion network into a sparse SLAM system using a multi-basis depth
representation, so that dense mapping can be performed online even on a mobile
phone. Specifically, we present a specifically optimized multi-basis depth
completion network, called BBC-Net, tailored to the characteristics of
traditional sparse SLAM systems. BBC-Net can predict multiple balanced bases
and a confidence map from a monocular image with sparse points generated by
off-the-shelf keypoint-based SLAM systems. The final depth is a linear
combination of predicted depth bases that can be optimized by tuning the
corresponding weights. To seamlessly incorporate the weights into traditional
SLAM optimization and ensure efficiency and robustness, we design a set of
depth weight factors, which makes our network a versatile plug-in module,
facilitating easy integration into various existing sparse SLAM systems and
significantly enhancing global depth consistency through bundle adjustment. To
verify the portability of our method, we integrate BBC-Net into two
representative SLAM systems. The experimental results on various datasets show
that the proposed method achieves better performance in monocular dense mapping
than the state-of-the-art methods. We provide an online demo running on a
mobile phone, which verifies the efficiency and mapping quality of the proposed
method in real-world scenarios.
</p></li>
</ul>

<h3>Title: Towards Practical Capture of High-Fidelity Relightable Avatars. (arXiv:2309.04247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04247">http://arxiv.org/abs/2309.04247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04247]] Towards Practical Capture of High-Fidelity Relightable Avatars(http://arxiv.org/abs/2309.04247)</code></li>
<li>Summary: <p>In this paper, we propose a novel framework, Tracking-free Relightable Avatar
(TRAvatar), for capturing and reconstructing high-fidelity 3D avatars. Compared
to previous methods, TRAvatar works in a more practical and efficient setting.
Specifically, TRAvatar is trained with dynamic image sequences captured in a
Light Stage under varying lighting conditions, enabling realistic relighting
and real-time animation for avatars in diverse scenes. Additionally, TRAvatar
allows for tracking-free avatar capture and obviates the need for accurate
surface tracking under varying illumination conditions. Our contributions are
two-fold: First, we propose a novel network architecture that explicitly builds
on and ensures the satisfaction of the linear nature of lighting. Trained on
simple group light captures, TRAvatar can predict the appearance in real-time
with a single forward pass, achieving high-quality relighting effects under
illuminations of arbitrary environment maps. Second, we jointly optimize the
facial geometry and relightable appearance from scratch based on image
sequences, where the tracking is implicitly learned. This tracking-free
approach brings robustness for establishing temporal correspondences between
frames under different lighting conditions. Extensive qualitative and
quantitative experiments demonstrate that our framework achieves superior
performance for photorealistic avatar animation and relighting.
</p></li>
</ul>

<h3>Title: WiSARD: A Labeled Visual and Thermal Image Dataset for Wilderness Search and Rescue. (arXiv:2309.04453v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04453">http://arxiv.org/abs/2309.04453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04453]] WiSARD: A Labeled Visual and Thermal Image Dataset for Wilderness Search and Rescue(http://arxiv.org/abs/2309.04453)</code></li>
<li>Summary: <p>Sensor-equipped unoccupied aerial vehicles (UAVs) have the potential to help
reduce search times and alleviate safety risks for first responders carrying
out Wilderness Search and Rescue (WiSAR) operations, the process of finding and
rescuing person(s) lost in wilderness areas. Unfortunately, visual sensors
alone do not address the need for robustness across all the possible terrains,
weather, and lighting conditions that WiSAR operations can be conducted in. The
use of multi-modal sensors, specifically visual-thermal cameras, is critical in
enabling WiSAR UAVs to perform in diverse operating conditions. However, due to
the unique challenges posed by the wilderness context, existing dataset
benchmarks are inadequate for developing vision-based algorithms for autonomous
WiSAR UAVs. To this end, we present WiSARD, a dataset with roughly 56,000
labeled visual and thermal images collected from UAV flights in various
terrains, seasons, weather, and lighting conditions. To the best of our
knowledge, WiSARD is the first large-scale dataset collected with multi-modal
sensors for autonomous WiSAR operations. We envision that our dataset will
provide researchers with a diverse and challenging benchmark that can test the
robustness of their algorithms when applied to real-world (life-saving)
applications.
</p></li>
</ul>

<h3>Title: GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue. (arXiv:2309.04162v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04162">http://arxiv.org/abs/2309.04162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04162]] GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue(http://arxiv.org/abs/2309.04162)</code></li>
<li>Summary: <p>Pre-trained models have achieved success in Chinese Short Text Matching (STM)
tasks, but they often rely on superficial clues, leading to a lack of robust
predictions. To address this issue, it is crucial to analyze and mitigate the
influence of superficial clues on STM models. Our study aims to investigate
their over-reliance on the edit distance feature, commonly used to measure the
semantic similarity of Chinese text pairs, which can be considered a
superficial clue. To mitigate STM models' over-reliance on superficial clues,
we propose a novel resampling training strategy called Gradually Learn Samples
Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of
In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we
demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the
robustness and generalization of Chinese STM models. Moreover, we conduct a
detailed analysis of existing methods and reveal their commonality.
</p></li>
</ul>

<h3>Title: Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04332">http://arxiv.org/abs/2309.04332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04332]] Graph Neural Networks Use Graphs When They Shouldn't(http://arxiv.org/abs/2309.04332)</code></li>
<li>Summary: <p>Predictions over graphs play a crucial role in various domains, including
social networks, molecular biology, medicine, and more. Graph Neural Networks
(GNNs) have emerged as the dominant approach for learning on graph data.
Instances of graph labeling problems consist of the graph-structure (i.e., the
adjacency matrix), along with node-specific feature vectors. In some cases,
this graph-structure is non-informative for the predictive task. For instance,
molecular properties such as molar mass depend solely on the constituent atoms
(node features), and not on the molecular structure. While GNNs have the
ability to ignore the graph-structure in such cases, it is not clear that they
will. In this work, we show that GNNs actually tend to overfit the
graph-structure in the sense that they use it even when a better solution can
be obtained by ignoring it. We examine this phenomenon with respect to
different graph distributions and find that regular graphs are more robust to
this overfitting. We then provide a theoretical explanation for this
phenomenon, via analyzing the implicit bias of gradient-descent-based learning
of GNNs in this setting. Finally, based on our empirical and theoretical
findings, we propose a graph-editing method to mitigate the tendency of GNNs to
overfit graph-structures that should be ignored. We show that this method
indeed improves the accuracy of GNNs across multiple benchmarks.
</p></li>
</ul>

<h3>Title: Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04344">http://arxiv.org/abs/2309.04344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04344]] Zero-Shot Robustification of Zero-Shot Models With Foundation Models(http://arxiv.org/abs/2309.04344)</code></li>
<li>Summary: <p>Zero-shot inference is a powerful paradigm that enables the use of large
pretrained models for downstream classification tasks without further training.
However, these models are vulnerable to inherited biases that can impact their
performance. The traditional solution is fine-tuning, but this undermines the
key advantage of pretrained models, which is their ability to be used
out-of-the-box. We propose RoboShot, a method that improves the robustness of
pretrained model embeddings in a fully zero-shot fashion. First, we use
zero-shot language models (LMs) to obtain useful insights from task
descriptions. These insights are embedded and used to remove harmful and boost
useful components in embeddings -- without any supervision. Theoretically, we
provide a simple and tractable model for biases in zero-shot embeddings and
give a result characterizing under what conditions our approach can boost
performance. Empirically, we evaluate RoboShot on nine image and NLP
classification tasks and show an average improvement of 15.98% over several
zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible
with a variety of pretrained and language models.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Long-Range Correlation Supervision for Land-Cover Classification from Remote Sensing Images. (arXiv:2309.04225v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04225">http://arxiv.org/abs/2309.04225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04225]] Long-Range Correlation Supervision for Land-Cover Classification from Remote Sensing Images(http://arxiv.org/abs/2309.04225)</code></li>
<li>Summary: <p>Long-range dependency modeling has been widely considered in modern deep
learning based semantic segmentation methods, especially those designed for
large-size remote sensing images, to compensate the intrinsic locality of
standard convolutions. However, in previous studies, the long-range dependency,
modeled with an attention mechanism or transformer model, has been based on
unsupervised learning, instead of explicit supervision from the objective
ground truth. In this paper, we propose a novel supervised long-range
correlation method for land-cover classification, called the supervised
long-range correlation network (SLCNet), which is shown to be superior to the
currently used unsupervised strategies. In SLCNet, pixels sharing the same
category are considered highly correlated and those having different categories
are less relevant, which can be easily supervised by the category consistency
information available in the ground truth semantic segmentation map. Under such
supervision, the recalibrated features are more consistent for pixels of the
same category and more discriminative for pixels of other categories,
regardless of their proximity. To complement the detailed information lacking
in the global long-range correlation, we introduce an auxiliary adaptive
receptive field feature extraction module, parallel to the long-range
correlation module in the encoder, to capture finely detailed feature
representations for multi-size objects in multi-scale remote sensing images. In
addition, we apply multi-scale side-output supervision and a hybrid loss
function as local and global constraints to further boost the segmentation
accuracy. Experiments were conducted on three remote sensing datasets. Compared
with the advanced segmentation methods from the computer vision, medicine, and
remote sensing communities, the SLCNet achieved a state-of-the-art performance
on all the datasets.
</p></li>
</ul>

<h3>Title: NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04146">http://arxiv.org/abs/2309.04146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04146]] NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus(http://arxiv.org/abs/2309.04146)</code></li>
<li>Summary: <p>The statistical analysis of large scale legal corpus can provide valuable
legal insights. For such analysis one needs to (1) select a subset of the
corpus using document retrieval tools, (2) structuralize text using information
extraction (IE) systems, and (3) visualize the data for the statistical
analysis. Each process demands either specialized tools or programming skills
whereas no comprehensive unified "no-code" tools have been available.
Especially for IE, if the target information is not predefined in the ontology
of the IE system, one needs to build their own system. Here we provide NESTLE,
a no code tool for large-scale statistical analysis of legal corpus. With
NESTLE, users can search target documents, extract information, and visualize
the structured data all via the chat interface with accompanying auxiliary GUI
for the fine-level control. NESTLE consists of three main components: a search
engine, an end-to-end IE system, and a Large Language Model (LLM) that glues
the whole components together and provides the chat interface. Powered by LLM
and the end-to-end IE system, NESTLE can extract any type of information that
has not been predefined in the IE system opening up the possibility of
unlimited customizable statistical analysis of the corpus without writing a
single line of code. The use of the custom end-to-end IE system also enables
faster and low-cost IE on large scale corpus. We validate our system on 15
Korean precedent IE tasks and 3 legal text classification tasks from LEXGLUE.
The comprehensive experiments reveal NESTLE can achieve GPT-4 comparable
performance by training the internal IE module with 4 human-labeled, and 192
LLM-labeled examples. The detailed analysis provides the insight on the
trade-off between accuracy, time, and cost in building such system.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning for Early Dropout Prediction on Healthy Ageing Applications. (arXiv:2309.04311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04311">http://arxiv.org/abs/2309.04311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04311]] Federated Learning for Early Dropout Prediction on Healthy Ageing Applications(http://arxiv.org/abs/2309.04311)</code></li>
<li>Summary: <p>The provision of social care applications is crucial for elderly people to
improve their quality of life and enables operators to provide early
interventions. Accurate predictions of user dropouts in healthy ageing
applications are essential since they are directly related to individual health
statuses. Machine Learning (ML) algorithms have enabled highly accurate
predictions, outperforming traditional statistical methods that struggle to
cope with individual patterns. However, ML requires a substantial amount of
data for training, which is challenging due to the presence of personal
identifiable information (PII) and the fragmentation posed by regulations. In
this paper, we present a federated machine learning (FML) approach that
minimizes privacy concerns and enables distributed training, without
transferring individual data. We employ collaborative training by considering
individuals and organizations under FML, which models both cross-device and
cross-silo learning scenarios. Our approach is evaluated on a real-world
dataset with non-independent and identically distributed (non-iid) data among
clients, class imbalance and label ambiguity. Our results show that data
selection and class imbalance handling techniques significantly improve the
predictive accuracy of models trained under FML, demonstrating comparable or
superior predictive performance than traditional ML models.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04027">http://arxiv.org/abs/2309.04027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04027]] TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models(http://arxiv.org/abs/2309.04027)</code></li>
<li>Summary: <p>Machine learning models can perpetuate unintended biases from unfair and
imbalanced datasets. Evaluating and debiasing these datasets and models is
especially hard in text datasets where sensitive attributes such as race,
gender, and sexual orientation may not be available. When these models are
deployed into society, they can lead to unfair outcomes for historically
underrepresented groups. In this paper, we present a dataset coupled with an
approach to improve text fairness in classifiers and language models. We create
a new, more comprehensive identity lexicon, TIDAL, which includes 15,123
identity terms and associated sense context across three demographic
categories. We leverage TIDAL to develop an identity annotation and
augmentation tool that can be used to improve the availability of identity
context and the effectiveness of ML fairness techniques. We evaluate our
approaches using human contributors, and additionally run experiments focused
on dataset and model debiasing. Results show our assistive annotation technique
improves the reliability and velocity of human-in-the-loop processes. Our
dataset and methods uncover more disparities during evaluation, and also
produce more fair models during remediation. These approaches provide a
practical path forward for scaling classifier and generative model fairness in
real-world settings.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval. (arXiv:2309.04171v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04171">http://arxiv.org/abs/2309.04171</a></li>
<li>Code URL: https://github.com/liuaxou/prista-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04171]] PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval(http://arxiv.org/abs/2309.04171)</code></li>
<li>Summary: <p>The problem of phase retrieval (PR) involves recovering an unknown image from
limited amplitude measurement data and is a challenge nonlinear inverse problem
in computational imaging and image processing. However, many of the PR methods
are based on black-box network models that lack interpretability and
plug-and-play (PnP) frameworks that are computationally complex and require
careful parameter tuning. To address this, we have developed PRISTA-Net, a deep
unfolding network (DUN) based on the first-order iterative shrinkage
thresholding algorithm (ISTA). This network utilizes a learnable nonlinear
transformation to address the proximal-point mapping sub-problem associated
with the sparse priors, and an attention mechanism to focus on phase
information containing image edges, textures, and structures. Additionally, the
fast Fourier transform (FFT) is used to learn global features to enhance local
information, and the designed logarithmic-based loss function leads to
significant improvements when the noise level is low. All parameters in the
proposed PRISTA-Net framework, including the nonlinear transformation,
threshold parameters, and step size, are learned end-to-end instead of being
manually set. This method combines the interpretability of traditional methods
with the fast inference ability of deep learning and is able to handle noise at
each iteration during the unfolding stage, thus improving recovery quality.
Experiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that
our approach outperforms the existing state-of-the-art methods in terms of
qualitative and quantitative evaluations. Our source codes are available at
\emph{https://github.com/liuaxou/PRISTA-Net}.
</p></li>
</ul>

<h3>Title: Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!. (arXiv:2309.03970v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03970">http://arxiv.org/abs/2309.03970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03970]] Automatic Concept Embedding Model (ACEM): No train-time concepts, No issue!(http://arxiv.org/abs/2309.03970)</code></li>
<li>Summary: <p>Interpretability and explainability of neural networks is continuously
increasing in importance, especially within safety-critical domains and to
provide the social right to explanation. Concept based explanations align well
with how humans reason, proving to be a good way to explain models. Concept
Embedding Models (CEMs) are one such concept based explanation architectures.
These have shown to overcome the trade-off between explainability and
performance. However, they have a key limitation -- they require concept
annotations for all their training data. For large datasets, this can be
expensive and infeasible. Motivated by this, we propose Automatic Concept
Embedding Models (ACEMs), which learn the concept annotations automatically.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models. (arXiv:2309.04109v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04109">http://arxiv.org/abs/2309.04109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04109]] From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models(http://arxiv.org/abs/2309.04109)</code></li>
<li>Summary: <p>Diffusion models have revolted the field of text-to-image generation
recently. The unique way of fusing text and image information contributes to
their remarkable capability of generating highly text-related images. From
another perspective, these generative models imply clues about the precise
correlation between words and pixels. In this work, a simple but effective
method is proposed to utilize the attention mechanism in the denoising network
of text-to-image diffusion models. Without re-training nor inference-time
optimization, the semantic grounding of phrases can be attained directly. We
evaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under
weakly-supervised semantic segmentation setting and our method achieves
superior performance to prior methods. In addition, the acquired word-pixel
correlation is found to be generalizable for the learned text embedding of
customized generation methods, requiring only a few modifications. To validate
our discovery, we introduce a new practical task called "personalized referring
image segmentation" with a new dataset. Experiments in various situations
demonstrate the advantages of our method compared to strong baselines on this
task. In summary, our work reveals a novel way to extract the rich multi-modal
knowledge hidden in diffusion models for segmentation.
</p></li>
</ul>

<h3>Title: MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers. (arXiv:2309.04372v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04372">http://arxiv.org/abs/2309.04372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04372]] MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers(http://arxiv.org/abs/2309.04372)</code></li>
<li>Summary: <p>Diffusion-model-based text-guided image generation has recently made
astounding progress, producing fascinating results in open-domain image
manipulation tasks. Few models, however, currently have complete zero-shot
capabilities for both global and local image editing due to the complexity and
diversity of image manipulation tasks. In this work, we propose a method with a
mixture-of-expert (MOE) controllers to align the text-guided capacity of
diffusion models with different kinds of human instructions, enabling our model
to handle various open-domain image manipulation tasks with natural language
instructions. First, we use large language models (ChatGPT) and conditional
image synthesis models (ControlNet) to generate a large number of global image
transfer dataset in addition to the instruction-based local image editing
dataset. Then, using an MOE technique and task-specific adaptation training on
a large-scale dataset, our conditional diffusion model can edit images globally
and locally. Extensive experiments demonstrate that our approach performs
surprisingly well on various image manipulation tasks when dealing with
open-domain images and arbitrary human instructions. Please refer to our
project page: [https://oppo-mente-lab.github.io/moe_controller/]
</p></li>
</ul>

<h3>Title: MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask. (arXiv:2309.04399v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04399">http://arxiv.org/abs/2309.04399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04399]] MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask(http://arxiv.org/abs/2309.04399)</code></li>
<li>Summary: <p>Recent advancements in diffusion models have showcased their impressive
capacity to generate visually striking images. Nevertheless, ensuring a close
match between the generated image and the given prompt remains a persistent
challenge. In this work, we identify that a crucial factor leading to the
text-image mismatch issue is the inadequate cross-modality relation learning
between the prompt and the output image. To better align the prompt and image
content, we advance the cross-attention with an adaptive mask, which is
conditioned on the attention maps and the prompt embeddings, to dynamically
adjust the contribution of each text token to the image features. This
mechanism explicitly diminishes the ambiguity in semantic information embedding
from the text encoder, leading to a boost of text-to-image consistency in the
synthesized images. Our method, termed MaskDiffusion, is training-free and
hot-pluggable for popular pre-trained diffusion models. When applied to the
latent diffusion models, our MaskDiffusion can significantly improve the
text-to-image consistency with negligible computation overhead compared to the
original diffusion models.
</p></li>
</ul>

<h3>Title: Create Your World: Lifelong Text-to-Image Diffusion. (arXiv:2309.04430v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04430">http://arxiv.org/abs/2309.04430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04430]] Create Your World: Lifelong Text-to-Image Diffusion(http://arxiv.org/abs/2309.04430)</code></li>
<li>Summary: <p>Text-to-image generative models can produce diverse high-quality images of
concepts with a text prompt, which have demonstrated excellent ability in image
generation, image translation, etc. We in this work study the problem of
synthesizing instantiations of a use's own concepts in a never-ending manner,
i.e., create your world, where the new concepts from user are quickly learned
with a few examples. To achieve this goal, we propose a Lifelong text-to-image
Diffusion Model (L2DM), which intends to overcome knowledge "catastrophic
forgetting" for the past encountered concepts, and semantic "catastrophic
neglecting" for one or more concepts in the text prompt. In respect of
knowledge "catastrophic forgetting", our L2DM framework devises a task-aware
memory enhancement module and a elastic-concept distillation module, which
could respectively safeguard the knowledge of both prior concepts and each past
personalized concept. When generating images with a user text prompt, the
solution to semantic "catastrophic neglecting" is that a concept attention
artist module can alleviate the semantic neglecting from concept aspect, and an
orthogonal attention module can reduce the semantic binding from attribute
aspect. To the end, our model can generate more faithful image across a range
of continual text prompts in terms of both qualitative and quantitative
metrics, when comparing with the related state-of-the-art models. The code will
be released at https://wenqiliang.github.io/.
</p></li>
</ul>

<h3>Title: Variations and Relaxations of Normalizing Flows. (arXiv:2309.04433v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04433">http://arxiv.org/abs/2309.04433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04433]] Variations and Relaxations of Normalizing Flows(http://arxiv.org/abs/2309.04433)</code></li>
<li>Summary: <p>Normalizing Flows (NFs) describe a class of models that express a complex
target distribution as the composition of a series of bijective transformations
over a simpler base distribution. By limiting the space of candidate
transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and
density evaluation, enabling NFs to flexibly behave as both discriminative and
generative models. Their restriction to diffeomorphisms, however, enforces that
input, output and all intermediary spaces share the same dimension, limiting
their ability to effectively represent target distributions with complex
topologies. Additionally, in cases where the prior and target distributions are
not homeomorphic, Normalizing Flows can leak mass outside of the support of the
target. This survey covers a selection of recent works that combine aspects of
other generative model classes, such as VAEs and score-based diffusion, and in
doing so loosen the strict bijectivity constraints of NFs to achieve a balance
of expressivity, training speed, sample efficiency and likelihood tractability.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Separable Self and Mixed Attention Transformers for Efficient Object Tracking. (arXiv:2309.03979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03979">http://arxiv.org/abs/2309.03979</a></li>
<li>Code URL: https://github.com/goutamyg/smat</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03979]] Separable Self and Mixed Attention Transformers for Efficient Object Tracking(http://arxiv.org/abs/2309.03979)</code></li>
<li>Summary: <p>The deployment of transformers for visual object tracking has shown
state-of-the-art results on several benchmarks. However, the transformer-based
models are under-utilized for Siamese lightweight tracking due to the
computational complexity of their attention blocks. This paper proposes an
efficient self and mixed attention transformer-based architecture for
lightweight tracking. The proposed backbone utilizes the separable mixed
attention transformers to fuse the template and search regions during feature
extraction to generate superior feature encoding. Our prediction head performs
global contextual modeling of the encoded features by leveraging efficient
self-attention blocks for robust target state estimation. With these
contributions, the proposed lightweight tracker deploys a transformer-based
backbone and head module concurrently for the first time. Our ablation study
testifies to the effectiveness of the proposed combination of backbone and head
modules. Simulations show that our Separable Self and Mixed Attention-based
Tracker, SMAT, surpasses the performance of related lightweight trackers on
GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at
37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it
significantly surpasses the closely related trackers E.T.Track and
MixFormerV2-S on GOT10k-test by a margin of 7.9% and 5.8%, respectively, in the
AO metric. The tracker code and model is available at
https://github.com/goutamyg/SMAT
</p></li>
</ul>

<h3>Title: Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04001">http://arxiv.org/abs/2309.04001</a></li>
<li>Code URL: https://github.com/csiplab/mmsformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04001]] Multimodal Transformer for Material Segmentation(http://arxiv.org/abs/2309.04001)</code></li>
<li>Summary: <p>Leveraging information across diverse modalities is known to enhance
performance on multimodal segmentation tasks. However, effectively fusing
information from different modalities remains challenging due to the unique
characteristics of each modality. In this paper, we propose a novel fusion
strategy that can effectively fuse information from different combinations of
four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of
Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model
named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the
proposed fusion strategy to perform multimodal material segmentation. MMSFormer
achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal
Material Segmentation (MCubeS) dataset. For instance, our method provides
significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes.
Ablation studies show that different modules in the fusion block are crucial
for overall model performance. Furthermore, our ablation studies also highlight
the capacity of different input modalities to improve performance in the
identification of different types of materials. The code and pretrained models
will be made available at https://github.com/csiplab/MMSFormer.
</p></li>
</ul>

<h3>Title: S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens. (arXiv:2309.04038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04038">http://arxiv.org/abs/2309.04038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04038]] S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens(http://arxiv.org/abs/2309.04038)</code></li>
<li>Summary: <p>Face Anti-Spoofing (FAS) aims to detect malicious attempts to invade a face
recognition system by presenting spoofed faces. State-of-the-art FAS techniques
predominantly rely on deep learning models but their cross-domain
generalization capabilities are often hindered by the domain shift problem,
which arises due to different distributions between training and testing data.
In this study, we develop a generalized FAS method under the Efficient
Parameter Transfer Learning (EPTL) paradigm, where we adapt the pre-trained
Vision Transformer models for the FAS task. During training, the adapter
modules are inserted into the pre-trained ViT model, and the adapters are
updated while other pre-trained parameters remain fixed. We find the
limitations of previous vanilla adapters in that they are based on linear
layers, which lack a spoofing-aware inductive bias and thus restrict the
cross-domain generalization. To address this limitation and achieve
cross-domain generalized FAS, we propose a novel Statistical Adapter
(S-Adapter) that gathers local discriminative and statistical information from
localized token histograms. To further improve the generalization of the
statistical tokens, we propose a novel Token Style Regularization (TSR), which
aims to reduce domain style variance by regularizing Gram matrices extracted
from tokens across different domains. Our experimental results demonstrate that
our proposed S-Adapter and TSR provide significant benefits in both zero-shot
and few-shot cross-domain testing, outperforming state-of-the-art methods on
several benchmark tests. We will release the source code upon acceptance.
</p></li>
</ul>

<h3>Title: Weakly Supervised Point Clouds Transformer for 3D Object Detection. (arXiv:2309.04105v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04105">http://arxiv.org/abs/2309.04105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04105]] Weakly Supervised Point Clouds Transformer for 3D Object Detection(http://arxiv.org/abs/2309.04105)</code></li>
<li>Summary: <p>The annotation of 3D datasets is required for semantic-segmentation and
object detection in scene understanding. In this paper we present a framework
for the weakly supervision of a point clouds transformer that is used for 3D
object detection. The aim is to decrease the required amount of supervision
needed for training, as a result of the high cost of annotating a 3D datasets.
We propose an Unsupervised Voting Proposal Module, which learns randomly preset
anchor points and uses voting network to select prepared anchor points of high
quality. Then it distills information into student and teacher network. In
terms of student network, we apply ResNet network to efficiently extract local
characteristics. However, it also can lose much global information. To provide
the input which incorporates the global and local information as the input of
student networks, we adopt the self-attention mechanism of transformer to
extract global features, and the ResNet layers to extract region proposals. The
teacher network supervises the classification and regression of the student
network using the pre-trained model on ImageNet. On the challenging KITTI
datasets, the experimental results have achieved the highest level of average
precision compared with the most recent weakly supervised 3D object detectors.
</p></li>
</ul>

<h3>Title: Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts. (arXiv:2309.04354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04354">http://arxiv.org/abs/2309.04354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04354]] Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts(http://arxiv.org/abs/2309.04354)</code></li>
<li>Summary: <p>Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due
to their ability to decouple model size from inference efficiency by only
activating a small subset of the model parameters for any given input token. As
such, sparse MoEs have enabled unprecedented scalability, resulting in
tremendous successes across domains such as natural language processing and
computer vision. In this work, we instead explore the use of sparse MoEs to
scale-down Vision Transformers (ViTs) to make them more attractive for
resource-constrained vision applications. To this end, we propose a simplified
and mobile-friendly MoE design where entire images rather than individual
patches are routed to the experts. We also propose a stable MoE training
procedure that uses super-class information to guide the router. We empirically
show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off
between performance and efficiency than the corresponding dense ViTs. For
example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense
counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only
54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.
</p></li>
</ul>

<h3>Title: CNN Injected Transformer for Image Exposure Correction. (arXiv:2309.04366v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04366">http://arxiv.org/abs/2309.04366</a></li>
<li>Code URL: https://github.com/rebeccaeexu/cit-ec</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04366]] CNN Injected Transformer for Image Exposure Correction(http://arxiv.org/abs/2309.04366)</code></li>
<li>Summary: <p>Capturing images with incorrect exposure settings fails to deliver a
satisfactory visual experience. Only when the exposure is properly set, can the
color and details of the images be appropriately preserved. Previous exposure
correction methods based on convolutions often produce exposure deviation in
images as a consequence of the restricted receptive field of convolutional
kernels. This issue arises because convolutions are not capable of capturing
long-range dependencies in images accurately. To overcome this challenge, we
can apply the Transformer to address the exposure correction problem,
leveraging its capability in modeling long-range dependencies to capture global
representation. However, solely relying on the window-based Transformer leads
to visually disturbing blocking artifacts due to the application of
self-attention in small patches. In this paper, we propose a CNN Injected
Transformer (CIT) to harness the individual strengths of CNN and Transformer
simultaneously. Specifically, we construct the CIT by utilizing a window-based
Transformer to exploit the long-range interactions among different regions in
the entire image. Within each CIT block, we incorporate a channel attention
block (CAB) and a half-instance normalization block (HINB) to assist the
window-based self-attention to acquire the global statistics and refine local
features. In addition to the hybrid architecture design for exposure
correction, we apply a set of carefully formulated loss functions to improve
the spatial coherence and rectify potential color deviations. Extensive
experiments demonstrate that our image exposure correction method outperforms
state-of-the-art approaches in terms of both quantitative and qualitative
metrics.
</p></li>
</ul>

<h3>Title: Language Prompt for Autonomous Driving. (arXiv:2309.04379v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04379">http://arxiv.org/abs/2309.04379</a></li>
<li>Code URL: https://github.com/wudongming97/prompt4driving</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04379]] Language Prompt for Autonomous Driving(http://arxiv.org/abs/2309.04379)</code></li>
<li>Summary: <p>A new trend in the computer vision community is to capture objects of
interest following flexible human command represented by a natural language
prompt. However, the progress of using language prompts in driving scenarios is
stuck in a bottleneck due to the scarcity of paired prompt-instance data. To
address this challenge, we propose the first object-centric language prompt set
for driving scenes within 3D, multi-view, and multi-frame space, named
NuPrompt. It expands Nuscenes dataset by constructing a total of 35,367
language descriptions, each referring to an average of 5.3 object tracks. Based
on the object-text pairs from the new benchmark, we formulate a new
prompt-based driving task, \ie, employing a language prompt to predict the
described object trajectory across views and frames. Furthermore, we provide a
simple end-to-end baseline model based on Transformer, named PromptTrack.
Experiments show that our PromptTrack achieves impressive performance on
NuPrompt. We hope this work can provide more new insights for the autonomous
driving community. Dataset and Code will be made public at
\href{https://github.com/wudongming97/Prompt4Driving}{https://github.com/wudongming97/Prompt4Driving}.
</p></li>
</ul>

<h3>Title: Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations. (arXiv:2309.04292v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04292">http://arxiv.org/abs/2309.04292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04292]] Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations(http://arxiv.org/abs/2309.04292)</code></li>
<li>Summary: <p>Fuzzy Fingerprints have been successfully used as an interpretable text
classification technique, but, like most other techniques, have been largely
surpassed in performance by Large Pre-trained Language Models, such as BERT or
RoBERTa. These models deliver state-of-the-art results in several Natural
Language Processing tasks, namely Emotion Recognition in Conversations (ERC),
but suffer from the lack of interpretability and explainability. In this paper,
we propose to combine the two approaches to perform ERC, as a means to obtain
simpler and more interpretable Large Language Models-based classifiers. We
propose to feed the utterances and their previous conversational turns to a
pre-trained RoBERTa, obtaining contextual embedding utterance representations,
that are then supplied to an adapted Fuzzy Fingerprint classification module.
We validate our approach on the widely used DailyDialog ERC benchmark dataset,
in which we obtain state-of-the-art level results using a much lighter model.
</p></li>
</ul>

<h3>Title: Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens. (arXiv:2309.04333v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04333">http://arxiv.org/abs/2309.04333</a></li>
<li>Code URL: https://github.com/ronaldseoh/multi2spe</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04333]] Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens(http://arxiv.org/abs/2309.04333)</code></li>
<li>Summary: <p>Many useful tasks on scientific documents, such as topic classification and
citation prediction, involve corpora that span multiple scientific domains.
Typically, such tasks are accomplished by representing the text with a vector
embedding obtained from a Transformer's single CLS token. In this paper, we
argue that using multiple CLS tokens could make a Transformer better specialize
to multiple scientific domains. We present Multi2SPE: it encourages each of
multiple CLS tokens to learn diverse ways of aggregating token embeddings, then
sums them up together to create a single vector representation. We also propose
our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector
encoders under multi-domain settings. We show that Multi2SPE reduces error by
up to 25 percent in multi-domain citation prediction, while requiring only a
negligible amount of computation in addition to one BERT forward pass.
</p></li>
</ul>

<h3>Title: Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04082">http://arxiv.org/abs/2309.04082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04082]] Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning(http://arxiv.org/abs/2309.04082)</code></li>
<li>Summary: <p>Real-world graphs naturally exhibit hierarchical or cyclical structures that
are unfit for the typical Euclidean space. While there exist graph neural
networks that leverage hyperbolic or spherical spaces to learn representations
that embed such structures more accurately, these methods are confined under
the message-passing paradigm, making the models vulnerable against side-effects
such as oversmoothing and oversquashing. More recent work have proposed global
attention-based graph Transformers that can easily model long-range
interactions, but their extensions towards non-Euclidean geometry are yet
unexplored. To bridge this gap, we propose Fully Product-Stereographic
Transformer, a generalization of Transformers towards operating entirely on the
product of constant curvature spaces. When combined with tokenized graph
Transformers, our model can learn the curvature appropriate for the input graph
in an end-to-end fashion, without the need of additional tuning on different
curvature initializations. We also provide a kernelized approach to
non-Euclidean attention, which enables our model to run in time and memory cost
linear to the number of nodes and edges while respecting the underlying
geometry. Experiments on graph reconstruction and node classification
demonstrate the benefits of generalizing Transformers to the non-Euclidean
domain.
</p></li>
</ul>

<h3>Title: Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System. (arXiv:2309.04361v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04361">http://arxiv.org/abs/2309.04361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04361]] Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System(http://arxiv.org/abs/2309.04361)</code></li>
<li>Summary: <p>As power quality becomes a higher priority in the electric utility industry,
the amount of disturbance event data continues to grow. Utilities do not have
the required personnel to analyze each event by hand. This work presents an
automated approach for analyzing power quality events recorded by digital fault
recorders and power quality monitors operating within a power transmission
system. The automated approach leverages rule-based analytics to examine the
time and frequency domain characteristics of the voltage and current signals.
Customizable thresholds are set to categorize each disturbance event. The
events analyzed within this work include various faults, motor starting, and
incipient instrument transformer failure. Analytics for fourteen different
event types have been developed. The analytics were tested on 160 signal files
and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data
analysis is performed using an approach coined as the cyclic histogram. The
cyclic histogram process will be integrated into the digital fault recorders
themselves to facilitate the detection of subtle signal variations that are too
small to trigger a disturbance event and that can occur over hours or days. In
addition to reducing memory requirements by a factor of 320, it is anticipated
that cyclic histogram processing will aid in identifying incipient events and
identifiers. This project is expected to save engineers time by automating the
classification of disturbance events and increase the reliability of the
transmission system by providing near real time detection and identification of
disturbances as well as prevention of problems before they occur.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry. (arXiv:2309.04147v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04147">http://arxiv.org/abs/2309.04147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04147]] Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry(http://arxiv.org/abs/2309.04147)</code></li>
<li>Summary: <p>Visual odometry (VO) and SLAM have been using multi-view geometry via local
structure from motion for decades. These methods have a slight disadvantage in
challenging scenarios such as low-texture images, dynamic scenarios, etc.
Meanwhile, use of deep neural networks to extract high level features is
ubiquitous in computer vision. For VO, we can use these deep networks to
extract depth and pose estimates using these high level features. The visual
odometry task then can be modeled as an image generation task where the pose
estimation is the by-product. This can also be achieved in a self-supervised
manner, thereby eliminating the data (supervised) intensive nature of training
deep neural networks. Although some works tried the similar approach [1], the
depth and pose estimation in the previous works are vague sometimes resulting
in accumulation of error (drift) along the trajectory. The goal of this work is
to tackle these limitations of past approaches and to develop a method that can
provide better depths and pose estimates. To address this, a couple of
approaches are explored: 1) Modeling: Using optical flow and recurrent neural
networks (RNN) in order to exploit spatio-temporal correlations which can
provide more information to estimate depth. 2) Loss function: Generative
adversarial network (GAN) [2] is deployed to improve the depth estimation (and
thereby pose too), as shown in Figure 1. This additional loss term improves the
realism in generated images and reduces artifacts.
</p></li>
</ul>

<h3>Title: Score-PA: Score-based 3D Part Assembly. (arXiv:2309.04220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04220">http://arxiv.org/abs/2309.04220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04220]] Score-PA: Score-based 3D Part Assembly(http://arxiv.org/abs/2309.04220)</code></li>
<li>Summary: <p>Autonomous 3D part assembly is a challenging task in the areas of robotics
and 3D computer vision. This task aims to assemble individual components into a
complete shape without relying on predefined instructions. In this paper, we
formulate this task from a novel generative perspective, introducing the
Score-based 3D Part Assembly framework (Score-PA) for 3D part assembly. Knowing
that score-based methods are typically time-consuming during the inference
stage. To address this issue, we introduce a novel algorithm called the Fast
Predictor-Corrector Sampler (FPC) that accelerates the sampling process within
the framework. We employ various metrics to assess assembly quality and
diversity, and our evaluation results demonstrate that our algorithm
outperforms existing state-of-the-art approaches. We release our code at
https://github.com/J-F-Cheng/Score-PA_Score-based-3D-Part-Assembly.
</p></li>
</ul>

<h3>Title: SSIG: A Visually-Guided Graph Edit Distance for Floor Plan Similarity. (arXiv:2309.04357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04357">http://arxiv.org/abs/2309.04357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04357]] SSIG: A Visually-Guided Graph Edit Distance for Floor Plan Similarity(http://arxiv.org/abs/2309.04357)</code></li>
<li>Summary: <p>We propose a simple yet effective metric that measures structural similarity
between visual instances of architectural floor plans, without the need for
learning. Qualitatively, our experiments show that the retrieval results are
similar to deeply learned methods. Effectively comparing instances of floor
plan data is paramount to the success of machine understanding of floor plan
data, including the assessment of floor plan generative models and floor plan
recommendation systems. Comparing visual floor plan images goes beyond a sole
pixel-wise visual examination and is crucially about similarities and
differences in the shapes and relations between subdivisions that compose the
layout. Currently, deep metric learning approaches are used to learn a
pair-wise vector representation space that closely mimics the structural
similarity, in which the models are trained on similarity labels that are
obtained by Intersection-over-Union (IoU). To compensate for the lack of
structural awareness in IoU, graph-based approaches such as Graph Matching
Networks (GMNs) are used, which require pairwise inference for comparing data
instances, making GMNs less practical for retrieval applications. In this
paper, an effective evaluation metric for judging the structural similarity of
floor plans, coined SSIG (Structural Similarity by IoU and GED), is proposed
based on both image and graph distances. In addition, an efficient algorithm is
developed that uses SSIG to rank a large-scale floor plan database. Code will
be openly available.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. (arXiv:2309.04041v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04041">http://arxiv.org/abs/2309.04041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04041]] Evaluation and Mitigation of Agnosia in Multimodal Large Language Models(http://arxiv.org/abs/2309.04041)</code></li>
<li>Summary: <p>While Multimodal Large Language Models (MLLMs) are widely used for a variety
of vision-language tasks, one observation is that they sometimes misinterpret
visual inputs or fail to follow textual instructions even in straightforward
cases, leading to irrelevant responses, mistakes, and ungrounded claims. This
observation is analogous to a phenomenon in neuropsychology known as Agnosia,
an inability to correctly process sensory modalities and recognize things
(e.g., objects, colors, relations). In our study, we adapt this similar concept
to define "agnosia in MLLMs", and our goal is to comprehensively evaluate and
mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process
in neuropsychology, we propose a novel framework EMMA (Evaluation and
Mitigation of Multimodal Agnosia). In EMMA, we develop an evaluation module
that automatically creates fine-grained and diverse visual question answering
examples to assess the extent of agnosia in MLLMs comprehensively. We also
develop a mitigation module to reduce agnosia in MLLMs through multimodal
instruction tuning on fine-grained conversations. To verify the effectiveness
of our framework, we evaluate and analyze agnosia in seven state-of-the-art
MLLMs using 9K test samples. The results reveal that most of them exhibit
agnosia across various aspects and degrees. We further develop a fine-grained
instruction set and tune MLLMs to mitigate agnosia, which led to notable
improvement in accuracy.
</p></li>
</ul>

<h3>Title: Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment. (arXiv:2309.04158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04158">http://arxiv.org/abs/2309.04158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04158]] Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment(http://arxiv.org/abs/2309.04158)</code></li>
<li>Summary: <p>Large-scale vision-language models (VLMs), e.g., CLIP, learn broad visual
concepts from tedious training data, showing superb generalization ability.
Amount of prompt learning methods have been proposed to efficiently adapt the
VLMs to downstream tasks with only a few training samples. We introduce a novel
method to improve the prompt learning of vision-language models by
incorporating pre-trained large language models (LLMs), called Dual-Aligned
Prompt Tuning (DuAl-PT). Learnable prompts, like CoOp, implicitly model the
context through end-to-end training, which are difficult to control and
interpret. While explicit context descriptions generated by LLMs, like GPT-3,
can be directly used for zero-shot classification, such prompts are overly
relying on LLMs and still underexplored in few-shot domains. With DuAl-PT, we
propose to learn more context-aware prompts, benefiting from both explicit and
implicit context modeling. To achieve this, we introduce a pre-trained LLM to
generate context descriptions, and we encourage the prompts to learn from the
LLM's knowledge by alignment, as well as the alignment between prompts and
local image features. Empirically, DuAl-PT achieves superior performance on 11
downstream datasets on few-shot recognition and base-to-new generalization.
Hopefully, DuAl-PT can serve as a strong baseline. Code will be available.
</p></li>
</ul>

<h3>Title: LanSER: Language-Model Supported Speech Emotion Recognition. (arXiv:2309.03978v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03978">http://arxiv.org/abs/2309.03978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03978]] LanSER: Language-Model Supported Speech Emotion Recognition(http://arxiv.org/abs/2309.03978)</code></li>
<li>Summary: <p>Speech emotion recognition (SER) models typically rely on costly
human-labeled data for training, making scaling methods to large speech
datasets and nuanced emotion taxonomies difficult. We present LanSER, a method
that enables the use of unlabeled data by inferring weak emotion labels via
pre-trained large language models through weakly-supervised learning. For
inferring weak labels constrained to a taxonomy, we use a textual entailment
approach that selects an emotion label with the highest entailment score for a
speech transcript extracted via automatic speech recognition. Our experimental
results show that models pre-trained on large datasets with this weak
supervision outperform other baseline models on standard SER datasets when
fine-tuned, and show improved label efficiency. Despite being pre-trained on
labels derived only from text, we show that the resulting representations
appear to model the prosodic content of speech.
</p></li>
</ul>

<h3>Title: ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03992">http://arxiv.org/abs/2309.03992</a></li>
<li>Code URL: https://github.com/amritabh/conda-gen-text-detection</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03992]] ConDA: Contrastive Domain Adaptation for AI-generated Text Detection(http://arxiv.org/abs/2309.03992)</code></li>
<li>Summary: <p>Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.
</p></li>
</ul>

<h3>Title: Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems. (arXiv:2309.04031v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04031">http://arxiv.org/abs/2309.04031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04031]] Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems(http://arxiv.org/abs/2309.04031)</code></li>
<li>Summary: <p>Transferring the knowledge of large language models (LLMs) is a promising
technique to incorporate linguistic knowledge into end-to-end automatic speech
recognition (ASR) systems. However, existing works only transfer a single
representation of LLM (e.g. the last layer of pretrained BERT), while the
representation of a text is inherently non-unique and can be obtained variously
from different layers, contexts and models. In this work, we explore a wide
range of techniques to obtain and transfer multiple representations of LLMs
into a transducer-based ASR system. While being conceptually simple, we show
that transferring multiple representations of LLMs can be an effective
alternative to transferring only a single representation.
</p></li>
</ul>

<h3>Title: Meta predictive learning model of natural languages. (arXiv:2309.04106v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04106">http://arxiv.org/abs/2309.04106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04106]] Meta predictive learning model of natural languages(http://arxiv.org/abs/2309.04106)</code></li>
<li>Summary: <p>Large language models based on self-attention mechanisms have achieved
astonishing performances not only in natural language itself, but also in a
variety of tasks of different nature. However, regarding processing language,
our human brain may not operate using the same principle. Then, a debate is
established on the connection between brain computation and artificial
self-supervision adopted in large language models. One of most influential
hypothesis in brain computation is the predictive coding framework, which
proposes to minimize the prediction error by local learning. However, the role
of predictive coding and the associated credit assignment in language
processing remains unknown. Here, we propose a mean-field learning model within
the predictive coding framework, assuming that the synaptic weight of each
connection follows a spike and slab distribution, and only the distribution is
trained. This meta predictive learning is successfully validated on classifying
handwritten digits where pixels are input to the network in sequence, and on
the toy and real language corpus. Our model reveals that most of the
connections become deterministic after learning, while the output connections
have a higher level of variability. The performance of the resulting network
ensemble changes continuously with data load, further improving with more
training data, in analogy with the emergent behavior of large language models.
Therefore, our model provides a starting point to investigate the physics and
biology correspondences of the language processing and the unexpected general
intelligence.
</p></li>
</ul>

<h3>Title: Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04175">http://arxiv.org/abs/2309.04175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04175]] Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese(http://arxiv.org/abs/2309.04175)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable success in diverse
natural language processing (NLP) tasks in general domains. However, LLMs
sometimes generate responses with the hallucination about medical facts due to
limited domain knowledge. Such shortcomings pose potential risks in the
utilization of LLMs within medical contexts. To address this challenge, we
propose knowledge-tuning, which leverages structured medical knowledge bases
for the LLMs to grasp domain knowledge efficiently and facilitate reliable
response generation. We also release cMedKnowQA, a Chinese medical knowledge
question-answering dataset constructed from medical knowledge bases to assess
the medical knowledge proficiency of LLMs. Experimental results show that the
LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of
accuracy in response generation compared with vanilla instruction-tuning and
offer a new reliable way for the domain adaptation of LLMs.
</p></li>
</ul>

<h3>Title: The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature. (arXiv:2309.04198v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04198">http://arxiv.org/abs/2309.04198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04198]] The CALLA Dataset: Probing LLMs' Interactive Knowledge Acquisition from Chinese Medical Literature(http://arxiv.org/abs/2309.04198)</code></li>
<li>Summary: <p>The application of Large Language Models (LLMs) to the medical domain has
stimulated the interest of researchers. Recent studies have focused on
constructing Instruction Fine-Tuning (IFT) data through medical knowledge
graphs to enrich the interactive medical knowledge of LLMs. However, the
medical literature serving as a rich source of medical knowledge remains
unexplored. Our work introduces the CALLA dataset to probe LLMs' interactive
knowledge acquisition from Chinese medical literature. It assesses the
proficiency of LLMs in mastering medical knowledge through a free-dialogue
fact-checking task. We identify a phenomenon called the ``fact-following
response``, where LLMs tend to affirm facts mentioned in questions and display
a reluctance to challenge them. To eliminate the inaccurate evaluation caused
by this phenomenon, for the golden fact, we artificially construct test data
from two perspectives: one consistent with the fact and one inconsistent with
the fact. Drawing from the probing experiment on the CALLA dataset, we conclude
that IFT data highly correlated with the medical literature corpus serves as a
potent catalyst for LLMs, enabling themselves to skillfully employ the medical
knowledge acquired during the pre-training phase within interactive scenarios,
enhancing accuracy. Furthermore, we design a framework for automatically
constructing IFT data based on medical literature and discuss some real-world
applications.
</p></li>
</ul>

<h3>Title: UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04213">http://arxiv.org/abs/2309.04213</a></li>
<li>Code URL: https://github.com/yanjiangjerry/alex</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04213]] UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media(http://arxiv.org/abs/2309.04213)</code></li>
<li>Summary: <p>As social media becomes increasingly popular, more and more activities
related to public health emerge. Current techniques for public health analysis
involve popular models such as BERT and large language models (LLMs). However,
the costs of training in-domain LLMs for public health are especially
expensive. Furthermore, such kinds of in-domain datasets from social media are
generally imbalanced. To tackle these challenges, the data imbalance issue can
be overcome by data augmentation and balanced training. Moreover, the ability
of the LLMs can be effectively utilized by prompting the model properly. In
this paper, a novel ALEX framework is proposed to improve the performance of
public health analysis on social media by adopting an LLMs explanation
mechanism. Results show that our ALEX model got the best performance among all
submissions in both Task 2 and Task 4 with a high score in Task 1 in Social
Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://
github.com/YanJiangJerry/ALEX.
</p></li>
</ul>

<h3>Title: Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04369">http://arxiv.org/abs/2309.04369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04369]] Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation(http://arxiv.org/abs/2309.04369)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have made progress in various real-world tasks,
which stimulates requirements for the evaluation of LLMs. Existing LLM
evaluation methods are mainly supervised signal-based which depends on static
datasets and cannot evaluate the ability of LLMs in dynamic real-world
scenarios where deep interaction widely exists. Other LLM evaluation methods
are human-based which are costly and time-consuming and are incapable of
large-scale evaluation of LLMs. To address the issues above, we propose a novel
Deep Interaction-based LLM-evaluation framework. In our proposed framework,
LLMs' performances in real-world domains can be evaluated from their deep
interaction with other LLMs in elaborately designed evaluation tasks.
Furthermore, our proposed framework is a general evaluation method that can be
applied to a host of real-world tasks such as machine translation and code
generation. We demonstrate the effectiveness of our proposed method through
extensive experiments on four elaborately designed evaluation tasks.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Random Expert Sampling for Deep Learning Segmentation of Acute Ischemic Stroke on Non-contrast CT. (arXiv:2309.03930v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03930">http://arxiv.org/abs/2309.03930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03930]] Random Expert Sampling for Deep Learning Segmentation of Acute Ischemic Stroke on Non-contrast CT(http://arxiv.org/abs/2309.03930)</code></li>
<li>Summary: <p>Purpose: Multi-expert deep learning training methods to automatically
quantify ischemic brain tissue on Non-Contrast CT Materials and Methods: The
data set consisted of 260 Non-Contrast CTs from 233 patients of acute ischemic
stroke patients recruited in the DEFUSE 3 trial. A benchmark U-Net was trained
on the reference annotations of three experienced neuroradiologists to segment
ischemic brain tissue using majority vote and random expert sampling training
schemes. We used a one-sided Wilcoxon signed-rank test on a set of segmentation
metrics to compare bootstrapped point estimates of the training schemes with
the inter-expert agreement and ratio of variance for consistency analysis. We
further compare volumes with the 24h-follow-up DWI (final infarct core) in the
patient subgroup with full reperfusion and we test volumes for correlation to
the clinical outcome (mRS after 30 and 90 days) with the Spearman method.
Results: Random expert sampling leads to a model that shows better agreement
with experts than experts agree among themselves and better agreement than the
agreement between experts and a majority-vote model performance (Surface Dice
at Tolerance 5mm improvement of 61% to 0.70 +- 0.03 and Dice improvement of 25%
to 0.50 +- 0.04). The model-based predicted volume similarly estimated the
final infarct volume and correlated better to the clinical outcome than CT
perfusion. Conclusion: A model trained on random expert sampling can identify
the presence and location of acute ischemic brain tissue on Non-Contrast CT
similar to CT perfusion and with better consistency than experts. This may
further secure the selection of patients eligible for endovascular treatment in
less specialized hospitals.
</p></li>
</ul>

<h3>Title: Grouping Boundary Proposals for Fast Interactive Image Segmentation. (arXiv:2309.04169v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04169">http://arxiv.org/abs/2309.04169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04169]] Grouping Boundary Proposals for Fast Interactive Image Segmentation(http://arxiv.org/abs/2309.04169)</code></li>
<li>Summary: <p>Geodesic models are known as an efficient tool for solving various image
segmentation problems. Most of existing approaches only exploit local pointwise
image features to track geodesic paths for delineating the objective
boundaries. However, such a segmentation strategy cannot take into account the
connectivity of the image edge features, increasing the risk of shortcut
problem, especially in the case of complicated scenario. In this work, we
introduce a new image segmentation model based on the minimal geodesic
framework in conjunction with an adaptive cut-based circular optimal path
computation scheme and a graph-based boundary proposals grouping scheme.
Specifically, the adaptive cut can disconnect the image domain such that the
target contours are imposed to pass through this cut only once. The boundary
proposals are comprised of precomputed image edge segments, providing the
connectivity information for our segmentation model. These boundary proposals
are then incorporated into the proposed image segmentation model, such that the
target segmentation contours are made up of a set of selected boundary
proposals and the corresponding geodesic paths linking them. Experimental
results show that the proposed model indeed outperforms state-of-the-art
minimal paths-based image segmentation approaches.
</p></li>
</ul>

<h3>Title: Have We Ever Encountered This Before? Retrieving Out-of-Distribution Road Obstacles from Driving Scenes. (arXiv:2309.04302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04302">http://arxiv.org/abs/2309.04302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04302]] Have We Ever Encountered This Before? Retrieving Out-of-Distribution Road Obstacles from Driving Scenes(http://arxiv.org/abs/2309.04302)</code></li>
<li>Summary: <p>In the life cycle of highly automated systems operating in an open and
dynamic environment, the ability to adjust to emerging challenges is crucial.
For systems integrating data-driven AI-based components, rapid responses to
deployment issues require fast access to related data for testing and
reconfiguration. In the context of automated driving, this especially applies
to road obstacles that were not included in the training data, commonly
referred to as out-of-distribution (OoD) road obstacles. Given the availability
of large uncurated recordings of driving scenes, a pragmatic approach is to
query a database to retrieve similar scenarios featuring the same safety
concerns due to OoD road obstacles. In this work, we extend beyond identifying
OoD road obstacles in video streams and offer a comprehensive approach to
extract sequences of OoD road obstacles using text queries, thereby proposing a
way of curating a collection of OoD data for subsequent analysis. Our proposed
method leverages the recent advances in OoD segmentation and multi-modal
foundation models to identify and efficiently extract safety-relevant scenes
from unlabeled videos. We present a first approach for the novel task of
text-based OoD object retrieval, which addresses the question ''Have we ever
encountered this before?''.
</p></li>
</ul>

<h3>Title: AMLP:Adaptive Masking Lesion Patches for Self-supervised Medical Image Segmentation. (arXiv:2309.04312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04312">http://arxiv.org/abs/2309.04312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04312]] AMLP:Adaptive Masking Lesion Patches for Self-supervised Medical Image Segmentation(http://arxiv.org/abs/2309.04312)</code></li>
<li>Summary: <p>Self-supervised masked image modeling has shown promising results on natural
images. However, directly applying such methods to medical images remains
challenging. This difficulty stems from the complexity and distinct
characteristics of lesions compared to natural images, which impedes effective
representation learning. Additionally, conventional high fixed masking ratios
restrict reconstructing fine lesion details, limiting the scope of learnable
information. To tackle these limitations, we propose a novel self-supervised
medical image segmentation framework, Adaptive Masking Lesion Patches (AMLP).
Specifically, we design a Masked Patch Selection (MPS) strategy to identify and
focus learning on patches containing lesions. Lesion regions are scarce yet
critical, making their precise reconstruction vital. To reduce
misclassification of lesion and background patches caused by unsupervised
clustering in MPS, we introduce an Attention Reconstruction Loss (ARL) to focus
on hard-to-reconstruct patches likely depicting lesions. We further propose a
Category Consistency Loss (CCL) to refine patch categorization based on
reconstruction difficulty, strengthening distinction between lesions and
background. Moreover, we develop an Adaptive Masking Ratio (AMR) strategy that
gradually increases the masking ratio to expand reconstructible information and
improve learning. Extensive experiments on two medical segmentation datasets
demonstrate AMLP's superior performance compared to existing self-supervised
approaches. The proposed strategies effectively address limitations in applying
masked modeling to medical images, tailored to capturing fine lesion details
vital for segmentation tasks.
</p></li>
</ul>

<h3>Title: Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving. (arXiv:2309.04422v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.04422">http://arxiv.org/abs/2309.04422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.04422]] Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving(http://arxiv.org/abs/2309.04422)</code></li>
<li>Summary: <p>Performing multiple heterogeneous visual tasks in dynamic scenes is a
hallmark of human perception capability. Despite remarkable progress in image
and video recognition via representation learning, current research still
focuses on designing specialized networks for singular, homogeneous, or simple
combination of tasks. We instead explore the construction of a unified model
for major image and video recognition tasks in autonomous driving with diverse
input and output structures. To enable such an investigation, we design a new
challenge, Video Task Decathlon (VTD), which includes ten representative image
and video tasks spanning classification, segmentation, localization, and
association of objects and pixels. On VTD, we develop our unified network,
VTDNet, that uses a single structure and a single set of weights for all ten
tasks. VTDNet groups similar tasks and employs task interaction stages to
exchange information within and between task groups. Given the impracticality
of labeling all tasks on all frames, and the performance degradation associated
with joint training of many tasks, we design a Curriculum training,
Pseudo-labeling, and Fine-tuning (CPF) scheme to successfully train VTDNet on
all tasks and mitigate performance loss. Armed with CPF, VTDNet significantly
outperforms its single-task counterparts on most tasks with only 20% overall
computations. VTD is a promising new direction for exploring the unification of
perception tasks in autonomous driving.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
