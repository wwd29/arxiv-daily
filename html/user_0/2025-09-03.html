<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-03</h1>
<h3>Title: Per-sender neural network classifiers for email authorship validation</h3>
<ul>
<li><strong>Authors: </strong>Rohit Dube</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00005">https://arxiv.org/abs/2509.00005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00005">https://arxiv.org/pdf/2509.00005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00005]] Per-sender neural network classifiers for email authorship validation(https://arxiv.org/abs/2509.00005)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Business email compromise and lateral spear phishing attacks are among modern organizations' most costly and damaging threats. While inbound phishing defenses have improved significantly, most organizations still trust internal emails by default, leaving themselves vulnerable to attacks from compromised employee accounts. In this work, we define and explore the problem of authorship validation: verifying whether a claimed sender actually authored a given email. Authorship validation is a lightweight, real-time defense that complements traditional detection methods by modeling per-sender writing style. Further, the paper presents a collection of new datasets based on the Enron corpus. These simulate inauthentic messages using both human-written and large language model-generated emails. The paper also evaluates two classifiers -- a Naive Bayes model and a character-level convolutional neural network (Char-CNN) -- for the authorship validation task. Our experiments show that the Char-CNN model achieves high accuracy and F1 scores under various circumstances. Finally, we discuss deployment considerations and show that per-sender authorship classifiers are practical for integrating into existing commercial email security systems with low overhead.</li>
</ul>

<h3>Title: Case Studies: Effective Approaches for Navigating Cross-Border Cloud Data Transfers Amid U.S. Government Privacy and Safety Concerns</h3>
<ul>
<li><strong>Authors: </strong>Motunrayo Adebayo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00006">https://arxiv.org/abs/2509.00006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00006">https://arxiv.org/pdf/2509.00006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00006]] Case Studies: Effective Approaches for Navigating Cross-Border Cloud Data Transfers Amid U.S. Government Privacy and Safety Concerns(https://arxiv.org/abs/2509.00006)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This study attempts to explain the impact of information exchange from one country to another, as well as the legal and technological implications for these exchanges. Due to the emergence of cloud technology, possibilities for free exchange of information between countries have increased rapidly, as it has become possible to save information in a country and access it in almost any part of the world. Countries all around the world have been confronted with developing frameworks to facilitate this process, although there are significant challenges which must be confronted on legal and technological fronts, as loopholes in the framework adopted by countries may hinder free access to information stored on cloud, and also compromise data privacy. Cloud technology is impacting a lot of issues, including domestic and international businesses, hence the need for a study to propose measures for safe exchange of information using cloud technology.</li>
</ul>

<h3>Title: Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?</h3>
<ul>
<li><strong>Authors: </strong>Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00026">https://arxiv.org/abs/2509.00026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00026">https://arxiv.org/pdf/2509.00026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00026]] Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?(https://arxiv.org/abs/2509.00026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mental disorders are clinically significant patterns of behavior that are associated with stress and/or impairment in social, occupational, or family activities. People suffering from such disorders are often misjudged and poorly diagnosed due to a lack of visible symptoms compared to other health complications. During emergency situations, identifying psychiatric issues is that's why challenging but highly required to save patients. In this paper, we have conducted research on how traditional machine learning and large language models (LLM) can assess these psychiatric patients based on their behavioral patterns to provide a diagnostic assessment. Data from emergency psychiatric patients were collected from a rescue station in Germany. Various machine learning models, including Llama 3.1, were used with rescue patient data to assess if the predictive capabilities of the models can serve as an efficient tool for identifying patients with unhealthy mental disorders, especially in rescue cases.</li>
</ul>

<h3>Title: Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Elie Thellier (EPIONE), Huiyu Li (EPIONE), Nicholas Ayache (EPIONE), Herv√© Delingette (EPIONE)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00027">https://arxiv.org/abs/2509.00027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00027">https://arxiv.org/pdf/2509.00027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00027]] Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning(https://arxiv.org/abs/2509.00027)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Data lakes enable the training of powerful machine learning models on sensitive, high-value medical datasets, but also introduce serious privacy risks due to potential leakage of protected health information. Recent studies show adversaries can exfiltrate training data by embedding latent representations into model parameters or inducing memorization via multi-task learning. These attacks disguise themselves as benign utility models while enabling reconstruction of high-fidelity medical images, posing severe privacy threats with legal and ethical implications. In this work, we propose a simple yet effective mitigation strategy that perturbs model parameters at export time through fine-tuning with a decaying layer-wise learning rate to corrupt embedded data without degrading task performance. Evaluations on DermaMNIST, ChestMNIST, and MIMIC-CXR show that our approach maintains utility task performance, effectively disrupts state-of-the-art exfiltration attacks, outperforms prior defenses, and renders exfiltrated data unusable for training. Ablations and discussions on adaptive attacks highlight challenges and future directions. Our findings offer a practical defense against data leakage in data lake-trained models and centralized federated learning.</li>
</ul>

<h3>Title: MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Marshall Thomas, Edward Fish, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00030">https://arxiv.org/abs/2509.00030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00030">https://arxiv.org/pdf/2509.00030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00030]] MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation(https://arxiv.org/abs/2509.00030)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite progress in gloss-free Sign Language Translation (SLT), monolithic end-to-end models consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in Automated Sign Language Translation with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names,places, and technical terms. We introduce MultiStream-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign benchmark with a BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging ChicagoFSWildPlus fingerspelling dataset. These results validate our core hypothesis: by isolating and solving distinct recogni tion tasks before fusion, our multi-expert approach provides a more powerful and effective pathway to robust, high-fidelity sign language translation.</li>
</ul>

<h3>Title: ZeroQAT: Your Quantization-aware Training but Efficient</h3>
<ul>
<li><strong>Authors: </strong>Qitao Tan, Xiaoying Song, Jin Lu, Guoming Li, Jun Liu, Lingzi Hong, Caiwen Ding, Jundong Li, Xiaoming Zhai, Shaoyi Huang, Wei Niu, Geng Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00031">https://arxiv.org/abs/2509.00031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00031">https://arxiv.org/pdf/2509.00031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00031]] ZeroQAT: Your Quantization-aware Training but Efficient(https://arxiv.org/abs/2509.00031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quantization is an effective technique to reduce the deployment cost of large language models (LLMs), and post-training quantization (PTQ) has been widely studied due to its efficiency. However, existing low-bit PTQ methods suffer from accuracy degradation because their layer-wise optimization introduces cumulative error propagation and misalignment between local reconstruction objectives and downstream performance. While quantization-aware training (QAT) provides a principled solution, its reliance on backpropagation incurs prohibitive data, time, and memory costs, limiting its practicality. To address these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT framework. ZeroQAT leverages forward-only gradient estimation to eliminate the need for backpropagation, significantly reducing computational and memory overhead while retaining the benefits of end-to-end optimization. Moreover, ZeroQAT jointly learns quantized weights, weight clipping thresholds, and equivalent transformations to mitigate quantization error and handle activation outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ while retaining the accuracy of QAT, offering a practical solution for high-quality low-bit quantization of LLMs.</li>
</ul>

<h3>Title: Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary</h3>
<ul>
<li><strong>Authors: </strong>Tahoshin Alam Ishat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00033">https://arxiv.org/abs/2509.00033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00033">https://arxiv.org/pdf/2509.00033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00033]] Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary(https://arxiv.org/abs/2509.00033)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This is a research exploring existing models and fine tuning them to combine a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to predict the recipe and generate text creating a step by step guide for the cooking procedure. All the data were gathered by the author for a robust task specific system to perform best in complex and challenging environments proving the extension and endless application of computer vision in daily activities such as kitchen work. This work extends the field for many more crucial task of our day to day life.</li>
</ul>

<h3>Title: Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications</h3>
<ul>
<li><strong>Authors: </strong>Mert Sehri, Ana Cardoso, Francisco de Assis Boldt, Patrick Dumond</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00034">https://arxiv.org/abs/2509.00034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00034">https://arxiv.org/pdf/2509.00034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00034]] Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications(https://arxiv.org/abs/2509.00034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Steel casting processes are vulnerable to financial losses due to slag flow contamination, making accurate slag flow condition detection essential. This study introduces a novel cross-domain diagnostic method using vibration data collected from an industrial steel foundry to identify various stages of slag flow. A hybrid deep learning model combining one-dimensional convolutional neural networks and long short-term memory layers is implemented, tested, and benchmarked against a standard one-dimensional convolutional neural network. The proposed method processes raw time-domain vibration signals from accelerometers and evaluates performance across 16 distinct domains using a realistic cross-domain dataset split. Results show that the hybrid convolutional neural network and long short-term memory architecture, when combined with root mean square preprocessing and a selective embedding data loading strategy, achieves robust classification accuracy, outperforming traditional models and loading techniques. The highest test accuracy of 99.10 +/- 0.30 demonstrates the method's capability for generalization and industrial relevance. This work presents a practical and scalable solution for real-time slag flow monitoring, contributing to improved reliability and operational efficiency in steel manufacturing.</li>
</ul>

<h3>Title: A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jin, Zhenyu Xiao, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00036">https://arxiv.org/abs/2509.00036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00036">https://arxiv.org/pdf/2509.00036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00036]] A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler(https://arxiv.org/abs/2509.00036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models deliver state-of-the-art generative performance across diverse modalities but remain computationally expensive due to their inherently iterative sampling process. Existing training-free acceleration methods typically improve numerical solvers for the reverse-time ODE, yet their effectiveness is fundamentally constrained by the inefficiency of the underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path Sampler), a principled, training-free framework that reparameterizes the sampling trajectory of any pre-trained diffusion model into a flow-matching form and augments it with an adaptive velocity decomposition. The reparameterization analytically maps diffusion scores to flow-compatible velocities, yielding integration-friendly trajectories without retraining. The adaptive mechanism further factorizes the velocity field into a linear drift term and a residual component whose temporal variation is actively suppressed, restoring the accuracy benefits of high-order integration even in extremely low-NFE regimes. Extensive experiments on conditional image generation and text-to-image synthesis show that A-FloPS consistently outperforms state-of-the-art training-free samplers in both sample quality and efficiency. Notably, with as few as $5$ function evaluations, A-FloPS achieves substantially lower FID and generates sharper, more coherent images. The adaptive mechanism also improves native flow-based generative models, underscoring its generality. These results position A-FloPS as a versatile and effective solution for high-quality, low-latency generative modeling.</li>
</ul>

<h3>Title: Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Teo Susnjak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00038">https://arxiv.org/abs/2509.00038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00038">https://arxiv.org/pdf/2509.00038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00038]] Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis(https://arxiv.org/abs/2509.00038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.</li>
</ul>

<h3>Title: ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization</h3>
<ul>
<li><strong>Authors: </strong>Poyraz Baydemir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00042">https://arxiv.org/abs/2509.00042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00042">https://arxiv.org/pdf/2509.00042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00042]] ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization(https://arxiv.org/abs/2509.00042)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present ARTPS (Autonomous Rover Target Prioritization System), a novel hybrid AI system that combines depth estimation, anomaly detection, and learnable curiosity scoring for autonomous exploration of planetary surfaces. Our approach integrates monocular depth estimation using Vision Transformers with multi-component anomaly detection and a weighted curiosity score that balances known value, anomaly signals, depth variance, and surface roughness. The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of 0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant improvements in target prioritization accuracy through ablation studies and provide comprehensive analysis of component contributions. The hybrid fusion approach reduces false positives by 23% while maintaining high detection sensitivity across diverse terrain types.</li>
</ul>

<h3>Title: Keystroke Detection by Exploiting Unintended RF Emission from Repaired USB Keyboards</h3>
<ul>
<li><strong>Authors: </strong>Md Faizul Bari, Yi Xie, Meghna Roy Choudhury, Shreyas Sen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00043">https://arxiv.org/abs/2509.00043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00043">https://arxiv.org/pdf/2509.00043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00043]] Keystroke Detection by Exploiting Unintended RF Emission from Repaired USB Keyboards(https://arxiv.org/abs/2509.00043)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>Electronic devices and cables inadvertently emit RF emissions as a byproduct of signal processing and/or transmission. Labeled as electromagnetic emanations, they form an EM side-channel for data leakage. Previously, it was believed that such leakage could be contained within a facility since they are weak signals with a short transmission range. However, in the preliminary version of this work [1], we found that the traditional cable repairing process forms a tiny monopole antenna that helps emanations transmit over a long range. Experimentation with three types of cables revealed that emanations from repaired cables remain detectable even at >4 m and can penetrate a 14 cm thick concrete wall. In this extended version, we show that such emanation can be exploited at a long distance for information extraction by detecting keystrokes typed on a repaired USB keyboard. By collecting data for 70 different keystrokes at different distances from the target in 3 diverse environments (open space, a corridor outside an office room, and outside a building) and developing an efficient detection algorithm, ~100% keystroke detection accuracy has been achieved up to 12 m distance, which is the highest reported accuracy at such a long range for USB keyboards in the literature. The effect of two experimental factors, interference and human-body coupling, has been investigated thoroughly. Along with exploring the vulnerability, multi-layer external metal shielding during the repairing process as a possible remedy has been explored. This work exposes a new attack surface caused by hardware modification, its exploitation, and potential countermeasures.</li>
</ul>

<h3>Title: Performance is not All You Need: Sustainability Considerations for Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Chong Zhang, Hongpeng Wang, Shreyank Narayana Gowda, Yushi Li, Xiaobo Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00045">https://arxiv.org/abs/2509.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00045">https://arxiv.org/pdf/2509.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00045]] Performance is not All You Need: Sustainability Considerations for Algorithms(https://arxiv.org/abs/2509.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This work focuses on the high carbon emissions generated by deep learning model training, specifically addressing the core challenge of balancing algorithm performance and energy consumption. It proposes an innovative two-dimensional sustainability evaluation system. Different from the traditional single performance-oriented evaluation paradigm, this study pioneered two quantitative indicators that integrate energy efficiency ratio and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy consumption and performance parameters through the harmonic mean to reveal the algorithm performance under unit energy consumption; the area under the sustainability curve (ASC) constructs a performance-power consumption curve to characterize the energy efficiency characteristics of the algorithm throughout the cycle. To verify the universality of the indicator system, the study constructed benchmarks in various multimodal tasks, including image classification, segmentation, pose estimation, and batch and online learning. Experiments demonstrate that the system can provide a quantitative basis for evaluating cross-task algorithms and promote the transition of green AI research from theory to practice. Our sustainability evaluation framework code can be found here, providing methodological support for the industry to establish algorithm energy efficiency standards.</li>
</ul>

<h3>Title: Exploring and Reshaping the Weight Distribution in LLM</h3>
<ul>
<li><strong>Authors: </strong>Chunming Ye, Songzhou Li, Xu Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00046">https://arxiv.org/abs/2509.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00046">https://arxiv.org/pdf/2509.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00046]] Exploring and Reshaping the Weight Distribution in LLM(https://arxiv.org/abs/2509.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models is influenced by their characteristics such as architecture, model sizes, decoding methods and so on. Due to differences in structure or function, the weights in different layers of large models have varying distributions. This paper explores the correlations between different types of layers in terms of weights distribution and studies the potential impact of these correlations on LoRA training effectiveness. Firstly, the study reveals that in the model the cosine distances between weights of different layers manifest power-law distribution. We extract Query-projection, down-projection and other weight matrices from the self-attention layers and MLP layers, calculate the singular values of the matrices using singular value decomposition, and organize a certain number of singular values into matrices according to projection's type. By analyzing the probability distribution of the cosine distances between these matrices, it is found that the cosine distances values between them have distinct power-law distribution characteristics. Secondly, based on the results of distance calculations and analysis across different layers of model, a qualitative method is proposed to describe the distribution characteristics of different models. Next, to construct weights that align with the distribution characteristics, a data generator is designed using a combination of Gaussian process and Pareto distribution functions. The generator is used to simulate the generation of data that aligns with specific distribution characteristics. Finally, based on the aforementioned distribution characteristics and data generation method, the weights in LoRA initialization are reshaped for training. Experimental results indicate that, without altering the model structure or training process, this method achieves a certain improvement in the performance of LoRA training.</li>
</ul>

<h3>Title: Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Nooraiepour, Mohammad Masoudi, Zezhang Song, Helge Hellevang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00049">https://arxiv.org/abs/2509.00049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00049">https://arxiv.org/pdf/2509.00049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00049]] Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals(https://arxiv.org/abs/2509.00049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Accurate prediction of hydrogen sorption in clays, shales, and coals is vital for advancing underground hydrogen storage, natural hydrogen exploration, and radioactive waste containment. Traditional experimental methods, while foundational, are time-consuming, error-prone, and limited in capturing geological heterogeneity. This study introduces an adaptive physics-informed neural network (PINN) framework with multi-category feature engineering to enhance hydrogen sorption prediction. The framework integrates classical isotherm models with thermodynamic constraints to ensure physical consistency while leveraging deep learning flexibility. A comprehensive dataset consisting of 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed, incorporating diverse compositional properties and experimental conditions. Multi-category feature engineering across seven categories captured complex sorption dynamics. The PINN employs deep residual networks with multi-head attention, optimized via adaptive loss functions and Monte Carlo dropout for uncertainty quantification. K-fold cross-validation and hyperparameter optimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg) with 67% faster convergence despite 15-fold increased complexity. The framework demonstrates robust lithology-specific performance across clay minerals (R2 = 0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91% reliability scores. Interpretability analysis via SHAP, accumulated local effects, and Friedman's H-statistics reveal that hydrogen adsorption capacity dominates predictions, while 86.7% of feature pairs exhibit strong interactions, validating the necessity of non-linear modeling approaches. This adaptive physics-informed framework accelerates site screening and enables risk-informed decision-making through robust uncertainty quantification.</li>
</ul>

<h3>Title: Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity</h3>
<ul>
<li><strong>Authors: </strong>David Kurtenbach, Megan Manly, Zach Metzinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00050">https://arxiv.org/abs/2509.00050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00050">https://arxiv.org/pdf/2509.00050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00050]] Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity(https://arxiv.org/abs/2509.00050)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>We apply deep learning techniques for anomaly detection to analyze activity of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and assess the results for any findings that can be used as indications and warnings (I&W) of aggressive military behavior for future conflicts. Through analysis of anomalous activity, an understanding of possible tactics and procedures can be established to assess the existence of statistically significant changes in Russian RSO pattern of life/pattern of behavior (PoL/PoB) using publicly available two-line element (TLE) data. This research looks at statistical and deep learning approaches to assess anomalous activity. The deep learning methods assessed are isolation forest (IF), traditional autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network (KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is used to establish a baseline of on-orbit activity based on a five-year data sample. The primary investigation period focuses on the six months leading up to the invasion date of February 24, 2022. Additional analysis looks at RSO activity during an active combat period by sampling TLE data after the invasion date. The deep learning autoencoder models identify anomalies based on reconstruction errors that surpass a threshold sigma. To capture the nuance and unique characteristics of each RSO an individual model was trained for each observed space object. The research made an effort to prioritize explainability and interpretability of the model results thus each observation was assessed for anomalous behavior of the individual six orbital elements versus analyzing the input data as a single monolithic observation. The results demonstrate not only statistically significant anomalies of Russian RSO activity but also details anomalous findings to the individual orbital element.</li>
</ul>

<h3>Title: MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Luu Tu Nguyen, Vu Tram Anh Khuong, Thanh Ha Le, Thi Duyen Ngo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00056">https://arxiv.org/abs/2509.00056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00056">https://arxiv.org/pdf/2509.00056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00056]] MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition(https://arxiv.org/abs/2509.00056)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Micro-expression recognition (MER) is a challenging task due to the subtle and fleeting nature of micro-expressions. Traditional input modalities, such as Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture these brief facial movements, resulting in suboptimal performance. In this study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel dynamic input modality that transforms a video sequence into a single image while preserving the essential characteristics of micro-movements. Additionally, we present the Micro-expression Gradient Attention Network (MEGANet), which incorporates a novel Gradient Attention block to enhance the extraction of fine-grained motion features from micro-expressions. By combining MESTI and MEGANet, we aim to establish a more effective approach to MER. Extensive experiments were conducted to evaluate the effectiveness of MESTI, comparing it with existing input modalities across three CNN architectures (VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing the input of previously published MER networks with MESTI leads to consistent performance improvements. The performance of MEGANet, both with MESTI and Dynamic Image, is also evaluated, showing that our proposed network achieves state-of-the-art results on the CASMEII and SAMM datasets. The combination of MEGANet and MESTI achieves the highest accuracy reported to date, setting a new benchmark for micro-expression recognition. These findings underscore the potential of MESTI as a superior input modality and MEGANet as an advanced recognition network, paving the way for more effective MER systems in a variety of applications.</li>
</ul>

<h3>Title: From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yousuf Moiz Ali, Jaroslaw E. Prilepsky, Nicola Sambo, Joao Pedro, Mohammad M. Hosseini, Antonio Napoli, Sergei K. Turitsyn, Pedro Freire</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00057">https://arxiv.org/abs/2509.00057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00057">https://arxiv.org/pdf/2509.00057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00057]] From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis(https://arxiv.org/abs/2509.00057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning-based failure management in optical networks has gained significant attention in recent years. However, severe class imbalance, where normal instances vastly outnumber failure cases, remains a considerable challenge. While pre- and in-processing techniques have been widely studied, post-processing methods are largely unexplored. In this work, we present a direct comparison of pre-, in-, and post-processing approaches for class imbalance mitigation in failure detection and identification using an experimental dataset. For failure detection, post-processing methods-particularly Threshold Adjustment-achieve the highest F1 score improvement (up to 15.3%), while Random Under-Sampling provides the fastest inference. In failure identification, GenAI methods deliver the most substantial performance gains (up to 24.2%), whereas post-processing shows limited impact in multi-class settings. When class overlap is present and latency is critical, over-sampling methods such as the SMOTE are most effective; without latency constraints, Meta-Learning yields the best results. In low-overlap scenarios, Generative AI approaches provide the highest performance with minimal inference time.</li>
</ul>

<h3>Title: Cryptographic Challenges: Masking Sensitive Data in Cyber Crimes through ASCII Art</h3>
<ul>
<li><strong>Authors: </strong>Andres Alejandre, Kassandra Delfin, Victor Castano</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00059">https://arxiv.org/abs/2509.00059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00059">https://arxiv.org/pdf/2509.00059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00059]] Cryptographic Challenges: Masking Sensitive Data in Cyber Crimes through ASCII Art(https://arxiv.org/abs/2509.00059)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>The use of ASCII art as a novel approach to masking sensitive information in cybercrime, focusing on its potential role in protecting personal data during the delivery process and beyond, is presented. By examining the unique properties of ASCII art and its historical context, this study discusses the advantages and limitations of employing this technique in various cybercrime scenarios. Additionally, providing recommendations for enhancing data security practices and fostering a culture of privacy awareness in both businesses and individuals. The findings suggest that ASCII art, with its simplicity and ambiguity, can serve as an effective tool against cybercriminals, emphasizing the need for robust data security measures and increased privacy awareness in today's interconnected world.</li>
</ul>

<h3>Title: Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Justin Jung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00062">https://arxiv.org/abs/2509.00062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00062">https://arxiv.org/pdf/2509.00062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00062]] Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion(https://arxiv.org/abs/2509.00062)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic sparse multi-category 3D voxel structures is difficult due to the cubic memory scaling of voxel structures and moreover the significant class imbalance caused by sparsity. We introduce Scaffold Diffusion, a generative model designed for sparse multi-category 3D voxel structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete diffusion language model to generate 3D voxel structures. We show that discrete diffusion language models can be extended beyond inherently sequential domains such as text to generate spatially coherent 3D structures. We evaluate on Minecraft house structures from the 3D-Craft dataset and demonstrate that, unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion produces realistic and coherent structures even when trained on data with over 98% sparsity. We provide an interactive viewer where readers can visualize generated samples and the generation process. Our results highlight discrete diffusion as a promising framework for 3D sparse voxel generative modeling.</li>
</ul>

<h3>Title: AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum</h3>
<ul>
<li><strong>Authors: </strong>Prasasthy Balasubramanian, Dumindu Kankanamge, Ekaterina Gilman, Mourad Oussalah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00069">https://arxiv.org/abs/2509.00069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00069">https://arxiv.org/pdf/2509.00069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00069]] AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum(https://arxiv.org/abs/2509.00069)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Conversational AI and Large Language Models (LLMs) have become powerful tools across domains, including cybersecurity, where they help detect threats early and improve response times. However, challenges such as false positives and complex model management still limit trust. Although Explainable AI (XAI) aims to make AI decisions more transparent, many security analysts remain uncertain about its usefulness. This study presents a framework that detects anomalies and provides high-quality explanations through visual tools BERTViz and Captum, combined with natural language reports based on attention outputs. This reduces manual effort and speeds up remediation. Our comparative analysis showed that RoBERTa offers high accuracy (99.6 %) and strong anomaly detection, outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback confirms the chatbot's ease of use and improved understanding of anomalies, demonstrating the ability of the developed framework to strengthen cybersecurity workflows.</li>
</ul>

<h3>Title: SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits</h3>
<ul>
<li><strong>Authors: </strong>Shang Liu, Jing Wang, Wenji Fang, Zhiyao Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00071">https://arxiv.org/abs/2509.00071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00071">https://arxiv.org/pdf/2509.00071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00071]] SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits(https://arxiv.org/abs/2509.00071)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, AI-assisted IC design methods have demonstrated great potential, but the availability of circuit design data is extremely limited, especially in the public domain. The lack of circuit data has become the primary bottleneck in developing AI-assisted IC design methods. In this work, we make the first attempt, SynCircuit, to generate new synthetic circuits with valid functionalities in the HDL format. SynCircuit automatically generates synthetic data using a framework with three innovative steps: 1) We propose a customized diffusion-based generative model to resolve the Directed Cyclic Graph (DCG) generation task, which has not been well explored in the AI community. 2) To ensure our circuit is valid, we enforce the circuit constraints by refining the initial graph generation outputs. 3) The Monte Carlo tree search (MCTS) method further optimizes the logic redundancy in the generated graph. Experimental results demonstrate that our proposed SynCircuit can generate more realistic synthetic circuits and enhance ML model performance in downstream circuit design tasks.</li>
</ul>

<h3>Title: Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00073">https://arxiv.org/abs/2509.00073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00073">https://arxiv.org/pdf/2509.00073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00073]] Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis(https://arxiv.org/abs/2509.00073)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), offer powerful capabilities for interpreting the complex data landscape in healthcare. In this paper, we present a comprehensive overview of the capabilities, requirements and applications of GenAI for deriving clinical insights and improving clinical efficiency. We first provide some background on the forms and sources of patient data, namely real-time Remote Patient Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The sheer volume and heterogeneity of this combined data present significant challenges to clinicians and contribute to information overload. In addition, we explore the potential of LLM-powered applications for improving clinical efficiency. These applications can enhance navigation of longitudinal patient data and provide actionable clinical decision support through natural language dialogue. We discuss the opportunities this presents for streamlining clinician workflows and personalizing care, alongside critical challenges such as data integration complexity, ensuring data quality and RPM data reliability, maintaining patient privacy, validating AI outputs for clinical safety, mitigating bias, and ensuring clinical acceptance. We believe this work represents the first summarization of GenAI techniques for managing clinician data overload due to combined RPM / EHR data complexities.</li>
</ul>

<h3>Title: Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor</h3>
<ul>
<li><strong>Authors: </strong>Zachery Dahm, Konstantinos Vasili, Vasileios Theos, Konstantinos Gkouliaras, William Richards, True Miller, Brian Jowers, Stylianos Chatzidakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00076">https://arxiv.org/abs/2509.00076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00076">https://arxiv.org/pdf/2509.00076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00076]] Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor(https://arxiv.org/abs/2509.00076)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>There is increased interest in applying Artificial Intelligence and Machine Learning (AI/ML) within the nuclear industry and nuclear engineering community. Effective implementation of AI/ML could offer benefits to the nuclear domain, including enhanced identification of anomalies, anticipation of system failures, and operational schedule optimization. However, limited work has been done to investigate the feasibility and applicability of AI/ML tools in a functioning nuclear reactor. Here, we go beyond the development of a single model and introduce a multi-layered AI/ML architecture that integrates both information technology and operational technology data streams to identify, characterize, and differentiate (i) among diverse cybersecurity events and (ii) between cyber events and other operational anomalies. Leveraging Purdue Universitys research reactor, PUR-1, we demonstrate this architecture through a representative use case that includes multiple concurrent false data injections and denial-of-service attacks of increasing complexity under realistic reactor conditions. The use case includes 14 system states (1 normal, 13 abnormal) and over 13.8 million multi-variate operational and information technology data points. The study demonstrated the capability of AI/ML to distinguish between normal, abnormal, and cybersecurity-related events, even under challenging conditions such as denial-of-service attacks. Combining operational and information technology data improved classification accuracy but posed challenges related to synchronization and collection during certain cyber events. While results indicate significant promise for AI/ML in nuclear cybersecurity, the findings also highlight the need for further refinement in handling complex event differentiation and multi-class architectures.</li>
</ul>

<h3>Title: Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies</h3>
<ul>
<li><strong>Authors: </strong>Luca Cotti, Anisa Rula, Devis Bianchini, Federico Cerutti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00081">https://arxiv.org/abs/2509.00081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00081">https://arxiv.org/pdf/2509.00081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00081]] Enabling Transparent Cyber Threat Intelligence Combining Large Language Models and Domain Ontologies(https://arxiv.org/abs/2509.00081)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Effective Cyber Threat Intelligence (CTI) relies upon accurately structured and semantically enriched information extracted from cybersecurity system logs. However, current methodologies often struggle to identify and interpret malicious events reliably and transparently, particularly in cases involving unstructured or ambiguous log entries. In this work, we propose a novel methodology that combines ontology-driven structured outputs with Large Language Models (LLMs), to build an Artificial Intelligence (AI) agent that improves the accuracy and explainability of information extraction from cybersecurity logs. Central to our approach is the integration of domain ontologies and SHACL-based constraints to guide the language model's output structure and enforce semantic validity over the resulting graph. Extracted information is organized into an ontology-enriched graph database, enabling future semantic analysis and querying. The design of our methodology is motivated by the analytical requirements associated with honeypot log data, which typically comprises predominantly malicious activity. While our case study illustrates the relevance of this scenario, the experimental evaluation is conducted using publicly available datasets. Results demonstrate that our method achieves higher accuracy in information extraction compared to traditional prompt-only approaches, with a deliberate focus on extraction quality rather than processing speed.</li>
</ul>

<h3>Title: Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Laksh Patel, Neel Shanbhag</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00083">https://arxiv.org/abs/2509.00083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00083">https://arxiv.org/pdf/2509.00083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00083]] Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models(https://arxiv.org/abs/2509.00083)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Modern generative models risk overfitting and unintentionally memorizing rare training examples, which can be extracted by adversaries or inflate benchmark performance. We propose Generative Data Cartography (GenDataCarto), a data-centric framework that assigns each pretraining sample a difficulty score (early-epoch loss) and a memorization score (frequency of ``forget events''), then partitions examples into four quadrants to guide targeted pruning and up-/down-weighting. We prove that our memorization score lower-bounds classical influence under smoothness assumptions and that down-weighting high-memorization hotspots provably decreases the generalization gap via uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary extraction success by over 40\% at just 10\% data pruning, while increasing validation perplexity by less than 0.5\%. These results demonstrate that principled data interventions can dramatically mitigate leakage with minimal cost to generative performance.</li>
</ul>

<h3>Title: Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Qibin Wang, Pu Zhao, Shaohan Huang, Fangkai Yang, Lu Wang, Furu Wei, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00084">https://arxiv.org/abs/2509.00084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00084">https://arxiv.org/pdf/2509.00084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00084]] Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs(https://arxiv.org/abs/2509.00084)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>To further enhance the ability of Large Language Models (LLMs) to solve complex, multi-step reasoning problems, test-time scaling (TTS) methods have gained widespread attention. Existing approaches such as Best-of-N and majority voting are limited as their performance depends on the quality of candidate responses, making them unable to produce a correct solution when all candidates are incorrect. Introducing an additional model to select the best response also incurs significant deployment costs. To this end, we introduce Generative Self-Refinement (GSR), a novel parallel test-time scaling framework where a unified model first generates a set of candidate responses in parallel and then performs self-refinement to synthesize a new superior solution based on a prompt consisting of the problem and these candidates. However, LLMs struggle to perform refinement effectively when prompted directly. Therefore, we design a hybrid training pipeline by jointly optimizing for two complementary objectives, solving problems directly and refining candidate responses. Experimental results demonstrate that our method achieves state-of-the-art performance across five mathematical benchmarks. We further show that this learned self-refinement skill is a model-agnostic enhancement, robust across different model scales and generalizing to out-of-distribution reasoning tasks.</li>
</ul>

<h3>Title: Private, Verifiable, and Auditable AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Tobin South</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00085">https://arxiv.org/abs/2509.00085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00085">https://arxiv.org/pdf/2509.00085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00085]] Private, Verifiable, and Auditable AI Systems(https://arxiv.org/abs/2509.00085)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>The growing societal reliance on artificial intelligence necessitates robust frameworks for ensuring its security, accountability, and trustworthiness. This thesis addresses the complex interplay between privacy, verifiability, and auditability in modern AI, particularly in foundation models. It argues that technical solutions that integrate these elements are critical for responsible AI innovation. Drawing from international policy contributions and technical research to identify key risks in the AI pipeline, this work introduces novel technical solutions for critical privacy and verifiability challenges. Specifically, the research introduces techniques for enabling verifiable and auditable claims about AI systems using zero-knowledge cryptography; utilizing secure multi-party computation and trusted execution environments for auditable, confidential deployment of large language models and information retrieval; and implementing enhanced delegation mechanisms, credentialing systems, and access controls to secure interactions with autonomous and multi-agent AI systems. Synthesizing these technical advancements, this dissertation presents a cohesive perspective on balancing privacy, verifiability, and auditability in foundation model-based AI systems, offering practical blueprints for system designers and informing policy discussions on AI safety and governance.</li>
</ul>

<h3>Title: Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Tertulino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00086">https://arxiv.org/abs/2509.00086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00086">https://arxiv.org/pdf/2509.00086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00086]] Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata(https://arxiv.org/abs/2509.00086)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>The application of data mining and artificial intelligence in education offers unprecedented potential for personalizing learning and early identification of at-risk students. However, the practical use of these techniques faces a significant barrier in privacy legislation, such as Brazil's General Data Protection Law (LGPD), which restricts the centralization of sensitive student data. To resolve this challenge, privacy-preserving computational approaches are required. The present study evaluates the feasibility and effectiveness of Federated Learning, specifically the FedProx algorithm, to predict student performance using microdata from the Brazilian Basic Education Assessment System (SAEB). A Deep Neural Network (DNN) model was trained in a federated manner, simulating a scenario with 50 schools, and its performance was rigorously benchmarked against a centralized eXtreme Gradient Boosting (XGBoost) model. The analysis, conducted on a universe of over two million student records, revealed that the centralized model achieved an accuracy of 63.96%. Remarkably, the federated model reached a peak accuracy of 61.23%, demonstrating a marginal performance loss in exchange for a robust privacy guarantee. The results indicate that Federated Learning is a viable and effective solution for building collaborative predictive models in the Brazilian educational context, in alignment with the requirements of the LGPD.</li>
</ul>

<h3>Title: AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema</h3>
<ul>
<li><strong>Authors: </strong>Ting-Chun Liu, Ching-Yu Hsu, Kuan-Yi Lee, Chi-An Fu, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00088">https://arxiv.org/abs/2509.00088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00088">https://arxiv.org/pdf/2509.00088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00088]] AEGIS : Automated Co-Evolutionary Framework for Guarding Prompt Injections Schema(https://arxiv.org/abs/2509.00088)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt injection attacks pose a significant challenge to the safe deployment of Large Language Models (LLMs) in real-world applications. While prompt-based detection offers a lightweight and interpretable defense strategy, its effectiveness has been hindered by the need for manual prompt engineering. To address this issue, we propose AEGIS , an Automated co-Evolutionary framework for Guarding prompt Injections Schema. Both attack and defense prompts are iteratively optimized against each other using a gradient-like natural language prompt optimization technique. This framework enables both attackers and defenders to autonomously evolve via a Textual Gradient Optimization (TGO) module, leveraging feedback from an LLM-guided evaluation loop. We evaluate our system on a real-world assignment grading dataset of prompt injection attacks and demonstrate that our method consistently outperforms existing baselines, achieving superior robustness in both attack success and detection. Specifically, the attack success rate (ASR) reaches 1.0, representing an improvement of 0.26 over the baseline. For detection, the true positive rate (TPR) improves by 0.23 compared to the previous best work, reaching 0.84, and the true negative rate (TNR) remains comparable at 0.89. Ablation studies confirm the importance of co-evolution, gradient buffering, and multi-objective optimization. We also confirm that this framework is effective in different LLMs. Our results highlight the promise of adversarial training as a scalable and effective approach for guarding prompt injections.</li>
</ul>

<h3>Title: Learning from Peers: Collaborative Ensemble Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Li Dengjin, Guo Yanming, Xie Yuxiang, Li Zheng, Chen Jiangming, Li Xiaolong, Lao Mingrui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00089">https://arxiv.org/abs/2509.00089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00089">https://arxiv.org/pdf/2509.00089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00089]] Learning from Peers: Collaborative Ensemble Adversarial Training(https://arxiv.org/abs/2509.00089)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Ensemble Adversarial Training (EAT) attempts to enhance the robustness of models against adversarial attacks by leveraging multiple models. However, current EAT strategies tend to train the sub-models independently, ignoring the cooperative benefits between sub-models. Through detailed inspections of the process of EAT, we find that that samples with classification disparities between sub-models are close to the decision boundary of ensemble, exerting greater influence on the robustness of ensemble. To this end, we propose a novel yet efficient Collaborative Ensemble Adversarial Training (CEAT), to highlight the cooperative learning among sub-models in the ensemble. To be specific, samples with larger predictive disparities between the sub-models will receive greater attention during the adversarial training of the other sub-models. CEAT leverages the probability disparities to adaptively assign weights to different samples, by incorporating a calibrating distance regularization. Extensive experiments on widely-adopted datasets show that our proposed method achieves the state-of-the-art performance over competitive EAT methods. It is noteworthy that CEAT is model-agnostic, which can be seamlessly adapted into various ensemble methods with flexible applicability.</li>
</ul>

<h3>Title: Robust Detection of Synthetic Tabular Data under Schema Variability</h3>
<ul>
<li><strong>Authors: </strong>G. Charbel N. Kindji (MALT), Elisa Fromont (MALT), Lina Maria Rojas-Barahona, Tanguy Urvoy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00092">https://arxiv.org/abs/2509.00092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00092">https://arxiv.org/pdf/2509.00092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00092]] Robust Detection of Synthetic Tabular Data under Schema Variability(https://arxiv.org/abs/2509.00092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>The rise of powerful generative models has sparked concerns over data authenticity. While detection methods have been extensively developed for images and text, the case of tabular data, despite its ubiquity, has been largely overlooked. Yet, detecting synthetic tabular data is especially challenging due to its heterogeneous structure and unseen formats at test time. We address the underexplored task of detecting synthetic tabular data in the wild, where tables have variable and previously unseen schemas. We introduce a novel datum-wise transformer architecture that significantly outperforms the only previously published baseline, improving both AUC and accuracy by 7 points. By incorporating a table-adaptation component, our model gains an additional 7 accuracy points, demonstrating enhanced robustness. This work provides the first strong evidence that detecting synthetic tabular data in real-world conditions is not only feasible, but can be done with high reliability.</li>
</ul>

<h3>Title: Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yao Fu, Runchao Li, Xianxuan Long, Haotian Yu, Xiaotian Han, Yu Yin, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00096">https://arxiv.org/abs/2509.00096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00096">https://arxiv.org/pdf/2509.00096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00096]] Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs(https://arxiv.org/abs/2509.00096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural network pruning has emerged as a promising approach for deploying LLMs in low-resource scenarios while preserving downstream task performance. However, for the first time, we reveal that such pruning disrupts LLMs' internal activation features crucial for lie detection, where probing classifiers (typically small logistic regression models) trained on these features assess the truthfulness of LLM-generated statements. This discovery raises a crucial open question: how can we prune LLMs without sacrificing these critical lie detection capabilities? Our investigation further reveals that naively adjusting layer-wise pruning sparsity based on importance inadvertently removes crucial weights, failing to improve lie detection performance despite its reliance on the most crucial LLM layer. To address this issue, we propose Truthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater emphasis on layers with more activation outliers and stronger discriminative features simultaneously. This preserves LLMs' original performance while retaining critical features of inner states needed for robust lie detection. Moreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for better calibrating LLM pruning. Empirical results show that our approach improves the hallucination detection for pruned LLMs (achieving 88% accuracy at 50% sparsity) and enhances their performance on TruthfulQA.</li>
</ul>

<h3>Title: LLM-QUBO: An End-to-End Framework for Automated QUBO Transformation from Natural Language Problem Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Huixiang Zhang, Mahzabeen Emu, Salimur Choudhury</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00099">https://arxiv.org/abs/2509.00099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00099">https://arxiv.org/pdf/2509.00099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00099]] LLM-QUBO: An End-to-End Framework for Automated QUBO Transformation from Natural Language Problem Descriptions(https://arxiv.org/abs/2509.00099)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Quantum annealing offers a promising paradigm for solving NP-hard combinatorial optimization problems, but its practical application is severely hindered by two challenges: the complex, manual process of translating problem descriptions into the requisite Quadratic Unconstrained Binary Optimization (QUBO) format and the scalability limitations of current quantum hardware. To address these obstacles, we propose a novel end-to-end framework, LLM-QUBO, that automates this entire formulation-to-solution pipeline. Our system leverages a Large Language Model (LLM) to parse natural language, automatically generating a structured mathematical representation. To overcome hardware limitations, we integrate a hybrid quantum-classical Benders' decomposition method. This approach partitions the problem, compiling the combinatorial complex master problem into a compact QUBO format, while delegating linearly structured sub-problems to classical solvers. The correctness of the generated QUBO and the scalability of the hybrid approach are validated using classical solvers, establishing a robust performance baseline and demonstrating the framework's readiness for quantum hardware. Our primary contribution is a synergistic computing paradigm that bridges classical AI and quantum computing, addressing key challenges in the practical application of optimization problem. This automated workflow significantly reduces the barrier to entry, providing a viable pathway to transform quantum devices into accessible accelerators for large-scale, real-world optimization challenges.</li>
</ul>

<h3>Title: Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Phu X. Nguyen, Huy Phan, Hieu Pham, Christos Chatzichristos, Bert Vandenberk, Maarten De Vos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00102">https://arxiv.org/abs/2509.00102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00102">https://arxiv.org/pdf/2509.00102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00102]] Exploiting a Mixture-of-Layers in an Electrocardiography Foundation Model(https://arxiv.org/abs/2509.00102)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based foundation models for Electrocardiograms (ECGs) have recently achieved impressive performance in many downstream applications. However, the internal representations of such models across layers have not been fully understood and exploited. An important question arises: Does the final layer of the pre-trained Transformer model, the \emph{de facto} representational layer, provide optimal performance for downstream tasks? Although our answer based on empirical and theoretical analyses for this question is negative, we propose a novel approach to leverage the representation diversity of the model's layers effectively. Specifically, we introduce a novel architecture called Post-pretraining Mixture-of-layers Aggregation (PMA), which enables a flexible combination of the layer-wise representations from the layer stack of a Transformer-based foundation model. We first pre-train the model from ECG signals using the 1-dimensional Vision Transformer (ViT) via masked modeling. In downstream applications, instead of relying solely on the last layer of the model, we employ a gating network to selectively fuse the representations from the pretrained model's layers, thereby enhancing representation power and improving performance of the downstream applications. In addition, we extend the proposed method to the pretraining stage by aggregating all representations through group-wise averaging before feeding them into the decoder-based Transformer.</li>
</ul>

<h3>Title: Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers</h3>
<ul>
<li><strong>Authors: </strong>Robert MacKnight, Jose Emilio Regio, Jeffrey G. Ethier, Luke A. Baldwin, Gabe Gomes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00103">https://arxiv.org/abs/2509.00103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00103">https://arxiv.org/pdf/2509.00103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00103]] Pre-trained knowledge elevates large language models beyond traditional chemical reaction optimizers(https://arxiv.org/abs/2509.00103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern optimization in experimental chemistry employs algorithmic search through black-box parameter spaces. Here we demonstrate that pre-trained knowledge in large language models (LLMs) fundamentally changes this paradigm. Using six fully enumerated categorical reaction datasets (768 - 5,684 experiments), we benchmark LLM-guided optimization (LLM-GO) against Bayesian optimization (BO) and random sampling. Frontier LLMs consistently match or exceed BO performance across five single-objective datasets, with advantages growing as parameter complexity increases and high-performing conditions become scarce (<5% of space). BO retains superiority only for explicit multi-objective trade-offs. To understand these contrasting behaviors, we introduce a topology-agnostic information theory framework quantifying sampling diversity throughout optimization campaigns. This analysis reveals that LLMs maintain systematically higher exploration entropy than BO across all datasets while achieving superior performance, with advantages most pronounced in solution-scarce parameter spaces where high-entropy exploration typically fails - suggesting that pre-trained domain knowledge enables more effective navigation of chemical parameter space rather than replacing structured exploration strategies. To enable transparent benchmarking and community validation, we release Iron Mind (this https URL), a no-code platform for side-by-side evaluation of human, algorithmic, and LLM optimization campaigns with public leaderboards and complete trajectories. Our findings establish that LLM-GO excels precisely where traditional methods struggle: complex categorical spaces requiring domain understanding rather than mathematical optimization.</li>
</ul>

<h3>Title: Enhanced R√©nyi Entropy-Based Post-Quantum Key Agreement with Provable Security and Information-Theoretic Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Ruopengyu Xu, Chenglian Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00104">https://arxiv.org/abs/2509.00104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00104">https://arxiv.org/pdf/2509.00104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00104]] Enhanced R√©nyi Entropy-Based Post-Quantum Key Agreement with Provable Security and Information-Theoretic Guarantees(https://arxiv.org/abs/2509.00104)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>This paper presents an enhanced post-quantum key agreement protocol based on R√©nyi entropy, addressing vulnerabilities in the original construction while preserving information-theoretic security properties. We develop a theoretical framework leveraging entropy-preserving operations and secret-shared verification to achieve provable security against quantum adversaries. Through entropy amplification techniques and quantum-resistant commitments, the protocol establishes $2^{128}$ quantum security guarantees under the quantum random oracle model. Key innovations include a confidentiality-preserving verification mechanism using distributed polynomial commitments, tightened min-entropy bounds with guaranteed non-negativity, and composable security proofs in the quantum universal composability framework. Unlike computational approaches, our method provides information-theoretic security without hardness assumptions while maintaining polynomial complexity. Theoretical analysis demonstrates resilience against known quantum attack vectors, including Grover-accelerated brute force and quantum memory attacks. The protocol achieves parameterization for 128-bit quantum security with efficient $\mathcal{O}(n^2)$ communication complexity. Extensions to secure multiparty computation and quantum network applications are established, providing a foundation for long-term cryptographic security. All security claims are derived from mathematical proofs; this theoretical work presents no experimental validation.</li>
</ul>

<h3>Title: Dual-Stage Global and Local Feature Framework for Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Anas M. Ali, Anis Koubaa, Bilel Benjdira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00108">https://arxiv.org/abs/2509.00108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00108">https://arxiv.org/pdf/2509.00108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00108]] Dual-Stage Global and Local Feature Framework for Image Dehazing(https://arxiv.org/abs/2509.00108)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Addressing the challenge of removing atmospheric fog or haze from digital images, known as image dehazing, has recently gained significant traction in the computer vision community. Although contemporary dehazing models have demonstrated promising performance, few have thoroughly investigated high-resolution imagery. In such scenarios, practitioners often resort to downsampling the input image or processing it in smaller patches, which leads to a notable performance degradation. This drop is primarily linked to the difficulty of effectively combining global contextual information with localized, fine-grained details as the spatial resolution grows. In this chapter, we propose a novel framework, termed the Streamlined Global and Local Features Combinator (SGLC), to bridge this gap and enable robust dehazing for high-resolution inputs. Our approach is composed of two principal components: the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The GFG produces an initial dehazed output by focusing on broad contextual understanding of the scene. Subsequently, the LFE refines this preliminary output by enhancing localized details and pixel-level features, thereby capturing the interplay between global appearance and local structure. To evaluate the effectiveness of SGLC, we integrated it with the Uformer architecture, a state-of-the-art dehazing model. Experimental results on high-resolution datasets reveal a considerable improvement in peak signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in addressing haze in large-scale imagery. Moreover, the SGLC design is model-agnostic, allowing any dehazing network to be augmented with the proposed global-and-local feature fusion mechanism. Through this strategy, practitioners can harness both scene-level cues and granular details, significantly improving visual fidelity in high-resolution environments.</li>
</ul>

<h3>Title: A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See</h3>
<ul>
<li><strong>Authors: </strong>Shaked Zychlinski</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00124">https://arxiv.org/abs/2509.00124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00124">https://arxiv.org/pdf/2509.00124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00124]] A Whole New World: Creating a Parallel-Poisoned Web Only AI-Agents Can See(https://arxiv.org/abs/2509.00124)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel attack vector that leverages website cloaking techniques to compromise autonomous web-browsing agents powered by Large Language Models (LLMs). As these agents become more prevalent, their unique and often homogenous digital fingerprints - comprising browser attributes, automation framework signatures, and network characteristics - create a new, distinguishable class of web traffic. The attack exploits this fingerprintability. A malicious website can identify an incoming request as originating from an AI agent and dynamically serve a different, "cloaked" version of its content. While human users see a benign webpage, the agent is presented with a visually identical page embedded with hidden, malicious instructions, such as indirect prompt injections. This mechanism allows adversaries to hijack agent behavior, leading to data exfiltration, malware execution, or misinformation propagation, all while remaining completely invisible to human users and conventional security crawlers. This work formalizes the threat model, details the mechanics of agent fingerprinting and cloaking, and discusses the profound security implications for the future of agentic AI, highlighting the urgent need for robust defenses against this stealthy and scalable attack.</li>
</ul>

<h3>Title: Principled Approximation Methods for Efficient and Scalable Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Pedro Savarese</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00174">https://arxiv.org/abs/2509.00174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00174">https://arxiv.org/pdf/2509.00174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00174]] Principled Approximation Methods for Efficient and Scalable Deep Learning(https://arxiv.org/abs/2509.00174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in deep learning has been driven by increasingly larger models. However, their computational and energy demands have grown proportionally, creating significant barriers to their deployment and to a wider adoption of deep learning technologies. This thesis investigates principled approximation methods for improving the efficiency of deep learning systems, with a particular focus on settings that involve discrete constraints and non-differentiability. We study three main approaches toward improved efficiency: architecture design, model compression, and optimization. For model compression, we propose novel approximations for pruning and quantization that frame the underlying discrete problem as continuous and differentiable, enabling gradient-based training of compression schemes alongside the model's parameters. These approximations allow for fine-grained sparsity and precision configurations, leading to highly compact models without significant fine-tuning. In the context of architecture design, we design an algorithm for neural architecture search that leverages parameter sharing across layers to efficiently explore implicitly recurrent architectures. Finally, we study adaptive optimization, revisiting theoretical properties of widely used methods and proposing an adaptive optimizer that allows for quick hyperparameter tuning. Our contributions center on tackling computationally hard problems via scalable and principled approximations. Experimental results on image classification, language modeling, and generative modeling tasks show that the proposed methods provide significant improvements in terms of training and inference efficiency while maintaining, or even improving, the model's performance.</li>
</ul>

<h3>Title: Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Ali, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00176">https://arxiv.org/abs/2509.00176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00176">https://arxiv.org/pdf/2509.00176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00176]] Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments(https://arxiv.org/abs/2509.00176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have paved the way for Vision Large Language Models (VLLMs) capable of performing a wide range of visual understanding tasks. While LLMs have demonstrated impressive performance on standard natural images, their capabilities have not been thoroughly explored in cluttered datasets where there is complex environment having deformed shaped objects. In this work, we introduce a novel dataset specifically designed for waste classification in real-world scenarios, characterized by complex environments and deformed shaped objects. Along with this dataset, we present an in-depth evaluation approach to rigorously assess the robustness and accuracy of VLLMs. The introduced dataset and comprehensive analysis provide valuable insights into the performance of VLLMs under challenging conditions. Our findings highlight the critical need for further advancements in VLLM's robustness to perform better in complex environments. The dataset and code for our experiments will be made publicly available.</li>
</ul>

<h3>Title: Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders</h3>
<ul>
<li><strong>Authors: </strong>Faizan Farooq Khan, Vladan Stojniƒá, Zakaria Laskar, Mohamed Elhoseiny, Giorgos Tolias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00177">https://arxiv.org/abs/2509.00177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00177">https://arxiv.org/pdf/2509.00177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00177]] Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders(https://arxiv.org/abs/2509.00177)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: this https URL</li>
</ul>

<h3>Title: Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Sheldon Yu, Yuxin Xiong, Junda Wu, Xintong Li, Tong Yu, Xiang Chen, Ritwik Sinha, Jingbo Shang, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00190">https://arxiv.org/abs/2509.00190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00190">https://arxiv.org/pdf/2509.00190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00190]] Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics(https://arxiv.org/abs/2509.00190)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.</li>
</ul>

<h3>Title: Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety</h3>
<ul>
<li><strong>Authors: </strong>Younggun Kim, Sirnam Swetha, Fazil Kagdi, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00192">https://arxiv.org/abs/2509.00192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00192">https://arxiv.org/pdf/2509.00192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00192]] Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety(https://arxiv.org/abs/2509.00192)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks. However, these models often infer and reveal sensitive biometric attributes - such as race, gender, age, body weight, and eye color - even when such information is not explicitly requested. This raises critical concerns, particularly in real-world applications and socially-sensitive domains. Despite increasing awareness, no publicly available dataset or benchmark exists to comprehensively evaluate or mitigate biometric leakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware Evaluation of Responses in Sensitive Modalities), a new benchmark designed to assess MLLMs on two fronts: (1) refuse biometric-related queries and (2) implicit biometric leakage in general responses while maintaining semantic faithfulness. Further, we conduct a detailed audit of the widely used LLaVA datasets and uncover extensive biometric leakage across pretraining and instruction data. To address this, we present Safe-LLaVA dataset, the first privacy-preserving MLLM training dataset constructed by systematically removing explicit and implicit biometric information from LLaVA dataset. Our evaluations on PRISM reveal biometric leakages across MLLMs for different attributes, highlighting the detailed privacy-violations. We also fine-tune a model on Safe-LLaVA dataset and show that it substantially reduces the biometric leakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned development and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark are publicly available at this https URL, and the source code is available at this https URL.</li>
</ul>

<h3>Title: Democratizing Agentic AI with Fast Test-Time Scaling on the Edge</h3>
<ul>
<li><strong>Authors: </strong>Hao Mark Chen, Zhiwen Mo, Guanxi Lu, Shuang Liang, Lingxiao Ma, Wayne Luk, Hongxiang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00195">https://arxiv.org/abs/2509.00195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00195">https://arxiv.org/pdf/2509.00195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00195]] Democratizing Agentic AI with Fast Test-Time Scaling on the Edge(https://arxiv.org/abs/2509.00195)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Deploying agentic AI on edge devices is crucial for privacy and responsiveness, but memory constraints typically relegate these systems to smaller Large Language Models (LLMs) with inferior reasoning capabilities. Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more compute during inference, but existing methods incur prohibitive overhead on edge hardware. To overcome this, we introduce FlashTTS, a serving system that makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces three synergistic optimizations: (i) Speculative Beam Extension to mitigate system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model Memory Allocation to dynamically balance memory between generation and verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on a single consumer GPU (24 GB) to match the accuracy and latency of large cloud models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x higher goodput and reduces latency by 38%-68% compared to a vLLM baseline, paving the way for democratized, high-performance agentic AI on edge devices.</li>
</ul>

<h3>Title: From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive Inference</h3>
<ul>
<li><strong>Authors: </strong>Zhongpan Tang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00202">https://arxiv.org/abs/2509.00202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00202">https://arxiv.org/pdf/2509.00202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00202]] From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive Inference(https://arxiv.org/abs/2509.00202)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Although the Transformer has become the cornerstone of modern AI, its autoregressive inference suffers from a linearly growing KV Cache and a computational complexity of O(N^2 d), severely hindering its ability to process ultra-long sequences. To overcome this limitation, this paper introduces the TConstFormer architecture, building upon our previous work, TLinFormer. TConstFormer employs an innovative periodic state update mechanism to achieve a truly constant-size O(1) KV Cache. The computational complexity of this mechanism is also O(1) in an amortized sense: it performs purely constant-time computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single linear-time global information synchronization only on the $k$-th step. Theoretical calculations and experimental results demonstrate that TConstFormer exhibits an overwhelming advantage over baseline models in terms of speed, memory efficiency, and overall performance on long-text inference tasks. This breakthrough paves the way for efficient and robust streaming language model applications.</li>
</ul>

<h3>Title: Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Li, Mahdi Masmoudi, Rami Gharbi, Nizar Lajnef, Vishnu Naresh Boddeti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00203">https://arxiv.org/abs/2509.00203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00203">https://arxiv.org/pdf/2509.00203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00203]] Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements(https://arxiv.org/abs/2509.00203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Parameterized partial differential equations (PDEs) underpin the mathematical modeling of complex systems in diverse domains, including engineering, healthcare, and physics. A central challenge in using PDEs for real-world applications is to accurately infer the parameters, particularly when the parameters exhibit non-linear and spatiotemporal variations. Existing parameter estimation methods, such as sparse identification and physics-informed neural networks (PINNs), struggle in such cases, especially with nonlinear dynamics, multiphysics interactions, or limited observations of the system response. To address these challenges, we introduce Neptune, a general-purpose method capable of inferring parameter fields from sparse measurements of system responses. Neptune employs independent coordinate neural networks to continuously represent each parameter field in physical space or in state variables. Across various physical and biomedical problems, where direct parameter measurements are prohibitively expensive or unattainable, Neptune significantly outperforms existing methods, achieving robust parameter estimation from as few as 50 observations, reducing parameter estimation errors by two orders of magnitude and dynamic response prediction errors by a factor of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation capabilities, enabling accurate predictions in regimes beyond training data where PINN fail. By facilitating reliable and data-efficient parameter inference, Neptune promises broad transformative impacts in engineering, healthcare, and beyond.</li>
</ul>

<h3>Title: Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Jaya Narain, Zakaria Aldeneh, Shirley Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00221">https://arxiv.org/abs/2509.00221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00221">https://arxiv.org/pdf/2509.00221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00221]] Speech Foundation Models Generalize to Time Series Tasks from Wearable Sensor Data(https://arxiv.org/abs/2509.00221)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Both speech and sensor time series data encode information in both the time- and frequency- domains, like spectral powers and waveform shapelets. We show that speech foundation models learn representations that are domain-independent and achieve state-of-the-art performance on time series tasks from wearable sensors. Probes trained on features extracted from HuBERT and wav2vec 2.0 outperform those extracted from self-supervised models trained directly on modality specific datasets for mood classification, arrhythmia detection, and activity classification tasks. We find a particularly strong relevance of the convolutional feature encoders from speech models for wearable sensor tasks. The methods proposed here improve performance and robustness for data-scarce time series tasks, using simple probing methods. This work is a step towards generalized time series models for speech and sensor data, a topic for further exploration.</li>
</ul>

<h3>Title: GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery</h3>
<ul>
<li><strong>Authors: </strong>Ren√© Parlange, Juan C. Cuevas-Tello, Octavio Valenzuela, Omar de J. Cabrera-Rosas, Tom√°s Verdugo, Anupreeta More, Anton T. Jaelani</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.GA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00226">https://arxiv.org/abs/2509.00226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00226">https://arxiv.org/pdf/2509.00226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00226]] GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery(https://arxiv.org/abs/2509.00226)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gravitational lensing offers a powerful probe into the properties of dark matter and is crucial to infer cosmological parameters. The Legacy Survey of Space and Time (LSST) is predicted to find O(10^5) gravitational lenses over the next decade, demanding automated classifiers. In this work, we introduce GraViT, a PyTorch pipeline for gravitational lens detection that leverages extensive pretraining of state-of-the-art Vision Transformer (ViT) models and MLP-Mixer. We assess the impact of transfer learning on classification performance by examining data quality (source and sample size), model architecture (selection and fine-tuning), training strategies (augmentation, normalization, and optimization), and ensemble predictions. This study reproduces the experiments in a previous systematic comparison of neural networks and provides insights into the detectability of strong gravitational lenses on that common test sample. We fine-tune ten architectures using datasets from HOLISMOKES VI and SuGOHI X, and benchmark them against convolutional baselines, discussing complexity and inference-time analysis.</li>
</ul>

<h3>Title: The Temporal Game: A New Perspective on Temporal Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Hugo Sousa, Ricardo Campos, Al√≠pio Jorge</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00250">https://arxiv.org/abs/2509.00250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00250">https://arxiv.org/pdf/2509.00250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00250]] The Temporal Game: A New Perspective on Temporal Relation Extraction(https://arxiv.org/abs/2509.00250)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper we demo the Temporal Game, a novel approach to temporal relation extraction that casts the task as an interactive game. Instead of directly annotating interval-level relations, our approach decomposes them into point-wise comparisons between the start and end points of temporal entities. At each step, players classify a single point relation, and the system applies temporal closure to infer additional relations and enforce consistency. This point-based strategy naturally supports both interval and instant entities, enabling more fine-grained and flexible annotation than any previous approach. The Temporal Game also lays the groundwork for training reinforcement learning agents, by treating temporal annotation as a sequential decision-making task. To showcase this potential, the demo presented in this paper includes a Game mode, in which users annotate texts from the TempEval-3 dataset and receive feedback based on a scoring system, and an Annotation mode, that allows custom documents to be annotated and resulting timeline to be exported. Therefore, this demo serves both as a research tool and an annotation interface. The demo is publicly available at this https URL, and the source code is open-sourced to foster further research and community-driven development in temporal reasoning and annotation.</li>
</ul>

<h3>Title: Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction</h3>
<ul>
<li><strong>Authors: </strong>Stefan-Alexandru Jura, Mihai Udrescu, Alexandru Topirceanu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00259">https://arxiv.org/abs/2509.00259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00259">https://arxiv.org/pdf/2509.00259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00259]] Quantum-Optimized Selective State Space Model for Efficient Time Series Prediction(https://arxiv.org/abs/2509.00259)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Long-range time series forecasting remains challenging, as it requires capturing non-stationary and multi-scale temporal dependencies while maintaining noise robustness, efficiency, and stability. Transformer-based architectures such as Autoformer and Informer improve generalization but suffer from quadratic complexity and degraded performance on very long time horizons. State space models, notably S-Mamba, provide linear-time updates but often face unstable training dynamics, sensitivity to initialization, and limited robustness for multivariate forecasting. To address such challenges, we propose the Quantum-Optimized Selective State Space Model (Q-SSM), a hybrid quantum-optimized approach that integrates state space dynamics with a variational quantum gate. Instead of relying on expensive attention mechanisms, Q-SSM employs a simple parametrized quantum circuit (RY-RX ansatz) whose expectation values regulate memory updates adaptively. This quantum gating mechanism improves convergence stability, enhances the modeling of long-term dependencies, and provides a lightweight alternative to attention. We empirically validate Q-SSM on three widely used benchmarks, i.e., ETT, Traffic, and Exchange Rate. Results show that Q-SSM consistently improves over strong baselines (LSTM, TCN, Reformer), Transformer-based models, and S-Mamba. These findings demonstrate that variational quantum gating can address current limitations in long-range forecasting, leading to accurate and robust multivariate predictions.</li>
</ul>

<h3>Title: A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Qishen Sam Liang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00266">https://arxiv.org/abs/2509.00266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00266">https://arxiv.org/pdf/2509.00266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00266]] A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report(https://arxiv.org/abs/2509.00266)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Academic and research Cyber Infrastructures (CI) present unique security challenges due to their collaborative nature, heterogeneous components, and the lack of practical, tailored security assessment frameworks. Existing standards can be too generic or complex for CI administrators to apply effectively. This report introduces a systematic, mission-centric approach to estimate and analyze the security posture of a CI. The framework guides administrators through a top-down process: (1) defining unacceptable losses and security missions, (2) identifying associated system hazards and critical assets, and (3) modeling the CI's components and their relationships as a security knowledge graph. The core of this methodology is the construction of directed attack graphs, which systematically map all potential paths an adversary could take from an entry point to a critical asset. By visualizing these attack paths alongside defense mechanisms, the framework provides a clear, comprehensive overview of the system's vulnerabilities and security gaps. This structured approach enables CI operators to proactively assess risks, prioritize mitigation strategies, and make informed, actionable decisions to strengthen the overall security posture of the CI.</li>
</ul>

<h3>Title: Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Liu, Tian Wang, Gourab Kundu, Tianyu Cao, Guang Cheng, Zhen Ge, Jianshu Chen, Qingjun Cui, Trishul Chilimbi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00276">https://arxiv.org/abs/2509.00276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00276">https://arxiv.org/pdf/2509.00276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00276]] Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval(https://arxiv.org/abs/2509.00276)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based models such as BERT and E5 have significantly advanced text embedding by capturing rich contextual representations. However, many complex real-world queries require sophisticated reasoning to retrieve relevant documents beyond surface-level lexical matching, where encoder-only retrievers often fall short. Decoder-only large language models (LLMs), known for their strong reasoning capabilities, offer a promising alternative. Despite this potential, existing LLM-based embedding methods primarily focus on contextual representation and do not fully exploit the reasoning strength of LLMs. To bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple but effective approach that integrates logical reasoning into the text embedding process using generative LLMs. RITE builds upon existing language model embedding techniques by generating intermediate reasoning texts in the token space before computing embeddings, thereby enriching representations with inferential depth. Experimental results on BRIGHT, a reasoning-intensive retrieval benchmark, demonstrate that RITE significantly enhances zero-shot retrieval performance across diverse domains, underscoring the effectiveness of incorporating reasoning into the embedding process.</li>
</ul>

<h3>Title: Generative AI for Industrial Contour Detection: A Language-Guided Vision System</h3>
<ul>
<li><strong>Authors: </strong>Liang Gong, Tommy (Zelin)Wang, Sara Chaker, Yanchen Dong, Fouad Bousetouane, Brenden Morton, Mark Mendez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00284">https://arxiv.org/abs/2509.00284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00284">https://arxiv.org/pdf/2509.00284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00284]] Generative AI for Industrial Contour Detection: A Language-Guided Vision System(https://arxiv.org/abs/2509.00284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Industrial computer vision systems often struggle with noise, material variability, and uncontrolled imaging conditions, limiting the effectiveness of classical edge detectors and handcrafted pipelines. In this work, we present a language-guided generative vision system for remnant contour detection in manufacturing, designed to achieve CAD-level precision. The system is organized into three stages: data acquisition and preprocessing, contour generation using a conditional GAN, and multimodal contour refinement through vision-language modeling, where standardized prompts are crafted in a human-in-the-loop process and applied through image-text guided synthesis. On proprietary FabTrack datasets, the proposed system improved contour fidelity, enhancing edge continuity and geometric alignment while reducing manual tracing. For the refinement stage, we benchmarked several vision-language models, including Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided workflow, and open-source baselines. Under standardized conditions, GPT-image-1 consistently outperformed Gemini 2.0 Flash in both structural accuracy and perceptual quality. These findings demonstrate the promise of VLM-guided generative workflows for advancing industrial computer vision beyond the limitations of classical pipelines.</li>
</ul>

<h3>Title: OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews</h3>
<ul>
<li><strong>Authors: </strong>Mir Tafseer Nayeem, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00285">https://arxiv.org/abs/2509.00285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00285">https://arxiv.org/pdf/2509.00285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00285]] OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews(https://arxiv.org/abs/2509.00285)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.</li>
</ul>

<h3>Title: Wage Sentiment Indices Derived from Survey Comments via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Taihei Sone</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00290">https://arxiv.org/abs/2509.00290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00290">https://arxiv.org/pdf/2509.00290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00290]] Wage Sentiment Indices Derived from Survey Comments via Large Language Models(https://arxiv.org/abs/2509.00290)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of generative Artificial Intelligence (AI) has created new opportunities for economic text analysis. This study proposes a Wage Sentiment Index (WSI) constructed with Large Language Models (LLMs) to forecast wage dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS), a monthly survey conducted by the Cabinet Office of Japan that captures real-time economic assessments from workers in industries highly sensitive to business conditions. The WSI extends the framework of the Price Sentiment Index (PSI) used in prior studies, adapting it specifically to wage related sentiment. To ensure scalability and adaptability, a data architecture is also developed that enables integration of additional sources such as newspapers and social media. Experimental results demonstrate that WSI models based on LLMs significantly outperform both baseline approaches and pretrained models. These findings highlight the potential of LLM-driven sentiment indices to enhance the timeliness and effectiveness of economic policy design by governments and central banks.</li>
</ul>

<h3>Title: ShadowScope: GPU Monitoring and Validation via Composable Side Channel Signals</h3>
<ul>
<li><strong>Authors: </strong>Ghadeer Almusaddar, Yicheng Zhang, Saber Ganjisaffar, Barry Williams, Yu David Liu, Dmitry Ponomare, Nael Abu-Ghazaleh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00300">https://arxiv.org/abs/2509.00300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00300">https://arxiv.org/pdf/2509.00300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00300]] ShadowScope: GPU Monitoring and Validation via Composable Side Channel Signals(https://arxiv.org/abs/2509.00300)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>As modern systems increasingly rely on GPUs for computationally intensive tasks such as machine learning acceleration, ensuring the integrity of GPU computation has become critically important. Recent studies have shown that GPU kernels are vulnerable to both traditional memory safety issues (e.g., buffer overflow attacks) and emerging microarchitectural threats (e.g., Rowhammer attacks), many of which manifest as anomalous execution behaviors observable through side-channel signals. However, existing golden model based validation approaches that rely on such signals are fragile, highly sensitive to interference, and do not scale well across GPU workloads with diverse scheduling behaviors. To address these challenges, we propose ShadowScope, a monitoring and validation framework that leverages a composable golden model. Instead of building a single monolithic reference, ShadowScope decomposes trusted kernel execution into modular, repeatable functions that encode key behavioral features. This composable design captures execution patterns at finer granularity, enabling robust validation that is resilient to noise, workload variation, and interference across GPU workloads. To further reduce reliance on noisy software-only monitoring, we introduce ShadowScope+, a hardware-assisted validation mechanism that integrates lightweight on-chip checks into the GPU pipeline. ShadowScope+ achieves high validation accuracy with an average runtime overhead of just 4.6%, while incurring minimal hardware and design complexity. Together, these contributions demonstrate that side-channel observability can be systematically repurposed into a practical defense for GPU kernel integrity.</li>
</ul>

<h3>Title: Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Zheng, Yiyuan Ma, Yuan Yang, Deyi Liu, Jing Liu, Zuquan Song, Yuxin Song, Cheng Ren, Hang Zhu, Xin Liu, Yiyuan Ma, Siyuan Qiao, Xun Zhou, Liang Xiang, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00309">https://arxiv.org/abs/2509.00309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00309">https://arxiv.org/pdf/2509.00309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00309]] Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models(https://arxiv.org/abs/2509.00309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The development of alignment and reasoning capabilities in large language models has seen remarkable progress through two paradigms: instruction tuning and reinforcement learning from human feedback (RLHF) alignment paradigm, and distillation-based reasoning fine-tuning paradigm. While both approaches prove effective independently, the third paradigm of applying RLHF to distillation-trained models presents significant challenges. Our investigation reveals two critical phenomena that emerge in this paradigm: Sequence Length Collapse, where language generation dramatically reduces during early RLHF training, and the Reward Hockey Stick Curve, featuring severe reward score drops followed by gradual recovery. These instabilities fundamentally compromise the model's alignment and reasoning capabilities. To address these challenges, we propose Balanced Actor Initialization (BAI), a two-stage weighted model merging approach. BAI first merges instruction-following and distillation-based reasoning fine-tuned models, then further combines this intermediate model with the pretrained model to preserve foundational knowledge. Through comprehensive experiments across diverse benchmarks and detailed analysis of training experiments, we demonstrate that BAI resolves Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables continuous sequence length improvement during training. Additionally, our analysis reveals that balanced merging ratios achieve optimal trade-offs between training stability and reasoning capability preservation. Our work provides the effective solution for stable training in this third paradigm, enabling more capable reasoning models that combine distillation efficiency with RLHF alignment.</li>
</ul>

<h3>Title: MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</h3>
<ul>
<li><strong>Authors: </strong>Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00311">https://arxiv.org/abs/2509.00311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00311">https://arxiv.org/pdf/2509.00311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00311]] MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification(https://arxiv.org/abs/2509.00311)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Domain generalization in computational histopathology is hindered by heterogeneity in whole slide images (WSIs), caused by variations in tissue preparation, staining, and imaging conditions across institutions. Unlike machine learning systems, pathologists rely on domain-invariant morphological cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia, chromatin texture, spatial disorganization), structural atypia (abnormal architecture and gland formation), and overall morphological atypia that remain diagnostic across diverse settings. Motivated by this, we hypothesize that explicitly modeling biologically robust nuclear morphology and spatial organization will enable the learning of cancer representations that are resilient to domain shifts. We propose MorphGen (Morphology-Guided Generalization), a method that integrates histopathology images, augmentations, and nuclear segmentation masks within a supervised contrastive learning framework. By aligning latent representations of images and nuclear masks, MorphGen prioritizes diagnostic features such as nuclear and morphological atypia and spatial organization over staining artifacts and domain-specific features. To further enhance out-of-distribution robustness, we incorporate stochastic weight averaging (SWA), steering optimization toward flatter minima. Attention map analyses revealed that MorphGen primarily relies on nuclear morphology, cellular composition, and spatial cell organization within tumors or normal regions for final classification. Finally, we demonstrate resilience of the learned representations to image corruptions (such as staining artifacts) and adversarial attacks, showcasing not only OOD generalization but also addressing critical vulnerabilities in current deep learning systems for digital pathology. Code, datasets, and trained models are available at: this https URL</li>
</ul>

<h3>Title: Continuously Tempered Diffusion Samplers</h3>
<ul>
<li><strong>Authors: </strong>Ezra Erives, Bowen Jing, Peter Holderrieth, Tommi Jaakkola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00316">https://arxiv.org/abs/2509.00316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00316">https://arxiv.org/pdf/2509.00316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00316]] Continuously Tempered Diffusion Samplers(https://arxiv.org/abs/2509.00316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Annealing-based neural samplers seek to amortize sampling from unnormalized distributions by training neural networks to transport a family of densities interpolating from source to target. A crucial design choice in the training phase of such samplers is the proposal distribution by which locations are generated at which to evaluate the loss. Previous work has obtained such a proposal distribution by combining a partially learned transport with annealed Langevin dynamics. However, isolated modes and other pathological properties of the annealing path imply that such proposals achieve insufficient exploration and thereby lower performance post training. To remedy this, we propose continuously tempered diffusion samplers, which leverage exploration techniques developed in the context of molecular dynamics to improve proposal distributions. Specifically, a family of distributions across different temperatures is introduced to lower energy barriers at higher temperatures and drive exploration at the lower temperature of interest. We empirically validate improved sampler performance driven by extended exploration. Code is available at this https URL.</li>
</ul>

<h3>Title: GIER: Gap-Driven Self-Refinement for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rinku Dewri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00325">https://arxiv.org/abs/2509.00325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00325">https://arxiv.org/pdf/2509.00325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00325]] GIER: Gap-Driven Self-Refinement for Large Language Models(https://arxiv.org/abs/2509.00325)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general framework for improving large language model (LLM) outputs through self-reflection and revision based on conceptual quality criteria. Unlike prompting strategies that rely on demonstrations, examples, or chain-of-thought templates, GIER utilizes natural language descriptions of reasoning gaps, and prompts a model to iteratively critique and refine its own outputs to better satisfy these criteria. Across three reasoning-intensive tasks (SciFact, PrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and Llama 3.3 70B), GIER improves rationale quality, grounding, and reasoning alignment without degrading task accuracy. Our analysis demonstrates that models can not only interpret abstract conceptual gaps but also translate them into concrete reasoning improvements.</li>
</ul>

<h3>Title: Chunked TabPFN: Exact Training-Free In-Context Learning for Long-Context Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Renat Sergazinov, Shao-An Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00326">https://arxiv.org/abs/2509.00326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00326">https://arxiv.org/pdf/2509.00326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00326]] Chunked TabPFN: Exact Training-Free In-Context Learning for Long-Context Tabular Data(https://arxiv.org/abs/2509.00326)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>TabPFN v2 achieves better results than tree-based models on several tabular benchmarks, which is notable since tree-based models are usually the strongest choice for tabular data. However, it cannot handle more than 10K context tokens because transformers have quadratic computation and memory costs. Unlike existing approaches that rely on context compression, such as selecting representative samples via K-nearest neighbors (KNN), we introduce a \textbf{tiled-block} strategy to compute attention within the TabPFN framework. This design is compatible with standard GPU setups and, to the best of our knowledge, is the first to enable TabPFN to \textbf{process long contexts without any pre-processing}. We demonstrate the effectiveness of our approach on the standard TabArena benchmark.</li>
</ul>

<h3>Title: CryptoFace: End-to-End Encrypted Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Wei Ao, Vishnu Naresh Boddeti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00332">https://arxiv.org/abs/2509.00332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00332">https://arxiv.org/pdf/2509.00332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00332]] CryptoFace: End-to-End Encrypted Face Recognition(https://arxiv.org/abs/2509.00332)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, biometric, extraction</a></li>
<li><strong>Abstract: </strong>Face recognition is central to many authentication, security, and personalized applications. Yet, it suffers from significant privacy risks, particularly arising from unauthorized access to sensitive biometric data. This paper introduces CryptoFace, the first end-to-end encrypted face recognition system with fully homomorphic encryption (FHE). It enables secure processing of facial data across all stages of a face-recognition process--feature extraction, storage, and matching--without exposing raw images or features. We introduce a mixture of shallow patch convolutional networks to support higher-dimensional tensors via patch-based processing while reducing the multiplicative depth and, thus, inference latency. Parallel FHE evaluation of these networks ensures near-resolution-independent latency. On standard face recognition benchmarks, CryptoFace significantly accelerates inference and increases verification accuracy compared to the state-of-the-art FHE neural networks adapted for face recognition. CryptoFace will facilitate secure face recognition systems requiring robust and provable security. The code is available at this https URL.</li>
</ul>

<h3>Title: Counterfactual Risk Minimization with IPS-Weighted BPR and Self-Normalized Evaluation in Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Rahul Raja, Arpita Vats</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00333">https://arxiv.org/abs/2509.00333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00333">https://arxiv.org/pdf/2509.00333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00333]] Counterfactual Risk Minimization with IPS-Weighted BPR and Self-Normalized Evaluation in Recommender Systems(https://arxiv.org/abs/2509.00333)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning and evaluating recommender systems from logged implicit feedback is challenging due to exposure bias. While inverse propensity scoring (IPS) corrects this bias, it often suffers from high variance and instability. In this paper, we present a simple and effective pipeline that integrates IPS-weighted training with an IPS-weighted Bayesian Personalized Ranking (BPR) objective augmented by a Propensity Regularizer (PR). We compare Direct Method (DM), IPS, and Self-Normalized IPS (SNIPS) for offline policy evaluation, and demonstrate how IPS-weighted training improves model robustness under biased exposure. The proposed PR further mitigates variance amplification from extreme propensity weights, leading to more stable estimates. Experiments on synthetic and MovieLens 100K data show that our approach generalizes better under unbiased exposure while reducing evaluation variance compared to naive and standard IPS methods, offering practical guidance for counterfactual learning and evaluation in real-world recommendation settings.</li>
</ul>

<h3>Title: Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>An B. Vuong, Michael T. McCann, Javier E. Santos, Yen Ting Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00336">https://arxiv.org/abs/2509.00336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00336">https://arxiv.org/pdf/2509.00336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00336]] Are We Really Learning the Score Function? Reinterpreting Diffusion Models Through Wasserstein Gradient Flow Matching(https://arxiv.org/abs/2509.00336)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are commonly interpreted as learning the score function, i.e., the gradient of the log-density of noisy data. However, this assumption implies that the target of learning is a conservative vector field, which is not enforced by the neural network architectures used in practice. We present numerical evidence that trained diffusion networks violate both integral and differential constraints required of true score functions, demonstrating that the learned vector fields are not conservative. Despite this, the models perform remarkably well as generative mechanisms. To explain this apparent paradox, we advocate a new theoretical perspective: diffusion training is better understood as flow matching to the velocity field of a Wasserstein Gradient Flow (WGF), rather than as score learning for a reverse-time stochastic differential equation. Under this view, the "probability flow" arises naturally from the WGF framework, eliminating the need to invoke reverse-time SDE theory and clarifying why generative sampling remains successful even when the neural vector field is not a true score. We further show that non-conservative errors from neural approximation do not necessarily harm density transport. Our results advocate for adopting the WGF perspective as a principled, elegant, and theoretically grounded framework for understanding diffusion generative models.</li>
</ul>

<h3>Title: LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanping Zhang, Yuhong Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00347">https://arxiv.org/abs/2509.00347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00347">https://arxiv.org/pdf/2509.00347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00347]] LLM-Driven Policy Diffusion: Enhancing Generalization in Offline Reinforcement Learning(https://arxiv.org/abs/2509.00347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) is known for its strong decision-making capabilities and has been widely applied in various real-world scenarios. However, with the increasing availability of offline datasets and the lack of well-designed online environments from human experts, the challenge of generalization in offline RL has become more prominent. Due to the limitations of offline data, RL agents trained solely on collected experiences often struggle to generalize to new tasks or environments. To address this challenge, we propose LLM-Driven Policy Diffusion (LLMDPD), a novel approach that enhances generalization in offline RL using task-specific prompts. Our method incorporates both text-based task descriptions and trajectory prompts to guide policy learning. We leverage a large language model (LLM) to process text-based prompts, utilizing its natural language understanding and extensive knowledge base to provide rich task-relevant context. Simultaneously, we encode trajectory prompts using a transformer model, capturing structured behavioral patterns within the underlying transition dynamics. These prompts serve as conditional inputs to a context-aware policy-level diffusion model, enabling the RL agent to generalize effectively to unseen tasks. Our experimental results demonstrate that LLMDPD outperforms state-of-the-art offline RL methods on unseen tasks, highlighting its effectiveness in improving generalization and adaptability in diverse settings.</li>
</ul>

<h3>Title: Theory Foundation of Physics-Enhanced Residual Learning</h3>
<ul>
<li><strong>Authors: </strong>Shixiao Liang, Wang Chen, Keke Long, Peng Zhang, Xiaopeng Li, Jintao Ke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00348">https://arxiv.org/abs/2509.00348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00348">https://arxiv.org/pdf/2509.00348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00348]] Theory Foundation of Physics-Enhanced Residual Learning(https://arxiv.org/abs/2509.00348)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Intensive studies have been conducted in recent years to integrate neural networks with physics models to balance model accuracy and interpretability. One recently proposed approach, named Physics-Enhanced Residual Learning (PERL), is to use learning to estimate the residual between the physics model prediction and the ground truth. Numeral examples suggested that integrating such residual with physics models in PERL has three advantages: (1) a reduction in the number of required neural network parameters; (2) faster convergence rates; and (3) fewer training samples needed for the same computational precision. However, these numerical results lack theoretical justification and cannot be adequately explained. This paper aims to explain these advantages of PERL from a theoretical perspective. We investigate a general class of problems with Lipschitz continuity properties. By examining the relationships between the bounds to the loss function and residual learning structure, this study rigorously proves a set of theorems explaining the three advantages of PERL. Several numerical examples in the context of automated vehicle trajectory prediction are conducted to illustrate the proposed theorems. The results confirm that, even with significantly fewer training samples, PERL consistently achieves higher accuracy than a pure neural network. These results demonstrate the practical value of PERL in real world autonomous driving applications where corner case data are costly or hard to obtain. PERL therefore improves predictive performance while reducing the amount of data required.</li>
</ul>

<h3>Title: Target-Oriented Single Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Marzi Heidari, Yuhong Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00351">https://arxiv.org/abs/2509.00351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00351">https://arxiv.org/pdf/2509.00351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00351]] Target-Oriented Single Domain Generalization(https://arxiv.org/abs/2509.00351)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep models trained on a single source domain often fail catastrophically under distribution shifts, a critical challenge in Single Domain Generalization (SDG). While existing methods focus on augmenting source data or learning invariant features, they neglect a readily available resource: textual descriptions of the target deployment environment. We propose Target-Oriented Single Domain Generalization (TO-SDG), a novel problem setup that leverages the textual description of the target domain, without requiring any target data, to guide model generalization. To address TO-SDG, we introduce Spectral TARget Alignment (STAR), a lightweight module that injects target semantics into source features by exploiting visual-language models (VLMs) such as CLIP. STAR uses a target-anchored subspace derived from the text embedding of the target description to recenter image features toward the deployment domain, then utilizes spectral projection to retain directions aligned with target cues while discarding source-specific noise. Moreover, we use a vision-language distillation to align backbone features with VLM's semantic geometry. STAR further employs feature-space Mixup to ensure smooth transitions between source and target-oriented representations. Experiments across various image classification and object detection benchmarks demonstrate STAR's superiority. This work establishes that minimal textual metadata, which is a practical and often overlooked resource, significantly enhances generalization under severe data constraints, opening new avenues for deploying robust models in target environments with unseen data.</li>
</ul>

<h3>Title: AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Koushik Ahmed Kushal, Abdullah Al Mamun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00353">https://arxiv.org/abs/2509.00353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00353">https://arxiv.org/pdf/2509.00353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00353]] AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data(https://arxiv.org/abs/2509.00353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Air pollution monitoring in resource-constrained regions remains challenging due to sparse sensor deployment and limited infrastructure. This work introduces AQFusionNet, a multimodal deep learning framework for robust Air Quality Index (AQI) prediction. The framework integrates ground-level atmospheric imagery with pollutant concentration data using lightweight CNN backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features are combined through semantically aligned embedding spaces, enabling accurate and efficient prediction. Experiments on more than 8,000 samples from India and Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines, achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the EfficientNet-B0 backbone. The model delivers an 18.5% improvement over single-modality approaches while maintaining low computational overhead, making it suitable for deployment on edge devices. AQFusionNet provides a scalable and practical solution for AQI monitoring in infrastructure-limited environments, offering robust predictive capability even under partial sensor availability.</li>
</ul>

<h3>Title: A Multimodal Head and Neck Cancer Dataset for AI-Driven Precision Oncology</h3>
<ul>
<li><strong>Authors: </strong>Numan Saeed (1), Salma Hassan (2), Shahad Hardan (2), Ahmed Aly (1), Darya Taratynova (2), Umair Nawaz (1), Ufaq Khan (1), Muhammad Ridzuan (1), Thomas Eugene (4), Rapha"el Metz (4), M'elanie Dore (5), Gregory Delpon (6), Vijay Ram Kumar Papineni (7), Kareem Wahid (8), Cem Dede (8), Alaa Mohamed Shawky Ali (8), Carlos Sjogreen (8), Mohamed Naser (8), Clifton D. Fuller (8), Valentin Oreiller (9), Mario Jreige (10), John O. Prior (10), Catherine Cheze Le Rest (11), Olena Tankyevych (11), Pierre Decazes (12), Su Ruan (12), Stephanie Tanadini-Lang (13), Martin Valli`eres (14), Hesham Elhalawani (16 and 17), Ronan Abgral (18), Romain Floch (18), Kevin Kerleguer (18), Ulrike Schick (19), Maelle Mauguen (19), Vincent Andrearczyk (9 and 10), Adrien Depeursinge (9 and 10), Mathieu Hatt (15), Arman Rahmim (3), Mohammad Yaqub (1) ((1) Department of Computer Vision, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, (2) Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, (3) Department of Integrative Oncology, BC Cancer Research Institute, Vancouver, BC, Canada, (4) Nantes Universit'e, CHU Nantes, Nuclear Medicine Department, Nantes, France, (5) Radiation Oncology Department, Institut de Canc'erologie de l'Ouest, Saint-Herblain, France, (6) Medical Physics Department, Institut de Canc'erologie de l'Ouest, Saint Herblain, France, (7) Radiology Department, Sheikh Shakhbout Medical City, Abu Dhabi, UAE, (8) MD Anderson Cancer Center, The University of Texas, Texas, United States, (9) Institute of Informatics, HES-SO Valais-Wallis University of Applied Sciences and Arts Western Switzerland, Sierre, Switzerland, (10) Department of Nuclear Medicine and Molecular Imaging, Lausanne University Hospital (CHUV), Lausanne, Switzerland, (11) Centre Hospitalier Universitaire de Poitiers (CHUP), Poitiers, France, (12) Center Henri Becquerel, LITIS laboratory, University of Rouen Normandy, Rouen, France, (13) University Hospital Z"urich, Zurich, Switzerland, (14) Department of Computer Science, Universit'e de Sherbrooke, Sherbrooke, Qu'ebec, Canada, (15) LaTIM, INSERM, UMR 1101, Univ Brest, Brest, France, (16) Department of Radiation Oncology, Brigham and Women's Hospital, Boston, United States, (17) Dana Farber Cancer Institute, Harvard Medical School, Boston, USA, (18) Nuclear medicine department, University Hospital of Brest, Brest, France, (19) Radiotherapy department, University Hospital of Brest, Brest, France)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00367">https://arxiv.org/abs/2509.00367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00367">https://arxiv.org/pdf/2509.00367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00367]] A Multimodal Head and Neck Cancer Dataset for AI-Driven Precision Oncology(https://arxiv.org/abs/2509.00367)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We describe a publicly available multimodal dataset of annotated Positron Emission Tomography/Computed Tomography (PET/CT) studies for head and neck cancer research. The dataset includes 1123 FDG-PET/CT studies from patients with histologically confirmed head and neck cancer, acquired from 10 international medical centers. All examinations consisted of co-registered PET/CT scans with varying acquisition protocols, reflecting real-world clinical diversity across institutions. Primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) were manually segmented by experienced radiation oncologists and radiologists following standardized guidelines and quality control measures. We provide anonymized NifTi files of all studies, along with expert-annotated segmentation masks, radiotherapy dose distribution for a subset of patients, and comprehensive clinical metadata. This metadata includes TNM staging, HPV status, demographics (age and gender), long-term follow-up outcomes, survival times, censoring indicators, and treatment information. We demonstrate how this dataset can be used for three key clinical tasks: automated tumor segmentation, recurrence-free survival prediction, and HPV status classification, providing benchmark results using state-of-the-art deep learning models, including UNet, SegResNet, and multimodal prognostic frameworks.</li>
</ul>

<h3>Title: Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Guangzong Si, Hao Yin, Xianfei Li, Qing Ding, Wenlong Liao, Tao He, Pai Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00371">https://arxiv.org/abs/2509.00371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00371">https://arxiv.org/pdf/2509.00371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00371]] Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs(https://arxiv.org/abs/2509.00371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved impressive advances, yet object hallucination remains a persistent challenge. Existing methods, based on the flawed assumption that omission and fabrication hallucinations share a common cause, often reduce omissions only to trigger more fabrications. In this work, we overturn this view by demonstrating that omission hallucinations arise from insufficient confidence when mapping perceived visual features to linguistic expressions, whereas fabrication hallucinations result from spurious associations within the cross-modal representation space due to statistical biases in the training corpus. Building on findings from visual attention intervention experiments, we propose the Visual-Semantic Attention Potential Field, a conceptual framework that reveals how the model constructs visual evidence to infer the presence or absence of objects. Leveraging this insight, we introduce Visual Potential Field Calibration (VPFC), a plug-and-play hallucination mitigation method that effectively reduces omission hallucinations without introducing additional fabrication hallucinations. Our findings reveal a critical oversight in current object hallucination research and chart new directions for developing more robust and balanced hallucination mitigation strategies.</li>
</ul>

<h3>Title: Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sihao Wu, Gaojie Jin, Wei Huang, Jianhong Wang, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00373">https://arxiv.org/abs/2509.00373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00373">https://arxiv.org/pdf/2509.00373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00373]] Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models(https://arxiv.org/abs/2509.00373)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) have demonstrated impressive capabilities in integrating visual and textual information for understanding and reasoning, but remain highly vulnerable to adversarial attacks. While activation steering has emerged as a promising defence, existing approaches often rely on task-specific contrastive prompts to extract harmful directions, which exhibit suboptimal performance and can degrade visual grounding performance. To address these limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM (\textit{SPO-VLM}), a novel two-stage defense framework that combines activation-level intervention with policy-level optimization to enhance model robustness. In \textit{Stage I}, we compute adaptive layer-specific steering vectors from diverse data sources, enabling generalized suppression of harmful behaviors during inference. In \textit{Stage II}, we refine these steering vectors through a sequence-level preference optimization process. This stage integrates automated toxicity assessment, as well as visual-consistency rewards based on caption-image alignment, to achieve safe and semantically grounded text generation. The two-stage structure of SPO-VLM balances efficiency and effectiveness by combining a lightweight mitigation foundation in Stage I with deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM enhances safety against attacks via activation steering and preference optimization, while maintaining strong performance on benign tasks without compromising visual understanding capabilities. We will release our code, model weights, and evaluation toolkit to support reproducibility and future research. \textcolor{red}{Warning: This paper may contain examples of offensive or harmful text and images.}</li>
</ul>

<h3>Title: Open Data Synthesis For Deep Research</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00375">https://arxiv.org/abs/2509.00375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00375">https://arxiv.org/pdf/2509.00375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00375]] Open Data Synthesis For Deep Research(https://arxiv.org/abs/2509.00375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{this https URL}{this repository}.</li>
</ul>

<h3>Title: NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shumpei Takezaki, Ryoma Bise, Shinnosuke Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00378">https://arxiv.org/abs/2509.00378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00378">https://arxiv.org/pdf/2509.00378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00378]] NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models(https://arxiv.org/abs/2509.00378)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this study, we propose a novel data augmentation method that introduces the concept of CutMix into the generation process of diffusion models, thereby exploiting both the ability of diffusion models to generate natural and high-resolution images and the characteristic of CutMix, which combines features from two classes to create diverse augmented data. Representative data augmentation methods for combining images from multiple classes include CutMix and MixUp. However, techniques like CutMix often result in unnatural boundaries between the two images due to contextual differences. Therefore, in this study, we propose a method, called NoiseCutMix, to achieve natural, high-resolution image generation featuring the fused characteristics of two classes by partially combining the estimated noise corresponding to two different classes in a diffusion model. In the classification experiments, we verified the effectiveness of the proposed method by comparing it with conventional data augmentation techniques that combine multiple classes, random image generation using Stable Diffusion, and combinations of these methods. Our codes are available at: this https URL</li>
</ul>

<h3>Title: Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Kang, Jiawen Wang, Dingsheng Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00379">https://arxiv.org/abs/2509.00379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00379">https://arxiv.org/pdf/2509.00379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00379]] Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation(https://arxiv.org/abs/2509.00379)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous driving. Traditional approaches rely on extensive annotated data for point cloud analysis, incurring high costs and time investments. In contrast, realworld image datasets offer abundant availability and substantial scale. To mitigate the burden of annotating 3D LiDAR point clouds, we propose two crossmodal knowledge distillation methods: Unsupervised Domain Adaptation Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge Distillation (FSKD). Leveraging readily available spatio-temporally synchronized data from cameras and LiDARs in autonomous driving scenarios, we directly apply a pretrained 2D image model to unlabeled 2D data. Through crossmodal knowledge distillation with known 2D-3D correspondence, we actively align the output of the 3D network with the corresponding points of the 2D network, thereby obviating the necessity for 3D annotations. Our focus is on preserving modality-general information while filtering out modality-specific details during crossmodal distillation. To achieve this, we deploy self-calibrated convolution on 3D point clouds as the foundation of our domain adaptation module. Rigorous experimentation validates the effectiveness of our proposed methods, consistently surpassing the performance of state-of-the-art approaches in the field.</li>
</ul>

<h3>Title: Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction</h3>
<ul>
<li><strong>Authors: </strong>Runtong Wu, Jiayao Song, Fei Teng, Xianhao Ren, Yuyan Gao, Kailun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00381">https://arxiv.org/abs/2509.00381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00381">https://arxiv.org/pdf/2509.00381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00381]] Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction(https://arxiv.org/abs/2509.00381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Narrative inquiry has been one of the prominent application domains for the analysis of human experience, aiming to know more about the complexity of human society. However, researchers are often required to transform various forms of data into coherent hand-drafted narratives in storied form throughout narrative analysis, which brings an immense burden of data analysis. Participants, too, are expected to engage in member checking and presentation of these narrative products, which involves reviewing and responding to large volumes of documents. Given the dual burden and the need for more efficient and participant-friendly approaches to narrative making and representation, we made a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt to push the field of narrative inquiry. Name is able to transfer research documents into coherent story images, alleviating the cognitive burden of interpreting extensive text-based materials during member checking for both researchers and participants. (ii) We develop an actor location and shape module to facilitate plausible image generation. (iii) We have designed a set of robust evaluation metrics comprising three key dimensions to objectively measure the perceptual quality and narrative consistency of generated characters. Our approach consistently demonstrates state-of-the-art performance across different data partitioning schemes. Remarkably, while the baseline relies on the full 100% of the available data, our method requires only 0.96% yet still reduces the FID score from 195 to 152. Under identical data volumes, our method delivers substantial improvements: for the 70:30 split, the FID score decreases from 175 to 152, and for the 95:5 split, it is nearly halved from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the newly introduced metric, surpassing the baseline score of 2.66.</li>
</ul>

<h3>Title: HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization</h3>
<ul>
<li><strong>Authors: </strong>Joohyun Chang, Soyeon Hong, Hyogun Lee, Seong Jong Ha, Dongho Lee, Seong Tae Kim, Jinwoo Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00385">https://arxiv.org/abs/2509.00385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00385">https://arxiv.org/pdf/2509.00385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00385]] HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization(https://arxiv.org/abs/2509.00385)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we tackle the egocentric visual query localization (VQL), where a model should localize the query object in a long-form egocentric video. Frequent and abrupt viewpoint changes in egocentric videos cause significant object appearance variations and partial occlusions, making it difficult for existing methods to achieve accurate localization. To tackle these challenges, we introduce Hierarchical, Egocentric and RObust Visual Query Localization (HERO-VQL), a novel method inspired by human cognitive process in object recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance refines the attention mechanism by leveraging the class token for high-level context and principal component score maps for fine-grained localization. To enhance learning in diverse and challenging matching scenarios, EgoAug enhances query diversity by replacing the query with a randomly selected corresponding object from groundtruth annotations and simulates extreme viewpoint changes by reordering video frames. Additionally, CT loss enforces stable object localization across different augmentation scenarios. Extensive experiments on VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges, significantly outperforming baselines.</li>
</ul>

<h3>Title: Unifying Adversarial Perturbation for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jinluan Yang, Ruihao Zhang, Zhengyu Chen, Fei Wu, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00387">https://arxiv.org/abs/2509.00387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00387">https://arxiv.org/pdf/2509.00387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00387]] Unifying Adversarial Perturbation for Graph Neural Networks(https://arxiv.org/abs/2509.00387)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>This paper studies the vulnerability of Graph Neural Networks (GNNs) to adversarial attacks on node features and graph structure. Various methods have implemented adversarial training to augment graph data, aiming to bolster the robustness and generalization of GNNs. These methods typically involve applying perturbations to the node feature, weights, or graph structure and subsequently minimizing the loss by learning more robust graph model parameters under the adversarial perturbations. Despite the effectiveness of adversarial training in enhancing GNNs' robustness and generalization abilities, its application has been largely confined to specific datasets and GNN types. In this paper, we propose a novel method, PerturbEmbedding, that integrates adversarial perturbation and training, enhancing GNNs' resilience to such attacks and improving their generalization ability. PerturbEmbedding performs perturbation operations directly on every hidden embedding of GNNs and provides a unified framework for most existing perturbation strategies/methods. We also offer a unified perspective on the forms of perturbations, namely random and adversarial perturbations. Through experiments on various datasets using different backbone models, we demonstrate that PerturbEmbedding significantly improves both the robustness and generalization abilities of GNNs, outperforming existing methods. The rejection of both random (non-targeted) and adversarial (targeted) perturbations further enhances the backbone model's performance.</li>
</ul>

<h3>Title: GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction</h3>
<ul>
<li><strong>Authors: </strong>Xuelin Li, Xiangqi Jin, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00388">https://arxiv.org/abs/2509.00388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00388">https://arxiv.org/pdf/2509.00388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00388]] GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction(https://arxiv.org/abs/2509.00388)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes will be released on Github.</li>
</ul>

<h3>Title: The Resurgence of GCG Adversarial Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuting Tan, Xuying Li, Zhuo Li, Huizhen Shu, Peikang Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00391">https://arxiv.org/abs/2509.00391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00391">https://arxiv.org/pdf/2509.00391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00391]] The Resurgence of GCG Adversarial Attacks on Large Language Models(https://arxiv.org/abs/2509.00391)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient (GCG) algorithm, has emerged as a powerful method for jailbreaking large language models (LLMs). In this paper, we present a systematic appraisal of GCG and its annealing-augmented variant, T-GCG, across open-source LLMs of varying scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack effectiveness on both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Our study reveals three key findings: (1) attack success rates (ASR) decrease with model size, reflecting the increasing complexity and non-convexity of larger models' loss landscapes; (2) prefix-based heuristics substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide a stricter and more realistic evaluation; and (3) coding-related prompts are significantly more vulnerable than adversarial safety prompts, suggesting that reasoning itself can be exploited as an attack vector. In addition, preliminary results with T-GCG show that simulated annealing can diversify adversarial search and achieve competitive ASR under prefix evaluation, though its benefits under semantic judgment remain limited. Together, these findings highlight the scalability limits of GCG, expose overlooked vulnerabilities in reasoning tasks, and motivate further development of annealing-inspired strategies for more robust adversarial evaluation.</li>
</ul>

<h3>Title: Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mengxiao Geng, Ran Hong, Bingxuan Li, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00395">https://arxiv.org/abs/2509.00395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00395">https://arxiv.org/pdf/2509.00395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00395]] Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction(https://arxiv.org/abs/2509.00395)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Ultra-low-dose positron emission tomography (PET) reconstruction holds significant potential for reducing patient radiation exposure and shortening examination times. However, it may also lead to increased noise and reduced imaging detail, which could decrease the image quality. In this study, we present a Double-Constraint Diffusion Model (DCDM), which freezes the weights of a pre-trained diffusion model and injects a trainable double-constraint controller into the encoding architecture, greatly reducing the number of trainable parameters for ultra-low-dose PET reconstruction. Unlike full fine-tuning models, DCDM can adapt to different dose levels without retraining all model parameters, thereby improving reconstruction flexibility. Specifically, the two constraint modules, named the Nuclear Transformer Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the pre-trained diffusion model. The NTC leverages the nuclear norm as an approximation for matrix rank minimization, integrates the low-rank property into the Transformer architecture, and enables efficient information extraction from low-dose images and conversion into compressed feature representations in the latent space. Subsequently, the ENC utilizes these compressed feature representations to encode and control the pre-trained diffusion model, ultimately obtaining reconstructed PET images in the pixel space. In clinical reconstruction, the compressed feature representations from NTC help select the most suitable ENC for efficient unknown low-dose PET reconstruction. Experiments conducted on the UDPET public dataset and the Clinical dataset demonstrated that DCDM outperforms state-of-the-art methods on known dose reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving valuable even at ultra-low dose levels, such as 1% of the full dose.</li>
</ul>

<h3>Title: Curriculum Guided Personalized Subgraph Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Minku Kang, Hogun Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00402">https://arxiv.org/abs/2509.00402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00402">https://arxiv.org/pdf/2509.00402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00402]] Curriculum Guided Personalized Subgraph Federated Learning(https://arxiv.org/abs/2509.00402)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs) across distributed private subgraphs, but it suffers from severe data heterogeneity. To mitigate data heterogeneity, weighted model aggregation personalizes each local GNN by assigning larger weights to parameters from clients with similar subgraph characteristics inferred from their current model states. However, the sparse and biased subgraphs often trigger rapid overfitting, causing the estimated client similarity matrix to stagnate or even collapse. As a result, aggregation loses effectiveness as clients reinforce their own biases instead of exploiting diverse knowledge otherwise available. To this end, we propose a novel personalized subgraph FL framework called Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges for training according to their reconstruction scores, exposing each GNN first to easier, generic cross-client substructures and only later to harder, client-specific ones. This paced exposure prevents early overfitting to biased patterns and enables gradual personalization. By regulating personalization, the curriculum also reshapes server aggregation from exchanging generic knowledge to propagating client-specific knowledge. Further, CUFL improves weighted aggregation by estimating client similarity using fine-grained structural indicators reconstructed on a random reference graph. Extensive experiments on six benchmark datasets confirm that CUFL achieves superior performance compared to relevant baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yushuo Chen, Ruizhi Shao, Youxin Pang, Hongwen Zhang, Xinyi Wu, Rihui Wu, Yebin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00403">https://arxiv.org/abs/2509.00403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00403">https://arxiv.org/pdf/2509.00403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00403]] DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective(https://arxiv.org/abs/2509.00403)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel framework to reconstruct human avatars from monocular videos. Recent approaches have struggled either to capture the fine-grained dynamic details from the input or to generate plausible details at novel viewpoints, which mainly stem from the limited representational capacity of the avatar model and insufficient observational data. To overcome these challenges, we propose to leverage the advanced video generative model, Human4DiT, to generate the human motions from alternative perspective as an additional supervision signal. This approach not only enriches the details in previously unseen regions but also effectively regularizes the avatar representation to mitigate artifacts. Furthermore, we introduce two complementary strategies to enhance video generation: To ensure consistent reproduction of human motion, we inject the physical identity into the model through video fine-tuning. For higher-resolution outputs with finer details, a patch-based denoising algorithm is employed. Experimental results demonstrate that our method outperforms recent state-of-the-art approaches and validate the effectiveness of our proposed strategies.</li>
</ul>

<h3>Title: Metis: Training Large Language Models with Advanced Low-Bit Quantization</h3>
<ul>
<li><strong>Authors: </strong>Hengjie Cao, Mengyi Chen, Yifeng Yang, Ruijun Huang, Fang Dong, Jixian Zhou, Anrui Chen, Mingzhi Dong, Yujiang Wang, Jinlong Hou, Yuan Cheng, Fan Wu, Fan Yang, Tun Lu, Ning Gu, Li Shang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00404">https://arxiv.org/abs/2509.00404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00404">https://arxiv.org/pdf/2509.00404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00404]] Metis: Training Large Language Models with Advanced Low-Bit Quantization(https://arxiv.org/abs/2509.00404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: this https URL.</li>
</ul>

<h3>Title: MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature</h3>
<ul>
<li><strong>Authors: </strong>Juraj Vladika, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00414">https://arxiv.org/abs/2509.00414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00414">https://arxiv.org/pdf/2509.00414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00414]] MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature(https://arxiv.org/abs/2509.00414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the digital age, people often turn to the Internet in search of medical advice and recommendations. With the increasing volume of online content, it has become difficult to distinguish reliable sources from misleading information. Similarly, millions of medical studies are published every year, making it challenging for researchers to keep track of the latest scientific findings. These evolving studies can reach differing conclusions, which is not reflected in traditional search tools. To address these challenges, we introduce MedSEBA, an interactive AI-powered system for synthesizing evidence-based answers to medical questions. It utilizes the power of Large Language Models to generate coherent and expressive answers, but grounds them in trustworthy medical studies dynamically retrieved from the research database PubMed. The answers consist of key points and arguments, which can be traced back to respective studies. Notably, the platform also provides an overview of the extent to which the most relevant studies support or refute the given medical claim, and a visualization of how the research consensus evolved through time. Our user study revealed that medical experts and lay users find the system usable and helpful, and the provided answers trustworthy and informative. This makes the system well-suited for both everyday health questions and advanced research insights.</li>
</ul>

<h3>Title: Memory Limitations of Prompt Tuning in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Maxime Meyer, Mario Michelessa, Caroline Chaux, Vincent Y. F. Tan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00421">https://arxiv.org/abs/2509.00421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00421">https://arxiv.org/pdf/2509.00421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00421]] Memory Limitations of Prompt Tuning in Transformers(https://arxiv.org/abs/2509.00421)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite the empirical success of prompt tuning in adapting pretrained language models to new tasks, theoretical analyses of its capabilities remain limited. Existing theoretical work primarily addresses universal approximation properties, demonstrating results comparable to standard weight tuning. In this paper, we explore a different aspect of the theory of transformers: the memorization capability of prompt tuning. We provide two principal theoretical contributions. First, we prove that the amount of information memorized by a transformer cannot scale faster than linearly with the prompt length. Second, and more importantly, we present the first formal proof of a phenomenon empirically observed in large language models: performance degradation in transformers with extended contexts. We rigorously demonstrate that transformers inherently have limited memory, constraining the amount of information they can retain, regardless of the context size. This finding offers a fundamental understanding of the intrinsic limitations of transformer architectures, particularly their ability to handle long sequences.</li>
</ul>

<h3>Title: The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang</h3>
<ul>
<li><strong>Authors: </strong>Fenghua Liu, Yulong Chen, Yixuan Liu, Zhujun Jin, Solomon Tsai, Ming Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00425">https://arxiv.org/abs/2509.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00425">https://arxiv.org/pdf/2509.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00425]] The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang(https://arxiv.org/abs/2509.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) achieve gold-medal performance across many benchmarks, yet it remains unclear whether such success reflects genuine reasoning or pattern matching. From a cognitive science perspective, an informative test is whether models can master an unfamiliar language through explicit metalinguistic deductive learning, a paradigm where human learners can reliably internalise grammatical systems through metalinguistic reasoning. We address this question with Camlang, a novel constructed language that exhibits naturalistic yet unattested feature combinations. Camlang consists of two explicit resources, a grammar book and a bilingual dictionary, which mirror adult second-language learning via explicit grammar rules and lexical lookup, and enable us to disentangle errors in morpho-syntax, lexical semantics, and sentence-level reasoning. Human experiments show that these resources are sufficient for participants to acquire Camlang and successfully solve Camlang tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang, creating Camlang-CSQA-v0, the first task in a broader suite where solving questions requires applying grammar rules and lexical mappings. Experimental results show that GPT-5 achieves 98\% EM accuracy in English but only 47\% in Camlang, far below human performance at 87\%, while other state-of-the-art reasoning LLMs perform even worse. Human verification further reveals that most model successes stem from shallow lexical alignment while GPT-5 shows emerging metalinguistic awareness to a limited extent but not systematic grammatical mastery as humans. Camlang establishes a cognitively grounded evaluation paradigm that exposes fundamental gaps between current models and human metalinguistic competence.</li>
</ul>

<h3>Title: Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Xuechao Zou, Shun Zhang, Xing Fu, Yue Li, Kai Li, Yushe Cao, Congyan Lang, Pin Tao, Junliang Xing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00428">https://arxiv.org/abs/2509.00428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00428">https://arxiv.org/pdf/2509.00428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00428]] Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation(https://arxiv.org/abs/2509.00428)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at this https URL.</li>
</ul>

<h3>Title: A Hybrid AI-based and Rule-based Approach to DICOM De-identification: A Solution for the MIDI-B Challenge</h3>
<ul>
<li><strong>Authors: </strong>Hamideh Haghiri, Rajesh Baidya, Stefan Dvoretskii, Klaus H. Maier-Hein, Marco Nolden</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00437">https://arxiv.org/abs/2509.00437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00437">https://arxiv.org/pdf/2509.00437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00437]] A Hybrid AI-based and Rule-based Approach to DICOM De-identification: A Solution for the MIDI-B Challenge(https://arxiv.org/abs/2509.00437)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Ensuring the de-identification of medical imaging data is a critical step in enabling safe data sharing. This paper presents a hybrid de-identification framework designed to process Digital Imaging and Communications in Medicine (DICOM) files. Our framework adopts a modified, pre-built rule-based component, updated with The Cancer Imaging Archive (TCIA)'s best practices guidelines, as outlined in DICOM PS 3.15, for improved performance. It incorporates PaddleOCR, a robust Optical Character Recognition (OCR) system for extracting text from images, and RoBERTa, a fine-tuned transformer-based model for identifying and removing Personally Identifiable Information (PII) and Protected Health Information (PHI). Initially, the transformer-based model and the rule-based component were integrated to process for both structured data and free text. However, this coarse-grained approach did not yield optimal results. To improve performance, we refined our approach by applying the transformer model exclusively to free text, while structured data was handled only by rule-based methods. In this framework the DICOM validator dciodvfy was leveraged to ensure the integrity of DICOM files after the deID process. Through iterative refinement, including the incorporation of custom rules and private tag handling, the framework achieved a de-identification accuracy of 99.91% on the MIDI-B test dataset. The results demonstrate the effectiveness of combining rule-based compliance with AI-enabled adaptability in addressing the complex challenges of DICOM de-identification.</li>
</ul>

<h3>Title: SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Lubin Gan, Xiaoman Wu, Jing Zhang, Zhifeng Wang, Linhao Qu, Siying Wu, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00442">https://arxiv.org/abs/2509.00442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00442">https://arxiv.org/pdf/2509.00442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00442]] SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification(https://arxiv.org/abs/2509.00442)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Multiple instance learning (MIL) has become the leading approach for extracting discriminative features from whole slide images (WSIs) in computational pathology. Attention-based MIL methods can identify key patches but tend to overlook contextual relationships. Transformer models are able to model interactions but require quadratic computational cost and are prone to overfitting. State space models (SSMs) offer linear complexity, yet shuffling patch order disrupts histological meaning and reduces interpretability. In this work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an adaptive method that clusters and arranges semantically similar patches in sequence through a reversible permutation, with a Semantic-guided Retrieval State Space Module (SRSM) that chooses a representative subset of queries to adjust state space parameters for improved global modeling. Evaluation on four WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves state-of-the-art accuracy with fewer FLOPs and parameters.</li>
</ul>

<h3>Title: GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework</h3>
<ul>
<li><strong>Authors: </strong>Xuecheng Zou, Ke Liu, Bingbing Wang, Huafei Deng, Li Zhang, Yu Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00449">https://arxiv.org/abs/2509.00449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00449">https://arxiv.org/pdf/2509.00449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00449]] GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework(https://arxiv.org/abs/2509.00449)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Building upon the standard graph-based Retrieval-Augmented Generation (RAG), the introduction of heterogeneous graphs and hypergraphs aims to enrich retrieval and generation by leveraging the relationships between multiple entities through the concept of semantic units (SUs). But this also raises a key issue: The extraction of high-level SUs limited to local text chunks is prone to ambiguity, complex coupling, and increased retrieval overhead due to the lack of global knowledge or the neglect of fine-grained relationships. To address these issues, we propose GOSU, a semantic unit-centric RAG framework that efficiently performs global disambiguation and utilizes SUs to capture interconnections between different nodes across the global context. In the graph construction phase, GOSU performs global merging on the pre-extracted SUs from local text chunks and guides entity and relationship extraction, reducing the difficulty of coreference resolution while uncovering global semantic objects across text chunks. In the retrieval and generation phase, we introduce hierarchical keyword extraction and semantic unit completion. The former uncovers the fine-grained binary relationships overlooked by the latter, while the latter compensates for the coarse-grained n-ary relationships missing from the former. Evaluation across multiple tasks demonstrates that GOSU outperforms the baseline RAG methods in terms of generation quality.</li>
</ul>

<h3>Title: Stage-wise Adaptive Label Distribution for Facial Age Estimation</h3>
<ul>
<li><strong>Authors: </strong>Bo Wu, Zhiqi Ai, Jun Jiang, Congcong Zhu, Shugong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00450">https://arxiv.org/abs/2509.00450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00450">https://arxiv.org/pdf/2509.00450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00450]] Stage-wise Adaptive Label Distribution for Facial Age Estimation(https://arxiv.org/abs/2509.00450)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Label ambiguity poses a significant challenge in age estimation tasks. Most existing methods address this issue by modeling correlations between adjacent age groups through label distribution learning. However, they often overlook the varying degrees of ambiguity present across different age stages. In this paper, we propose a Stage-wise Adaptive Label Distribution Learning (SA-LDL) algorithm, which leverages the observation -- revealed through our analysis of embedding similarities between an anchor and all other ages -- that label ambiguity exhibits clear stage-wise patterns. By jointly employing stage-wise adaptive variance modeling and weighted loss function, SA-LDL effectively captures the complex and structured nature of label ambiguity, leading to more accurate and robust age estimation. Extensive experiments demonstrate that SA-LDL achieves competitive performance, with MAE of 1.74 and 2.15 on the MORPH-II and FG-NET datasets.</li>
</ul>

<h3>Title: Encoder-Only Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Xiang Chen, Renjiu Hu, Jinwei Zhang, Yuxi Zhang, Xinyao Yue, Min Liu, Yaonan Wang, Hang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00451">https://arxiv.org/abs/2509.00451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00451">https://arxiv.org/pdf/2509.00451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00451]] Encoder-Only Image Registration(https://arxiv.org/abs/2509.00451)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Learning-based techniques have significantly improved the accuracy and speed of deformable image registration. However, challenges such as reducing computational complexity and handling large deformations persist. To address these challenges, we analyze how convolutional neural networks (ConvNets) influence registration performance using the Horn-Schunck optical flow equation. Supported by prior studies and our empirical experiments, we observe that ConvNets play two key roles in registration: linearizing local intensities and harmonizing global contrast variations. Based on these insights, we propose the Encoder-Only Image Registration (EOIR) framework, designed to achieve a better accuracy-efficiency trade-off. EOIR separates feature learning from flow estimation, employing only a 3-layer ConvNet for feature extraction and a set of 3-layer flow estimators to construct a Laplacian feature pyramid, progressively composing diffeomorphic deformations under a large-deformation model. Results on five datasets across different modalities and anatomical regions demonstrate EOIR's effectiveness, achieving superior accuracy-efficiency and accuracy-smoothness trade-offs. With comparable accuracy, EOIR provides better efficiency and smoothness, and vice versa. The source code of EOIR will be publicly available on this https URL.</li>
</ul>

<h3>Title: Universal Properties of Activation Sparsity in Modern Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Filip Szatkowski, Patryk Bƒôdkowski, Alessio Devoto, Jan Dubi≈Ñski, Pasquale Minervini, Miko≈Çaj Pi√≥rczy≈Ñski, Simone Scardapane, Bartosz W√≥jcik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00454">https://arxiv.org/abs/2509.00454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00454">https://arxiv.org/pdf/2509.00454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00454]] Universal Properties of Activation Sparsity in Modern Large Language Models(https://arxiv.org/abs/2509.00454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Input-dependent activation sparsity is a notable property of deep learning models, which has been extensively studied in networks with ReLU activations and is associated with efficiency, robustness, and interpretability. However, the approaches developed for ReLU-based models depend on exact zero activations and do not transfer directly to modern large language models~(LLMs), which have abandoned ReLU in favor of other activation functions. As a result, current work on activation sparsity in LLMs is fragmented, model-specific, and lacks consensus on which components to target. We propose a general framework to assess sparsity robustness and present a systematic study of the phenomenon in the FFN layers of modern LLMs, including diffusion LLMs. Our findings reveal universal patterns of activation sparsity in LLMs, provide insights into this phenomenon, and offer practical guidelines for exploiting it in model design and acceleration.</li>
</ul>

<h3>Title: CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Salah Eddine Bekhouche, Abdellah Zakaria Sellam, Hichem Telli, Cosimo Distante, Abdenour Hadid</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00457">https://arxiv.org/abs/2509.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00457">https://arxiv.org/pdf/2509.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00457]] CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning(https://arxiv.org/abs/2509.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Islamic inheritance law (Ilm al-Mawarith) requires precise identification of heirs and calculation of shares, which poses a challenge for AI. In this paper, we present a lightweight framework for solving multiple-choice inheritance questions using a specialised Arabic text encoder and Attentive Relevance Scoring (ARS). The system ranks answer options according to semantic relevance, and enables fast, on-device inference without generative reasoning. We evaluate Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an accuracy of up to 87.6%, they require more resources and are context-dependent. Our MARBERT-based approach achieves 69.87% accuracy, presenting a compelling case for efficiency, on-device deployability, and privacy. While this is lower than the 87.6% achieved by the best-performing LLM, our work quantifies a critical trade-off between the peak performance of large models and the practical advantages of smaller, specialized systems in high-stakes domains.</li>
</ul>

<h3>Title: TECP: Token-Entropy Conformal Prediction for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Beining Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00461">https://arxiv.org/abs/2509.00461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00461">https://arxiv.org/pdf/2509.00461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00461]] TECP: Token-Entropy Conformal Prediction for LLMs(https://arxiv.org/abs/2509.00461)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) for open-ended language generation remains a critical yet underexplored challenge, especially under black-box constraints where internal model signals are inaccessible. In this paper, we introduce Token-Entropy Conformal Prediction (TECP), a novel framework that leverages token-level entropy as a logit-free, reference-free uncertainty measure and integrates it into a split conformal prediction (CP) pipeline to construct prediction sets with formal coverage guarantees. Unlike existing approaches that rely on semantic consistency heuristics or white-box features, TECP directly estimates epistemic uncertainty from the token entropy structure of sampled generations and calibrates uncertainty thresholds via CP quantiles to ensure provable error control. Empirical evaluations across six large language models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP consistently achieves reliable coverage and compact prediction sets, outperforming prior self-consistency-based UQ methods. Our method provides a principled and efficient solution for trustworthy generation in black-box LLM settings.</li>
</ul>

<h3>Title: Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models</h3>
<ul>
<li><strong>Authors: </strong>Omar Khalid Ali Mohamed</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00476">https://arxiv.org/abs/2509.00476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00476">https://arxiv.org/pdf/2509.00476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00476]] Cross-Domain Malware Detection via Probability-Level Fusion of Lightweight Gradient Boosting Models(https://arxiv.org/abs/2509.00476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The escalating sophistication of malware necessitates robust detection mechanisms that generalize across diverse data sources. Traditional single-dataset models struggle with cross-domain generalization and often incur high computational costs. This paper presents a novel, lightweight framework for malware detection that employs probability-level fusion across three distinct datasets: EMBER (static features), API Call Sequences (behavioral features), and CIC Obfuscated Memory (memory patterns). Our method trains individual LightGBM classifiers on each dataset, selects top predictive features to ensure efficiency, and fuses their prediction probabilities using optimized weights determined via grid search. Extensive experiments demonstrate that our fusion approach achieves a macro F1-score of 0.823 on a cross-domain validation set, significantly outperforming individual models and providing superior generalization. The framework maintains low computational overhead, making it suitable for real-time deployment, and all code and data are provided for full reproducibility.</li>
</ul>

<h3>Title: Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</h3>
<ul>
<li><strong>Authors: </strong>Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00482">https://arxiv.org/abs/2509.00482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00482">https://arxiv.org/pdf/2509.00482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00482]] Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting(https://arxiv.org/abs/2509.00482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at this https URL.</li>
</ul>

<h3>Title: VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00484">https://arxiv.org/abs/2509.00484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00484">https://arxiv.org/pdf/2509.00484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00484]] VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding(https://arxiv.org/abs/2509.00484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal reward models (MRMs) play a crucial role in the training, inference, and evaluation of Large Vision Language Models (LVLMs) by assessing response quality. However, existing benchmarks for evaluating MRMs in the video domain suffer from a limited number and diversity of questions, a lack of comprehensive evaluation dimensions, and inadequate evaluation of diverse types of MRMs. To address these gaps, we introduce VideoRewardBench, the first comprehensive benchmark covering four core aspects of video understanding: perception, knowledge, reasoning, and safety. Through our AI-assisted data pipeline, we curate a high-quality preference dataset of 1,563 annotated samples, including 1,482 unique videos and 1,559 distinct questions--15 times the number found in the most question-rich prior benchmark. Each sample is a triplet consisting of a video-text prompt, a chosen response, and a rejected response. We also conduct a comprehensive evaluation across 28 multimodal reward models spanning three categories: generative, discriminative, and semi-scalar. Results show that even the top-performing model GPT-4o achieves only 57.0% overall accuracy, and the state-of-the-art open-source model Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL; (ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling; and (iii) variations in input video frame count have different effects on different types of MRMs. We believe VideoRewardBench offers a challenging and valuable benchmark for advancing the evaluation and development of MRMs in the video domain.</li>
</ul>

<h3>Title: Localizing and Mitigating Memorization in Image Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kasliwal, Franziska Boenisch, Adam Dziedzic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00488">https://arxiv.org/abs/2509.00488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00488">https://arxiv.org/pdf/2509.00488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00488]] Localizing and Mitigating Memorization in Image Autoregressive Models(https://arxiv.org/abs/2509.00488)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, generative</a></li>
<li><strong>Abstract: </strong>Image AutoRegressive (IAR) models have achieved state-of-the-art performance in speed and quality of generated images. However, they also raise concerns about memorization of their training data and its implications for privacy. This work explores where and how such memorization occurs within different image autoregressive architectures by measuring a fine-grained memorization. The analysis reveals that memorization patterns differ across various architectures of IARs. In hierarchical per-resolution architectures, it tends to emerge early and deepen with resolutions, while in IARs with standard autoregressive per token prediction, it concentrates in later processing stages. These localization of memorization patterns are further connected to IARs' ability to memorize and leak training data. By intervening on their most memorizing components, we significantly reduce the capacity for data extraction from IARs with minimal impact on the quality of generated images. These findings offer new insights into the internal behavior of image generative models and point toward practical strategies for mitigating privacy risks.</li>
</ul>

<h3>Title: Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yasser Benigmim, Subhankar Roy, Khalid Oublal, Imad Eddine Marouf, Slim Essid, Vicky Kalogeiton, St√©phane Lathuili√®re</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00509">https://arxiv.org/abs/2509.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00509">https://arxiv.org/pdf/2509.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00509]] Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation(https://arxiv.org/abs/2509.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The rise of Artificial Intelligence as a Service (AIaaS) democratizes access to pre-trained models via Application Programming Interfaces (APIs), but also raises a fundamental question: how can local models be effectively trained using black-box models that do not expose their weights, training data, or logits, a constraint in which current domain adaptation paradigms are impractical ? To address this challenge, we introduce the Black-Box Distillation (B2D) setting, which enables local model adaptation under realistic constraints: (1) the API model is open-vocabulary and trained on large-scale general-purpose data, and (2) access is limited to one-hot predictions only. We identify that open-vocabulary models exhibit significant sensitivity to input resolution, with different object classes being segmented optimally at different scales, a limitation termed the "curse of resolution". Our method, ATtention-Guided sCaler (ATGC), addresses this challenge by leveraging DINOv2 attention maps to dynamically select optimal scales for black-box model inference. ATGC scores the attention maps with entropy to identify informative scales for pseudo-labelling, enabling effective distillation. Experiments demonstrate substantial improvements under black-box supervision across multiple datasets while requiring only one-hot API predictions. Our code is available at this https URL.</li>
</ul>

<h3>Title: Biological Pathway Informed Models with Graph Attention Networks (GATs)</h3>
<ul>
<li><strong>Authors: </strong>Gavin Wong, Ping Shu Ho, Ivan Au Yeung, Ka Chun Cheung, Simon See</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00524">https://arxiv.org/abs/2509.00524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00524">https://arxiv.org/pdf/2509.00524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00524]] Biological Pathway Informed Models with Graph Attention Networks (GATs)(https://arxiv.org/abs/2509.00524)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Biological pathways map gene-gene interactions that govern all human processes. Despite their importance, most ML models treat genes as unstructured tokens, discarding known pathway structure. The latest pathway-informed models capture pathway-pathway interactions, but still treat each pathway as a "bag of genes" via MLPs, discarding its topology and gene-gene interactions. We propose a Graph Attention Network (GAT) framework that models pathways at the gene level. We show that GATs generalize much better than MLPs, achieving an 81% reduction in MSE when predicting pathway dynamics under unseen treatment conditions. We further validate the correctness of our biological prior by encoding drug mechanisms via edge interventions, boosting model robustness. Finally, we show that our GAT model is able to correctly rediscover all five gene-gene interactions in the canonical TP53-MDM2-MDM4 feedback loop from raw time-series mRNA data, demonstrating potential to generate novel biological hypotheses directly from experimental data.</li>
</ul>

<h3>Title: Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Ruitao Wu, Yifan Zhao, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00527">https://arxiv.org/abs/2509.00527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00527">https://arxiv.org/pdf/2509.00527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00527]] Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement(https://arxiv.org/abs/2509.00527)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Class-Incremental Semantic Segmentation (CISS) requires continuous learning of newly introduced classes while retaining knowledge of past classes. By abstracting mainstream methods into two stages (visual feature extraction and prototype-feature matching), we identify a more fundamental challenge termed catastrophic semantic entanglement. This phenomenon involves Prototype-Feature Entanglement caused by semantic misalignment during the incremental process, and Background-Increment Entanglement due to dynamic data evolution. Existing techniques, which rely on visual feature learning without sufficient cues to distinguish targets, introduce significant noise and errors. To address these issues, we introduce a Language-inspired Bootstrapped Disentanglement framework (LBD). We leverage the prior class semantics of pre-trained visual-language models (e.g., CLIP) to guide the model in autonomously disentangling features through Language-guided Prototypical Disentanglement and Manifold Mutual Background Disentanglement. The former guides the disentangling of new prototypes by treating hand-crafted text features as topological templates, while the latter employs multiple learnable prototypes and mask-pooling-based supervision for background-incremental class disentanglement. By incorporating soft prompt tuning and encoder adaptation modifications, we further bridge the capability gap of CLIP between dense and sparse tasks, achieving state-of-the-art performance on both Pascal VOC and ADE20k, particularly in multi-step scenarios.</li>
</ul>

<h3>Title: Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization</h3>
<ul>
<li><strong>Authors: </strong>Eunjung Cho, Alexander Hoyle, Yoan Hermstr√ºwer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00529">https://arxiv.org/abs/2509.00529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00529">https://arxiv.org/pdf/2509.00529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00529]] Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization(https://arxiv.org/abs/2509.00529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to generate user-tailored summaries, adapting outputs to specific stakeholders. In legal contexts, this raises important questions about motivated reasoning -- how models strategically frame information to align with a stakeholder's position within the legal system. Building on theories of legal realism and recent trends in legal practice, we investigate how LLMs respond to prompts conditioned on different legal roles (e.g., judges, prosecutors, attorneys) when summarizing judicial decisions. We introduce an evaluation framework grounded in legal fact and reasoning inclusion, also considering favorability towards stakeholders. Our results show that even when prompts include balancing instructions, models exhibit selective inclusion patterns that reflect role-consistent perspectives. These findings raise broader concerns about how similar alignment may emerge as LLMs begin to infer user roles from prior interactions or context, even without explicit role instructions. Our results underscore the need for role-aware evaluation of LLM summarization behavior in high-stakes legal settings.</li>
</ul>

<h3>Title: FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Zhang, Mang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00540">https://arxiv.org/abs/2509.00540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00540">https://arxiv.org/pdf/2509.00540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00540]] FedThief: Harming Others to Benefit Oneself in Self-Centered Federated Learning(https://arxiv.org/abs/2509.00540)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>In federated learning, participants' uploaded model updates cannot be directly verified, leaving the system vulnerable to malicious attacks. Existing attack strategies have adversaries upload tampered model updates to degrade the global model's performance. However, attackers also degrade their own private models, gaining no advantage. In real-world scenarios, attackers are driven by self-centered motives: their goal is to gain a competitive advantage by developing a model that outperforms those of other participants, not merely to cause disruption. In this paper, we study a novel Self-Centered Federated Learning (SCFL) attack paradigm, in which attackers not only degrade the performance of the global model through attacks but also enhance their own models within the federated learning process. We propose a framework named FedThief, which degrades the performance of the global model by uploading modified content during the upload stage. At the same time, it enhances the private model's performance through divergence-aware ensemble techniques, where "divergence" quantifies the deviation between private and global models, that integrate global updates and local knowledge. Extensive experiments show that our method effectively degrades the global model performance while allowing the attacker to obtain an ensemble model that significantly outperforms the global model.</li>
</ul>

<h3>Title: Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hanqi Yan, Hainiu Xu, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00544">https://arxiv.org/abs/2509.00544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00544">https://arxiv.org/pdf/2509.00544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00544]] Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs(https://arxiv.org/abs/2509.00544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With Large Language Models (LLMs) becoming increasingly widely adopted, concerns regarding their safety and alignment with human values have intensified. Previous studies have shown that fine-tuning LLMs on narrow and malicious datasets induce misaligned behaviors. In this work, we report a more concerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe that LLMs become more responsive to malicious requests when reasoning is strengthened, via switching to "think-mode" or fine-tuning on benign math datasets, with dense models particularly vulnerable. Moreover, we analyze internal model states and find that both attention shifts and specialized experts in mixture-of-experts models help redirect excessive reasoning towards safety guardrails. These findings provide new insights into the emerging reasoning-safety trade-off and underscore the urgency of advancing alignment for advanced reasoning models.</li>
</ul>

<h3>Title: Advanced spectral clustering for heterogeneous data in credit risk monitoring systems</h3>
<ul>
<li><strong>Authors: </strong>Lu Han, Mengyan Li, Jiping Qiang, Zhi Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00546">https://arxiv.org/abs/2509.00546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00546">https://arxiv.org/pdf/2509.00546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00546]] Advanced spectral clustering for heterogeneous data in credit risk monitoring systems(https://arxiv.org/abs/2509.00546)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Heterogeneous data, which encompass both numerical financial variables and textual records, present substantial challenges for credit monitoring. To address this issue, we propose Advanced Spectral Clustering (ASC), a method that integrates financial and textual similarities through an optimized weight parameter and selects eigenvectors using a novel eigenvalue-silhouette optimization approach. Evaluated on a dataset comprising 1,428 small and medium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18% higher than that of a single-type data baseline method. Furthermore, the resulting clusters offer actionable insights; for instance, 51% of low-risk firms are found to include the term 'social recruitment' in their textual records. The robustness of ASC is confirmed across multiple clustering algorithms, including k-means, k-medians, and k-medoids, with {\Delta}Intra/Inter < 0.13 and {\Delta}Silhouette Coefficient < 0.02. By bridging spectral clustering theory with heterogeneous data applications, ASC enables the identification of meaningful clusters, such as recruitment-focused SMEs exhibiting a 30% lower default risk, thereby supporting more targeted and effective credit interventions.</li>
</ul>

<h3>Title: A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging</h3>
<ul>
<li><strong>Authors: </strong>Peirong Liu, Oula Puonti, Xiaoling Hu, Karthik Gopinath, Annabel Sorby-Adams, Daniel C. Alexander, W. Taylor Kimberly, Juan E. Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00549">https://arxiv.org/abs/2509.00549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00549">https://arxiv.org/pdf/2509.00549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00549]] A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging(https://arxiv.org/abs/2509.00549)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recent learning-based approaches have made astonishing advances in calibrated medical imaging like computerized tomography (CT), yet they struggle to generalize in uncalibrated modalities -- notably magnetic resonance (MR) imaging, where performance is highly sensitive to the differences in MR contrast, resolution, and orientation. This prevents broad applicability to diverse real-world clinical protocols. Here we introduce BrainFM, a modality-agnostic, multi-task vision foundation model for human brain imaging. With the proposed "mild-to-severe" intra-subject generation and "real-synth" mix-up training strategy, BrainFM is resilient to the appearance of acquired images (e.g., modality, contrast, deformation, resolution, artifacts), and can be directly applied to five fundamental brain imaging tasks, including image synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical distance, bias field estimation, and registration. We evaluate the efficacy of BrainFM on eleven public datasets, and demonstrate its robustness and effectiveness across all tasks and input modalities. Code is available at this https URL.</li>
</ul>

<h3>Title: Integrated Multivariate Segmentation Tree for the Analysis of Heterogeneous Credit Data in Small and Medium-Sized Enterprises</h3>
<ul>
<li><strong>Authors: </strong>Lu Han, Xiuying Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00550">https://arxiv.org/abs/2509.00550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00550">https://arxiv.org/pdf/2509.00550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00550]] Integrated Multivariate Segmentation Tree for the Analysis of Heterogeneous Credit Data in Small and Medium-Sized Enterprises(https://arxiv.org/abs/2509.00550)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Traditional decision tree models, which rely exclusively on numerical variables, often encounter difficulties in handling high-dimensional data and fail to effectively incorporate textual information. To address these limitations, we propose the Integrated Multivariate Segmentation Tree (IMST), a comprehensive framework designed to enhance credit evaluation for small and medium-sized enterprises (SMEs) by integrating financial data with textual sources. The methodology comprises three core stages: (1) transforming textual data into numerical matrices through matrix factorization; (2) selecting salient financial features using Lasso regression; and (3) constructing a multivariate segmentation tree based on the Gini index or Entropy, with weakest-link pruning applied to regulate model complexity. Experimental results derived from a dataset of 1,428 Chinese SMEs demonstrate that IMST achieves an accuracy of 88.9%, surpassing baseline decision trees (87.4%) as well as conventional models such as logistic regression and support vector machines (SVM). Furthermore, the proposed model exhibits superior interpretability and computational efficiency, featuring a more streamlined architecture and enhanced risk detection capabilities.</li>
</ul>

<h3>Title: FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks</h3>
<ul>
<li><strong>Authors: </strong>Yuwen Pu, Zhou Feng, Chunyi Zhou, Jiahao Chen, Chunqiang Hu, Haibo Hu, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00561">https://arxiv.org/abs/2509.00561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00561">https://arxiv.org/pdf/2509.00561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00561]] FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks(https://arxiv.org/abs/2509.00561)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Recently, speech assistant and speech verification have been used in many fields, which brings much benefit and convenience for us. However, when we enjoy these speech applications, our speech may be collected by attackers for speech synthesis. For example, an attacker generates some inappropriate political opinions with the characteristic of the victim's voice by obtaining a piece of the victim's speech, which will greatly influence the victim's reputation. Specifically, with the appearance of some zero-shot voice conversion methods, the cost of speech synthesis attacks has been further reduced, which also brings greater challenges to user voice security and privacy. Some researchers have proposed the corresponding privacy-preserving methods. However, the existing approaches have some non-negligible drawbacks: low transferability and robustness, high computational overhead. These deficiencies seriously limit the existing method deployed in practical scenarios. Therefore, in this paper, we propose a lightweight, robust, plug-and-play privacy preservation method against speech synthesis attacks in a black-box setting. Our method generates and adds a frequency-domain perturbation to the original speech to achieve privacy protection and high speech quality. Then, we present a data augmentation strategy and noise smoothing mechanism to improve the robustness of the proposed method. Besides, to reduce the user's defense overhead, we also propose a novel identity-wise protection mechanism. It can generate a universal perturbation for one speaker and support privacy preservation for speech of any length. Finally, we conduct extensive experiments on 5 speech synthesis models, 5 speech verification models, 1 speech recognition model, and 2 datasets. The experimental results demonstrate that our method has satisfying privacy-preserving performance, high speech quality, and utility.</li>
</ul>

<h3>Title: C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Abdellah Zakaria Sellam, Ilyes Benaissa, Salah Eddine Bekhouche, Abdenour Hadid, Vito Ren√≥, Cosimo Distante</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00578">https://arxiv.org/abs/2509.00578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00578">https://arxiv.org/pdf/2509.00578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00578]] C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection(https://arxiv.org/abs/2509.00578)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fine-grained object detection in challenging visual domains, such as vehicle damage assessment, presents a formidable challenge even for human experts to resolve reliably. While DiffusionDet has advanced the state-of-the-art through conditional denoising diffusion, its performance remains limited by local feature conditioning in context-dependent scenarios. We address this fundamental limitation by introducing Context-Aware Fusion (CAF), which leverages cross-attention mechanisms to integrate global scene context with local proposal features directly. The global context is generated using a separate dedicated encoder that captures comprehensive environmental information, enabling each object proposal to attend to scene-level understanding. Our framework significantly enhances the generative detection paradigm by enabling each object proposal to attend to comprehensive environmental information. Experimental results demonstrate an improvement over state-of-the-art models on the CarDD benchmark, establishing new performance benchmarks for context-aware object detection in fine-grained domains</li>
</ul>

<h3>Title: StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Lang Xiong, Nishant Bhargava, Wesley Chang, Jianhang Hong, Haihao Liu, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00591">https://arxiv.org/abs/2509.00591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00591">https://arxiv.org/pdf/2509.00591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00591]] StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks(https://arxiv.org/abs/2509.00591)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as "evaluation awareness." This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model's true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from "test-like" to "deploy-like" and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten "deploy-like" prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment.</li>
</ul>

<h3>Title: DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Boyi Li, Ce Zhang, Richard M. Timmerman, Wenxuan Bao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00598">https://arxiv.org/abs/2509.00598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00598">https://arxiv.org/pdf/2509.00598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00598]] DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation(https://arxiv.org/abs/2509.00598)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The emergence of vision language models (VLMs) has bridged vision and language, enabling joint multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the limited category diversity in RS datasets and the domain gap between natural and RS imagery. Here, we propose a training-free framework, DGL-RSIS, that decouples visual and textual inputs, performing visual-language alignment at both the local semantic and global contextual levels through tailored strategies. Specifically, we first introduce a global-local decoupling (GLD) module, where text inputs are divided into local class nouns and global modifiers using natural language processing (NLP) techniques; image inputs are partitioned into a set of class-agnostic mask proposals via unsupervised mask proposal networks. Second, visual and textual features are aligned at local scale, through a novel context-aware cropping strategy for extracting image patches with proper boundaries and introducing RS-specific knowledge to enrich the text inputs. By matching the enhanced text features with mask-guided visual features, we enable the mask classification, supporting open-vocabulary semantic segmentation (OVSS). Third, at the global scale, we propose a Cross-Scale Grad-CAM module to refine Grad-CAM maps using contextual information from global modifiers. A subsequent mask selection module integrates pixel-level Grad-CAM activations into the mask-level segmentation output, such that accurate and interpretable alignment can be realized across global and local dimensions for referring expression segmentation (RES).</li>
</ul>

<h3>Title: TranCIT: Transient Causal Interaction Toolbox</h3>
<ul>
<li><strong>Authors: </strong>Salar Nouri, Kaidi Shao, Shervin Safavi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00602">https://arxiv.org/abs/2509.00602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00602">https://arxiv.org/pdf/2509.00602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00602]] TranCIT: Transient Causal Interaction Toolbox(https://arxiv.org/abs/2509.00602)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Quantifying transient causal interactions from non-stationary neural signals is a fundamental challenge in neuroscience. Traditional methods are often inadequate for brief neural events, and advanced, event-specific techniques have lacked accessible implementations within the Python ecosystem. Here, we introduce trancit (Transient Causal Interaction Toolbox), an open-source Python package designed to bridge this gap. TranCIT implements a comprehensive analysis pipeline, including Granger Causality, Transfer Entropy, and the more robust Structural Causal Model-based Dynamic Causal Strength (DCS) and relative Dynamic Causal Strength (rDCS) for accurately detecting event-driven causal effects. We demonstrate TranCIT's utility by successfully capturing causality in high-synchrony regimes where traditional methods fail and by identifying the known transient information flow from hippocampal CA3 to CA1 during sharp-wave ripple events in real-world data. The package offers a user-friendly, validated solution for investigating the transient causal dynamics that govern complex systems.</li>
</ul>

<h3>Title: Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Rishiraj Acharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00605">https://arxiv.org/abs/2509.00605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00605">https://arxiv.org/pdf/2509.00605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00605]] Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling(https://arxiv.org/abs/2509.00605)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.</li>
</ul>

<h3>Title: RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Shikun Liu, Deyu Zou, Nima Shoghi, Victor Fung, Kai Liu, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00614">https://arxiv.org/abs/2509.00614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00614">https://arxiv.org/pdf/2509.00614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00614]] RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models(https://arxiv.org/abs/2509.00614)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the era of foundation models, fine-tuning pre-trained models for specific downstream tasks has become crucial. This drives the need for robust fine-tuning methods to address challenges such as model overfitting and sparse labeling. Molecular graph foundation models (MGFMs) face unique difficulties that complicate fine-tuning. These models are limited by smaller pre-training datasets and more severe data scarcity for downstream tasks, both of which require enhanced model generalization. Moreover, MGFMs must accommodate diverse objectives, including both regression and classification tasks. To better understand and improve fine-tuning techniques under these conditions, we classify eight fine-tuning methods into three mechanisms: weight-based, representation-based, and partial fine-tuning. We benchmark these methods on downstream regression and classification tasks across supervised and self-supervised pre-trained models in diverse labeling settings. This extensive evaluation provides valuable insights and informs the design of a refined robust fine-tuning method, ROFT-MOL. This approach combines the strengths of simple post-hoc weight interpolation with more complex weight ensemble fine-tuning methods, delivering improved performance across both task types while maintaining the ease of use inherent in post-hoc weight interpolation.</li>
</ul>

<h3>Title: Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves</h3>
<ul>
<li><strong>Authors: </strong>Narasimha Raghavan Veeraragavan, Jan Franz Nyg√•rd</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00615">https://arxiv.org/abs/2509.00615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00615">https://arxiv.org/pdf/2509.00615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00615]] Federated Survival Analysis with Node-Level Differential Privacy: Private Kaplan-Meier Curves(https://arxiv.org/abs/2509.00615)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>We investigate how to calculate Kaplan-Meier survival curves across multiple health-care jurisdictions while protecting patient privacy with node-level differential privacy. Each site discloses its curve only once, adding Laplace noise whose scale is determined by the length of the common time grid; the server then averages the noisy curves, so the overall privacy budget remains unchanged. We benchmark four one-shot smoothing techniques: Discrete Cosine Transform, Haar Wavelet shrinkage, adaptive Total-Variation denoising, and a parametric Weibull fit on the NCCTG lung-cancer cohort under five privacy levels and three partition scenarios (uniform, moderately skewed, highly imbalanced). Total-Variation gives the best mean accuracy, whereas the frequency-domain smoothers offer stronger worst-case robustness and the Weibull model shows the most stable behaviour at the strictest privacy setting. Across all methods the released curves keep the empirical log-rank type-I error below fifteen percent for privacy budgets of 0.5 and higher, demonstrating that clinically useful survival information can be shared without iterative training or heavy cryptography.</li>
</ul>

<h3>Title: TimeCopilot</h3>
<ul>
<li><strong>Authors: </strong>Azul Garza, Rene√© Rosillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00616">https://arxiv.org/abs/2509.00616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00616">https://arxiv.org/pdf/2509.00616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00616]] TimeCopilot(https://arxiv.org/abs/2509.00616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.</li>
</ul>

<h3>Title: A Multi-Strategy Approach for AI-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Ali Zain, Sareem Farooqui, Muhammad Rafi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00623">https://arxiv.org/abs/2509.00623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00623">https://arxiv.org/pdf/2509.00623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00623]] A Multi-Strategy Approach for AI-Generated Text Detection(https://arxiv.org/abs/2509.00623)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents presents three distinct systems developed for the M-DAIGT shared task on detecting AI generated content in news articles and academic abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2) A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An Innovative ensemble model named Candace, leveraging probabilistic features extracted from multiple Llama-3.2 models processed by a customTransformer this http URL RoBERTa-based system emerged as the most performant, achieving near-perfect results on both development and test sets.</li>
</ul>

<h3>Title: Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Acciarini, Simone Mestici, Halil Kelebek, Linnea Wolniewicz, Michael Vergalla, Madhulika Guhathakurta, Umaa Rebbapragada, Bala Poduval, Atƒ±lƒ±m G√ºne≈ü Baydin, Frank Soboczenski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00631">https://arxiv.org/abs/2509.00631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00631">https://arxiv.org/pdf/2509.00631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00631]] Forecasting the Ionosphere from Sparse GNSS Data with Temporal-Fusion Transformers(https://arxiv.org/abs/2509.00631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The ionosphere critically influences Global Navigation Satellite Systems (GNSS), satellite communications, and Low Earth Orbit (LEO) operations, yet accurate prediction of its variability remains challenging due to nonlinear couplings between solar, geomagnetic, and thermospheric drivers. Total Electron Content (TEC), a key ionospheric parameter, is derived from GNSS observations, but its reliable forecasting is limited by the sparse nature of global measurements and the limited accuracy of empirical models, especially during strong space weather conditions. In this work, we present a machine learning framework for ionospheric TEC forecasting that leverages Temporal Fusion Transformers (TFT) to predict sparse ionosphere data. Our approach accommodates heterogeneous input sources, including solar irradiance, geomagnetic indices, and GNSS-derived vertical TEC, and applies preprocessing and temporal alignment strategies. Experiments spanning 2010-2025 demonstrate that the model achieves robust predictions up to 24 hours ahead, with root mean square errors as low as 3.33 TECU. Results highlight that solar EUV irradiance provides the strongest predictive signals. Beyond forecasting accuracy, the framework offers interpretability through attention-based analysis, supporting both operational applications and scientific discovery. To encourage reproducibility and community-driven development, we release the full implementation as the open-source toolkit \texttt{ionopy}.</li>
</ul>

<h3>Title: Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats</h3>
<ul>
<li><strong>Authors: </strong>Chaoyu Zhang, Heng Jin, Shanghao Shi, Hexuan Yu, Sydney Johns, Y. Thomas Hou, Wenjing Lou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00634">https://arxiv.org/abs/2509.00634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00634">https://arxiv.org/pdf/2509.00634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00634]] Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats(https://arxiv.org/abs/2509.00634)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has gained significant attention for its privacy-preserving capabilities, enabling distributed devices to collaboratively train a global model without sharing raw data. However, its distributed nature forces the central server to blindly trust the local training process and aggregate uncertain model updates, making it susceptible to Byzantine attacks from malicious participants, especially in mission-critical scenarios. Detecting such attacks is challenging due to the diverse knowledge across clients, where variations in model updates may stem from benign factors, such as non-IID data, rather than adversarial behavior. Existing data-driven defenses struggle to distinguish malicious updates from natural variations, leading to high false positive rates and poor filtering performance. To address this challenge, we propose Sentinel, a remote attestation (RA)-based scheme for FL systems that regains client-side transparency and mitigates Byzantine attacks from a system security perspective. Our system employs code instrumentation to track control-flow and monitor critical variables in the local training process. Additionally, we utilize a trusted training recorder within a Trusted Execution Environment (TEE) to generate an attestation report, which is cryptographically signed and securely transmitted to the server. Upon verification, the server ensures that legitimate client training processes remain free from program behavior violation or data manipulation, allowing only trusted model updates to be aggregated into the global model. Experimental results on IoT devices demonstrate that Sentinel ensures the trustworthiness of the local training integrity with low runtime and memory overhead.</li>
</ul>

<h3>Title: Disentangling Slow and Fast Temporal Dynamics in Degradation Inference with Hierarchical Differential Models</h3>
<ul>
<li><strong>Authors: </strong>Mengjie Zhao, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00639">https://arxiv.org/abs/2509.00639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00639">https://arxiv.org/pdf/2509.00639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00639]] Disentangling Slow and Fast Temporal Dynamics in Degradation Inference with Hierarchical Differential Models(https://arxiv.org/abs/2509.00639)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reliable inference of system degradation from sensor data is fundamental to condition monitoring and prognostics in engineered systems. Since degradation is rarely observable and measurable, it must be inferred to enable accurate health assessment and decision-making. This is particularly challenging because operational variations dominate system behavior, while degradation introduces only subtle, long-term changes. Consequently, sensor data mainly reflect short-term operational variability, making it difficult to disentangle the underlying degradation process. Residual-based methods are widely employed, but the residuals remain entangled with operational history, often resulting in noisy and unreliable degradation estimation, particularly in systems with dynamic responses. Neural Ordinary Equations (NODEs) offer a promising framework for inferring latent dynamics, but the time-scale separation in slow-fast systems introduces numerical stiffness and complicates training, while degradation disentanglement remains difficult. To address these limitations, we propose a novel Hierarchical Controlled Differential Equation (H-CDE) framework that incorporates a slow (degradation) and a fast (operation) CDE component in a unified architecture. It introduces three key innovations: a multi-scale time integration scheme to mitigate numerical stiffness; a learnable path transformation that extracts latent degradation drivers to control degradation evolution; and a novel activation function that enforces monotonicity on inferred degradation as a regularizer for disentanglement. Through comprehensive evaluations on both dynamic response (e.g., bridges) and steady state (e.g., aero-engine) systems, we demonstrate that H-CDE effectively disentangles degradation from operational dynamics and outperforms residual-based baselines, yielding more accurate, robust, and interpretable inference.</li>
</ul>

<h3>Title: AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Yin, Zichong Wang, Avash Palikhe, Zhen Liu, Jun Liu, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00641">https://arxiv.org/abs/2509.00641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00641">https://arxiv.org/pdf/2509.00641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00641]] AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models(https://arxiv.org/abs/2509.00641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.</li>
</ul>

<h3>Title: LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement</h3>
<ul>
<li><strong>Authors: </strong>Yu-Zheng Lin, Sujan Ghimire, Abhiram Nandimandalam, Jonah Michael Camacho, Unnati Tripathi, Rony Macwan, Sicong Shao, Setareh Rafatirad, Rozhin Yasaei, Pratik Satam, Soheil Salehi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00647">https://arxiv.org/abs/2509.00647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00647">https://arxiv.org/pdf/2509.00647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00647]] LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement(https://arxiv.org/abs/2509.00647)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The rapid growth of hardware vulnerabilities has created an urgent need for systematic and scalable analysis methods. Unlike software flaws, which are often patchable post-deployment, hardware weaknesses remain embedded across product lifecycles, posing persistent risks to processors, embedded devices, and IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021) relied on expert-driven Delphi surveys, which lack statistical rigor and introduce subjective bias, while large-scale data-driven foundations for hardware weaknesses have been largely absent. In this work, we propose LLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction and refinement from vulnerability corpora. Our approach integrates zero-shot LLM classification, contextualized embeddings, unsupervised clustering, and prompt-driven summarization to mine hardware-related CVEs at scale. Applying LLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742 hardware-related vulnerabilities. We distilled them into five recurring themes, including privilege escalation via firmware and BIOS, memory corruption in mobile and IoT systems, and physical access exploits. Benchmarking across seven LLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy (99.5%) on a curated validation set. Beyond methodological contributions, our framework directly supported the MITRE CWE Most Important Hardware Weaknesses (MIHW) 2025 update by narrowing the candidate search space. Specifically, our pipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis, thereby reducing expert workload and accelerating evidence gathering. These results establish LLM-HyPZ as the first data-driven, scalable approach for systematically discovering hardware vulnerabilities, thereby bridging the gap between expert knowledge and real-world vulnerability evidence.</li>
</ul>

<h3>Title: MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Aviral Chharia, Wenbo Gou, Haoye Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00649">https://arxiv.org/abs/2509.00649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00649">https://arxiv.org/pdf/2509.00649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00649]] MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation(https://arxiv.org/abs/2509.00649)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>While significant progress has been made in single-view 3D human pose estimation, multi-view 3D human pose estimation remains challenging, particularly in terms of generalizing to new camera configurations. Existing attention-based transformers often struggle to accurately model the spatial arrangement of keypoints, especially in occluded scenarios. Additionally, they tend to overfit specific camera arrangements and visual scenes from training data, resulting in substantial performance drops in new settings. In this study, we introduce a novel Multi-View State Space Modeling framework, named MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the joint spatial sequence at two distinct levels: the feature level from multi-view images and the person keypoint level. We propose a Projective State Space (PSS) block to learn a generalized representation of joint spatial arrangements using state space modeling. Moreover, we modify Mamba's traditional scanning into an effective Grid Token-guided Bidirectional Scanning (GTBS), which is integral to the PSS block. Multiple experiments demonstrate that MV-SSM achieves strong generalization, outperforming state-of-the-art methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP (+38%) on Campus A1 in cross-dataset evaluations. Project Website: this https URL</li>
</ul>

<h3>Title: Missing Data Imputation using Neural Cellular Automata</h3>
<ul>
<li><strong>Authors: </strong>Tin Luu, Binh Nguyen, Man Ngo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00651">https://arxiv.org/abs/2509.00651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00651">https://arxiv.org/pdf/2509.00651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00651]] Missing Data Imputation using Neural Cellular Automata(https://arxiv.org/abs/2509.00651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>When working with tabular data, missingness is always one of the most painful problems. Throughout many years, researchers have continuously explored better and better ways to impute missing data. Recently, with the rapid development evolution in machine learning and deep learning, there is a new trend of leveraging generative models to solve the imputation task. While the imputing version of famous models such as Variational Autoencoders or Generative Adversarial Networks were investigated, prior work has overlooked Neural Cellular Automata (NCA), a powerful computational model. In this paper, we propose a novel imputation method that is inspired by NCA. We show that, with some appropriate adaptations, an NCA-based model is able to address the missing data imputation problem. We also provide several experiments to evidence that our model outperforms state-of-the-art methods in terms of imputation error and post-imputation performance.</li>
</ul>

<h3>Title: IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India</h3>
<ul>
<li><strong>Authors: </strong>Tung Nguyen, Harkanwar Singh, Nilay Naharas, Lucas Bandarkar, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00653">https://arxiv.org/abs/2509.00653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00653">https://arxiv.org/pdf/2509.00653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00653]] IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India(https://arxiv.org/abs/2509.00653)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Regional weather forecasting is a critical problem for localized climate adaptation, disaster mitigation, and sustainable development. While machine learning has shown impressive progress in global weather forecasting, regional forecasting remains comparatively underexplored. Existing efforts often use different datasets and experimental setups, limiting fair comparison and reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for data-driven regional weather forecasting focused on the Indian subcontinent. IndiaWeatherBench provides a curated dataset built from high-resolution regional reanalysis products, along with a suite of deterministic and probabilistic metrics to facilitate consistent training and evaluation. To establish strong baselines, we implement and evaluate a range of models across diverse architectures, including UNets, Transformers, and Graph-based networks, as well as different boundary conditioning strategies and training objectives. While focused on India, IndiaWeatherBench is easily extensible to other geographic regions. We open-source all raw and preprocessed datasets, model implementations, and evaluation pipelines to promote accessibility and future development. We hope IndiaWeatherBench will serve as a foundation for advancing regional weather forecasting research. Code is available at this https URL.</li>
</ul>

<h3>Title: Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains</h3>
<ul>
<li><strong>Authors: </strong>Yumeng Lin, Dong Li, Xintao Wu, Minglai Shao, Xujiang Zhao, Zhong Chen, Chen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00658">https://arxiv.org/abs/2509.00658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00658">https://arxiv.org/pdf/2509.00658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00658]] Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains(https://arxiv.org/abs/2509.00658)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Ensuring fairness and robustness in machine learning models remains a challenge, particularly under domain shifts. We present Face4FairShifts, a large-scale facial image benchmark designed to systematically evaluate fairness-aware learning and domain generalization. The dataset includes 100,000 images across four visually distinct domains with 39 annotations within 14 attributes covering demographic and facial features. Through extensive experiments, we analyze model performance under distribution shifts and identify significant gaps. Our findings emphasize the limitations of existing related datasets and the need for more effective fairness-aware domain adaptation techniques. Face4FairShifts provides a comprehensive testbed for advancing equitable and reliable AI systems. The dataset is available online at this https URL.</li>
</ul>

<h3>Title: Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware</h3>
<ul>
<li><strong>Authors: </strong>Vamsi Shankar Simhadri, Yichang Xiong, Habiba Farrukh, Xiaokuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00662">https://arxiv.org/abs/2509.00662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00662">https://arxiv.org/pdf/2509.00662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00662]] Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware(https://arxiv.org/abs/2509.00662)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Virtual Reality (VR) technology is rapidly growing in recent years. VR devices such as Meta Quest 3 utilize numerous sensors to collect users' data to provide an immersive experience. Due to the extensive data collection and the immersive nature, the security of VR devices is paramount. Leading VR devices often adopt and customize Android systems, which makes them susceptible to both Android-based vulnerabilities and new issues introduced by VR-specific customizations (e.g., system services to support continuous head and hand tracking). While prior work has extensively examined the security properties of the Android software stack, how these security properties hold for VR systems remains unexplored. In this paper, we present the first comprehensive security analysis of VR firmware. We collect over 300 versions of VR firmware from two major vendors, Quest and Pico, and perform a longitudinal analysis across the kernel layer, the system binary and library layer, and the application layer. We have identified several security issues in these VR firmware, including missing kernel-level security features, insufficient binary hardening, inconsistent permission enforcement, and inadequate SELinux policy enforcement. Based on our findings, we synthesize recommendations for VR vendors to improve security and trust for VR devices. This paper will act as an important security resource for VR developers, users, and vendors, and will also direct future advancements in secure VR ecosystem.</li>
</ul>

<h3>Title: An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator Learning Network</h3>
<ul>
<li><strong>Authors: </strong>Binghang Lu, Changhong Mou, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00663">https://arxiv.org/abs/2509.00663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00663">https://arxiv.org/pdf/2509.00663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00663]] An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator Learning Network(https://arxiv.org/abs/2509.00663)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Operator learning Network, which is a novel operator learning network to efficiently solve parametric partial differential equations. In forward and inverse settings, this operator learning network only admits minimum requirement of noisy observational data. While physics-informed neural networks and operator learning approaches such as Deep Operator Networks and Fourier Neural Operators offer promising alternatives to traditional numerical solvers, they struggle with balancing operator and physics losses, maintaining robustness under noisy or sparse data, and providing uncertainty quantification. The proposed framework addresses these limitations by integrating: (i) evolutionary multi-objective optimization to adaptively balance operator and physics-based losses in the Pareto front; (ii) replica exchange stochastic gradient Langevin dynamics to improve global parameter-space exploration and accelerate convergence; and (iii) built-in Bayesian uncertainty quantification from stochastic sampling. The proposed operator learning method is tested numerically on several different problems including one-dimensional Burgers equation and the time-fractional mixed diffusion-wave equation. The results indicate that our framework consistently outperforms the general operator learning methods in accuracy, noise robustness, and the ability to quantify uncertainty.</li>
</ul>

<h3>Title: Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yifei She, Huangxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00664">https://arxiv.org/abs/2509.00664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00664">https://arxiv.org/pdf/2509.00664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00664]] Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model(https://arxiv.org/abs/2509.00664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have made significant progress in bridging visual perception with high-level textual reasoning. However, they face a fundamental contradiction: while excelling at complex semantic understanding, these models often fail at basic visual tasks that require precise detail perception. This deficiency primarily stems from the prevalent architectural reliance on a single vision encoder optimized for high-level semantic alignment, which inherently sacrifices the ability to capture fine-grained visual information. To address this issue, we introduce Fusion to Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the single-encoder design by innovatively composing a semantically powerful anchor encoder with a perception-rich augmenting encoder via a lightweight Multi-Head Cross-Attention mechanism. Experimental results demonstrate that on several challenging benchmarks demanding fine-grained visual understanding, such as TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms baselines that use only a single encoder or existing feature fusion methods. This work proves that composing heterogeneous expert encoders is an efficient and effective path to overcoming the visual perception bottleneck in current MLLMs, offering a new design paradigm for building next-generation AI systems with stronger perceptual capabilities.</li>
</ul>

<h3>Title: ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Weilong Yan, Xin Zhang, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00665">https://arxiv.org/abs/2509.00665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00665">https://arxiv.org/pdf/2509.00665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00665]] ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation(https://arxiv.org/abs/2509.00665)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog, snow, and nighttime) remains highly challenging due to the lack of reliable ground truth and the difficulty of learning from unlabeled real-world data. Existing methods often rely on synthetic adverse data with pseudo-labels, which suffer from domain gaps, or employ self-supervised learning, which violates photometric assumptions in adverse scenarios. In this work, we propose to achieve weather--generalized depth estimation by Parameter--Efficient Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small amount of high--visibility (normal) data. While PEFT has shown strong performance in semantic tasks such as segmentation, it remains underexplored for geometry--centric tasks like depth estimation -- especially in terms of balancing effective adaptation with the preservation of pretrained knowledge. To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy, which structurally decomposes the pretrained weights of VFMs based on two kinds of effective ranks (entropy--rank and stable--rank). In the tuning phase, we adaptively select the proper rank number as well as the task--aware singular directions for initialization, based on the entropy--rank and full--tuned weight; while in the maintaining stage, we enforce a principal direction regularization based on the stable--rank. This design guarantees flexible task adaptation while preserving the strong generalization capability of the pretrained VFM. Extensive experiments on four real--world benchmarks across diverse weather conditions demonstrate that STM not only outperforms existing PEFT methods and full fine--tuning but also surpasses methods trained with adverse synthetic data, and even the depth foundation model</li>
</ul>

<h3>Title: Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech</h3>
<ul>
<li><strong>Authors: </strong>Sanjeeevan Selvaganapathy, Mehwish Nasim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00673">https://arxiv.org/abs/2509.00673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00673">https://arxiv.org/pdf/2509.00673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00673]] Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech(https://arxiv.org/abs/2509.00673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>We investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. Furthermore, we identify critical failures across all models in understanding nuanced language such as irony. We also find alarming fairness disparities in performance across different targeted groups and systemic overconfidence that renders self-reported certainty unreliable. These findings challenge the notion of LLMs as objective arbiters and highlight the need for more sophisticated auditing frameworks that account for fairness, calibration, and ideological consistency.</li>
</ul>

<h3>Title: LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model</h3>
<ul>
<li><strong>Authors: </strong>Xiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang, Bo Liu, Tianyi Xiong, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00676">https://arxiv.org/abs/2509.00676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00676">https://arxiv.org/pdf/2509.00676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00676]] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model(https://arxiv.org/abs/2509.00676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.</li>
</ul>

<h3>Title: CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Wang, Xue Jiang, Guozheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00677">https://arxiv.org/abs/2509.00677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00677">https://arxiv.org/pdf/2509.00677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00677]] CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification(https://arxiv.org/abs/2509.00677)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal fusion has made great progress in the field of remote sensing image classification due to its ability to exploit the complementary spatial-spectral information. Deep learning methods such as CNN and Transformer have been widely used in these domains. State Space Models recently highlighted that prior methods suffer from quadratic computational complexity. As a result, modeling longer-range dependencies of spatial-spectral features imposes an overwhelming burden on the network. Mamba solves this problem by incorporating time-varying parameters into ordinary SSM and performing hardware optimization, but it cannot perform feature fusion directly. In order to make full use of Mamba's low computational burden and explore the potential of internal structure in multimodal feature fusion, we propose Cross State Fusion Mamba (CSFMamba) Network. Specifically, we first design the preprocessing module of remote sensing image information for the needs of Mamba structure, and combine it with CNN to extract multi-layer features. Secondly, a cross-state module based on Mamba operator is creatively designed to fully fuse the feature of the two modalities. The advantages of Mamba and CNN are combined by designing a more powerful backbone. We capture the fusion relationship between HSI and LiDAR modalities with stronger full-image understanding. The experimental results on two datasets of MUUFL and Houston2018 show that the proposed method outperforms the experimental results of Transformer under the premise of reducing the network training burden.</li>
</ul>

<h3>Title: Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Amartya Banerjee, Somnath Kar, Anirban Pal, Debabrata Maiti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00684">https://arxiv.org/abs/2509.00684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00684">https://arxiv.org/pdf/2509.00684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00684]] Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design(https://arxiv.org/abs/2509.00684)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Efficiently steering generative models toward pharmacologically relevant regions of chemical space remains a major obstacle in molecular drug discovery under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling, a framework that couples property-guided representation learning with controllable molecule generation. VECTOR+ applies to both regression and classification tasks and enables interpretable, data-efficient exploration of functional chemical space. We evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056 molecules by binding mode). Despite limited training data, VECTOR+ generates novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of 8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor ($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl pharmacophore while introducing novel motifs. Molecular dynamics (250 ns) confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes to kinase inhibitors, producing compounds with stronger docking scores than established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity highlights the superior performance of our method. These results position our work as a robust, extensible approach for property-conditioned molecular design in low-data settings, bridging contrastive learning and generative modeling for reproducible, AI-accelerated discovery.</li>
</ul>

<h3>Title: CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Alex Gulko, Yusen Peng, Sachin Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00691">https://arxiv.org/abs/2509.00691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00691">https://arxiv.org/pdf/2509.00691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00691]] CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders(https://arxiv.org/abs/2509.00691)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Probing with sparse autoencoders is a promising approach for uncovering interpretable features in large language models (LLMs). However, the lack of automated evaluation methods has hindered their broader adoption and development. In this work, we introduce CE-Bench, a novel and lightweight contrastive evaluation benchmark for sparse autoencoders, built on a curated dataset of contrastive story pairs. We conduct comprehensive ablation studies to validate the effectiveness of our approach. Our results show that CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks, all without requiring an external LLM. The official implementation and evaluation dataset are open-sourced under the MIT License.</li>
</ul>

<h3>Title: CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yusen Peng, Alper Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00692">https://arxiv.org/abs/2509.00692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00692">https://arxiv.org/pdf/2509.00692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00692]] CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition(https://arxiv.org/abs/2509.00692)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Skeleton-based human action recognition leverages sequences of human joint coordinates to identify actions performed in videos. Owing to the intrinsic spatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs) have been the dominant architecture in this field. However, recent advances in transformer models and masked pretraining frameworks open new avenues for representation learning. In this work, we propose CascadeFormer, a family of two-stage cascading transformers for skeleton-based human action recognition. Our framework consists of a masked pretraining stage to learn generalizable skeleton representations, followed by a cascading fine-tuning stage tailored for discriminative action classification. We evaluate CascadeFormer across three benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achieving competitive performance on all tasks. To promote reproducibility, we release our code and model checkpoints.</li>
</ul>

<h3>Title: DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming</h3>
<ul>
<li><strong>Authors: </strong>Arun Vignesh Malarkkan, Haoyue Bai, Anjali Kaushik, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00693">https://arxiv.org/abs/2509.00693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00693">https://arxiv.org/pdf/2509.00693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00693]] DELTA: Variational Disentangled Learning for Privacy-Preserving Data Reprogramming(https://arxiv.org/abs/2509.00693)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>In real-world applications, domain data often contains identifiable or sensitive attributes, is subject to strict regulations (e.g., HIPAA, GDPR), and requires explicit data feature engineering for interpretability and transparency. Existing feature engineering primarily focuses on advancing downstream task performance, often risking privacy leakage. We generalize this learning task under such new requirements as Privacy-Preserving Data Reprogramming (PPDR): given a dataset, transforming features to maximize target attribute prediction accuracy while minimizing sensitive attribute prediction accuracy. PPDR poses challenges for existing systems: 1) generating high-utility feature transformations without being overwhelmed by a large search space, and 2) disentangling and eliminating sensitive information from utility-oriented features to reduce privacy inferability. To tackle these challenges, we propose DELTA, a two-phase variational disentangled generative learning framework. Phase I uses policy-guided reinforcement learning to discover feature transformations with downstream task utility, without any regard to privacy inferability. Phase II employs a variational LSTM seq2seq encoder-decoder with a utility-privacy disentangled latent space design and adversarial-causal disentanglement regularization to suppress privacy signals during feature generation. Experiments on eight datasets show DELTA improves predictive performance by ~9.3% and reduces privacy leakage by ~35%, demonstrating robust, privacy-aware data transformation.</li>
</ul>

<h3>Title: Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Wei, Jinpeng Gao, Jiang Zhong, Yuming Yang, Fengmao Lv, Zhenyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00698">https://arxiv.org/abs/2509.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00698">https://arxiv.org/pdf/2509.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00698]] Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs(https://arxiv.org/abs/2509.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown strong potential in recommendation tasks due to their strengths in language understanding, reasoning and knowledge integration. These capabilities are especially beneficial for review-based recommendation, which relies on semantically rich user-generated texts to reveal fine-grained user preferences and item attributes. However, effectively incorporating reviews into LLM-based recommendation remains challenging due to (1) inefficient to dynamically utilize user reviews under LLMs' constrained context windows, and (2) lacking effective mechanisms to prioritize reviews most relevant to the user's current decision context. To address these challenges, we propose RevBrowse, a review-driven recommendation framework inspired by the "browse-then-decide" decision process commonly observed in online user behavior. RevBrowse integrates user reviews into the LLM-based reranking process to enhance its ability to distinguish between candidate items. To improve the relevance and efficiency of review usage, we introduce PrefRAG, a retrieval-augmented module that disentangles user and item representations into structured forms and adaptively retrieves preference-relevant content conditioned on the target item. Extensive experiments on four Amazon review datasets demonstrate that RevBrowse achieves consistent and significant improvements over strong baselines, highlighting its generalizability and effectiveness in modeling dynamic user preferences. Furthermore, since the retrieval-augmented process is transparent, RevBrowse offers a certain level of interpretability by making visible which reviews influence the final recommendation.</li>
</ul>

<h3>Title: Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision</h3>
<ul>
<li><strong>Authors: </strong>Raehyuk Jung, Seungjun Yu, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00700">https://arxiv.org/abs/2509.00700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00700">https://arxiv.org/pdf/2509.00700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00700]] Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision(https://arxiv.org/abs/2509.00700)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) combine a vision encoder and a large language model (LLM) through alignment training, showing strong performance on multimodal tasks. A central component in this architecture is the projection layer, which maps visual features into the LLM's embedding space. Despite its importance, its ability to generalize to unseen visual concepts has not been systematically evaluated. To address this, we propose a benchmark for evaluating projection-layer generalization. We adapt object detection datasets (rich in fine-grained annotations) into a prompting format and design train/test splits with disjoint label sets, enabling precise control over seen and unseen concept separation. Experimental results show that the projection layer retains about 79 to 88 percent of the performance on unseen classes compared to seen ones across various settings, suggesting a non-trivial level of generalization even without explicit alignment supervision on those concepts. We further analyze this behavior through a mechanistic interpretability lens. Our findings indicate that the feed-forward network in the projection layer functions like a key-value memory, processing seen and unseen tokens in similar ways. This study introduces a new evaluation framework for alignment generalization and highlights the potential for efficient VLM training with limited aligned data.</li>
</ul>

<h3>Title: Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Osama Ahmad, Lukas Wesemann, Fabian Waschkowski, Zubair Khalid</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00703">https://arxiv.org/abs/2509.00703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00703">https://arxiv.org/pdf/2509.00703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00703]] Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition(https://arxiv.org/abs/2509.00703)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate spatiotemporal forecasting is critical for numerous complex systems but remains challenging due to complex volatility patterns and spectral entanglement in conventional graph neural networks (GNNs). While decomposition-integrated approaches like variational mode graph convolutional network (VMGCN) improve accuracy through signal decomposition, they suffer from computational inefficiency and manual hyperparameter tuning. To address these limitations, we propose the mode adaptive graph network (MAGN) that transforms iterative variational mode decomposition (VMD) into a trainable neural module. Our key innovations include (1) an unfolded VMD (UVMD) module that replaces iterative optimization with a fixed-depth network to reduce the decomposition time (by 250x for the LargeST benchmark), and (2) mode-specific learnable bandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminate manual tuning while preventing spectral overlap. Evaluated on the LargeST benchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction in the prediction error over VMGCN and outperforms state-of-the-art baselines.</li>
</ul>

<h3>Title: Why Pool When You Can Flow? Active Learning with GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Renfei Zhang, Mohit Pandey, Artem Cherkasov, Martin Ester</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00704">https://arxiv.org/abs/2509.00704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00704">https://arxiv.org/pdf/2509.00704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00704]] Why Pool When You Can Flow? Active Learning with GFlowNets(https://arxiv.org/abs/2509.00704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scalability of pool-based active learning is limited by the computational cost of evaluating large unlabeled datasets, a challenge that is particularly acute in virtual screening for drug discovery. While active learning strategies such as Bayesian Active Learning by Disagreement (BALD) prioritize informative samples, it remains computationally intensive when scaled to libraries containing billions samples. In this work, we introduce BALD-GFlowNet, a generative active learning framework that circumvents this issue. Our method leverages Generative Flow Networks (GFlowNets) to directly sample objects in proportion to the BALD reward. By replacing traditional pool-based acquisition with generative sampling, BALD-GFlowNet achieves scalability that is independent of the size of the unlabeled pool. In our virtual screening experiment, we show that BALD-GFlowNet achieves a performance comparable to that of standard BALD baseline while generating more structurally diverse molecules, offering a promising direction for efficient and scalable molecular discovery.</li>
</ul>

<h3>Title: X-PRINT:Platform-Agnostic and Scalable Fine-Grained Encrypted Traffic Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>YuKun Zhu, ManYuan Hua, Hai Huang, YongZhao Zhang, Jie Yang, FengHua Xu, RuiDong Chen, XiaoSong Zhang, JiGuo Yu, Yong Ma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00706">https://arxiv.org/abs/2509.00706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00706">https://arxiv.org/pdf/2509.00706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00706]] X-PRINT:Platform-Agnostic and Scalable Fine-Grained Encrypted Traffic Fingerprinting(https://arxiv.org/abs/2509.00706)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Although encryption protocols such as TLS are widely de-ployed,side-channel metadata in encrypted traffic still reveals patterns that allow application and behavior this http URL-ever,existing fine-grained fingerprinting approaches face two key limitations:(i)reliance on platform-dependent charac-teristics,which restricts generalization across heterogeneous platforms,and(ii)poor scalability for fine-grained behavior identification in open-world settings. In this paper,we present X-PRINT,the first server-centric,URI-based framework for cross-platform fine-grained encrypted-traffic fingerprinting.X-PRINT systematically demonstrates that backend URI invocation patterns can serve as platform-agnostic invariants and are effective for mod-eling fine-grained this http URL achieve robust identifica-tion,X-PRINT further leverages temporally structured URI maps for behavior inference and emphasizes the exclusion of platform-or application-specific private URIs to handle unseen cases,thereby improving reliability in open-world and cross-platform this http URL experiments across diverse cross-platform and open-world settings show that X-PRINT achieves state-of-the-art accuracy in fine-grained fingerprint-ing and exhibits strong scalability and robustness.</li>
</ul>

<h3>Title: Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00707">https://arxiv.org/abs/2509.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00707">https://arxiv.org/pdf/2509.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00707]] Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs(https://arxiv.org/abs/2509.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) offer a promising non-autoregressive alternative for large language modeling. Standard decoding methods for MDMs, such as confidence-based sampling, select tokens independently based on individual token confidences at each diffusion step. However, we observe that this independent token selection often results in generation orders resembling sequential autoregressive processes, limiting the advantages of non-autoregressive modeling. To mitigate this pheonomenon, we propose Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an external reward model to provide a principled global signal during the iterative diffusion process. Specifically, at each diffusion step, RWS evaluates the quality of the entire intermediate sequence and scales token logits accordingly, guiding token selection by integrating global sequence-level coherence. This method selectively increases the confidence of tokens that initially have lower scores, thereby promoting a more non-autoregressive generation order. Furthermore, we provide theoretical justification showing that reward-weighted logit scaling induces beneficial rank reversals in token selection and consistently improves expected reward. Experiments demonstrate that RWS significantly promotes non-autoregressive generation orders, leading to improvements across multiple evaluation metrics. These results highlight the effectiveness of integrating global signals in enhancing both the non-autoregressive properties and overall performance of MDMs.</li>
</ul>

<h3>Title: Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI</h3>
<ul>
<li><strong>Authors: </strong>Elias Ra, Seung Je Kim, Eui-Yeong Seo, Geunju So</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00709">https://arxiv.org/abs/2509.00709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00709">https://arxiv.org/pdf/2509.00709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00709]] Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI(https://arxiv.org/abs/2509.00709)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Higher education faces growing challenges in delivering personalized, scalable, and pedagogically coherent learning experiences. This study introduces a structured framework for designing an AI-powered Learning Management System (AI-LMS) that integrates generative and conversational AI to support adaptive, interactive, and learner-centered instruction. Using a design-based research (DBR) methodology, the framework unfolds through five phases: literature review, SWOT analysis, development of ethical-pedagogical principles, system design, and instructional strategy formulation. The resulting AI-LMS features modular components -- including configurable prompts, adaptive feedback loops, and multi-agent conversation flows -- aligned with pedagogical paradigms such as behaviorist, constructivist, and connectivist learning theories. By combining AI capabilities with human-centered design and ethical safeguards, this study advances a practical model for AI integration in education. Future research will validate and refine the system through real-world implementation.</li>
</ul>

<h3>Title: LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA</h3>
<ul>
<li><strong>Authors: </strong>Houji Jin, Negin Ashrafi, Armin Abdollahi, Wei Liu, Jian Wang, Ganyu Gui, Maryam Pishgar, Huanghao Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00731">https://arxiv.org/abs/2509.00731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00731">https://arxiv.org/pdf/2509.00731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00731]] LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA(https://arxiv.org/abs/2509.00731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of large language models (LLMs) has heightened the demand for accurate detection of AI-generated text, particularly in languages like Chinese, where subtle linguistic nuances pose significant challenges to current methods. In this study, we conduct a systematic comparison of encoder-based Transformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM (Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank Adaptation, LoRA), and a FastText baseline using the publicly available dataset from the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models were fine-tuned using a novel prompt-based masked language modeling approach, while Qwen2.5-7B was adapted for classification with an instruction-format input and a lightweight classification head trained via LoRA. Experiments reveal that although encoder models nearly memorize training data, they suffer significant performance degradation under distribution shifts (RoBERTa: 76.3% test accuracy; BERT: 79.3%). FastText demonstrates surprising lexical robustness (83.5% accuracy) yet lacks deeper semantic understanding. In contrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with balanced precision-recall metrics, indicating superior generalization and resilience to dataset-specific artifacts. These findings underscore the efficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust Chinese AI-generated text detection. Future work will explore next-generation Qwen3 models, distilled variants, and ensemble strategies to enhance cross-domain robustness further.</li>
</ul>

<h3>Title: Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning</h3>
<ul>
<li><strong>Authors: </strong>Kuniko Paxton, Koorosh Aslansefat, Dhavalkumar Thakker, Yiannis Papadopoulos, Tanaya Maslekar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00745">https://arxiv.org/abs/2509.00745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00745">https://arxiv.org/pdf/2509.00745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00745]] Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning(https://arxiv.org/abs/2509.00745)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have significantly improved the accuracy of skin lesion classification models, supporting medical diagnoses and promoting equitable healthcare. However, concerns remain about potential biases related to skin color, which can impact diagnostic outcomes. Ensuring fairness is challenging due to difficulties in classifying skin tones, high computational demands, and the complexity of objectively verifying fairness. To address these challenges, we propose a fairness algorithm for skin lesion classification that overcomes the challenges associated with achieving diagnostic fairness across varying skin tones. By calculating the skewness of the feature map in the convolution layer of the VGG (Visual Geometry Group) network and the patches and the heads of the Vision Transformer, our method reduces unnecessary channels related to skin tone, focusing instead on the lesion area. This approach lowers computational costs and mitigates bias without relying on conventional statistical methods. It potentially reduces model size while maintaining fairness, making it more practical for real-world applications.</li>
</ul>

<h3>Title: Causal Interpretation of Sparse Autoencoder Features in Vision</h3>
<ul>
<li><strong>Authors: </strong>Sangyu Han, Yearim Kim, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00749">https://arxiv.org/abs/2509.00749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00749">https://arxiv.org/pdf/2509.00749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00749]] Causal Interpretation of Sparse Autoencoder Features in Vision(https://arxiv.org/abs/2509.00749)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Understanding what sparse auto-encoder (SAE) features in vision transformers truly represent is usually done by inspecting the patches where a feature's activation is highest. However, self-attention mixes information across the entire image, so an activated patch often co-occurs with-but does not cause-the feature's firing. We propose Causal Feature Explanation (CaFE), which leverages Effective Receptive Field (ERF). We consider each activation of an SAE feature to be a target and apply input-attribution methods to identify the image patches that causally drive that activation. Across CLIP-ViT features, ERF maps frequently diverge from naive activation maps, revealing hidden context dependencies (e.g., a "roaring face" feature that requires the co-occurrence of eyes and nose, rather than merely an open mouth). Patch insertion tests confirm that CaFE more effectively recovers or suppresses feature activations than activation-ranked patches. Our results show that CaFE yields more faithful and semantically precise explanations of vision-SAE features, highlighting the risk of misinterpretation when relying solely on activation location.</li>
</ul>

<h3>Title: EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions</h3>
<ul>
<li><strong>Authors: </strong>Dinh-Khoi Vo, Van-Loc Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00751">https://arxiv.org/abs/2509.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00751">https://arxiv.org/pdf/2509.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00751]] EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions(https://arxiv.org/abs/2509.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Event-based image retrieval from free-form captions presents a significant challenge: models must understand not only visual features but also latent event semantics, context, and real-world knowledge. Conventional vision-language retrieval approaches often fall short when captions describe abstract events, implicit causality, temporal context, or contain long, complex narratives. To tackle these issues, we introduce a multi-stage retrieval framework combining dense article retrieval, event-aware language model reranking, and efficient image collection, followed by caption-guided semantic matching and rank-aware selection. We leverage Qwen3 for article search, Qwen3-Reranker for contextual alignment, and Qwen2-VL for precise image scoring. To further enhance performance and robustness, we fuse outputs from multiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves the top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand Challenge, demonstrating the effectiveness of combining language-based reasoning and multimodal retrieval for complex, real-world image understanding. The code is available at this https URL.</li>
</ul>

<h3>Title: Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Y Hop Nguyen, Doan Anh Phan Huu, Trung Thai Tran, Nhat Nam Mai, Van Toi Giap, Thao Thi Phuong Dao, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00752">https://arxiv.org/abs/2509.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00752">https://arxiv.org/pdf/2509.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00752]] Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification(https://arxiv.org/abs/2509.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a unified vision-language framework tailored for ENT endoscopy image analysis that simultaneously tackles three clinically-relevant tasks: image classification, image-to-image retrieval, and text-to-image retrieval. Unlike conventional CNN-based pipelines that struggle to capture cross-modal semantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it through Low-Rank Adaptation, multi-level CLS token aggregation, and spherical feature interpolation. These components collectively enable efficient fine-tuning on limited medical data while improving representation diversity and semantic alignment across modalities. To bridge the gap between visual inputs and textual diagnostic context, we introduce class-specific natural language prompts that guide the image encoder through a joint training objective combining supervised classification with contrastive learning. We validated our framework through participation in the ACM MM'25 ENTRep Grand Challenge, achieving 95% accuracy and F1-score in classification, Recall@1 of 0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and MRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental benefits of each architectural component, validating the effectiveness of our design for robust multimodal medical understanding in low-resource clinical settings.</li>
</ul>

<h3>Title: Attribute Fusion-based Classifier on Framework of Belief Structure</h3>
<ul>
<li><strong>Authors: </strong>Qiying Hu, Yingying Liang, Qianli Zhou, Witold Pedrycz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00754">https://arxiv.org/abs/2509.00754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00754">https://arxiv.org/pdf/2509.00754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00754]] Attribute Fusion-based Classifier on Framework of Belief Structure(https://arxiv.org/abs/2509.00754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dempster-Shafer Theory (DST) provides a powerful framework for modeling uncertainty and has been widely applied to multi-attribute classification tasks. However, traditional DST-based attribute fusion-based classifiers suffer from oversimplified membership function modeling and limited exploitation of the belief structure brought by basic probability assignment (BPA), reducing their effectiveness in complex real-world scenarios. This paper presents an enhanced attribute fusion-based classifier that addresses these limitations through two key innovations. First, we adopt a selective modeling strategy that utilizes both single Gaussian and Gaussian Mixture Models (GMMs) for membership function construction, with model selection guided by cross-validation and a tailored evaluation metric. Second, we introduce a novel method to transform the possibility distribution into a BPA by combining simple BPAs derived from normalized possibility distributions, enabling a much richer and more flexible representation of uncertain information. Furthermore, we apply the belief structure-based BPA generation method to the evidential K-Nearest Neighbors classifier, enhancing its ability to incorporate uncertainty information into decision-making. Comprehensive experiments on benchmark datasets are conducted to evaluate the performance of the proposed attribute fusion-based classifier and the enhanced evidential K-Nearest Neighbors classifier in comparison with both evidential classifiers and conventional machine learning classifiers. The results demonstrate that our proposed classifier outperforms the best existing evidential classifier, achieving an average accuracy improvement of 4.84%, while maintaining low variance, thus confirming its superior effectiveness and robustness.</li>
</ul>

<h3>Title: MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure</h3>
<ul>
<li><strong>Authors: </strong>Xiufeng Huang, Ziyuan Luo, Qi Song, Ruofei Wang, Renjie Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00757">https://arxiv.org/abs/2509.00757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00757">https://arxiv.org/pdf/2509.00757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00757]] MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure(https://arxiv.org/abs/2509.00757)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction, watermark, segmentation</a></li>
<li><strong>Abstract: </strong>The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: this https URL.</li>
</ul>

<h3>Title: No More Sibling Rivalry: Debiasing Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Bin Yang, Yulin Zhang, Hong-Yu Zhou, Sibei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00760">https://arxiv.org/abs/2509.00760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00760">https://arxiv.org/pdf/2509.00760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00760]] No More Sibling Rivalry: Debiasing Human-Object Interaction Detection(https://arxiv.org/abs/2509.00760)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detection transformers have been applied to human-object interaction (HOI) detection, enhancing the localization and recognition of human-action-object triplets in images. Despite remarkable progress, this study identifies a critical issue-"Toxic Siblings" bias-which hinders the interaction decoder's learning, as numerous similar yet distinct HOI triplets interfere with and even compete against each other both input side and output side to the interaction decoder. This bias arises from high confusion among sibling triplets/categories, where increased similarity paradoxically reduces precision, as one's gain comes at the expense of its toxic sibling's decline. To address this, we propose two novel debiasing learning objectives-"contrastive-then-calibration" and "merge-then-split"-targeting the input and output perspectives, respectively. The former samples sibling-like incorrect HOI triplets and reconstructs them into correct ones, guided by strong positional priors. The latter first learns shared features among sibling categories to distinguish them from other groups, then explicitly refines intra-group differentiation to preserve uniqueness. Experiments show that we significantly outperform both the baseline (+9.18% mAP on HICO-Det) and the state-of-the-art (+3.59% mAP) across various settings.</li>
</ul>

<h3>Title: Decomposing and Revising What Language Models Generate</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Yan, Jiaoyan Chen, Jiapu Wang, Xiaoli Li, Ru Li, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00765">https://arxiv.org/abs/2509.00765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00765">https://arxiv.org/pdf/2509.00765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00765]] Decomposing and Revising What Language Models Generate(https://arxiv.org/abs/2509.00765)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attribution is crucial in question answering (QA) with Large Language Models (LLMs).SOTA question decomposition-based approaches use long form answers to generate questions for retrieving related documents. However, the generated questions are often irrelevant and incomplete, resulting in a loss of facts in this http URL approaches also fail to aggregate evidence snippets from different documents and paragraphs. To tackle these problems, we propose a new fact decomposition-based framework called FIDES (\textit{faithful context enhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES uses a contextually enhanced two-stage faithful decomposition method to decompose long form answers into sub-facts, which are then used by a retriever to retrieve related evidence snippets. If the retrieved evidence snippets conflict with the related sub-facts, such sub-facts will be revised accordingly. Finally, the evidence snippets are aggregated according to the original this http URL evaluation has been conducted with six datasets, with an additionally proposed new metric called $Attr_{auto-P}$ for evaluating the evidence precision. FIDES outperforms the SOTA methods by over 14\% in average with GPT-3.5-turbo, Gemini and Llama 70B series.</li>
</ul>

<h3>Title: InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos</h3>
<ul>
<li><strong>Authors: </strong>Yangsong Zhang, Abdul Ahad Butt, G√ºl Varol, Ivan Laptev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00767">https://arxiv.org/abs/2509.00767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00767">https://arxiv.org/pdf/2509.00767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00767]] InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos(https://arxiv.org/abs/2509.00767)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.</li>
</ul>

<h3>Title: Bayesian and Multi-Objective Decision Support for Real-Time Cyber-Physical Incident Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Shaofei Huang, Christopher M. Poskitt, Lwin Khin Shar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00770">https://arxiv.org/abs/2509.00770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00770">https://arxiv.org/pdf/2509.00770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00770]] Bayesian and Multi-Objective Decision Support for Real-Time Cyber-Physical Incident Mitigation(https://arxiv.org/abs/2509.00770)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>This research proposes a real-time, adaptive decision-support framework for mitigating cyber incidents in cyber-physical systems, developed in response to an increasing reliance on these systems within critical infrastructure and evolving adversarial tactics. Existing decision-support systems often fall short in accounting for multi-agent, multi-path attacks and trade-offs between safety and operational continuity. To address this, our framework integrates hierarchical system modelling with Bayesian probabilistic reasoning, constructing Bayesian Network Graphs from system architecture and vulnerability data. Models are encoded using a Domain Specific Language to enhance computational efficiency and support dynamic updates. In our approach, we use a hybrid exposure probability estimation framework, which combines Exploit Prediction Scoring System and Common Vulnerability Scoring System scores via Bayesian confidence calibration to handle epistemic uncertainty caused by incomplete or heterogeneous vulnerability metadata. Mitigation recommendations are generated as countermeasure portfolios, refined using multi-objective optimisation to identify Pareto-optimal strategies balancing attack likelihood, impact severity, and system availability. To accommodate time- and resource-constrained incident response, frequency-based heuristics are applied to prioritise countermeasures across the optimised portfolios. The framework was evaluated through three representative cyber-physical attack scenarios, demonstrating its versatility in handling complex adversarial behaviours under real-time response constraints. The results affirm its utility in operational contexts and highlight the robustness of our proposed approach across diverse threat environments.</li>
</ul>

<h3>Title: Secure and Scalable Face Retrieval via Cancelable Product Quantization</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Tang, Wenjie Li, Yixiang Qiu, Genping Wang, Shu-Tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00781">https://arxiv.org/abs/2509.00781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00781">https://arxiv.org/pdf/2509.00781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00781]] Secure and Scalable Face Retrieval via Cancelable Product Quantization(https://arxiv.org/abs/2509.00781)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>Despite the ubiquity of modern face retrieval systems, their retrieval stage is often outsourced to third-party entities, posing significant risks to user portrait privacy. Although homomorphic encryption (HE) offers strong security guarantees by enabling arithmetic computations in the cipher space, its high computational inefficiency makes it unsuitable for real-time, real-world applications. To address this issue, we propose Cancelable Product Quantization, a highly efficient framework for secure face representation retrieval. Our hierarchical two-stage framework comprises: (i) a high-throughput cancelable PQ indexing module for fast candidate filtering, and (ii) a fine-grained cipher-space retrieval module for final precise face ranking. A tailored protection mechanism is designed to secure the indexing module for cancelable biometric authentication while ensuring efficiency. Experiments on benchmark datasets demonstrate that our method achieves an decent balance between effectiveness, efficiency and security.</li>
</ul>

<h3>Title: Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses</h3>
<ul>
<li><strong>Authors: </strong>Ganxi Xu, Jinyi Long, Jia Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00787">https://arxiv.org/abs/2509.00787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00787">https://arxiv.org/pdf/2509.00787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00787]] Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses(https://arxiv.org/abs/2509.00787)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual prostheses have shown great potential in restoring vision for blind individuals. On the one hand, researchers have been continuously improving the brain decoding framework of visual prostheses by leveraging the powerful image generation capabilities of diffusion models. On the other hand, the brain encoding stage of visual prostheses struggles to generate brain signals with sufficient biological similarity. Although existing works have recognized this problem, the quality of predicted stimuli still remains a critical issue, as existing approaches typically lack supervised signals from real brain responses to validate the biological plausibility of predicted stimuli. To address this issue, we propose a novel image-to-brain framework based on denoising diffusion probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our framework consists of two key architectural components: a pre-trained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that learns to reconstruct biologically plausible brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules enable dynamic interaction between visual features and brain signal representations, facilitating fine-grained alignment during the generation process. We evaluate our framework on two multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its effectiveness in generating biologically plausible brain signals. Moreover, we visualize the training and test M/EEG topographies for all subjects on both datasets to intuitively demonstrate the intra-subject variations and inter-subject variations in M/EEG signals.</li>
</ul>

<h3>Title: OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Pei Liu, Qingtian Ning, Xinyan Lu, Haipeng Liu, Weiliang Ma, Dangen She, Peng Jia, Xianpeng Lang, Jun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00789">https://arxiv.org/abs/2509.00789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00789">https://arxiv.org/pdf/2509.00789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00789]] OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving(https://arxiv.org/abs/2509.00789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in vision-language models (VLMs) have demonstrated impressive spatial reasoning capabilities for autonomous driving, yet existing methods predominantly focus on static scene understanding while neglecting the essential temporal dimension of real-world driving scenarios. To address this critical limitation, we propose the OmniReason framework, which establishes robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and their underlying decision-making processes. Our work makes two fundamental advances: (1) We introduce OmniReason-Data, two large-scale vision-language-action (VLA) datasets with dense spatiotemporal annotations and natural language explanations, generated through a novel hallucination-mitigated auto-labeling pipeline that ensures both physical plausibility and temporal coherence; (2) We develop the OmniReason-Agent architecture, which integrates a sparse temporal memory module for persistent scene context modeling and an explanation generator that produces human-interpretable decision rationales, facilitated by our spatiotemporal knowledge distillation approach that effectively captures spatiotemporal causal reasoning patterns. Comprehensive experiments demonstrate state-of-the-art performance, where OmniReason-Agent achieves significant improvements in both open-loop planning tasks and visual question answering (VQA) benchmarks, while establishing new capabilities for interpretable, temporally-aware autonomous vehicles operating in complex, dynamic environments.</li>
</ul>

<h3>Title: ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods</h3>
<ul>
<li><strong>Authors: </strong>Jakob De Moor, Hans Weytjens, Johannes De Smedt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00797">https://arxiv.org/abs/2509.00797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00797">https://arxiv.org/pdf/2509.00797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00797]] ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods(https://arxiv.org/abs/2509.00797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining that focuses on optimizing processes through real-time interventions based on event log data. Evaluating PresPM methods is challenging due to the lack of ground-truth outcomes for all intervention actions in datasets. A generative deep learning approach from the field of Causal Inference (CI), RealCause, has been commonly used to estimate the outcomes for proposed intervention actions to evaluate a new policy. However, RealCause overlooks the temporal dependencies in process data, and relies on a single CI model architecture, TARNet, limiting its effectiveness. To address both shortcomings, we introduce ProCause, a generative approach that supports both sequential (e.g., LSTMs) and non-sequential models while integrating multiple CI architectures (S-Learner, T-Learner, TARNet, and an ensemble). Our research using a simulator with known ground truths reveals that TARNet is not always the best choice; instead, an ensemble of models offers more consistent reliability, and leveraging LSTMs shows potential for improved evaluations when temporal dependencies are present. We further validate ProCause's practical effectiveness through a real-world data analysis, ensuring a more reliable evaluation of PresPM methods.</li>
</ul>

<h3>Title: Multimodal Iterative RAG for Knowledge Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Changin Choi, Wonseok Lee, Jungmin Ko, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00798">https://arxiv.org/abs/2509.00798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00798">https://arxiv.org/pdf/2509.00798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00798]] Multimodal Iterative RAG for Knowledge Visual Question Answering(https://arxiv.org/abs/2509.00798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) have significantly advanced multimodal understanding, their performance remains limited on knowledge-intensive visual questions that require external knowledge beyond the image. Retrieval-Augmented Generation (RAG) has become a promising solution for providing models with external knowledge, its conventional single-pass framework often fails to gather sufficient knowledge. To overcome this limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that leverages reasoning to enhance retrieval and update reasoning over newly retrieved knowledge across modalities. At each iteration, MI-RAG leverages an accumulated reasoning record to dynamically formulate a multi-query. These queries then drive a joint search across heterogeneous knowledge bases containing both visually-grounded and textual knowledge. The newly acquired knowledge is synthesized into the reasoning record, progressively refining understanding across iterations. Experiments on challenging benchmarks, including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG significantly improves both retrieval recall and answer accuracy, establishing a scalable approach for compositional reasoning in knowledge-intensive VQA.</li>
</ul>

<h3>Title: Fairness in Federated Learning: Trends, Challenges, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Noorain Mukhtiar, Adnan Mahmood, Quan Z. Sheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00799">https://arxiv.org/abs/2509.00799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00799">https://arxiv.org/pdf/2509.00799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00799]] Fairness in Federated Learning: Trends, Challenges, and Opportunities(https://arxiv.org/abs/2509.00799)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>At the intersection of the cutting-edge technologies and privacy concerns, Federated Learning (FL) with its distributed architecture, stands at the forefront in a bid to facilitate collaborative model training across multiple clients while preserving data privacy. However, the applicability of FL systems is hindered by fairness concerns arising from numerous sources of heterogeneity that can result in biases and undermine a system's effectiveness, with skewed predictions, reduced accuracy, and inefficient model convergence. This survey thus explores the diverse sources of bias, including but not limited to, data, client, and model biases, and thoroughly discusses the strengths and limitations inherited within the array of the state-of-the-art techniques utilized in the literature to mitigate such disparities in the FL training process. We delineate a comprehensive overview of the several notions, theoretical underpinnings, and technical aspects associated with fairness and their adoption in FL-based multidisciplinary environments. Furthermore, we examine salient evaluation metrics leveraged to measure fairness quantitatively. Finally, we envisage exciting open research directions that have the potential to drive future advancements in achieving fairer FL frameworks, in turn, offering a strong foundation for future research in this pivotal area.</li>
</ul>

<h3>Title: SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00800">https://arxiv.org/abs/2509.00800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00800">https://arxiv.org/pdf/2509.00800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00800]] SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting(https://arxiv.org/abs/2509.00800)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate 3D reconstruction in underwater environments remains a complex challenge due to issues such as light distortion, turbidity, and limited visibility. AI-based techniques have been applied to address these issues, however, existing methods have yet to fully exploit the potential of AI, particularly in integrating language models with visual processing. In this paper, we propose a novel framework that leverages multimodal cross-knowledge to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity deep-sea scene reconstruction. By embedding an extra semantic feature into each Gaussian primitive and supervised by the CLIP extracted semantic feature, our method enforces semantic and structural awareness throughout the training. The dedicated semantic consistency loss ensures alignment with high-level scene understanding. Besides, we propose a novel stage-wise training strategy, combining coarse-to-fine learning with late-stage parameter refinement, to further enhance both stability and reconstruction quality. Extensive results show that our approach consistently outperforms state-of-the-art methods on SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for applications in underwater exploration and marine perception.</li>
</ul>

<h3>Title: XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Feriel Amel Sellal, Ahmed Ayoub Bellachia, Meryem Malak Dif, Enguerrand De Rautlin De La Roy, Mouhamed Amine Bouchiha, Yacine Ghamri-Doudane</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00802">https://arxiv.org/abs/2509.00802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00802">https://arxiv.org/pdf/2509.00802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00802]] XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations(https://arxiv.org/abs/2509.00802)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is increasingly used in the automotive industry for applications such as driving style classification, which aims to improve road safety, efficiency, and personalize user experiences. While deep learning (DL) models, such as Long Short-Term Memory (LSTM) networks, excel at this task, their black-box nature limits interpretability and trust. This paper proposes a machine learning (ML)-based method that balances high accuracy with interpretability. We introduce a high-quality dataset, CARLA-Drive, and leverage ML techniques like Random Forest (RF), Gradient Boosting (XGBoost), and Support Vector Machine (SVM), which are efficient, lightweight, and interpretable. In addition, we apply the SHAP (Shapley Additive Explanations) explainability technique to provide personalized recommendations for safer driving. Achieving an accuracy of 0.92 on a three-class classification task with both RF and XGBoost classifiers, our approach matches DL models in performance while offering transparency and practicality for real-world deployment in intelligent transportation systems.</li>
</ul>

<h3>Title: CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA</h3>
<ul>
<li><strong>Authors: </strong>Reem Abdel-Salam, Mary Adewunmi, Modinat A. Abayomi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00806">https://arxiv.org/abs/2509.00806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00806">https://arxiv.org/pdf/2509.00806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00806]] CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA(https://arxiv.org/abs/2509.00806)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly evident for accurate question answering across various domains. However, rigorous evaluation of their performance on complex question-answering (QA) capabilities is essential before deployment in real-world biomedical and healthcare applications. This paper presents our approach to the MedHopQA track of the BioCreative IX shared task, which focuses on multi-hop biomedical question answering involving diseases, genes, and chemicals. We adopt a supervised fine-tuning strategy leveraging LLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled from external sources including BioASQ, MedQuAD, and TREC. Three experimental setups are explored: fine-tuning on combined short and long answers, short answers only, and long answers only. While our models demonstrate strong domain understanding, achieving concept-level accuracy scores of up to 0.8, their Exact Match (EM) scores remain significantly lower, particularly in the test phase. We introduce a two-stage inference pipeline for precise short-answer extraction to mitigate verbosity and improve alignment with evaluation metrics. Despite partial improvements, challenges persist in generating strictly formatted outputs. Our findings highlight the gap between semantic understanding and exact answer evaluation in biomedical LLM applications, motivating further research in output control and post-processing strategies.</li>
</ul>

<h3>Title: Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Sanglin Zhao, Baoyu Chen, Mans Gustaf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00808">https://arxiv.org/abs/2509.00808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00808">https://arxiv.org/pdf/2509.00808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00808]] Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification(https://arxiv.org/abs/2509.00808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fetal ultrasound standard plane classification is essential for reliable prenatal diagnosis but faces inherent challenges, including low tissue contrast, boundary ambiguity, and operator-dependent image quality variations. To overcome these limitations, we propose a plug-and-play adaptive contrast adjustment module (ACAM), whose core design is inspired by the clinical practice of doctors adjusting image contrast to obtain clearer and more discriminative structural information. The module employs a shallow texture-sensitive network to predict clinically plausible contrast parameters, transforms input images into multiple contrast-enhanced views through differentiable mapping, and fuses them within downstream classifiers. Validated on a multi-center dataset of 12,400 images across six anatomical categories, the module consistently improves performance across diverse models, with accuracy of lightweight models increasing by 2.02 percent, accuracy of traditional models increasing by 1.29 percent, and accuracy of state-of-the-art models increasing by 1.15 percent. The innovation of the module lies in its content-aware adaptation capability, replacing random preprocessing with physics-informed transformations that align with sonographer workflows while improving robustness to imaging heterogeneity through multi-view fusion. This approach effectively bridges low-level image features with high-level semantics, establishing a new paradigm for medical image analysis under real-world image quality variations.</li>
</ul>

<h3>Title: MAESTROCUT: Dynamic, Noise-Adaptive, and Secure Quantum Circuit Cutting on Near-Term Hardware</h3>
<ul>
<li><strong>Authors: </strong>Samuel Punch, Krishnendu Guha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00811">https://arxiv.org/abs/2509.00811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00811">https://arxiv.org/pdf/2509.00811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00811]] MAESTROCUT: Dynamic, Noise-Adaptive, and Secure Quantum Circuit Cutting on Near-Term Hardware(https://arxiv.org/abs/2509.00811)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We present MaestroCut, a closed-loop framework for quantum circuit cutting that adapts partitioning and shot allocation to device drift and workload variation. MaestroCut tracks a variance proxy in real time, triggers re-cutting when accuracy degrades, and routes shots using topology-aware priors. An online estimator cascade (MLE, Bayesian, GP-assisted) selects the lowest-error reconstruction within a fixed budget. Tier-1 simulations show consistent variance contraction and reduced mean-squared error versus uniform and proportional baselines. Tier-2 emulation with realistic queueing and noise demonstrates stable latency targets, high reliability, and ~1% software overhead under stress scenarios. These results indicate that adaptive circuit cutting can provide accuracy and efficiency improvements with minimal operational cost on near-term hardware.</li>
</ul>

<h3>Title: Adaptive t Design Dummy-Gate Obfuscation for Cryogenic Scale Enforcement</h3>
<ul>
<li><strong>Authors: </strong>Samuel Punch, Krishnendu Guha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00812">https://arxiv.org/abs/2509.00812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00812">https://arxiv.org/pdf/2509.00812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00812]] Adaptive t Design Dummy-Gate Obfuscation for Cryogenic Scale Enforcement(https://arxiv.org/abs/2509.00812)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, fair</a></li>
<li><strong>Abstract: </strong>Cloud quantum services can reveal circuit structure and timing through scheduler metadata, latency patterns, and co-tenant interference. We introduce NADGO (Noise-Adaptive Dummy-Gate Obfuscation), a scheduling and obfuscation stack that enforces operational privacy for gate-model workloads by applying per-interval limits on observable information leakage. To support confidentiality and fair multi-tenancy, operators require a method to audit compliance at acceptable overheads. NADGO combines: (i) hardware-aware t-design padding for structured cover traffic, (ii) particle-filter timing randomization to mask queue patterns, (iii) CASQUE subcircuit routing across heterogeneous backends, and (iv) a per-interval leakage estimator with locked calibration artifacts and a dual-threshold kill-switch. We prototype the approach on a 4-qubit superconducting tile with cryo-CMOS control and evaluate both depth-varied local-random circuits and small QAOA instances. Monitoring runs at a 6.3 microsecond control interval, and per-interval decisions are recorded in an append-only, hash-chained audit log. Across Monte Carlo (Tier 1) and cloud-hardware emulation (Tier 2) evaluations, NADGO maintains leakage within budget in nominal operation (interval-abort rate below 1 percent) and under attack yields high separation with concentrated aborts. At matched leakage targets, microbenchmarks indicate lower latency and cryogenic power consumption than static padding, while end-to-end workloads maintain competitive cost envelopes.</li>
</ul>

<h3>Title: Unlocking the Effectiveness of LoRA-FP for Seamless Transfer Implantation of Fingerprints in Downstream Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Xu, Zhaokun Yan, Binhan Xu, Xin Tong, Haitao Xu, Yourong Chen, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00820">https://arxiv.org/abs/2509.00820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00820">https://arxiv.org/pdf/2509.00820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00820]] Unlocking the Effectiveness of LoRA-FP for Seamless Transfer Implantation of Fingerprints in Downstream Models(https://arxiv.org/abs/2509.00820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models (LLMs), safeguarding intellectual property (IP) has become increasingly critical. To address the challenges of high costs and potential contamination in fingerprint integration, we propose LoRA-FP, a lightweight, plug-and-play framework that embeds backdoor fingerprints into LoRA adapters through constrained fine-tuning. This design enables seamless fingerprint transplantation via parameter fusion, eliminating the need for full-parameter updates while preserving model integrity. Experimental results demonstrate that LoRA-FP not only significantly reduces computational overhead compared to conventional approaches but also achieves superior robustness across diverse scenarios, including incremental training and model fusion. Our code and datasets are publicly available at this https URL.</li>
</ul>

<h3>Title: TMT: A Simple Way to Translate Topic Models Using Dictionaries</h3>
<ul>
<li><strong>Authors: </strong>Felix Engl, Andreas Henrich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00822">https://arxiv.org/abs/2509.00822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00822">https://arxiv.org/pdf/2509.00822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00822]] TMT: A Simple Way to Translate Topic Models Using Dictionaries(https://arxiv.org/abs/2509.00822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The training of topic models for a multilingual environment is a challenging task, requiring the use of sophisticated algorithms, topic-aligned corpora, and manual evaluation. These difficulties are further exacerbated when the developer lacks knowledge of the target language or is working in an environment with limited data, where only small or unusable multilingual corpora are available. Considering these challenges, we introduce Topic Model Translation (TMT), a novel, robust and transparent technique designed to transfer topic models (e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language to another, without the need for metadata, embeddings, or aligned corpora. TMT enables the reuse of topic models across languages, making it especially suitable for scenarios where large corpora in the target language are unavailable or manual translation is infeasible. Furthermore, we evaluate TMT extensively using both quantitative and qualitative methods, demonstrating that it produces semantically coherent and consistent topic translations.</li>
</ul>

<h3>Title: Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinlei Liu, Tao Hu, Peng Yi, Weitao Han, Jichao Xie, Baolin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00826">https://arxiv.org/abs/2509.00826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00826">https://arxiv.org/pdf/2509.00826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00826]] Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization(https://arxiv.org/abs/2509.00826)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Efficient adversarial attack methods are critical for assessing the robustness of computer vision models. In this paper, we reconstruct the optimization objective for generating adversarial examples as "maximizing the difference between the non-true labels' probability upper bound and the true label's probability," and propose a gradient-based attack method termed Sequential Difference Maximization (SDM). SDM establishes a three-layer optimization framework of "cycle-stage-step." The processes between cycles and between iterative steps are respectively identical, while optimization stages differ in terms of loss functions: in the initial stage, the negative probability of the true label is used as the loss function to compress the solution space; in subsequent stages, we introduce the Directional Probability Difference Ratio (DPDR) loss function to gradually increase the non-true labels' probability upper bound by compressing the irrelevant labels' probabilities. Experiments demonstrate that compared with previous SOTA methods, SDM not only exhibits stronger attack performance but also achieves higher attack cost-effectiveness. Additionally, SDM can be combined with adversarial training methods to enhance their defensive effects. The code is available at this https URL.</li>
</ul>

<h3>Title: Surface Defect Detection with Gabor Filter Using Reconstruction-Based Blurring U-Net-ViT</h3>
<ul>
<li><strong>Authors: </strong>Jongwook Si, Sungyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00827">https://arxiv.org/abs/2509.00827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00827">https://arxiv.org/pdf/2509.00827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00827]] Surface Defect Detection with Gabor Filter Using Reconstruction-Based Blurring U-Net-ViT(https://arxiv.org/abs/2509.00827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel approach to enhance the accuracy and reliability of texture-based surface defect detection using Gabor filters and a blurring U-Net-ViT model. By combining the local feature training of U-Net with the global processing of the Vision Transformer(ViT), the model effectively detects defects across various textures. A Gaussian filter-based loss function removes background noise and highlights defect patterns, while Salt-and-Pepper(SP) masking in the training process reinforces texture-defect boundaries, ensuring robust performance in noisy environments. Gabor filters are applied in post-processing to emphasize defect orientation and frequency characteristics. Parameter optimization, including filter size, sigma, wavelength, gamma, and orientation, maximizes performance across datasets like MVTec-AD, Surface Crack Detection, and Marble Surface Anomaly Dataset, achieving an average Area Under the Curve(AUC) of 0.939. The ablation studies validate that the optimal filter size and noise probability significantly enhance defect detection performance.</li>
</ul>

<h3>Title: SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Yang, Hongqiu Wang, Zhaohu Xing, Sixiang Chen, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00833">https://arxiv.org/abs/2509.00833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00833">https://arxiv.org/pdf/2509.00833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00833]] SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3(https://arxiv.org/abs/2509.00833)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The DINO family of self-supervised vision models has shown remarkable transferability, yet effectively adapting their representations for segmentation remains challenging. Existing approaches often rely on heavy decoders with multi-scale fusion or complex upsampling, which introduce substantial parameter overhead and computational cost. In this work, we propose SegDINO, an efficient segmentation framework that couples a frozen DINOv3 backbone with a lightweight decoder. SegDINO extracts multi-level features from the pretrained encoder, aligns them to a common resolution and channel width, and utilizes a lightweight MLP head to directly predict segmentation masks. This design minimizes trainable parameters while preserving the representational power of foundation features. Extensive experiments across six benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO consistently achieves state-of-the-art performance compared to existing methods. Code is available at this https URL.</li>
</ul>

<h3>Title: Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss</h3>
<ul>
<li><strong>Authors: </strong>Jongwook Si, Sungyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00835">https://arxiv.org/abs/2509.00835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00835">https://arxiv.org/pdf/2509.00835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00835]] Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss(https://arxiv.org/abs/2509.00835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Satellite imagery plays a crucial role in various fields; however, atmospheric interference and haze significantly degrade image clarity and reduce the accuracy of information extraction. To address these challenges, this paper proposes a hybrid dehazing framework that integrates Swin Transformer and U-Net to balance global context learning and local detail restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin Transformer-based Residual-in-Residual Dense Block, in both the encoder and decoder to effectively extract features. This module enables the joint learning of global contextual information and fine spatial structures, which is crucial for structural preservation in satellite image. Furthermore, we introduce a composite loss function that combines L2 loss, guided loss, and a novel watershed loss, which enhances structural boundary preservation and ensures pixel-level accuracy. This architecture enables robust dehazing under diverse atmospheric conditions while maintaining structural consistency across restored images. Experimental results demonstrate that the proposed method outperforms state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically, on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an SSIM of 0.967, which is a significant improvement over existing method. This study provides an effective solution for mitigating atmospheric interference in satellite imagery and highlights its potential applicability across diverse remote sensing applications.</li>
</ul>

<h3>Title: Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations</h3>
<ul>
<li><strong>Authors: </strong>Michelle Elizabeth, Alicja Kasicka, Natalia Krawczyk, Magalie Ochs, Gw√©nol√© Lecorv√©, Justyna Gromada, Lina M. Rojas-Barahona</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00841">https://arxiv.org/abs/2509.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00841">https://arxiv.org/pdf/2509.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00841]] Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations(https://arxiv.org/abs/2509.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The growing number of generative AI-based dialogue systems has made their evaluation a crucial challenge. This paper presents our contribution to this important problem through the Dialogue System Technology Challenge (DSTC-12, Track 1), where we developed models to predict dialogue-level, dimension-specific scores. Given the constraint of using relatively small models (i.e. fewer than 13 billion parameters) our work follows two main strategies: employing Language Models (LMs) as evaluators through prompting, and training encoder-based classification and regression models. Our results show that while LM prompting achieves only modest correlations with human judgments, it still ranks second on the test set, outperformed only by the baseline. The regression and classification models, with significantly fewer parameters, demonstrate high correlation for some dimensions on the validation set. Although their performance decreases on the test set, it is important to note that the test set contains annotations with significantly different score ranges for some of the dimensions with respect to the train and validation sets.</li>
</ul>

<h3>Title: Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Tengyu Pan, Zhichao Duan, Zhenyu Li, Bowen Dong, Ning Liu, Xiuxing Li, Jianyong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00842">https://arxiv.org/abs/2509.00842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00842">https://arxiv.org/pdf/2509.00842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00842]] Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings(https://arxiv.org/abs/2509.00842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text embedding models are essential for various natural language processing tasks, enabling the effective encoding of semantic information into dense vector representations. These models are typically optimized using triplets of (query, positive, negative) data pairs for contrastive learning, where the negative samples play a critical role in enhancing the model's ability to discern subtle semantic distinctions. In this work, we introduce a Multi-Granularity Hard-negative (MGH) synthesis framework that leverages large language models (LLMs) to generate diverse negative samples with varying levels of similarity with the query. This approach facilitates a coarse-to-fine curriculum learning strategy during supervised training, allowing the embedding model to progressively learn more nuanced semantic representations. Meanwhile, we propose an Anchor Token Aware (ATA) pooling method that assigns higher weights to anchor tokens based on aggregation patterns observed in LLMs, improving text embedding accuracy without increasing model complexity. Comprehensive experiments on the MTEB benchmark demonstrate that our methods achieve state-of-the-art performance, surpassing existing synthesis strategies both with synthetic data and when combined with public retrieval datasets.</li>
</ul>

<h3>Title: Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Kang, Zhengkang Xiang, Zezheng Zhang, Kourosh Khoshelham</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00843">https://arxiv.org/abs/2509.00843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00843">https://arxiv.org/pdf/2509.00843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00843]] Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion(https://arxiv.org/abs/2509.00843)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery</h3>
<ul>
<li><strong>Authors: </strong>Woon Yee Ng, Li Rong Wang, Siyuan Liu, Xiuyi Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00846">https://arxiv.org/abs/2509.00846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00846">https://arxiv.org/pdf/2509.00846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00846]] Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery(https://arxiv.org/abs/2509.00846)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Explaining machine learning (ML) predictions has become crucial as ML models are increasingly deployed in high-stakes domains such as healthcare. While SHapley Additive exPlanations (SHAP) is widely used for model interpretability, it fails to differentiate between causality and correlation, often misattributing feature importance when features are highly correlated. We propose Causal SHAP, a novel framework that integrates causal relationships into feature attribution while preserving many desirable properties of SHAP. By combining the Peter-Clark (PC) algorithm for causal discovery and the Intervention Calculus when the DAG is Absent (IDA) algorithm for causal strength quantification, our approach addresses the weakness of SHAP. Specifically, Causal SHAP reduces attribution scores for features that are merely correlated with the target, as validated through experiments on both synthetic and real-world datasets. This study contributes to the field of Explainable AI (XAI) by providing a practical framework for causal-aware model explanations. Our approach is particularly valuable in domains such as healthcare, where understanding true causal relationships is critical for informed decision-making.</li>
</ul>

<h3>Title: Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Maximus Powers, Partha Pratim Saha, Mahveen Raza, Rizwan Qureshi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00849">https://arxiv.org/abs/2509.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00849">https://arxiv.org/pdf/2509.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00849]] Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations(https://arxiv.org/abs/2509.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-Image (TTI) models are powerful creative tools but risk amplifying harmful social biases. We frame representational societal bias assessment as an image curation and evaluation task and introduce a pilot benchmark of occupational portrayals spanning five socially salient roles (CEO, Nurse, Software Engineer, Teacher, Athlete). Using five state-of-the-art models: closed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable Diffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against fairness-aware controlled prompts designed to encourage demographic diversity. All outputs are annotated for gender (male, female) and race (Asian, Black, White), enabling structured distributional analysis. Results show that prompting can substantially shift demographic representations, but with highly model-specific effects: some systems diversify effectively, others overcorrect into unrealistic uniformity, and some show little responsiveness. These findings highlight both the promise and the limitations of prompting as a fairness intervention, underscoring the need for complementary model-level strategies. We release all code and data for transparency and reproducibility this https URL.</li>
</ul>

<h3>Title: Predicting Multi-Type Talented Students in Secondary School Using Semi-Supervised Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinzhe Zheng, Zhen-Qun Yang, Jiannong Cao, Jiabei Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00863">https://arxiv.org/abs/2509.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00863">https://arxiv.org/pdf/2509.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00863]] Predicting Multi-Type Talented Students in Secondary School Using Semi-Supervised Machine Learning(https://arxiv.org/abs/2509.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Talent identification plays a critical role in promoting student development. However, traditional approaches often rely on manual processes or focus narrowly on academic achievement, and typically delaying intervention until the higher education stage. This oversight overlooks diverse non-academic talents and misses opportunities for early intervention. To address this gap, this study introduces TalentPredictor, a novel semi-supervised multi-modal neural network that combines Transformer, LSTM, and ANN architectures. This model is designed to predict seven different talent types--academic, sport, art, leadership, service, technology, and others--in secondary school students within an offline educational setting. Drawing on existing offline educational data from 1,041 local secondary students, TalentPredictor overcomes the limitations of traditional talent identification methods. By clustering various award records into talent categories and extracting features from students' diverse learning behaviors, it achieves high prediction accuracy (0.908 classification accuracy, 0.908 ROCAUC). This demonstrates the potential of machine learning to identify diverse talents early in student development.</li>
</ul>

<h3>Title: Exploring and Mitigating Fawning Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Shangguan, Yanjie Dong, Lanjun Wang, Xiaoyi Fan, Victor C. M. Leung, Xiping Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00869">https://arxiv.org/abs/2509.00869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00869">https://arxiv.org/pdf/2509.00869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00869]] Exploring and Mitigating Fawning Hallucinations in Large Language Models(https://arxiv.org/abs/2509.00869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated exceptional proficiency in language understanding. However, when LLMs align their outputs with deceptive and/or misleading prompts, the generated responses could deviate from the de facto information. Such observations are known as fawning hallucinations, where the model prioritizes alignment with the input's implied perspective over accuracy and truthfulness. In this work, we analyze fawning hallucinations in various natural language processing tasks and tailor the so-termed contrastive decoding method for fawning-hallucination mitigation. Specifically, we design two paradigms to generate corresponding deceptive and/or misleading inputs for the consistent fawning hallucinations induction. Then, we propose the collaborative contrastive decoding (CCD) to handle the fawning hallucinations across different tasks in LLMs. By contrasting the deviation in output distribution between induced and transformed neutral inputs, the proposed CCD can reduce reliance on deceptive and/or misleading information without requiring additional training. Extensive experiments demonstrate that the proposed CCD can effectively mitigate fawning hallucinations and improve the factuality of the generated responses over various tasks.</li>
</ul>

<h3>Title: Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening</h3>
<ul>
<li><strong>Authors: </strong>Zirui Zhou, Zizhao Peng, Dongyang Jin, Chao Fan, Fengwei An, Shiqi Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00872">https://arxiv.org/abs/2509.00872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00872">https://arxiv.org/pdf/2509.00872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00872]] Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening(https://arxiv.org/abs/2509.00872)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Recent AI-based scoliosis screening methods primarily rely on large-scale silhouette datasets, often neglecting clinically relevant postural asymmetries-key indicators in traditional screening. In contrast, pose data provide an intuitive skeletal representation, enhancing clinical interpretability across various medical applications. However, pose-based scoliosis screening remains underexplored due to two main challenges: (1) the scarcity of large-scale, annotated pose datasets; and (2) the discrete and noise-sensitive nature of raw pose coordinates, which hinders the modeling of subtle asymmetries. To address these limitations, we introduce Scoliosis1K-Pose, a 2D human pose annotation set that extends the original Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050 adolescents. Building on this dataset, we introduce the Dual Representation Framework (DRF), which integrates a continuous skeleton map to preserve spatial structure with a discrete Postural Asymmetry Vector (PAV) that encodes clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA) module further uses the PAV as clinical prior to direct feature extraction from the skeleton map, focusing on clinically meaningful asymmetries. Extensive experiments demonstrate that DRF achieves state-of-the-art performance. Visualizations further confirm that the model leverages clinical asymmetry cues to guide feature extraction and promote synergy between its dual representations. The dataset and code are publicly available at this https URL.</li>
</ul>

<h3>Title: Tabular Diffusion Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Brian Barr, John Paisley</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00876">https://arxiv.org/abs/2509.00876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00876">https://arxiv.org/pdf/2509.00876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00876]] Tabular Diffusion Counterfactual Explanations(https://arxiv.org/abs/2509.00876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations methods provide an important tool in the field of {interpretable machine learning}. Recent advances in this direction have focused on diffusion models to explain a deep classifier. However, these techniques have predominantly focused on problems in computer vision. In this paper, we focus on tabular data typical in finance and the social sciences and propose a novel guided reverse process for categorical features based on an approximation to the Gumbel-softmax distribution. Furthermore, we study the effect of the temperature $\tau$ and derive a theoretical bound between the Gumbel-softmax distribution and our proposed approximated distribution. We perform experiments on several large-scale credit lending and other tabular datasets, assessing their performance in terms of the quantitative measures of interpretability, diversity, instability, and validity. These results indicate that our approach outperforms popular baseline methods, producing robust and realistic counterfactual explanations.</li>
</ul>

<h3>Title: EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes</h3>
<ul>
<li><strong>Authors: </strong>Yuqin Dai, Guoqing Wang, Yuan Wang, Kairan Dou, Kaichen Zhou, Zhanwei Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Can Yi, Changhua Meng, Yuchen Zhou, Yongliang Shen, Shuai Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00877">https://arxiv.org/abs/2509.00877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00877">https://arxiv.org/pdf/2509.00877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00877]] EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes(https://arxiv.org/abs/2509.00877)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) empowered with retrieval mechanisms have achieved strong progress in open-domain question answering (QA). Yet, the conventional retrieve--then--answer paradigm often suffers from two key limitations: (1) low signal-to-noise ratio in retrieved evidence, where useful information is buried under irrelevant content, and (2) error accumulation in multi-hop reasoning when incomplete or noisy passages are involved. To address these challenges, we present EviNote-RAG, an agentic RAG framework that introduces a structured retrieve--note--answer pipeline. Instead of directly reasoning over raw retrievals, the model is trained to compose Supportive-Evidence Notes (SENs), concise, human-like notes that preserve only answer-relevant information, highlight uncertainty, and explicitly state when no useful evidence exists. This distillation process is further reinforced by the Evidence Quality Reward (EQR), an entailment-based signal that evaluates whether SENs logically support the final answer. Together, SENs and EQR guide the model toward faithful and robust reasoning, while reducing the impact of noise. Experiments on in-domain and out-of-domain QA benchmarks show that EviNote-RAG consistently outperforms strong baselines in accuracy, generalization, and training stability. In particular, it achieves state-of-the-art results while enhancing robustness and efficiency, yielding relative F1 gains of 20\% on HotpotQA (+0.093), 40\% on Bamboogle (+0.151), and 91\% on 2Wiki (+0.256) via denser rewards and reduced verbosity.</li>
</ul>

<h3>Title: VULSOVER: Vulnerability Detection via LLM-Driven Constraint Solving</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Yueci Su, Jiahao Liu, Zhiwei Lin, Yuebing Hou, Peiming Gao, Yuanchao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00882">https://arxiv.org/abs/2509.00882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00882">https://arxiv.org/pdf/2509.00882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00882]] VULSOVER: Vulnerability Detection via LLM-Driven Constraint Solving(https://arxiv.org/abs/2509.00882)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Traditional vulnerability detection methods rely heavily on predefined rule matching, which often fails to capture vulnerabilities accurately. With the rise of large language models (LLMs), leveraging their ability to understand code semantics has emerged as a promising direction for achieving more accurate and efficient vulnerability detection. However, current LLM-based approaches face significant challenges: instability in model outputs, limitations in context length, and hallucination. As a result, many existing solutions either use LLMs merely to enrich predefined rule sets, thereby keeping the detection process fundamentally rule-based, or over-rely on them, leading to poor robustness. To address these challenges, we propose a constraint-solving approach powered by LLMs named VULSOLVER. By modeling vulnerability detection as a constraint-solving problem, and by integrating static application security testing (SAST) with the semantic reasoning capabilities of LLMs, our method enables the LLM to act like a professional human security expert. We assess VULSOLVER on the OWASP Benchmark (1,023 labeled samples), achieving 96.29% accuracy, 96.55% F1-score, and 100% recall. Applied to popular GitHub repositories, VULSOLVER also identified 15 previously unknown high-severity vulnerabilities (CVSS 7.5-9.8), demonstrating its effectiveness in real-world security analysis.</li>
</ul>

<h3>Title: An Explainable Gaussian Process Auto-encoder for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Brian Barr, John Paisley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00884">https://arxiv.org/abs/2509.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00884">https://arxiv.org/pdf/2509.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00884]] An Explainable Gaussian Process Auto-encoder for Tabular Data(https://arxiv.org/abs/2509.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Explainable machine learning has attracted much interest in the community where the stakes are high. Counterfactual explanations methods have become an important tool in explaining a black-box model. The recent advances have leveraged the power of generative models such as an autoencoder. In this paper, we propose a novel method using a Gaussian process to construct the auto-encoder architecture for generating counterfactual samples. The resulting model requires fewer learnable parameters and thus is less prone to overfitting. We also introduce a novel density estimator that allows for searching for in-distribution samples. Furthermore, we introduce an algorithm for selecting the optimal regularization rate on density estimator while searching for counterfactuals. We experiment with our method in several large-scale tabular datasets and compare with other auto-encoder-based methods. The results show that our method is capable of generating diversified and in-distribution counterfactual samples.</li>
</ul>

<h3>Title: SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset</h3>
<ul>
<li><strong>Authors: </strong>RƒÉzvan-Alexandru SmƒÉdu, Andreea Iuga, Dumitru-Clementin Cercel, Florin Pop</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00893">https://arxiv.org/abs/2509.00893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00893">https://arxiv.org/pdf/2509.00893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00893]] SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset(https://arxiv.org/abs/2509.00893)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Satire, irony, and sarcasm are techniques typically used to express humor and critique, rather than deceive; however, they can occasionally be mistaken for factual reporting, akin to fake news. These techniques can be applied at a more granular level, allowing satirical information to be incorporated into news articles. In this paper, we introduce the first sentence-level dataset for Romanian satire detection for news articles, called SeLeRoSa. The dataset comprises 13,873 manually annotated sentences spanning various domains, including social issues, IT, science, and movies. With the rise and recent progress of large language models (LLMs) in the natural language processing literature, LLMs have demonstrated enhanced capabilities to tackle various tasks in zero-shot settings. We evaluate multiple baseline models based on LLMs in both zero-shot and fine-tuning settings, as well as baseline transformer-based models. Our findings reveal the current limitations of these models in the sentence-level satire detection task, paving the way for new research directions.</li>
</ul>

<h3>Title: Hybrid AI-Driven Intrusion Detection: Framework Leveraging Novel Feature Selection for Enhanced Network Security</h3>
<ul>
<li><strong>Authors: </strong>Maryam Mahdi Alhusseini, Mohammad Reza Feizi Derakhshi</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00896">https://arxiv.org/abs/2509.00896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00896">https://arxiv.org/pdf/2509.00896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00896]] Hybrid AI-Driven Intrusion Detection: Framework Leveraging Novel Feature Selection for Enhanced Network Security(https://arxiv.org/abs/2509.00896)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, fair</a></li>
<li><strong>Abstract: </strong>In today's rapidly evolving digital landscape, safeguarding network infrastructures against cyberattacks has become a critical priority. This research presents an innovative AI-driven real-time intrusion detection framework designed to enhance network security, particularly in Wireless Sensor Networks (WSNs) and Cloud Computing (CC) environments. The system employs classical machine learning models, Logistic Regression, Decision Tree, and K-Nearest Neighbors, optimized through the novel Energy Valley Optimization (EVO) method using the NSL-KDD dataset. Feature selection significantly reduced the number of input features from 42 to 18 while maintaining strong detection capabilities. The proposed system achieved 98.95 percent accuracy with Decision Tree, 98.47 percent with K-Nearest Neighbors, and 88.84 percent with Logistic Regression. Moreover, high precision, recall, and F1-scores were attained across all classifiers while substantially reducing training and testing times, making the framework highly suitable for real-time applications. To ensure fair detection across diverse attack types, dataset balancing via downsampling was applied to address class imbalance challenges. This investigation focuses on the significance of advancing intrusion detection systems in cloud computing and WSNs. Overall, this work advances secure communications by delivering a scalable, low-latency, and high-accuracy intrusion detection solution aligned with the latest trends in artificial intelligence, cybersecurity, and real-time digital networks</li>
</ul>

<h3>Title: Spotlighter: Revisiting Prompt Tuning from a Representative Mining View</h3>
<ul>
<li><strong>Authors: </strong>Yutong Gao, Maoyuan Shao, Xinyang Huang, Chuang Zhu, Lijuan Sun, Yu Weng, Xuan Liu, Guoshun Nan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00905">https://arxiv.org/abs/2509.00905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00905">https://arxiv.org/pdf/2509.00905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00905]] Spotlighter: Revisiting Prompt Tuning from a Representative Mining View(https://arxiv.org/abs/2509.00905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>CLIP's success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual token's activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at this https URL.</li>
</ul>

<h3>Title: PREE: Towards Harmless and Adaptive Fingerprint Editing in Large Language Models via Knowledge Prefix Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xubin Yue, Zhenhua Xu, Wenpeng Xing, Jiahui Yu, Mohan Li, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00918">https://arxiv.org/abs/2509.00918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00918">https://arxiv.org/pdf/2509.00918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00918]] PREE: Towards Harmless and Adaptive Fingerprint Editing in Large Language Models via Knowledge Prefix Enhancement(https://arxiv.org/abs/2509.00918)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Addressing the intellectual property protection challenges in commercial deployment of large language models (LLMs), existing black-box fingerprinting techniques face dual challenges from incremental fine-tuning erasure and feature-space defense due to their reliance on overfitting high-perplexity trigger patterns. Recent work has revealed that model editing in the fingerprinting domain offers distinct advantages, including significantly lower false positive rates, enhanced harmlessness, and superior robustness. Building on this foundation, this paper innovatively proposes a $\textbf{Pr}$efix-$\textbf{e}$nhanced Fingerprint $\textbf{E}$diting Framework (PREE), which encodes copyright information into parameter offsets through dual-channel knowledge edit to achieve covert embedding of fingerprint features. Experimental results demonstrate that the proposed solution achieves the 90\% trigger precision in mainstream architectures including LLaMA-3 and Qwen-2.5. The minimal parameter offset (change rate < 0.03) effectively preserves original knowledge representation while demonstrating strong robustness against incremental fine-tuning and multi-dimensional defense strategies, maintaining zero false positive rate throughout evaluations.</li>
</ul>

<h3>Title: Supervised In-Context Fine-Tuning for Generative Sequence Labeling</h3>
<ul>
<li><strong>Authors: </strong>David Dukiƒá, Goran Glava≈°, Jan ≈†najder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00921">https://arxiv.org/abs/2509.00921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00921">https://arxiv.org/pdf/2509.00921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00921]] Supervised In-Context Fine-Tuning for Generative Sequence Labeling(https://arxiv.org/abs/2509.00921)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sequence labeling (SL) tasks, where labels are assigned to tokens, are abundant in NLP (e.g., named entity recognition and aspect-based sentiment analysis). Owing to the intuition that they require bidirectional context, SL tasks are commonly tackled with encoder-only models. Recent work also shows that removing the causal mask in fine-tuning enables decoder-based LLMs to become effective token classifiers. Less work, however, focused on (supervised) generative SL, a more natural setting for causal LLMs. Due to their rapid scaling, causal LLMs applied to SL are expected to outperform encoders, whose own development has stagnated. In this work, we propose supervised in-context fine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained response generation, natural to LLMs, combining (1) in-context learning (ICL) from demonstrations with (2) supervised fine-tuning. SIFT considerably outperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of standard SL tasks. We further find that although long context hinders the performance of generative SL in both ICL and SIFT, this deficiency can be mitigated by removing the instruction, as instructions are shown to be largely unnecessary for achieving strong SL performance with SIFT. Our findings highlight strengths and limitations of SL with LLMs, underscoring the importance of a response-based generative task formulation for effective SL performance.</li>
</ul>

<h3>Title: DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Aman Sharma, Saeed Najafi, Parsa Farinneya, Benyamin Jamialahmadi, Marzieh S. Tahaei, Yuhe Fan, Mehdi Rezagholizadeh, Boxing Chen, Aref Jafari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00925">https://arxiv.org/abs/2509.00925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00925">https://arxiv.org/pdf/2509.00925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00925]] DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers(https://arxiv.org/abs/2509.00925)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers achieve state-of-the-art results across many tasks, but their uniform application of quadratic self-attention to every token at every layer makes them computationally expensive. We introduce DTRNet (Dynamic Token Routing Network), an improved Transformer architecture that allows tokens to dynamically skip the quadratic cost of cross-token mixing while still receiving lightweight linear updates. By preserving the MLP module and reducing the attention cost for most tokens to linear, DTRNet ensures that every token is explicitly updated while significantly lowering overall computation. This design offers an efficient and effective alternative to standard dense attention. Once trained, DTRNet blocks routes only ~10% of tokens through attention at each layer while maintaining performance comparable to a full Transformer. It consistently outperforms routing-based layer skipping methods such as MoD and D-LLM in both accuracy and memory at matched FLOPs, while routing fewer tokens to full attention. Its efficiency gains, scales with sequence length, offering significant reduction in FLOPs for long-context inputs. By decoupling token updates from attention mixing, DTRNet substantially reduces the quadratic share of computation, providing a simple, efficient, and scalable alternative to Transformers.</li>
</ul>

<h3>Title: MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework</h3>
<ul>
<li><strong>Authors: </strong>Md Shahidul Salim, Lian Fu, Arav Adikesh Ramakrishnan, Zonghai Yao, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00934">https://arxiv.org/abs/2509.00934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00934">https://arxiv.org/pdf/2509.00934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00934]] MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework(https://arxiv.org/abs/2509.00934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed to improve English-to-Spanish medical translation by integrating domain-specific structured knowledge into large language models (LLMs). MedCOD integrates domain-specific knowledge from both the Unified Medical Language System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance structured prompting and fine-tuning. We constructed a parallel corpus of 2,999 English-Spanish MedlinePlus articles and a 100-sentence test set annotated with structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B, Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that incorporated multilingual variants, medical synonyms, and UMLS-derived definitions, combined with LoRA-based fine-tuning. Experimental results demonstrate that MedCOD significantly improves translation quality across all models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23, chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model adaptation independently contribute to performance gains, with their combination yielding the highest improvements. These findings highlight the potential of structured knowledge integration to enhance LLMs for medical translation tasks.</li>
</ul>

<h3>Title: SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Aref Jafari, Yuhe Fan, Benyamin Jamialahmadi, Parsa Farinneya, Boxing Chen, Marzieh S. Tahaei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00935">https://arxiv.org/abs/2509.00935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00935">https://arxiv.org/pdf/2509.00935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00935]] SCOUT: Toward Sub-Quadratic Attention via Segment Compression for Optimized Utility in Transformers(https://arxiv.org/abs/2509.00935)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have demonstrated strong performance across a wide range of sequence modeling tasks, but their quadratic attention complexity limits scalability to long sequences. Linear models such as Mamba and sliding-window attention (SWA) address this by mixing tokens through recurrent or localized operations with fixed-size memory, achieving efficient inference. However, these methods risk degrading performance on long sequences due to their inability to retain detailed information from distant tokens. We propose SCOUT (Segment Compression for Optimized Utility in Transformers), a hybrid architecture that compresses tokens locally within fixed-size segments and applies attention only over these compressed representations. Each token embedding is first enriched via a linear local mixer, Mamba or SWA, that integrates recent context. Then, instead of attending to all previous tokens, each token sparsely attends to a small number of compressed checkpoint tokens that summarize the input history. This design retains much of the expressivity of full attention while substantially reducing the computational and memory cost. By attending to compressed history rather than all previous tokens, SCOUT incurs slightly higher memory than purely linear models, but its growth rate remains sub-quadratic and far more scalable than that of full Transformers. We analyze SCOUT's computational and memory efficiency and evaluate it empirically on long-context language modeling and reasoning tasks. SCOUT with both Mamba and SWA mixers outperforms strong long-sequence baselines under the same computational budget, matches full-attention Transformers on language modeling and common-sense reasoning tasks at 400M and 1.3B scales. Moreover, our SCOUT achieves higher end-to-end throughput than SOTA models, while delivering comparable results on long sequence benchmarks.</li>
</ul>

<h3>Title: Structure and Destructure: Dual Forces in the Making of Knowledge Engines</h3>
<ul>
<li><strong>Authors: </strong>Yihong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00949">https://arxiv.org/abs/2509.00949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00949">https://arxiv.org/pdf/2509.00949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00949]] Structure and Destructure: Dual Forces in the Making of Knowledge Engines(https://arxiv.org/abs/2509.00949)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The making of knowledge engines in natural language processing has been shaped by two seemingly distinct paradigms: one grounded in structure, the other driven by massively available unstructured data. The structured paradigm leverages predefined symbolic interactions, such as knowledge graphs, as priors and designs models to capture them. In contrast, the unstructured paradigm centers on scaling transformer architectures with increasingly vast data and model sizes, as seen in modern large language models. Despite their divergence, this thesis seeks to establish conceptual connections bridging these paradigms. Two complementary forces, structure and destructure, emerge across both paradigms: structure organizes seen symbolic interactions, while destructure, through periodic embedding resets, improves model plasticity and generalization to unseen scenarios. These connections form a new recipe for developing general knowledge engines that can support transparent, controllable, and adaptable intelligent systems.</li>
</ul>

<h3>Title: Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kanchon Gharami, Hansaka Aluvihare, Shafika Showkat Moni, Berker Pek√∂z</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00973">https://arxiv.org/abs/2509.00973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00973">https://arxiv.org/pdf/2509.00973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00973]] Clone What You Can't Steal: Black-Box LLM Replication via Logit Leakage and Distillation(https://arxiv.org/abs/2509.00973)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust, steal, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in mission-critical systems, facilitating tasks such as satellite operations, command-and-control, military decision support, and cyber defense. Many of these systems are accessed through application programming interfaces (APIs). When such APIs lack robust access controls, they can expose full or top-k logits, creating a significant and often overlooked attack surface. Prior art has mainly focused on reconstructing the output projection layer or distilling surface-level behaviors. However, regenerating a black-box model under tight query constraints remains underexplored. We address that gap by introducing a constrained replication pipeline that transforms partial logit leakage into a functional deployable substitute model clone. Our two-stage approach (i) reconstructs the output projection matrix by collecting top-k logits from under 10k black-box queries via singular value decomposition (SVD) over the logits, then (ii) distills the remaining architecture into compact student models with varying transformer depths, trained on an open source dataset. A 6-layer student recreates 97.6% of the 6-layer teacher model's hidden-state geometry, with only a 7.31% perplexity increase, and a 7.58 Negative Log-Likelihood (NLL). A 4-layer variant achieves 17.1% faster inference and 18.1% parameter reduction with comparable performance. The entire attack completes in under 24 graphics processing unit (GPU) hours and avoids triggering API rate-limit defenses. These results demonstrate how quickly a cost-limited adversary can clone an LLM, underscoring the urgent need for hardened inference APIs and secure on-premise defense deployments.</li>
</ul>

<h3>Title: RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Feng Liu, Fang-Ming Hung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00974">https://arxiv.org/abs/2509.00974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00974">https://arxiv.org/pdf/2509.00974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00974]] RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning(https://arxiv.org/abs/2509.00974)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley-Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B-13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs.</li>
</ul>

<h3>Title: Towards Integrating Multi-Spectral Imaging with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Josef Gr√ºn, Lukas Meyer, Maximilian Weiherer, Bernhard Egger, Marc Stamminger, Linus Franke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00989">https://arxiv.org/abs/2509.00989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00989">https://arxiv.org/pdf/2509.00989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00989]] Towards Integrating Multi-Spectral Imaging with Gaussian Splatting(https://arxiv.org/abs/2509.00989)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a study of how to integrate color (RGB) and multi-spectral imagery (red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS) framework, a state-of-the-art explicit radiance-field-based method for fast and high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on RGB data, naive per-band optimization of additional spectra yields poor reconstructions due to inconsistently appearing geometry in the spectral domain. This problem is prominent, even though the actual geometry is the same, regardless of spectral modality. To investigate this, we evaluate three strategies: 1) Separate per-band reconstruction with no shared structure. 2) Splitting optimization, in which we first optimize RGB geometry, copy it, and then fit each new band to the model by optimizing both geometry and band representation. 3) Joint, in which the modalities are jointly optimized, optionally with an initial RGB-only phase. We showcase through quantitative metrics and qualitative novel-view renderings on multi-spectral datasets the effectiveness of our dedicated optimized Joint strategy, increasing overall spectral reconstruction as well as enhancing RGB results through spectral cross-talk. We therefore suggest integrating multi-spectral data directly into the spherical harmonics color components to compactly model each Gaussian's multi-spectral reflectance. Moreover, our analysis reveals several key trade-offs in when and how to introduce spectral bands during optimization, offering practical insights for robust multi-modal 3DGS reconstruction.</li>
</ul>

<h3>Title: Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Olusola Odeyomi, Sofiat Olaosebikan, Ajibuwa Opeyemi, Oluwadoyinsola Ige</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.00992">https://arxiv.org/abs/2509.00992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.00992">https://arxiv.org/pdf/2509.00992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.00992]] Online Decentralized Federated Multi-task Learning With Trustworthiness in Cyber-Physical Systems(https://arxiv.org/abs/2509.00992)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Multi-task learning is an effective way to address the challenge of model personalization caused by high data heterogeneity in federated learning. However, extending multi-task learning to the online decentralized federated learning setting is yet to be explored. The online decentralized federated learning setting considers many real-world applications of federated learning, such as autonomous systems, where clients communicate peer-to-peer and the data distribution of each client is time-varying. A more serious problem in real-world applications of federated learning is the presence of Byzantine clients. Byzantine-resilient approaches used in federated learning work only when the number of Byzantine clients is less than one-half the total number of clients. Yet, it is difficult to put a limit on the number of Byzantine clients within a system in reality. However, recent work in robotics shows that it is possible to exploit cyber-physical properties of a system to predict clients' behavior and assign a trust probability to received signals. This can help to achieve resiliency in the presence of a dominating number of Byzantine clients. Therefore, in this paper, we develop an online decentralized federated multi-task learning algorithm to provide model personalization and resiliency when the number of Byzantine clients dominates the number of honest clients. Our proposed algorithm leverages cyber-physical properties, such as the received signal strength in wireless systems or side information, to assign a trust probability to local models received from neighbors in each iteration. Our simulation results show that the proposed algorithm performs close to a Byzantine-free setting.</li>
</ul>

<h3>Title: Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions</h3>
<ul>
<li><strong>Authors: </strong>Ghazal Farhani, Taufiq Rahman, Dominique Charlebois</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01013">https://arxiv.org/abs/2509.01013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01013">https://arxiv.org/pdf/2509.01013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01013]] Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions(https://arxiv.org/abs/2509.01013)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rainy weather significantly increases the risk of road accidents due to reduced visibility and vehicle traction. Understanding how experienced drivers adapt their visual perception through gaze behavior under such conditions is critical for designing robust driver monitoring systems (DMS) and for informing advanced driver assistance systems (ADAS). This case study investigates the eye gaze behavior of a driver operating the same highway route under both clear and rainy conditions. To this end, gaze behavior was analyzed by a two-step clustering approach: first, clustering gaze points within 10-second intervals, and then aggregating cluster centroids into meta-clusters. This, along with Markov transition matrices and metrics such as fixation duration, gaze elevation, and azimuth distributions, reveals meaningful behavioral shifts. While the overall gaze behavior focused on the road with occasional mirror checks remains consistent, rainy conditions lead to more frequent dashboard glances, longer fixation durations, and higher gaze elevation, indicating increased cognitive focus. These findings offer valuable insight into visual attention patterns under adverse conditions and highlight the potential of leveraging gaze modeling to aid in the design of more robust ADAS and DMS.</li>
</ul>

<h3>Title: Any-Order Flexible Length Masked Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Lee Cheuk-Kit, Carles Domingo-Enrich, Yilun Du, Sham Kakade, Timothy Ngotiaoco, Sitan Chen, Michael Albergo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01025">https://arxiv.org/abs/2509.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01025">https://arxiv.org/pdf/2509.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01025]] Any-Order Flexible Length Masked Diffusion(https://arxiv.org/abs/2509.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have recently emerged as a promising alternative to autoregressive models over discrete domains. MDMs generate sequences in an any-order, parallel fashion, enabling fast inference and strong performance on non-causal tasks. However, a crucial limitation is that they do not support token insertions and are thus limited to fixed-length generations. To this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a discrete diffusion paradigm that simultaneously can model sequences of flexible length while provably retaining MDMs' flexibility of any-order inference. Grounded in an extension of the stochastic interpolant framework, FlexMDMs generate sequences by inserting mask tokens and unmasking them. Empirically, we show that FlexMDMs match MDMs in perplexity while modeling length statistics with much higher fidelity. On a synthetic maze planning task, they achieve $\approx 60 \%$ higher success rate than MDM baselines. Finally, we show pretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes only three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior performance on math (GSM8K, $58\% \to 67\%$) and code infilling performance ($52\% \to 65\%$).</li>
</ul>

<h3>Title: Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhou Ye, Kevin I-Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01031">https://arxiv.org/abs/2509.01031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01031">https://arxiv.org/pdf/2509.01031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01031]] Reinforcement Learning Driven Generalizable Feature Representation for Cross-User Activity Recognition(https://arxiv.org/abs/2509.01031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) using wearable sensors is crucial for healthcare, fitness tracking, and smart environments, yet cross-user variability -- stemming from diverse motion patterns, sensor placements, and physiological traits -- hampers generalization in real-world settings. Conventional supervised learning methods often overfit to user-specific patterns, leading to poor performance on unseen users. Existing domain generalization approaches, while promising, frequently overlook temporal dependencies or depend on impractical domain-specific labels. We propose Temporal-Preserving Reinforcement Learning Domain Generalization (TPRL-DG), a novel framework that redefines feature extraction as a sequential decision-making process driven by reinforcement learning. TPRL-DG leverages a Transformer-based autoregressive generator to produce temporal tokens that capture user-invariant activity dynamics, optimized via a multi-objective reward function balancing class discrimination and cross-user invariance. Key innovations include: (1) an RL-driven approach for domain generalization, (2) autoregressive tokenization to preserve temporal coherence, and (3) a label-free reward design eliminating the need for target user annotations. Evaluations on the DSADS and PAMAP2 datasets show that TPRL-DG surpasses state-of-the-art methods in cross-user generalization, achieving superior accuracy without per-user calibration. By learning robust, user-invariant temporal patterns, TPRL-DG enables scalable HAR systems, facilitating advancements in personalized healthcare, adaptive fitness tracking, and context-aware environments.</li>
</ul>

<h3>Title: We Politely Insist: Your LLM Must Learn the Persian Art of Taarof</h3>
<ul>
<li><strong>Authors: </strong>Nikta Gohari Sadr, Sahar Heidariasl, Karine Megerdoomian, Laleh Seyyed-Kalantari, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01035">https://arxiv.org/abs/2509.01035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01035">https://arxiv.org/pdf/2509.01035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01035]] We Politely Insist: Your LLM Must Learn the Persian Art of Taarof(https://arxiv.org/abs/2509.01035)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) struggle to navigate culturally specific communication norms, limiting their effectiveness in global contexts. We focus on Persian taarof, a social norm in Iranian interactions, which is a sophisticated system of ritual politeness that emphasizes deference, modesty, and indirectness, yet remains absent from existing cultural benchmarks. We introduce TaarofBench, the first benchmark for evaluating LLM understanding of taarof, comprising 450 role-play scenarios covering 12 common social interaction topics, validated by native speakers. Our evaluation of five frontier LLMs reveals substantial gaps in cultural competence, with accuracy rates 40-48% below native speakers when taarof is culturally appropriate. Performance varies between interaction topics, improves with Persian-language prompts, and exhibits gender-based asymmetries. We also show that responses rated "polite" by standard metrics often violate taarof norms, indicating the limitations of Western politeness frameworks. Through supervised fine-tuning and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in model alignment with cultural expectations. Our human study with 33 participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers) forms baselines in varying degrees of familiarity with Persian norms. This work lays the foundation for developing diverse and culturally aware LLMs, enabling applications that better navigate complex social interactions.</li>
</ul>

<h3>Title: MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature</h3>
<ul>
<li><strong>Authors: </strong>Hirofumi Tsuruta, Masaya Kumagai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01042">https://arxiv.org/abs/2509.01042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01042">https://arxiv.org/pdf/2509.01042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01042]] MatPROV: A Provenance Graph Dataset of Material Synthesis Extracted from Scientific Literature(https://arxiv.org/abs/2509.01042)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Synthesis procedures play a critical role in materials research, as they directly affect material properties. With data-driven approaches increasingly accelerating materials discovery, there is growing interest in extracting synthesis procedures from scientific literature as structured data. However, existing studies often rely on rigid, domain-specific schemas with predefined fields for structuring synthesis procedures or assume that synthesis procedures are linear sequences of operations, which limits their ability to capture the structural complexity of real-world procedures. To address these limitations, we adopt PROV-DM, an international standard for provenance information, which supports flexible, graph-based modeling of procedures. We present MatPROV, a dataset of PROV-DM-compliant synthesis procedures extracted from scientific literature using large language models. MatPROV captures structural complexities and causal relationships among materials, operations, and conditions through visually intuitive directed graphs. This representation enables machine-interpretable synthesis knowledge, opening opportunities for future research such as automated synthesis planning and optimization.</li>
</ul>

<h3>Title: Lightening the Load: A Cluster-Based Framework for A Lower-Overhead, Provable Website Fingerprinting Defense</h3>
<ul>
<li><strong>Authors: </strong>Khashayar Khajavi, Tao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01046">https://arxiv.org/abs/2509.01046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01046">https://arxiv.org/pdf/2509.01046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01046]] Lightening the Load: A Cluster-Based Framework for A Lower-Overhead, Provable Website Fingerprinting Defense(https://arxiv.org/abs/2509.01046)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Website fingerprinting (WF) attacks remain a significant threat to encrypted traffic, prompting the development of a wide range of defenses. Among these, two prominent classes are regularization-based defenses, which shape traffic using fixed padding rules, and supersequence-based approaches, which conceal traces among predefined patterns. In this work, we present a unified framework for designing an adaptive WF defense that combines the effectiveness of regularization with the provable security of supersequence-style grouping. The scheme first extracts behavioural patterns from traces and clusters them into (k,l)-diverse anonymity sets; an early-time-series classifier (adapted from ECDIRE) then switches from a conservative global set of regularization parameters to the lighter, set-specific parameters. We instantiate the design as Adaptive Tamaraw, a variant of Tamaraw that assigns padding parameters on a per-cluster basis while retaining its original information-theoretic guarantee. Comprehensive experiments on public real-world datasets confirm the benefits. By tuning k, operators can trade privacy for efficiency: in its high-privacy mode Adaptive Tamaraw pushes the bound on any attacker's accuracy below 30%, whereas in efficiency-centred settings it cuts total overhead by 99% compared with classic Tamaraw.</li>
</ul>

<h3>Title: A Unified Low-level Foundation Model for Enhancing Pathology Image Quality</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Liu, Zhe Xu, Jiabo Ma, Wenqaing Li, Junlin Hou, Fuxiang Huang, Xi Wang, Ronald Cheong Kin Chan, Terence Tsz Wai Wong, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01071">https://arxiv.org/abs/2509.01071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01071">https://arxiv.org/pdf/2509.01071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01071]] A Unified Low-level Foundation Model for Enhancing Pathology Image Quality(https://arxiv.org/abs/2509.01071)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized computational pathology by achieving remarkable success in high-level diagnostic tasks, yet the critical challenge of low-level image enhancement remains largely unaddressed. Real-world pathology images frequently suffer from degradations such as noise, blur, and low resolution due to slide preparation artifacts, staining variability, and imaging constraints, while the reliance on physical staining introduces significant costs, delays, and inconsistency. Although existing methods target individual problems like denoising or super-resolution, their task-specific designs lack the versatility to handle the diverse low-level vision challenges encountered in practice. To bridge this gap, we propose the first unified Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality in restoration tasks, including super-resolution, deblurring, and denoising, as well as facilitating image translation tasks like virtual staining (H&E and special stains), all through a single adaptable architecture. Our approach introduces a contrastive pre-trained encoder that learns transferable, stain-invariant feature representations from 190 million unlabeled pathology images, enabling robust identification of degradation patterns. A unified conditional diffusion process dynamically adapts to specific tasks via textual prompts, ensuring precise control over output quality. Trained on a curated dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5 staining protocols, LPFM demonstrates statistically significant improvements (p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual staining.</li>
</ul>

<h3>Title: IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Zhang, Xusheng Zhu, Yuchen Xu, ChiaEn Lu, Hsinyu Shih, Gert Cauwenberghs, Tzyy-Ping Jung</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01073">https://arxiv.org/abs/2509.01073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01073">https://arxiv.org/pdf/2509.01073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01073]] IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models(https://arxiv.org/abs/2509.01073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) is a non-invasive method for measuring brain activity with high temporal resolution; however, EEG signals often exhibit low signal-to-noise ratios because of contamination from physiological and environmental artifacts. One of the major challenges hindering the real-world deployment of brain-computer interfaces (BCIs) involves the frequent occurrence of motion-related EEG artifacts. Most prior studies on EEG motion artifact removal rely on single-modality approaches, such as Artifact Subspace Reconstruction (ASR) and Independent Component Analysis (ICA), without incorporating simultaneously recorded modalities like inertial measurement units (IMUs), which directly capture the extent and dynamics of motion. This work proposes a fine-tuned large brain model (LaBraM)-based correlation attention mapping method that leverages spatial channel relationships in IMU data to identify motion-related artifacts in EEG signals. The fine-tuned model contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU recordings for training, just 0.2346\% of the 2500 hours used to train the base model. We compare our results against the established ASR-ICA benchmark across varying time scales and motion activities, showing that incorporating IMU reference signals significantly improves robustness under diverse motion scenarios.</li>
</ul>

<h3>Title: SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yao Wang, Dong Yang, Zhi Qiao, Wenjian Huang, Liuzhi Yang, Zhen Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01080">https://arxiv.org/abs/2509.01080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01080">https://arxiv.org/pdf/2509.01080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01080]] SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection(https://arxiv.org/abs/2509.01080)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Abnormality detection in medical imaging is a critical task requiring both high efficiency and accuracy to support effective diagnosis. While convolutional neural networks (CNNs) and Transformer-based models are widely used, both face intrinsic challenges: CNNs have limited receptive fields, restricting their ability to capture broad contextual information, and Transformers encounter prohibitive computational costs when processing high-resolution medical images. Mamba, a recent innovation in natural language processing, has gained attention for its ability to process long sequences with linear complexity, offering a promising alternative. Building on this foundation, we present SpectMamba, the first Mamba-based architecture designed for medical image detection. A key component of SpectMamba is the Hybrid Spatial-Frequency Attention (HSFA) block, which separately learns high- and low-frequency features. This approach effectively mitigates the loss of high-frequency information caused by frequency bias and correlates frequency-domain features with spatial features, thereby enhancing the model's ability to capture global context. To further improve long-range dependencies, we propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert Curve Scanning technique to strengthen spatial correlations and local dependencies, further optimizing the Mamba framework. Comprehensive experiments show that SpectMamba achieves state-of-the-art performance while being both effective and efficient across various medical image detection tasks.</li>
</ul>

<h3>Title: Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutez Al-Khatib, Mohammed Ghaly</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01081">https://arxiv.org/abs/2509.01081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01081">https://arxiv.org/pdf/2509.01081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01081]] Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation(https://arxiv.org/abs/2509.01081)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: this https URL</li>
</ul>

<h3>Title: REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Madhav Kanda, Shubham Ugare, Sasa Misailovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01082">https://arxiv.org/abs/2509.01082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01082">https://arxiv.org/pdf/2509.01082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01082]] REFINESTAT: Efficient Exploration for Probabilistic Program Synthesis(https://arxiv.org/abs/2509.01082)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Probabilistic programming offers a powerful framework for modeling uncertainty, yet statistical model discovery in this domain entails navigating an immense search space under strict domain-specific constraints. When small language models are tasked with generating probabilistic programs, they frequently produce outputs that suffer from both syntactic and semantic errors, such as flawed inference constructs. Motivated by probabilistic programmers' domain expertise and debugging strategies, we introduce RefineStat, a language model--driven framework that enforces semantic constraints ensuring synthesized programs contain valid distributions and well-formed parameters, and then applies diagnostic-aware refinement by resampling prior or likelihood components whenever reliability checks fail. We evaluate RefineStat on multiple probabilistic-programming code-generation tasks using smaller language models (SLMs) and find that it produces programs that are both syntactically sound and statistically reliable, often matching or surpassing those from closed-source large language models (e.g., OpenAI o3).</li>
</ul>

<h3>Title: Bidirectional Sparse Attention for Faster Video Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01085">https://arxiv.org/abs/2509.01085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01085">https://arxiv.org/pdf/2509.01085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01085]] Bidirectional Sparse Attention for Faster Video Diffusion Training(https://arxiv.org/abs/2509.01085)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.</li>
</ul>

<h3>Title: Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinwen Chen, Hainan Zhang, Liang Pang, Yongxin Tong, Haibo Zhou, Yuan Zhan, Wei Lin, Zhiming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01088">https://arxiv.org/abs/2509.01088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01088">https://arxiv.org/pdf/2509.01088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01088]] Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation(https://arxiv.org/abs/2509.01088)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The current RAG system requires uploading plaintext documents to the cloud, risking private data leakage. Parametric RAG (PRAG) addresses this by encoding documents as LoRA within LLMs, enabling reasoning without exposing raw content. However, it still faces two issues: (1) PRAG demands synthesizing QA pairs and fine-tuning LLM for each individual document to create its corresponding LoRA, leading to unacceptable inference latency. (2) The performance of PRAG relies solely on synthetic QA data, lacking internal alignment with standard RAG, resulting in poor generalization on out-of-distribution(OOD) inputs. Therefore, achieving high-efficiency parameterization while maintaining RAG-level performance remains a critical challenge for privacy-preserving reasoning. In this paper, we propose DistilledPRAG, a generalizable knowledge-distilled parametric RAG model aligned with standard RAG in document structure and parameter activation. We first synthesize QA pairs from single and multi-documents to enhance cross-document reasoning. Then, we mask the plaintext documents with a special token and translate them to LoRA via a parameter generator, maintaining the standard RAG document structure. Finally, guided by synthetic QA data, we train the parameter generator to match standard RAG's hidden states and output logits, enabling RAG-style reasoning without original documents. Experiments on four QA datasets show that DistilledPRAG outperforms baselines in accuracy and generalizes well on OOD data.</li>
</ul>

<h3>Title: REFRAG: Rethinking RAG based Decoding</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01092">https://arxiv.org/abs/2509.01092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01092">https://arxiv.org/pdf/2509.01092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01092]] REFRAG: Rethinking RAG based Decoding(https://arxiv.org/abs/2509.01092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.</li>
</ul>

<h3>Title: Natural Context Drift Undermines the Natural Language Understanding of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yulong Wu, Viktor Schlegel, Riza Batista-Navarro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01093">https://arxiv.org/abs/2509.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01093">https://arxiv.org/pdf/2509.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01093]] Natural Context Drift Undermines the Natural Language Understanding of Large Language Models(https://arxiv.org/abs/2509.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>How does the natural evolution of context paragraphs affect question answering in generative Large Language Models (LLMs)? To investigate this, we propose a framework for curating naturally evolved, human-edited variants of reading passages from contemporary QA benchmarks and for analyzing LLM performance across a range of semantic similarity scores, which quantify how closely each variant aligns with content seen during pretraining. Using this framework, we evaluate six QA datasets and eight LLMs with publicly available training data. Our experiments reveal that LLM performance declines as reading passages naturally diverge from the versions encountered during pretraining-even when the question and all necessary information remains present at inference time. For instance, average model accuracy on BoolQ drops by over 30% from the highest to lowest similarity bins, with slopes exceeding 70 across several LLMs. These findings suggest that natural text evolution poses a significant challenge to the language understanding capabilities of LLMs.</li>
</ul>

<h3>Title: An End-to-End Framework for Video Multi-Person Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhihong Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01095">https://arxiv.org/abs/2509.01095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01095">https://arxiv.org/pdf/2509.01095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01095]] An End-to-End Framework for Video Multi-Person Pose Estimation(https://arxiv.org/abs/2509.01095)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video-based human pose estimation models aim to address scenarios that cannot be effectively solved by static image models such as motion blur, out-of-focus and occlusion. Most existing approaches consist of two stages: detecting human instances in each image frame and then using a temporal model for single-person pose estimation. This approach separates the spatial and temporal dimensions and cannot capture the global spatio-temporal context between spatial instances for end-to-end optimization. In addition, it relies on separate detectors and complex post-processing such as RoI cropping and NMS, which reduces the inference efficiency of the video scene. To address the above problems, we propose VEPE (Video End-to-End Pose Estimation), a simple and flexible framework for end-to-end pose estimation in video. The framework utilizes three crucial spatio-temporal Transformer components: the Spatio-Temporal Pose Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the Spatio-Temporal Pose Decoder (STPD). These components are designed to effectively utilize temporal context for optimizing human body pose estimation. Furthermore, to reduce the mismatch problem during the cross-frame pose query matching process, we propose an instance consistency mechanism, which aims to enhance the consistency and discrepancy of the cross-frame instance query and realize the instance tracking function, which in turn accurately guides the pose query to perform cross-frame matching. Extensive experiments on the Posetrack dataset show that our approach outperforms most two-stage models and improves inference efficiency by 300%.</li>
</ul>

<h3>Title: PVINet: Point-Voxel Interlaced Network for Point Cloud Compression</h3>
<ul>
<li><strong>Authors: </strong>Xuan Deng, Xingtao Wang, Xiandong Meng, Xiaopeng Fan, Debin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01097">https://arxiv.org/abs/2509.01097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01097">https://arxiv.org/pdf/2509.01097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01097]] PVINet: Point-Voxel Interlaced Network for Point Cloud Compression(https://arxiv.org/abs/2509.01097)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In point cloud compression, the quality of a reconstructed point cloud relies on both the global structure and the local context, with existing methods usually processing global and local information sequentially and lacking communication between these two types of information. In this paper, we propose a point-voxel interlaced network (PVINet), which captures global structural features and local contextual features in parallel and performs interactions at each scale to enhance feature perception efficiency. Specifically, PVINet contains a voxel-based encoder (Ev) for extracting global structural features and a point-based encoder (Ep) that models local contexts centered at each voxel. Particularly, a novel conditional sparse convolution is introduced, which applies point embeddings to dynamically customize kernels for voxel feature extraction, facilitating feature interactions from Ep to Ev. During decoding, a voxel-based decoder employs conditional sparse convolutions to incorporate point embeddings as guidance to reconstruct the point cloud. Experiments on benchmark datasets show that PVINet delivers competitive performance compared to state-of-the-art methods.</li>
</ul>

<h3>Title: CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Zhong, Zhiwen Yu, Yiu-ming Cheung, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01098">https://arxiv.org/abs/2509.01098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01098">https://arxiv.org/pdf/2509.01098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01098]] CCE: Confidence-Consistency Evaluation for Time Series Anomaly Detection(https://arxiv.org/abs/2509.01098)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time Series Anomaly Detection metrics serve as crucial tools for model evaluation. However, existing metrics suffer from several limitations: insufficient discriminative power, strong hyperparameter dependency, sensitivity to perturbations, and high computational overhead. This paper introduces Confidence-Consistency Evaluation (CCE), a novel evaluation metric that simultaneously measures prediction confidence and uncertainty consistency. By employing Bayesian estimation to quantify the uncertainty of anomaly scores, we construct both global and event-level confidence and consistency scores for model predictions, resulting in a concise CCE metric. Theoretically and experimentally, we demonstrate that CCE possesses strict boundedness, Lipschitz robustness against score perturbations, and linear time complexity $\mathcal{O}(n)$. Furthermore, we establish RankEval, a benchmark for comparing the ranking capabilities of various metrics. RankEval represents the first standardized and reproducible evaluation pipeline that enables objective comparison of evaluation metrics. Both CCE and RankEval implementations are fully open-source.</li>
</ul>

<h3>Title: FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuang Wang, Yifan Zhao, Mingcan Ma, Ming Liu, Zhonglin Jiang, Yong Chen, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01107">https://arxiv.org/abs/2509.01107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01107">https://arxiv.org/pdf/2509.01107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01107]] FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation(https://arxiv.org/abs/2509.01107)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Layout-to-image (L2I) generation has exhibited promising results in natural domains, but suffers from limited generative fidelity and weak alignment with user-provided layouts when applied to degraded scenes (i.e., low-light, underwater). We primarily attribute these limitations to the "contextual illusion dilemma" in degraded conditions, where foreground instances are overwhelmed by context-dominant frequency distributions. Motivated by this, our paper proposes a new Frequency-Inspired Contextual Disentanglement Generative (FICGen) paradigm, which seeks to transfer frequency knowledge of degraded images into the latent diffusion space, thereby facilitating the rendering of degraded instances and their surroundings via contextual frequency-aware guidance. To be specific, FICGen consists of two major steps. Firstly, we introduce a learnable dual-query mechanism, each paired with a dedicated frequency resampler, to extract contextual frequency prototypes from pre-collected degraded exemplars in the training set. Secondly, a visual-frequency enhanced attention is employed to inject frequency prototypes into the degraded generation process. To alleviate the contextual illusion and attribute leakage, an instance coherence map is developed to regulate latent-space disentanglement between individual instances and their surroundings, coupled with an adaptive spatial-frequency aggregation module to reconstruct spatial-frequency mixed degraded representations. Extensive experiments on 5 benchmarks involving a variety of degraded scenarios-from severe low-light to mild blur-demonstrate that FICGen consistently surpasses existing L2I methods in terms of generative fidelity, alignment and downstream auxiliary trainability.</li>
</ul>

<h3>Title: GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhengqiang Zhang, Rongyuan Wu, Lingchen Sun, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01109">https://arxiv.org/abs/2509.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01109">https://arxiv.org/pdf/2509.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01109]] GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation(https://arxiv.org/abs/2509.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Effective and efficient tokenization plays an important role in image representation and generation. Conventional methods, constrained by uniform 2D/1D grid tokenization, are inflexible to represent regions with varying shapes and textures and at different locations, limiting their efficacy of feature representation. In this work, we propose $\textbf{GPSToken}$, a novel $\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive $\textbf{Token}$ization framework, to achieve non-uniform image tokenization by leveraging parametric 2D Gaussians to dynamically model the shape, position, and textures of different image regions. We first employ an entropy-driven algorithm to partition the image into texture-homogeneous regions of variable sizes. Then, we parameterize each region as a 2D Gaussian (mean for position, covariance for shape) coupled with texture features. A specialized transformer is trained to optimize the Gaussian parameters, enabling continuous adaptation of position/shape and content-aware feature extraction. During decoding, Gaussian parameterized tokens are reconstructed into 2D feature maps through a differentiable splatting-based renderer, bridging our adaptive tokenization with standard decoders for end-to-end training. GPSToken disentangles spatial layout (Gaussian parameters) from texture features to enable efficient two-stage generation: structural layout synthesis using lightweight networks, followed by structure-conditioned texture generation. Experiments demonstrate the state-of-the-art performance of GPSToken, which achieves rFID and FID scores of 0.65 and 1.50 on image reconstruction and generation tasks using 128 tokens, respectively. Codes and models of GPSToken can be found at $\href{this https URL}{this https URL}$.</li>
</ul>

<h3>Title: Dream-Coder 7B: An Open Diffusion Language Model for Code</h3>
<ul>
<li><strong>Authors: </strong>Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01142">https://arxiv.org/abs/2509.01142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01142">https://arxiv.org/pdf/2509.01142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01142]] Dream-Coder 7B: An Open Diffusion Language Model for Code(https://arxiv.org/abs/2509.01142)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Dream-Coder 7B, an open-source discrete diffusion language model for code generation that exhibits emergent any-order generation capabilities. Unlike traditional autoregressive (AR) models that decode strictly left-to-right, Dream-Coder 7B adaptively determines its decoding strategy based on the coding task: sketch-first generation for complex algorithms, left-to-right generation for straightforward completions, and interleaved reasoning generation for code understanding tasks. We adapt a pretrained AR checkpoint to a discrete diffusion frameworks with a continuous-time weighted cross-entropy objective. Our post-training recipe comprises (i) supervised fine-tuning, where we mitigate padding pathologies via random truncation and a padding penalty to improve sample efficiency and stabilize generation; and (ii) reinforcement learning with verifiable rewards over a curated high-quality prompt set drawn from open-source datasets, using a tailored reinforcement learning recipe for diffusion language models. The resulting Dream-Coder 7B Instruct attains 21.4\% pass@1 on LiveCodeBench (2410--2505) and demonstrates competitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We release Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training recipes, preprocessing pipelines, and inference code to facilitate reproducibility and further research.</li>
</ul>

<h3>Title: MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weiren Zhao, Lanfeng Zhong, Xin Liao, Wenjun Liao, Sichuan Zhang, Shaoting Zhang, Guotai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01144">https://arxiv.org/abs/2509.01144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01144">https://arxiv.org/pdf/2509.01144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01144]] MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2509.01144)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-Supervised Learning (SSL) is important for reducing the annotation cost for medical image segmentation models. State-of-the-art SSL methods such as Mean Teacher, FixMatch and Cross Pseudo Supervision (CPS) are mainly based on consistency regularization or pseudo-label supervision between a reference prediction and a supervised prediction. Despite the effectiveness, they have overlooked the potential noise in the labeled data, and mainly focus on strategies to generate the reference prediction, while ignoring the heterogeneous values of different unlabeled pixels. We argue that effectively mining the rich information contained by the two predictions in the loss function, instead of the specific strategy to obtain a reference prediction, is more essential for SSL, and propose a universal framework MetaSSL based on a spatially heterogeneous loss that assigns different weights to pixels by simultaneously leveraging the uncertainty and consistency information between the reference and supervised predictions. Specifically, we split the predictions on unlabeled data into four regions with decreasing weights in the loss: Unanimous and Confident (UC), Unanimous and Suspicious (US), Discrepant and Confident (DC), and Discrepant and Suspicious (DS), where an adaptive threshold is proposed to distinguish confident predictions from suspicious ones. The heterogeneous loss is also applied to labeled images for robust learning considering the potential annotation noise. Our method is plug-and-play and general to most existing SSL methods. The experimental results showed that it improved the segmentation performance significantly when integrated with existing SSL frameworks on different datasets. Code is available at this https URL.</li>
</ul>

<h3>Title: Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhang, Sophia Yat Mei Lee, Dong Zhang, Shoushan Li, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01147">https://arxiv.org/abs/2509.01147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01147">https://arxiv.org/pdf/2509.01147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01147]] Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective(https://arxiv.org/abs/2509.01147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge from high-resource languages to low-resource languages. However, existing zero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language (LSL), where shared linguistic features facilitate effective knowledge transfer. In contrast, for non-Latin script language (NSL), such as Chinese and Japanese, performance often degrades due to deep structural differences. To address these challenges, we propose an entity-aligned translation (EAT) approach. Leveraging large language models (LLMs), EAT employs a dual-translation strategy to align entities between NSL and English. In addition, we fine-tune LLMs using multilingual Wikipedia data to enhance the entity alignment from source to target languages.</li>
</ul>

<h3>Title: MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost</h3>
<ul>
<li><strong>Authors: </strong>Taiga Yamane, Ryo Masumura, Satoshi Suzuki, Shota Orihashi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01157">https://arxiv.org/abs/2509.01157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01157">https://arxiv.org/pdf/2509.01157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01157]] MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost(https://arxiv.org/abs/2509.01157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-View Pedestrian Tracking (MVPT) aims to track pedestrians in the form of a bird's eye view occupancy map from multi-view videos. End-to-end methods that detect and associate pedestrians within one model have shown great progress in MVPT. The motion and appearance information of pedestrians is important for the association, but previous end-to-end MVPT methods rely only on the current and its single adjacent past timestamp, discarding the past trajectories before that. This paper proposes a novel end-to-end MVPT method called Multi-View Trajectory Tracker (MVTrajecter) that utilizes information from multiple timestamps in past trajectories for robust association. MVTrajecter introduces trajectory motion cost and trajectory appearance cost to effectively incorporate motion and appearance information, respectively. These costs calculate which pedestrians at the current and each past timestamp are likely identical based on the information between those timestamps. Even if a current pedestrian could be associated with a false pedestrian at some past timestamp, these costs enable the model to associate that current pedestrian with the correct past trajectory based on other past timestamps. In addition, MVTrajecter effectively captures the relationships between multiple timestamps leveraging the attention mechanism. Extensive experiments demonstrate the effectiveness of each component in MVTrajecter and show that it outperforms the previous state-of-the-art methods.</li>
</ul>

<h3>Title: Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA</h3>
<ul>
<li><strong>Authors: </strong>Xuemei Tang, Chengxi Yan, Jinghang Gu, Chu-Ren Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01158">https://arxiv.org/abs/2509.01158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01158">https://arxiv.org/pdf/2509.01158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01158]] Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA(https://arxiv.org/abs/2509.01158)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Chinese information extraction (IE) involves multiple tasks across diverse temporal domains, including Classical and Modern documents. Fine-tuning a single model on heterogeneous tasks and across different eras may lead to interference and reduced performance. Therefore, in this paper, we propose Tea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with a Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in different IE tasks and eras, while a task-era-aware router mechanism dynamically allocates expert contributions. Experiments show that Tea-MOELoRA outperforms both single-task and joint LoRA baselines, demonstrating its ability to leverage task and temporal knowledge effectively.</li>
</ul>

<h3>Title: A Multimodal Deep Learning Framework for Early Diagnosis of Liver Cancer via Optimized BiLSTM-AM-VMD Architecture</h3>
<ul>
<li><strong>Authors: </strong>Cheng Cheng, Zeping Chen, Xavier Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01164">https://arxiv.org/abs/2509.01164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01164">https://arxiv.org/pdf/2509.01164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01164]] A Multimodal Deep Learning Framework for Early Diagnosis of Liver Cancer via Optimized BiLSTM-AM-VMD Architecture(https://arxiv.org/abs/2509.01164)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel multimodal deep learning framework integrating bidirectional LSTM, multi-head attention mechanism, and variational mode decomposition (BiLSTM-AM-VMD) for early liver cancer diagnosis. Using heterogeneous data that include clinical characteristics, biochemical markers, and imaging-derived variables, our approach improves both prediction accuracy and interpretability. Experimental results on real-world datasets demonstrate superior performance over traditional machine learning and baseline deep learning models.</li>
</ul>

<h3>Title: Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01166">https://arxiv.org/abs/2509.01166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01166">https://arxiv.org/pdf/2509.01166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01166]] Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning(https://arxiv.org/abs/2509.01166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%.</li>
</ul>

<h3>Title: Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyunjong Ok, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01167">https://arxiv.org/abs/2509.01167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01167">https://arxiv.org/pdf/2509.01167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01167]] Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models(https://arxiv.org/abs/2509.01167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have led to much progress in video understanding tasks. To avoid the heavy computational cost of processing all frames, these models typically rely on keyframe sampling methods guided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remains unclear whether such encoders can truly identify the most informative frames. In this work, we provide several empirical pieces of evidence revealing that popular vision encoders critically suffer from their limited capability to identify where the MLLM should look inside the video to handle the given textual query appropriately. Our findings suggest that the development of better keyframe identification techniques may be necessary for efficient video MLLMs.</li>
</ul>

<h3>Title: DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Junxiang Liu, Junming Lin, Jiangtong Li, Jie Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01177">https://arxiv.org/abs/2509.01177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01177">https://arxiv.org/pdf/2509.01177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01177]] DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion(https://arxiv.org/abs/2509.01177)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstruction dynamic visual scenes from electroencephalography (EEG) signals remains a primary challenge in brain decoding, limited by the low spatial resolution of EEG, a temporal mismatch between neural recordings and video dynamics, and the insufficient use of semantic information within brain activity. Therefore, existing methods often inadequately resolve both the dynamic coherence and the complex semantic context of the perceived visual stimuli. To overcome these limitations, we introduce DynaMind, a novel framework that reconstructs video by jointly modeling neural dynamics and semantic features via three core modules: a Regional-aware Semantic Mapper (RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to extract multimodal semantic features from EEG signals across distinct brain regions, aggregating them into a unified diffusion prior. In the mean time, the TDA generates a dynamic latent sequence, or blueprint, to enforce temporal consistency between the feature representations and the original neural recordings. Together, guided by the semantic diffusion prior, the DGVR translates the temporal-aware blueprint into a high-fidelity video reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art (SOTA), boosting reconstructed video accuracies (video- and frame-based) by 12.5 and 10.3 percentage points, respectively. It also achieves a leap in pixel-level quality, showing exceptional visual fidelity and temporal coherence with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical advancement, bridging the gap between neural dynamics and high-fidelity visual semantics.</li>
</ul>

<h3>Title: Efficient and High-Accuracy Secure Two-Party Protocols for a Class of Functions with Real-number Inputs</h3>
<ul>
<li><strong>Authors: </strong>Hao Guo, Zhaoqian Liu, Liqiang Peng, Shuaishuai Li, Ximing Fu, Weiran Liu, Lin Qu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01178">https://arxiv.org/abs/2509.01178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01178">https://arxiv.org/pdf/2509.01178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01178]] Efficient and High-Accuracy Secure Two-Party Protocols for a Class of Functions with Real-number Inputs(https://arxiv.org/abs/2509.01178)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>In two-party secret sharing scheme, values are typically encoded as unsigned integers $\mathsf{uint}(x)$, whereas real-world applications often require computations on signed real numbers $\mathsf{Real}(x)$. To enable secure evaluation of practical functions, it is essential to computing $\mathsf{Real}(x)$ from shared inputs, as protocols take shares as input. At USENIX'25, Guo et al. proposed an efficient method for computing signed integer values $\mathsf{int}(x)$ from shares, which can be extended to compute $\mathsf{Real}(x)$. However, their approach imposes a restrictive input constraint $|x| < \frac{L}{3}$ for $x \in \mathbb{Z}_L$, limiting its applicability in real-world scenarios. In this work, we significantly relax this constraint to $|x| < B$ for any $B \leq \frac{L}{2}$, where $B = \frac{L}{2}$ corresponding to the natural representable range in $x \in \mathbb{Z}_L$. This relaxes the restrictions and enables the computation of $\mathsf{Real}(x)$ with loose or no input constraints. Building upon this foundation, we present a generalized framework for designing secure protocols for a broad class of functions, including integer division ($\lfloor \frac{x}{d} \rfloor$), trigonometric ($\sin(x)$) and exponential ($e^{-x}$) functions. Our experimental evaluation demonstrates that the proposed protocols achieve both high efficiency and high accuracy. Notably, our protocol for evaluating $e^{-x}$ reduces communication costs to approximately 31% of those in SirNN (S&P 21) and Bolt (S&P 24), with runtime speedups of up to $5.53 \times$ and $3.09 \times$, respectively. In terms of accuracy, our protocol achieves a maximum ULP error of $1.435$, compared to $2.64$ for SirNN and $8.681$ for Bolt.</li>
</ul>

<h3>Title: FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus</h3>
<ul>
<li><strong>Authors: </strong>Qiaoqiao Jin, Siming Fu, Dong She, Weinan Jia, Hualiang Wang, Mu Liu, Jidong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01181">https://arxiv.org/abs/2509.01181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01181">https://arxiv.org/pdf/2509.01181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01181]] FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus(https://arxiv.org/abs/2509.01181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-subject personalized image generation aims to synthesize customized images containing multiple specified subjects without requiring test-time optimization. However, achieving fine-grained independent control over multiple subjects remains challenging due to difficulties in preserving subject fidelity and preventing cross-subject attribute leakage. We present FocusDPO, a framework that adaptively identifies focus regions based on dynamic semantic correspondence and supervision image complexity. During training, our method progressively adjusts these focal areas across noise timesteps, implementing a weighted strategy that rewards information-rich patches while penalizing regions with low prediction confidence. The framework dynamically adjusts focus allocation during the DPO process according to the semantic complexity of reference images and establishes robust correspondence mappings between generated and reference subjects. Extensive experiments demonstrate that our method substantially enhances the performance of existing pre-trained personalized generation models, achieving state-of-the-art results on both single-subject and multi-subject personalized image synthesis benchmarks. Our method effectively mitigates attribute leakage while preserving superior subject fidelity across diverse generation scenarios, advancing the frontier of controllable multi-subject image synthesis.</li>
</ul>

<h3>Title: SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</h3>
<ul>
<li><strong>Authors: </strong>Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, Jianya Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01183">https://arxiv.org/abs/2509.01183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01183">https://arxiv.org/pdf/2509.01183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01183]] SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment(https://arxiv.org/abs/2509.01183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments across 32 datasets derived from 6 sources demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks, establishing PQM via SegAssess as a robust and transferable solution for unsupervised SQA. The code is available at this https URL.</li>
</ul>

<h3>Title: Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seganrasan Subramanian, Abhigya Verma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01185">https://arxiv.org/abs/2509.01185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01185">https://arxiv.org/pdf/2509.01185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01185]] Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation(https://arxiv.org/abs/2509.01185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability of large language models (LLMs) to process and reason over long textual inputs is critical for a wide range of real-world applications. However, progress in this area is significantly constrained by the absence of high-quality, diverse, and verifiable long-context datasets suitable for both training and evaluation. This work introduces a modular, extensible framework for synthetic long-context data generation via prompt-based interaction with LLMs. The framework supports multiple training and alignment objectives, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). It encompasses four core generation paradigms: multi-turn conversational dialogues, document-grounded input-output pairs, verifiable instruction-response tasks, and long-context reasoning examples. Through templated prompting, a model-agnostic architecture, and metadata-enriched outputs, the proposed approach facilitates scalable, controllable, and purpose-aligned dataset creation for advancing long-context capabilities in LLMs.</li>
</ul>

<h3>Title: Statutory Construction and Interpretation for Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Luxi He, Nimra Nadeem, Michel Liao, Howard Chen, Danqi Chen, Mariano-Florentino Cu√©llar, Peter Henderson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01186">https://arxiv.org/abs/2509.01186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01186">https://arxiv.org/pdf/2509.01186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01186]] Statutory Construction and Interpretation for Artificial Intelligence(https://arxiv.org/abs/2509.01186)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity arises both from how these principles are written and how they are applied. But while legal systems use institutional safeguards to manage such ambiguity, such as transparent appellate review policing interpretive constraints, AI alignment pipelines offer no comparable protections. Different interpretations of the same rule can lead to inconsistent or unstable model behavior. Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity at both the rule creation and rule application steps. We then propose a computational framework that mirrors two legal mechanisms: (1) a rule refinement pipeline that minimizes interpretive disagreement by revising ambiguous rules (analogous to agency rulemaking or iterative legislative action), and (2) prompt-based interpretive constraints that reduce inconsistency in rule application (analogous to legal canons that guide judicial discretion). We evaluate our framework on a 5,000-scenario subset of the WildChat dataset and show that both interventions significantly improve judgment consistency across a panel of reasonable interpreters. Our approach offers a first step toward systematically managing interpretive ambiguity, an essential step for building more robust, law-following AI systems.</li>
</ul>

<h3>Title: StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Yunjie Li, Lingmin Zan, Zheng Gong, Mengtao Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01187">https://arxiv.org/abs/2509.01187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01187">https://arxiv.org/pdf/2509.01187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01187]] StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting(https://arxiv.org/abs/2509.01187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Extended Long Short-Term Memory (xLSTM) network has attracted widespread research interest due to its enhanced capability to model complex temporal dependencies in diverse time series applications. Despite its success, there is still potential to further improve its representational capacity and forecasting performance, particularly on challenging real-world datasets with unknown, intricate, and hierarchical dynamics. In this work, we propose a stochastic xLSTM, termed StoxLSTM, that improves the original architecture into a state space modeling framework by incorporating stochastic latent variables within xLSTM. StoxLSTM models the latent dynamic evolution through specially designed recurrent blocks, enabling it to effectively capture the underlying temporal patterns and dependencies. Extensive experiments on publicly available benchmark datasets from multiple research communities demonstrate that StoxLSTM consistently outperforms state-of-the-art baselines with better robustness and stronger generalization ability.</li>
</ul>

<h3>Title: Efficient Large Language Models with Zero-Shot Adjustable Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Kachuee, Mohammad Sharifkhani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01190">https://arxiv.org/abs/2509.01190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01190">https://arxiv.org/pdf/2509.01190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01190]] Efficient Large Language Models with Zero-Shot Adjustable Acceleration(https://arxiv.org/abs/2509.01190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Using Large Language Models (LLMs) in real-world applications presents significant challenges, particularly in balancing computational efficiency and performance. Optimizing acceleration after the fine-tuning phase and during inference is crucial for building an efficient architecture. This paper introduces Zero-Shot Adjustable Acceleration, a novel training and inference method that dynamically adjusts hardware usage during inference without requiring additional fine-tuning. The proposed approach is applied to newly developed models and evaluated across multiple classification and text generation tasks. Experimental results demonstrate that the method enables a wide range of acceleration in a zero-shot manner and achieves up to a 11x speedup compared to the baseline.</li>
</ul>

<h3>Title: Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework</h3>
<ul>
<li><strong>Authors: </strong>Eddi Weinwurm, Alexander Kovalenko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01198">https://arxiv.org/abs/2509.01198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01198">https://arxiv.org/pdf/2509.01198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01198]] Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework(https://arxiv.org/abs/2509.01198)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Dimensionality reduction can distort vector space properties such as orthogonality and linear independence, which are critical for tasks including cross-modal retrieval, clustering, and classification. We propose a Relationship Preserving Loss (RPL), a loss function that preserves these properties by minimizing discrepancies between relationship matrices (e.g., Gram or cosine) of high-dimensional data and their low-dimensional embeddings. RPL trains neural networks for non-linear projections and is supported by error bounds derived from matrix perturbation theory. Initial experiments suggest that RPL reduces embedding dimensions while largely retaining performance on downstream tasks, likely due to its preservation of key vector space properties. While we describe here the use of RPL in dimensionality reduction, this loss can also be applied more broadly, for example to cross-domain alignment and transfer learning, knowledge distillation, fairness and invariance, dehubbing, graph and manifold learning, and federated learning, where distributed embeddings must remain geometrically consistent.</li>
</ul>

<h3>Title: SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01200">https://arxiv.org/abs/2509.01200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01200">https://arxiv.org/pdf/2509.01200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01200]] SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation(https://arxiv.org/abs/2509.01200)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.</li>
</ul>

<h3>Title: DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency</h3>
<ul>
<li><strong>Authors: </strong>Tianwei Ye, Yong Ma, Xiaoguang Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01204">https://arxiv.org/abs/2509.01204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01204">https://arxiv.org/pdf/2509.01204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01204]] DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency(https://arxiv.org/abs/2509.01204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Establishing point-to-point correspondences across multiple 3D shapes is a fundamental problem in computer vision and graphics. In this paper, we introduce DcMatch, a novel unsupervised learning framework for non-rigid multi-shape matching. Unlike existing methods that learn a canonical embedding from a single shape, our approach leverages a shape graph attention network to capture the underlying manifold structure of the entire shape collection. This enables the construction of a more expressive and robust shared latent space, leading to more consistent shape-to-universe correspondences via a universe predictor. Simultaneously, we represent these correspondences in both the spatial and spectral domains and enforce their alignment in the shared universe space through a novel cycle consistency loss. This dual-level consistency fosters more accurate and coherent mappings. Extensive experiments on several challenging benchmarks demonstrate that our method consistently outperforms previous state-of-the-art approaches across diverse multi-shape matching scenarios. Code is available at this https URL.</li>
</ul>

<h3>Title: Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Ma√´lic Neau, Zoe Falomir, C√©dric Buche, Akihiro Sugimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01209">https://arxiv.org/abs/2509.01209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01209">https://arxiv.org/pdf/2509.01209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01209]] Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation(https://arxiv.org/abs/2509.01209)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Scene Graph Generation (SGG) encodes visual relationships between objects in images as graph structures. Thanks to the advances of Vision-Language Models (VLMs), the task of Open-Vocabulary SGG has been recently proposed where models are evaluated on their functionality to learn a wide and diverse range of relations. Current benchmarks in SGG, however, possess a very limited vocabulary, making the evaluation of open-source models inefficient. In this paper, we propose a new reference-free metric to fairly evaluate the open-vocabulary capabilities of VLMs for relation prediction. Another limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of poor quality for pre-training. We also propose a new solution for quickly generating high-quality synthetic data through region-specific prompt tuning of VLMs. Experimental results show that pre-training with this new data split can benefit the generalization capabilities of Open-Voc SGG models.</li>
</ul>

<h3>Title: Web Fraud Attacks Against LLM-Driven Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Dezhang Kong, Hujin Peng, Yilun Zhang, Lele Zhao, Zhenhua Xu, Shi Lin, Changting Lin, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01211">https://arxiv.org/abs/2509.01211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01211">https://arxiv.org/pdf/2509.01211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01211]] Web Fraud Attacks Against LLM-Driven Multi-Agent Systems(https://arxiv.org/abs/2509.01211)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>With the proliferation of applications built upon LLM-driven multi-agent systems (MAS), the security of Web links has become a critical concern in ensuring system reliability. Once an agent is induced to visit a malicious website, attackers can use it as a springboard to conduct diverse subsequent attacks, which will drastically expand the attack surface. In this paper, we propose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to visit malicious websites. We design 11 representative attack variants that encompass domain name tampering (homoglyph deception, character substitution, etc.), link structure camouflage (sub-directory nesting, sub-domain grafting, parameter obfuscation, etc.), and other deceptive techniques tailored to exploit MAS's vulnerabilities in link validation. Through extensive experiments on these crafted attack vectors, we demonstrate that Web fraud attacks not only exhibit significant destructive potential across different MAS architectures but also possess a distinct advantage in evasion: they circumvent the need for complex input formats such as jailbreaking, which inherently carry higher exposure risks. These results underscore the importance of addressing Web fraud attacks in LLM-driven MAS, as their stealthiness and destructiveness pose non-negligible threats to system security and user safety.</li>
</ul>

<h3>Title: Mitigating Catastrophic Forgetting in Continual Learning through Model Growth</h3>
<ul>
<li><strong>Authors: </strong>Ege S√ºalp, Mina Rezaei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01213">https://arxiv.org/abs/2509.01213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01213">https://arxiv.org/pdf/2509.01213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01213]] Mitigating Catastrophic Forgetting in Continual Learning through Model Growth(https://arxiv.org/abs/2509.01213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting is a significant challenge in continual learning, in which a model loses prior knowledge when it is fine-tuned on new tasks. This problem is particularly critical for large language models (LLMs) undergoing continual learning, as retaining performance across diverse domains is important for their general utility. In this paper, we explore model growth, a promising strategy that leverages smaller models to expedite and structure the training of larger ones for mitigating the catastrophic forgetting problem. Although growth-based pretraining, particularly via transformer stacking, has shown promise in accelerating convergence, its impact on forgetting remains under-explored. Therefore, we evaluate whether growth-based models can retain previously learned capabilities more effectively across a sequence of fine-tuning tasks involving domain knowledge, reasoning, reading comprehension, and bias. Our findings show that both models -- one trained with growth (Stack LLM) and one without (LLM) -- exhibit improvements in domain knowledge. However, reasoning and reading comprehension degrade over time, indicating signs of catastrophic forgetting. Stack LLM consistently shows less degradation, especially in reading comprehension, suggesting enhanced retention capabilities. Interestingly, in bias evaluation, the baseline LLM becomes progressively more neutral with continued fine-tuning, while Stack LLM maintains a steady bias ratio around 60--61\%. These results indicate that growth-based pretraining may deliver modest improvements in resisting catastrophic forgetting, though trade-offs remain in handling social biases.</li>
</ul>

<h3>Title: PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01214">https://arxiv.org/abs/2509.01214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01214">https://arxiv.org/pdf/2509.01214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01214]] PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity(https://arxiv.org/abs/2509.01214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tumor spatial heterogeneity analysis requires precise correlation between Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker expression, yet current methods suffer from spatial misalignment in consecutive sections, severely compromising in situ pathological interpretation. In order to obtain a more accurate virtual staining pattern, We propose PRINTER, a weakly-supervised framework that integrates PRototype-drIven content and staiNing patTERn decoupling and deformation-aware adversarial learning strategies designed to accurately learn IHC staining patterns while preserving H&E staining details. Our approach introduces three key innovations: (1) A prototype-driven staining pattern transfer with explicit content-style decoupling; and (2) A cyclic registration-synthesis framework GapBridge that bridges H&E and IHC domains through deformable structural alignment, where registered features guide cross-modal style transfer while synthesized outputs iteratively refine the registration;(3) Deformation-Aware Adversarial Learning: We propose a training framework where a generator and deformation-aware registration network jointly adversarially optimize a style-focused discriminator. Extensive experiments demonstrate that PRINTER effectively achieves superior performance in preserving H&E staining details and virtual staining fidelity, outperforming state-of-the-art methods. Our work provides a robust and scalable solution for virtual staining, advancing the field of computational pathology.</li>
</ul>

<h3>Title: POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion</h3>
<ul>
<li><strong>Authors: </strong>Yuan Liu, Zhongyin Zhao, Le Tian, Haicheng Wang, Xubing Ye, Yangxiu You, Zilin Yu, Chuhan Wu, Xiao Zhou, Yang Yu, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01215">https://arxiv.org/abs/2509.01215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01215">https://arxiv.org/pdf/2509.01215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01215]] POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion(https://arxiv.org/abs/2509.01215)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at this https URL.</li>
</ul>

<h3>Title: DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Huang Wei, Yinggui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01221">https://arxiv.org/abs/2509.01221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01221">https://arxiv.org/pdf/2509.01221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01221]] DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression(https://arxiv.org/abs/2509.01221)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&A, financial Q&A, general Q&A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.</li>
</ul>

<h3>Title: FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework</h3>
<ul>
<li><strong>Authors: </strong>Lingzhou Mu, Qiang Wang, Fan Jiang, Mengchao Wang, Yaqi Fan, Mu Xu, Kai Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01232">https://arxiv.org/abs/2509.01232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01232">https://arxiv.org/pdf/2509.01232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01232]] FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework(https://arxiv.org/abs/2509.01232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human-Scene Interaction (HSI) seeks to generate realistic human behaviors within complex environments, yet it faces significant challenges in handling long-horizon, high-level tasks and generalizing to unseen scenes. To address these limitations, we introduce FantasyHSI, a novel HSI framework centered on video generation and multi-agent systems that operates without paired data. We model the complex interaction process as a dynamic directed graph, upon which we build a collaborative multi-agent system. This system comprises a scene navigator agent for environmental perception and high-level path planning, and a planning agent that decomposes long-horizon goals into atomic actions. Critically, we introduce a critic agent that establishes a closed-loop feedback mechanism by evaluating the deviation between generated actions and the planned path. This allows for the dynamic correction of trajectory drifts caused by the stochasticity of the generative model, thereby ensuring long-term logical consistency. To enhance the physical realism of the generated motions, we leverage Direct Preference Optimization (DPO) to train the action generator, significantly reducing artifacts such as limb distortion and foot-sliding. Extensive experiments on our custom SceneBench benchmark demonstrate that FantasyHSI significantly outperforms existing methods in terms of generalization, long-horizon task completion, and physical realism. Ours project page: this https URL</li>
</ul>

<h3>Title: Geometric origin of adversarial vulnerability in deep learning</h3>
<ul>
<li><strong>Authors: </strong>Yixiong Ren, Wenkang Du, Jianhui Zhou, Haiping Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01235">https://arxiv.org/abs/2509.01235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01235">https://arxiv.org/pdf/2509.01235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01235]] Geometric origin of adversarial vulnerability in deep learning(https://arxiv.org/abs/2509.01235)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>How to balance training accuracy and adversarial robustness has become a challenge since the birth of deep learning. Here, we introduce a geometry-aware deep learning framework that leverages layer-wise local training to sculpt the internal representations of deep neural networks. This framework promotes intra-class compactness and inter-class separation in feature space, leading to manifold smoothness and adversarial robustness against white or black box attacks. The performance can be explained by an energy model with Hebbian coupling between elements of the hidden representation. Our results thus shed light on the physics of learning in the direction of alignment between biological and artificial intelligence systems. Using the current framework, the deep network can assimilate new information into existing knowledge structures while reducing representation interference.</li>
</ul>

<h3>Title: Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Zhiyu Yang, Yunjie Zhang, Shanyi Zhu, Lin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01236">https://arxiv.org/abs/2509.01236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01236">https://arxiv.org/pdf/2509.01236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01236]] Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors(https://arxiv.org/abs/2509.01236)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing model inference capabilities. Despite growing interest in Chain-of-Thought reasoning, its underlying mechanisms remain unclear. This paper explores the working mechanisms of Chain-of-Thought reasoning from the perspective of the dual relationship between in-context learning and pretrained priors. We first conduct a fine-grained lexical-level analysis of rationales to examine the model's reasoning behavior. Then, by incrementally introducing noisy exemplars, we examine how the model balances pretrained priors against erroneous in-context information. Finally, we investigate whether prompt engineering can induce slow thinking in large language models. Our extensive experiments reveal three key findings: (1) The model not only quickly learns the reasoning structure at the lexical level but also grasps deeper logical reasoning patterns, yet it heavily relies on pretrained priors. (2) Providing sufficient exemplars shifts the model's decision-making from pretrained priors to in-context signals, while misleading prompts introduce instability. (3) Long Chain-of-Thought prompting can induce the model to generate longer reasoning chains, thereby improving its performance on downstream tasks.</li>
</ul>

<h3>Title: RT-DETRv2 Explained in 8 Illustrations</h3>
<ul>
<li><strong>Authors: </strong>Ethan Qi Yang Chua, Jen Hong Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01241">https://arxiv.org/abs/2509.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01241">https://arxiv.org/pdf/2509.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01241]] RT-DETRv2 Explained in 8 Illustrations(https://arxiv.org/abs/2509.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Object detection architectures are notoriously difficult to understand, often more so than large language models. While RT-DETRv2 represents an important advance in real-time detection, most existing diagrams do little to clarify how its components actually work and fit together. In this article, we explain the architecture of RT-DETRv2 through a series of eight carefully designed illustrations, moving from the overall pipeline down to critical components such as the encoder, decoder, and multi-scale deformable attention. Our goal is to make the existing one genuinely understandable. By visualizing the flow of tensors and unpacking the logic behind each module, we hope to provide researchers and practitioners with a clearer mental model of how RT-DETRv2 works under the hood.</li>
</ul>

<h3>Title: Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views</h3>
<ul>
<li><strong>Authors: </strong>Xiangdong Zhang, Shaofeng Zhang, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01250">https://arxiv.org/abs/2509.01250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01250">https://arxiv.org/pdf/2509.01250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01250]] Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views(https://arxiv.org/abs/2509.01250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at this https URL.</li>
</ul>

<h3>Title: Practical and Private Hybrid ML Inference with Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Sayan Biswas, Philippe Chartier, Akash Dhasade, Tom Jurien, David Kerriou, Anne-Marie Kerrmarec, Mohammed Lemou, Franklin Tranie, Martijn de Vos, Milos Vujasinovic</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01253">https://arxiv.org/abs/2509.01253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01253">https://arxiv.org/pdf/2509.01253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01253]] Practical and Private Hybrid ML Inference with Fully Homomorphic Encryption(https://arxiv.org/abs/2509.01253)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, extraction</a></li>
<li><strong>Abstract: </strong>In contemporary cloud-based services, protecting users' sensitive data and ensuring the confidentiality of the server's model are critical. Fully homomorphic encryption (FHE) enables inference directly on encrypted inputs, but its practicality is hindered by expensive bootstrapping and inefficient approximations of non-linear activations. We introduce Safhire, a hybrid inference framework that executes linear layers under encryption on the server while offloading non-linearities to the client in plaintext. This design eliminates bootstrapping, supports exact activations, and significantly reduces computation. To safeguard model confidentiality despite client access to intermediate outputs, Safhire applies randomized shuffling, which obfuscates intermediate values and makes it practically impossible to reconstruct the model. To further reduce latency, Safhire incorporates advanced optimizations such as fast ciphertext packing and partial extraction. Evaluations on multiple standard models and datasets show that Safhire achieves 1.5X - 10.5X lower inference latency than Orion, a state-of-the-art baseline, with manageable communication overhead and comparable accuracy, thereby establishing the practicality of hybrid FHE inference.</li>
</ul>

<h3>Title: ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization</h3>
<ul>
<li><strong>Authors: </strong>Thinh-Phuc Nguyen, Thanh-Hai Nguyen, Gia-Huy Dinh, Lam-Huy Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01259">https://arxiv.org/abs/2509.01259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01259">https://arxiv.org/pdf/2509.01259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01259]] ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization(https://arxiv.org/abs/2509.01259)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Image captioning systems often produce generic descriptions that fail to capture event-level semantics which are crucial for applications like news reporting and digital archiving. We present ReCap, a novel pipeline for event-enriched image retrieval and captioning that incorporates broader contextual information from relevant articles to generate narrative-rich, factually grounded captions. Our approach addresses the limitations of standard vision-language models that typically focus on visible content while missing temporal, social, and historical contexts. ReCap comprises three integrated components: (1) a robust two-stage article retrieval system using DINOv2 embeddings with global feature similarity for initial candidate selection followed by patch-level mutual nearest neighbor similarity re-ranking; (2) a context extraction framework that synthesizes information from article summaries, generic captions, and original source metadata; and (3) a large language model-based caption generation system with Semantic Gaussian Normalization to enhance fluency and relevance. Evaluated on the OpenEvents V1 dataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a strong overall score of 0.54666, ranking 2nd on the private test set. These results highlight ReCap's effectiveness in bridging visual perception with real-world knowledge, offering a practical solution for context-aware image understanding in high-stakes domains. The code is available at this https URL.</li>
</ul>

<h3>Title: An Automated Attack Investigation Approach Leveraging Threat-Knowledge-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rujie Dai, Peizhuo Lv, Yujiang Gui, Qiujian Lv, Yuanyuan Qiao, Yan Wang, Degang Sun, Weiqing Huang, Yingjiu Li, XiaoFeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01271">https://arxiv.org/abs/2509.01271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01271">https://arxiv.org/pdf/2509.01271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01271]] An Automated Attack Investigation Approach Leveraging Threat-Knowledge-Augmented Large Language Models(https://arxiv.org/abs/2509.01271)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) are prolonged, stealthy intrusions by skilled adversaries that compromise high-value systems to steal data or disrupt operations. Reconstructing complete attack chains from massive, heterogeneous logs is essential for effective attack investigation, yet existing methods suffer from poor platform generality, limited generalization to evolving tactics, and an inability to produce analyst-ready reports. Large Language Models (LLMs) offer strong semantic understanding and summarization capabilities, but in this domain they struggle to capture the long-range, cross-log dependencies critical for accurate reconstruction. To solve these problems, we present an LLM-empowered attack investigation framework augmented with a dynamically adaptable Kill-Chain-aligned threat knowledge base. We organizes attack-relevant behaviors into stage-aware knowledge units enriched with semantic annotations, enabling the LLM to iteratively retrieve relevant intelligence, perform causal reasoning, and progressively expand the investigation context. This process reconstructs multi-phase attack scenarios and generates coherent, human-readable investigation reports. Evaluated on 15 attack scenarios spanning single-host and multi-host environments across Windows and Linux (over 4.3M log events, 7.2 GB of data), the system achieves an average True Positive Rate (TPR) of 97.1% and an average False Positive Rate (FPR) of 0.2%, significantly outperforming the SOTA method ATLAS, which achieves an average TPR of 79.2% and an average FPR of 29.1%.</li>
</ul>

<h3>Title: Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Li Yang Lu, Yachao Zhang, Fangyong Wang, Yuan Xie, Yanyun Qu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01275">https://arxiv.org/abs/2509.01275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01275">https://arxiv.org/pdf/2509.01275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01275]] Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2509.01275)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation (OVSS) conducts pixel-level classification via text-driven alignment, where the domain discrepancy between base category training and open-vocabulary inference poses challenges in discriminative modeling of latent unseen category. To address this challenge, existing vision-language model (VLM)-based approaches demonstrate commendable performance through pre-trained multi-modal representations. However, the fundamental mechanisms of latent semantic comprehension remain underexplored, making the bottleneck for OVSS. In this work, we initiate a probing experiment to explore distribution patterns and dynamics of latent semantics in VLMs under inductive learning paradigms. Building on these insights, we propose X-Agent, an innovative OVSS framework employing latent semantic-aware ``agent'' to orchestrate cross-modal attention mechanisms, simultaneously optimizing latent semantic dynamic and amplifying its perceptibility. Extensive benchmark evaluations demonstrate that X-Agent achieves state-of-the-art performance while effectively enhancing the latent semantic saliency.</li>
</ul>

<h3>Title: Multi-Representation Adapter with Neural Architecture Search for Efficient Range-Doppler Radar Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Lin, Weicheng Zheng, Yongtao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01280">https://arxiv.org/abs/2509.01280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01280">https://arxiv.org/pdf/2509.01280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01280]] Multi-Representation Adapter with Neural Architecture Search for Efficient Range-Doppler Radar Object Detection(https://arxiv.org/abs/2509.01280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting objects efficiently from radar sensors has recently become a popular trend due to their robustness against adverse lighting and weather conditions compared with cameras. This paper presents an efficient object detection model for Range-Doppler (RD) radar maps. Specifically, we first represent RD radar maps with multi-representation, i.e., heatmaps and grayscale images, to gather high-level object and fine-grained texture features. Then, we design an additional Adapter branch, an Exchanger Module with two modes, and a Primary-Auxiliary Fusion Module to effectively extract, exchange, and fuse features from the multi-representation inputs, respectively. Furthermore, we construct a supernet with various width and fusion operations in the Adapter branch for the proposed model and employ a One-Shot Neural Architecture Search method to further improve the model's efficiency while maintaining high performance. Experimental results demonstrate that our model obtains favorable accuracy and efficiency trade-off. Moreover, we achieve new state-of-the-art performance on RADDet and CARRADA datasets with mAP@50 of 71.9 and 57.1, respectively.</li>
</ul>

<h3>Title: Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals</h3>
<ul>
<li><strong>Authors: </strong>Huan Ni, Qingshan Liu, Xiaonan Niu, Danfeng Hong, Lingli Zhao, Haiyan Guan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01299">https://arxiv.org/abs/2509.01299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01299">https://arxiv.org/pdf/2509.01299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01299]] Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals(https://arxiv.org/abs/2509.01299)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation of unseen categories with very limited samples, but also improves cross-domain generalization ability within the few-shot segmentation framework. Currently, existing CD-FSS studies typically design multiple independent modules to enhance the cross-domain generalization ability of feature representations. However, the independence among these modules hinders the effective flow of knowledge, making it difficult to fully leverage their collective potential. In contrast, this paper proposes an all-in-one module based on ordinary differential equations and Fourier transform, resulting in a structurally concise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs assumes the existence of an ODE relationship between the spectra (including amplitude and phase spectra) of domain-specific features and domain-agnostic features. This ODE formulation yields an iterative transformation process along a sequence of time intervals, while simultaneously applying affine transformations with randomized perturbations to the spectra. In doing so, the exploration of domain-agnostic feature representation spaces and the simulation of diverse potential target-domain distributions are reformulated as an optimization process over the intrinsic parameters of the ODE. Moreover, we strictly constrain the support-sample selection during target-domain fine-tuning so that it is consistent with the requirements of real-world few-shot segmentation tasks. For evaluation, we introduce five datasets from substantially different domains and define two sets of cross-domain few-shot segmentation tasks to comprehensively analyze the performance of FSS-TIs. Experimental results demonstrate the superiority of FSS-TIs over existing CD-FSS methods, and in-depth ablation studies further validate the cross-domain adaptability of FSS-TIs.</li>
</ul>

<h3>Title: Culture is Everywhere: A Call for Intentionally Cultural Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Juhyun Oh, Inha Cha, Michael Saxon, Hyunseung Lim, Shaily Bhatt, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01301">https://arxiv.org/abs/2509.01301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01301">https://arxiv.org/pdf/2509.01301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01301]] Culture is Everywhere: A Call for Intentionally Cultural Evaluation(https://arxiv.org/abs/2509.01301)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The prevailing ``trivia-centered paradigm'' for evaluating the cultural alignment of large language models (LLMs) is increasingly inadequate as these models become more advanced and widely deployed. Existing approaches typically reduce culture to static facts or values, testing models via multiple-choice or short-answer questions that treat culture as isolated trivia. Such methods neglect the pluralistic and interactive realities of culture, and overlook how cultural assumptions permeate even ostensibly ``neutral'' evaluation settings. In this position paper, we argue for \textbf{intentionally cultural evaluation}: an approach that systematically examines the cultural assumptions embedded in all aspects of evaluation, not just in explicitly cultural tasks. We systematically characterize the what, how, and circumstances by which culturally contingent considerations arise in evaluation, and emphasize the importance of researcher positionality for fostering inclusive, culturally aligned NLP research. Finally, we discuss implications and future directions for moving beyond current benchmarking practices, discovering important applications that we don't know exist, and involving communities in evaluation design through HCI-inspired participatory methodologies.</li>
</ul>

<h3>Title: TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Sishi Xiong, Ziyang He, Zhongjiang He, Yu Zhao, Changzai Pan, Jie Zhang, Zhenhe Wu, Shuangyong Song, Yongxiang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01312">https://arxiv.org/abs/2509.01312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01312">https://arxiv.org/pdf/2509.01312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01312]] TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering(https://arxiv.org/abs/2509.01312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have shown promise in the table question answering (TQA) task through prompt engineering, they face challenges in industrial applications, including structural heterogeneity, difficulties in target data localization, and bottlenecks in complex reasoning. To address these limitations, this paper presents TableZoomer, a novel LLM-powered, programming-based agent framework. It introduces three key innovations: (1) replacing the original fully verbalized table with structured table schema to bridge the semantic gap and reduce computational complexity; (2) a query-aware table zooming mechanism that dynamically generates sub-table schema through column selection and entity linking, significantly improving target localization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that transforms queries into executable code to mitigate numerical hallucination. Additionally, we integrate the reasoning workflow with the ReAct paradigm to enable iterative reasoning. Extensive experiments demonstrate that our framework maintains the usability advantages while substantially enhancing performance and scalability across tables of varying scales. When implemented with the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of 19.34% and 25% over conventional PoT methods on the large-scale DataBench dataset and the small-scale Fact Checking task of TableBench dataset, respectively.</li>
</ul>

<h3>Title: Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Anum Afzal, Mehul Kumawat, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01314">https://arxiv.org/abs/2509.01314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01314">https://arxiv.org/pdf/2509.01314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01314]] Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization(https://arxiv.org/abs/2509.01314)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), being generic task solvers, are versatile. However, despite the vast amount of data they are trained on, there are speculations about their adaptation capabilities to a new domain. Additionally, the simple fine-tuning of the model to incorporate knowledge of a new domain is computationally expensive and time-consuming. This becomes more challenging when the domain in question is also low-resource, and labeled data is unavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on high-resource datasets to address these challenges to improve performance on unseen low-resource domains. Throughout our experiments, we evaluate whether intrinsic linguistic commonalities between datasets can be leveraged for efficient domain adaptation. We benchmark six PEFTs with \texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific, Medical, Legal, and News domains for a Text Summarization task. Our experiments show that for low-resource domains, inference using Within-Domain Adapters can achieve better performance than Few-Shot as well as a much larger \texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain Adapters, we explore the concept of using Cross-Domain Adapters as well as the strategic combinations of adapters to leverage intrinsic language similarities across domains, facilitating better adaptability and performance in low-resource settings.</li>
</ul>

<h3>Title: Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01317">https://arxiv.org/abs/2509.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01317">https://arxiv.org/pdf/2509.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01317]] Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation(https://arxiv.org/abs/2509.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>High-resolution LiDAR data plays a critical role in 3D semantic segmentation for autonomous driving, but the high cost of advanced sensors limits large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR produce sparse point clouds that degrade segmentation accuracy. To overcome this, we introduce the first end-to-end framework that jointly addresses LiDAR super-resolution (SR) and semantic segmentation. The framework employs joint optimization during training, allowing the SR module to incorporate semantic cues and preserve fine details, particularly for smaller object classes. A new SR loss function further directs the network to focus on regions of interest. The proposed lightweight, model-based SR architecture uses significantly fewer parameters than existing LiDAR SR approaches, while remaining easily compatible with segmentation networks. Experiments show that our method achieves segmentation performance comparable to models operating on high-resolution and costly 64-channel LiDAR data.</li>
</ul>

<h3>Title: KoBLEX: Open Legal Question Answering with Multi-hop Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jihyung Lee, Daehui Kim, Seonjeong Hwang, Hyounghun Kim, Gary Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01324">https://arxiv.org/abs/2509.01324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01324">https://arxiv.org/pdf/2509.01324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01324]] KoBLEX: Open Legal Question Answering with Multi-hop Reasoning(https://arxiv.org/abs/2509.01324)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) have achieved remarkable performances in general domains and are now extending into the expert domain of law. Several benchmarks have been proposed to evaluate LLMs' legal capabilities. However, these benchmarks fail to evaluate open-ended and provision-grounded Question Answering (QA). To address this, we introduce a Korean Benchmark for Legal EXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop legal reasoning. KoBLEX includes 226 scenario-based QA instances and their supporting provisions, created using a hybrid LLM-human expert pipeline. We also propose a method called Parametric provision-guided Selection Retrieval (ParSeR), which uses LLM-generated parametric provisions to guide legally grounded and reliable answers. ParSeR facilitates multi-hop reasoning on complex legal questions by generating parametric provisions and employing a three-stage sequential retrieval process. Furthermore, to better evaluate the legal fidelity of the generated answers, we propose Legal Fidelity Evaluation (LF-Eval). LF-Eval is an automatic metric that jointly considers the question, answer, and supporting provisions and shows a high correlation with human judgments. Experimental results show that ParSeR consistently outperforms strong baselines, achieving the best results across multiple LLMs. Notably, compared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1 and +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently delivers consistent performance across reasoning depths, with ablations confirming the effectiveness of ParSeR.</li>
</ul>

<h3>Title: Can Large Language Models Master Complex Card Games?</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Fuqing Bie, Junzhe Chen, Dan Zhang, Shiyu Huang, Evgeny Kharlamov, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01328">https://arxiv.org/abs/2509.01328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01328">https://arxiv.org/pdf/2509.01328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01328]] Can Large Language Models Master Complex Card Games?(https://arxiv.org/abs/2509.01328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can master multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs.</li>
</ul>

<h3>Title: Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fuyou Mao, Beining Wu, Yanfeng Jiang, Han Xue, Yan Tang, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01330">https://arxiv.org/abs/2509.01330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01330">https://arxiv.org/pdf/2509.01330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01330]] Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation(https://arxiv.org/abs/2509.01330)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Ambiguity in medical image segmentation calls for models that capture full conditional distributions rather than a single point estimate. We present Prior-Guided Residual Diffusion (PGRD), a diffusion-based framework that learns voxel-wise distributions while maintaining strong calibration and practical sampling efficiency. PGRD embeds discrete labels as one-hot targets in a continuous space to align segmentation with diffusion modeling. A coarse prior predictor provides step-wise guidance; the diffusion network then learns the residual to the prior, accelerating convergence and improving calibration. A deep diffusion supervision scheme further stabilizes training by supervising intermediate time steps. Evaluated on representative MRI and CT datasets, PGRD achieves higher Dice scores and lower NLL/ECE values than Bayesian, ensemble, Probabilistic U-Net, and vanilla diffusion baselines, while requiring fewer sampling steps to reach strong performance.</li>
</ul>

<h3>Title: Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunus Serhat Bicakci, Joseph Shingleton, Anahid Basiri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01341">https://arxiv.org/abs/2509.01341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01341">https://arxiv.org/pdf/2509.01341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01341]] Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation(https://arxiv.org/abs/2509.01341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Street-level geolocalization from images is crucial for a wide range of essential applications and services, such as navigation, location-based recommendations, and urban planning. With the growing popularity of social media data and cameras embedded in smartphones, applying traditional computer vision techniques to localize images has become increasingly challenging, yet highly valuable. This paper introduces a novel approach that integrates open-weight and publicly accessible multimodal large language models with retrieval-augmented generation. The method constructs a vector database using the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query images are augmented with prompts containing both similar and dissimilar geolocation information retrieved from this database before being processed by the multimodal large language models. Our approach has demonstrated state-of-the-art performance, achieving higher accuracy compared against three widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our solution eliminates the need for expensive fine-tuning or retraining and scales seamlessly to incorporate new data sources. The effectiveness of retrieval-augmented generation-based multimodal large language models in geolocation estimation demonstrated by this paper suggests an alternative path to the traditional methods which rely on the training models from scratch, opening new possibilities for more accessible and scalable solutions in GeoAI.</li>
</ul>

<h3>Title: AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling</h3>
<ul>
<li><strong>Authors: </strong>Vishal Pandey, Ranjita Das, Debasmita Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01344">https://arxiv.org/abs/2509.01344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01344">https://arxiv.org/pdf/2509.01344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01344]] AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling(https://arxiv.org/abs/2509.01344)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>Meeting the increasing global demand for food security and sustainable farming requires intelligent crop recommendation systems that operate in real time. Traditional soil analysis techniques are often slow, labor-intensive, and not suitable for on-field decision-making. To address these limitations, we introduce AgroSense, a deep-learning framework that integrates soil image classification and nutrient profiling to produce accurate and contextually relevant crop recommendations. AgroSense comprises two main components: a Soil Classification Module, which leverages ResNet-18, EfficientNet-B0, and Vision Transformer architectures to categorize soil types from images; and a Crop Recommendation Module, which employs a Multi-Layer Perceptron, XGBoost, LightGBM, and TabNet to analyze structured soil data, including nutrient levels, pH, and rainfall. We curated a multimodal dataset of 10,000 paired samples drawn from publicly available Kaggle repositories, approximately 50,000 soil images across seven classes, and 25,000 nutrient profiles for experimental evaluation. The fused model achieves 98.0% accuracy, with a precision of 97.8%, a recall of 97.7%, and an F1-score of 96.75%, while RMSE and MAE drop to 0.32 and 0.27, respectively. Ablation studies underscore the critical role of multimodal coupling, and statistical validation via t-tests and ANOVA confirms the significance of our improvements. AgroSense offers a practical, scalable solution for real-time decision support in precision agriculture and paves the way for future lightweight multimodal AI systems in resource-constrained environments.</li>
</ul>

<h3>Title: Causal Sensitivity Identification using Generative Learning</h3>
<ul>
<li><strong>Authors: </strong>Soma Bandyopadhyay, Sudeshna Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01352">https://arxiv.org/abs/2509.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01352">https://arxiv.org/pdf/2509.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01352]] Causal Sensitivity Identification using Generative Learning(https://arxiv.org/abs/2509.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we propose a novel generative method to identify the causal impact and apply it to prediction tasks. We conduct causal impact analysis using interventional and counterfactual perspectives. First, applying interventions, we identify features that have a causal influence on the predicted outcome, which we refer to as causally sensitive features, and second, applying counterfactuals, we evaluate how changes in the cause affect the effect. Our method exploits the Conditional Variational Autoencoder (CVAE) to identify the causal impact and serve as a generative predictor. We are able to reduce confounding bias by identifying causally sensitive features. We demonstrate the effectiveness of our method by recommending the most likely locations a user will visit next in their spatiotemporal trajectory influenced by the causal relationships among various features. Experiments on the large-scale GeoLife [Zheng et al., 2010] dataset and the benchmark Asia Bayesian network validate the ability of our method to identify causal impact and improve predictive performance.</li>
</ul>

<h3>Title: DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Anda Cheng, Zhao Zhang, Yinggui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01354">https://arxiv.org/abs/2509.01354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01354">https://arxiv.org/pdf/2509.01354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01354]] DPF-CM: A Data Processing Framework with Privacy-Preserving Vector Databases for Chinese Medical LLMs Training and Deployment(https://arxiv.org/abs/2509.01354)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Current open-source training pipelines for Chinese medical language models predominantly emphasize optimizing training methodologies to enhance the performance of large language models (LLMs), yet lack comprehensive exploration into training data processing. To address this gap, we propose DPF-CM, a holistic Data Processing Framework for Chinese Medical LLMs training and deployment. DPF-CM comprises two core modules. The first module is a data processing pipeline tailored for model training. Beyond standard data processing operations, we (1) introduce a chained examples context-learning strategy to generate question-oriented instructions to mitigate the lack of instruction content, and (2) implement an ensemble-based filtering mechanism for preference data curation that averages multiple reward models to suppress noisy samples. The second module focuses on privacy preservation during model deployment. To prevent privacy risks from the inadvertent exposure of training data, we propose a Privacy Preserving Vector Database (PPVD) approach, which involves model memory search, high-risk database construction, secure database construction, and match-and-replace, four key stages to minimize privacy leakage during inference collectively. Experimental results show that DPF-CM significantly improves model accuracy, enabling our trained Chinese medical LLM to achieve state-of-the-art performance among open-source counterparts. Moreover, the framework reduces training data privacy leakage by 27%.</li>
</ul>

<h3>Title: M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision</h3>
<ul>
<li><strong>Authors: </strong>Che Liu, Zheng Jiang, Chengyu Fang, Heng Guo, Yan-Jie Zhou, Jiaqi Qu, Le Lu, Minfeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01360">https://arxiv.org/abs/2509.01360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01360">https://arxiv.org/pdf/2509.01360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01360]] M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision(https://arxiv.org/abs/2509.01360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding.</li>
</ul>

<h3>Title: Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Gao, Changcheng Hua, Qingchao Chen, Yuxin Peng, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01362">https://arxiv.org/abs/2509.01362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01362">https://arxiv.org/pdf/2509.01362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01362]] Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement(https://arxiv.org/abs/2509.01362)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Identity-preserving text-to-video (IPT2V) generation creates videos faithful to both a reference subject image and a text prompt. While fine-tuning large pretrained video diffusion models on ID-matched data achieves state-of-the-art results on IPT2V, data scarcity and high tuning costs hinder broader improvement. We thus introduce a Training-Free Prompt, Image, and Guidance Enhancement (TPIGE) framework that bridges the semantic gap between the video description and the reference image and design sampling guidance that enhances identity preservation and video quality, achieving performance gains at minimal this http URL, we first propose Face Aware Prompt Enhancement, using GPT-4o to enhance the text prompt with facial details derived from the reference image. We then propose Prompt Aware Reference Image Enhancement, leveraging an identity-preserving image generator to refine the reference image, rectifying conflicts with the text prompt. The above mutual refinement significantly improves input quality before video generation. Finally, we propose ID-Aware Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize identity preservation and video quality jointly during this http URL method outperforms prior work and is validated by automatic and human evaluations on a 1000 video test set, winning first place in the ACM Multimedia 2025 Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art performance and strong generality. The code is available at this https URL.</li>
</ul>

<h3>Title: Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zbeeb, Hasan Abed Al Kader Hammoud, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01363">https://arxiv.org/abs/2509.01363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01363">https://arxiv.org/pdf/2509.01363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01363]] Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic(https://arxiv.org/abs/2509.01363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: $v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.</li>
</ul>

<h3>Title: CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Cao, Zhiyang Zhang, Heming Wang, Jun Xu, Ling Lan, Ran Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01370">https://arxiv.org/abs/2509.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01370">https://arxiv.org/pdf/2509.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01370]] CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function(https://arxiv.org/abs/2509.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Nowadays, the nanostructure inverse problem is an attractive problem that helps researchers to understand the relationship between the properties and the structure of nanomaterials. This article focuses on the problem of using PDF to recover the nanostructure, which this article views as a conditional generation problem. This article propose a deep learning model CbLDM, Condition-based Latent Diffusion Model. Based on the original latent diffusion model, the sampling steps of the diffusion model are reduced and the sample generation efficiency is improved by using the conditional prior to estimate conditional posterior distribution, which is the approximated distribution of p(z|x). In addition, this article uses the Laplacian matrix instead of the distance matrix to recover the nanostructure, which can reduce the reconstruction error. Finally, this article compares CbLDM with existing models which were used to solve the nanostructure inverse problem, and find that CbLDM demonstrates significantly higher prediction accuracy than these models, which reflects the ability of CbLDM to solve the nanostructure inverse problem and the potential to cope with other continuous conditional generation tasks.</li>
</ul>

<h3>Title: Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework</h3>
<ul>
<li><strong>Authors: </strong>Wei Lu, Lingyu Zhu, Si-Bao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01373">https://arxiv.org/abs/2509.01373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01373">https://arxiv.org/pdf/2509.01373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01373]] Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework(https://arxiv.org/abs/2509.01373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs) performance in critical applications. Existing Low-light Image Enhancement (LIE) methods struggle with the unique challenges of aerial imagery, including Ultra-High Resolution (UHR), lack of paired data, severe non-uniform illumination, and deployment constraints. To address these issues, we propose three key contributions. First, we present U3D, the first unsupervised UHR UAV dataset for LIE, with a unified evaluation toolkit. Second, we introduce the Edge Efficiency Index (EEI), a novel metric balancing perceptual quality with key deployment factors: speed, resolution, model complexity, and memory footprint. Third, we develop U3LIE, an efficient framework with two training-only designs-Adaptive Pre-enhancement Augmentation (APA) for input normalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE achieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making it ideal for real-time on-board deployment. In summary, these contributions provide a holistic solution (dataset, metric, and method) for advancing robust 24/7 UAV vision. The code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Anomaly detection in network flows using unsupervised online machine learning</h3>
<ul>
<li><strong>Authors: </strong>Alberto Miguel-Diez, Adri√°n Campazas-Vega, √Ångel Manuel Guerrero-Higueras, Claudia √Ålvarez-Aparicio, Vicente Matell√°n-Olivera</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01375">https://arxiv.org/abs/2509.01375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01375">https://arxiv.org/pdf/2509.01375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01375]] Anomaly detection in network flows using unsupervised online machine learning(https://arxiv.org/abs/2509.01375)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Nowadays, the volume of network traffic continues to grow, along with the frequency and sophistication of attacks. This scenario highlights the need for solutions capable of continuously adapting, since network behavior is dynamic and changes over time. This work presents an anomaly detection model for network flows using unsupervised machine learning with online learning capabilities. This approach allows the system to dynamically learn the normal behavior of the network and detect deviations without requiring labeled data, which is particularly useful in real-world environments where traffic is constantly changing and labeled data is scarce. The model was implemented using the River library with a One-Class SVM and evaluated on the NF-UNSW-NB15 dataset and its extended version v2, which contain network flows labeled with different attack categories. The results show an accuracy above 98%, a false positive rate below 3.1%, and a recall of 100% in the most advanced version of the dataset. In addition, the low processing time per flow (<0.033 ms) demonstrates the feasibility of the approach for real-time applications.</li>
</ul>

<h3>Title: WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data</h3>
<ul>
<li><strong>Authors: </strong>Paloma Piot, Diego S√°nchez, Javier Parapar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01379">https://arxiv.org/abs/2509.01379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01379">https://arxiv.org/pdf/2509.01379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01379]] WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data(https://arxiv.org/abs/2509.01379)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online harms are a growing problem in digital spaces, putting user safety at risk and reducing trust in social media platforms. One of the most persistent forms of harm is hate speech. To address this, we need tools that combine the speed and scale of automated systems with the judgment and insight of human moderators. These tools should not only find harmful content but also explain their decisions clearly, helping to build trust and understanding. In this paper, we present WATCHED, a chatbot designed to support content moderators in tackling hate speech. The chatbot is built as an Artificial Intelligence Agent system that uses Large Language Models along with several specialised tools. It compares new posts with real examples of hate speech and neutral content, uses a BERT-based classifier to help flag harmful messages, looks up slang and informal language using sources like Urban Dictionary, generates chain-of-thought reasoning, and checks platform guidelines to explain and support its decisions. This combination allows the chatbot not only to detect hate speech but to explain why content is considered harmful, grounded in both precedent and policy. Experimental results show that our proposed method surpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91. Designed for moderators, safety teams, and researchers, the tool helps reduce online harms by supporting collaboration between AI and human oversight.</li>
</ul>

<h3>Title: Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning</h3>
<ul>
<li><strong>Authors: </strong>Long Zhang, Peipei Song, Jianfeng Dong, Kun Li, Xun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01383">https://arxiv.org/abs/2509.01383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01383">https://arxiv.org/pdf/2509.01383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01383]] Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning(https://arxiv.org/abs/2509.01383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos partially relevant to a given query. The core challenge lies in learning robust query-video alignment against spurious semantic correlations arising from inherent data uncertainty: 1) query ambiguity, where the query incompletely characterizes the target video and often contains uninformative tokens, and 2) partial video relevance, where abundant query-irrelevant segments introduce contextual noise in cross-modal alignment. Existing methods often focus on enhancing multi-scale clip representations and retrieving the most relevant clip. However, the inherent data uncertainty in PRVR renders them vulnerable to distractor videos with spurious similarities, leading to suboptimal performance. To fill this research gap, we propose Robust Alignment Learning (RAL) framework, which explicitly models the uncertainty in data. Key innovations include: 1) we pioneer probabilistic modeling for PRVR by encoding videos and queries as multivariate Gaussian distributions. This not only quantifies data uncertainty but also enables proxy-level matching to capture the variability in cross-modal correspondences; 2) we consider the heterogeneous informativeness of query words and introduce learnable confidence gates to dynamically weight similarity. As a plug-and-play solution, RAL can be seamlessly integrated into the existing architectures. Extensive experiments across diverse retrieval backbones demonstrate its effectiveness.</li>
</ul>

<h3>Title: Analysing the Language of Neural Audio Codecs</h3>
<ul>
<li><strong>Authors: </strong>Joonyong Park, Shinnosuke Takamichi, David M. Chan, Shunsuke Kando, Yuki Saito, Hiroshi Saruwatari</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01390">https://arxiv.org/abs/2509.01390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01390">https://arxiv.org/pdf/2509.01390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01390]] Analysing the Language of Neural Audio Codecs(https://arxiv.org/abs/2509.01390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study presents a comparative analysis of the statistical and linguistic properties of neural audio codecs (NACs). We investigate discrete speech tokens produced by various NAC models, examining their adherence to linguistic statistical laws such as Zipf's law and Heaps' law, as well as their entropy and redundancy. To assess how these token-level properties relate to semantic and acoustic preservation in synthesized speech, we evaluate intelligibility using error rates of automatic speech recognition, and quality using the UTMOS score. Our results reveal that NAC tokens, particularly 3-grams, exhibit language-like statistical patterns. Moreover, these properties, together with measures of information content, are found to correlate with improved performances in speech recognition and resynthesis tasks. These findings offer insights into the structure of NAC token sequences and inform the design of more effective generative speech models.</li>
</ul>

<h3>Title: LLMs cannot spot math errors, even when allowed to peek into the solution</h3>
<ul>
<li><strong>Authors: </strong>KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01395">https://arxiv.org/abs/2509.01395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01395">https://arxiv.org/pdf/2509.01395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01395]] LLMs cannot spot math errors, even when allowed to peek into the solution(https://arxiv.org/abs/2509.01395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance.</li>
</ul>

<h3>Title: Distillation of a tractable model from the VQ-VAE</h3>
<ul>
<li><strong>Authors: </strong>Armin Had≈æiƒá, Milan Papez, Tom√°≈° Pevn√Ω</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01400">https://arxiv.org/abs/2509.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01400">https://arxiv.org/pdf/2509.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01400]] Distillation of a tractable model from the VQ-VAE(https://arxiv.org/abs/2509.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models with discrete latent space, such as the Vector-Quantized Variational Autoencoder (VQ-VAE), offer excellent data generation capabilities, but, due to the large size of their latent space, their probabilistic inference is deemed intractable. We demonstrate that the VQ-VAE can be distilled into a tractable model by selecting a subset of latent variables with high probabilities. This simple strategy is particularly efficient, especially if the VQ-VAE underutilizes its latent space, which is, indeed, very often the case. We frame the distilled model as a probabilistic circuit, and show that it preserves expressiveness of the VQ-VAE while providing tractable probabilistic inference. Experiments illustrate competitive performance in density estimation and conditional generation tasks, challenging the view of the VQ-VAE as an inherently intractable model.</li>
</ul>

<h3>Title: RibPull: Implicit Occupancy Fields and Medial Axis Extraction for CT Ribcage Scans</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Nikolakakis, Amine Ouasfi, Julie Digne, Razvan Marinescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01402">https://arxiv.org/abs/2509.01402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01402">https://arxiv.org/pdf/2509.01402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01402]] RibPull: Implicit Occupancy Fields and Medial Axis Extraction for CT Ribcage Scans(https://arxiv.org/abs/2509.01402)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We present RibPull, a methodology that utilizes implicit occupancy fields to bridge computational geometry and medical imaging. Implicit 3D representations use continuous functions that handle sparse and noisy data more effectively than discrete methods. While voxel grids are standard for medical imaging, they suffer from resolution limitations, topological information loss, and inefficient handling of sparsity. Coordinate functions preserve complex geometrical information and represent a better solution for sparse data representation, while allowing for further morphological operations. Implicit scene representations enable neural networks to encode entire 3D scenes within their weights. The result is a continuous function that can implicitly compesate for sparse signals and infer further information about the 3D scene by passing any combination of 3D coordinates as input to the model. In this work, we use neural occupancy fields that predict whether a 3D point lies inside or outside an object to represent CT-scanned ribcages. We also apply a Laplacian-based contraction to extract the medial axis of the ribcage, thus demonstrating a geometrical operation that benefits greatly from continuous coordinate-based 3D scene representations versus voxel-based representations. We evaluate our methodology on 20 medical scans from the RibSeg dataset, which is itself an extension of the RibFrac dataset. We will release our code upon publication.</li>
</ul>

<h3>Title: Neural Scene Designer: Self-Styled Semantic Image Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Jianman Lin, Tianshui Chen, Chunmei Qing, Zhijing Yang, Shuangping Huang, Yuheng Ren, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01405">https://arxiv.org/abs/2509.01405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01405">https://arxiv.org/pdf/2509.01405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01405]] Neural Scene Designer: Self-Styled Semantic Image Manipulation(https://arxiv.org/abs/2509.01405)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Maintaining stylistic consistency is crucial for the cohesion and aesthetic appeal of images, a fundamental requirement in effective image editing and inpainting. However, existing methods primarily focus on the semantic control of generated content, often neglecting the critical task of preserving this consistency. In this work, we introduce the Neural Scene Designer (NSD), a novel framework that enables photo-realistic manipulation of user-specified scene regions while ensuring both semantic alignment with user intent and stylistic consistency with the surrounding environment. NSD leverages an advanced diffusion model, incorporating two parallel cross-attention mechanisms that separately process text and style information to achieve the dual objectives of semantic control and style consistency. To capture fine-grained style representations, we propose the Progressive Self-style Representational Learning (PSRL) module. This module is predicated on the intuitive premise that different regions within a single image share a consistent style, whereas regions from different images exhibit distinct styles. The PSRL module employs a style contrastive loss that encourages high similarity between representations from the same image while enforcing dissimilarity between those from different images. Furthermore, to address the lack of standardized evaluation protocols for this task, we establish a comprehensive benchmark. This benchmark includes competing algorithms, dedicated style-related metrics, and diverse datasets and settings to facilitate fair comparisons. Extensive experiments conducted on our benchmark demonstrate the effectiveness of the proposed framework.</li>
</ul>

<h3>Title: Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring</h3>
<ul>
<li><strong>Authors: </strong>Matteo Ballegeer, Matthias Bogaert, Dries F. Benoit</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01409">https://arxiv.org/abs/2509.01409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01409">https://arxiv.org/pdf/2509.01409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01409]] Evaluating the stability of model explanations in instance-dependent cost-sensitive credit scoring(https://arxiv.org/abs/2509.01409)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Instance-dependent cost-sensitive (IDCS) classifiers offer a promising approach to improving cost-efficiency in credit scoring by tailoring loss functions to instance-specific costs. However, the impact of such loss functions on the stability of model explanations remains unexplored in literature, despite increasing regulatory demands for transparency. This study addresses this gap by evaluating the stability of Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) when applied to IDCS models. Using four publicly available credit scoring datasets, we first assess the discriminatory power and cost-efficiency of IDCS classifiers, introducing a novel metric to enhance cross-dataset comparability. We then investigate the stability of SHAP and LIME feature importance rankings under varying degrees of class imbalance through controlled resampling. Our results reveal that while IDCS classifiers improve cost-efficiency, they produce significantly less stable explanations compared to traditional models, particularly as class imbalance increases, highlighting a critical trade-off between cost optimization and interpretability in credit scoring. Amid increasing regulatory scrutiny on explainability, this research underscores the pressing need to address stability issues in IDCS classifiers to ensure that their cost advantages are not undermined by unstable or untrustworthy explanations.</li>
</ul>

<h3>Title: MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization</h3>
<ul>
<li><strong>Authors: </strong>Uƒüur √áoƒüalan, Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Colin Groth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01411">https://arxiv.org/abs/2509.01411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01411">https://arxiv.org/pdf/2509.01411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01411]] MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization(https://arxiv.org/abs/2509.01411)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present MILO (Metric for Image- and Latent-space Optimization), a lightweight, multiscale, perceptual metric for full-reference image quality assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score) supervision, in which reproducible distortions are applied to diverse images and scored via an ensemble of recent quality metrics that account for visual masking effects. This approach enables accurate learning without requiring large-scale human-labeled datasets. Despite its compact architecture, MILO outperforms existing metrics across standard FR-IQA benchmarks and offers fast inference suitable for real-time applications. Beyond quality prediction, we demonstrate the utility of MILO as a perceptual loss in both image and latent domains. In particular, we show that spatial masking modeled by MILO, when applied to latent representations from a VAE encoder within Stable Diffusion, enables efficient and perceptually aligned optimization. By combining spatial masking with a curriculum learning strategy, we first process perceptually less relevant regions before progressively shifting the optimization to more visually distorted areas. This strategy leads to significantly improved performance in tasks like denoising, super-resolution, and face restoration, while also reducing computational overhead. MILO thus functions as both a state-of-the-art image quality metric and as a practical tool for perceptual optimization in generative pipelines.</li>
</ul>

<h3>Title: Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kaviraj Pather, Elena Hadjigeorgiou, Arben Krasniqi, Claire Schmit, Irina Rusu, Marc Pons, Kabir Khan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01412">https://arxiv.org/abs/2509.01412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01412">https://arxiv.org/pdf/2509.01412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01412]] Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning(https://arxiv.org/abs/2509.01412)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.</li>
</ul>

<h3>Title: Bangladeshi Street Food Calorie Estimation Using Improved YOLOv8 and Regression Model</h3>
<ul>
<li><strong>Authors: </strong>Aparup Dhar (1), MD Tamim Hossain (1), Pritom Barua (1) ((1) Department of Computer Science and Engineering, Premier University, Chittagong, Bangladesh)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01415">https://arxiv.org/abs/2509.01415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01415">https://arxiv.org/pdf/2509.01415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01415]] Bangladeshi Street Food Calorie Estimation Using Improved YOLOv8 and Regression Model(https://arxiv.org/abs/2509.01415)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As obesity rates continue to increase, automated calorie tracking has become a vital tool for people seeking to maintain a healthy lifestyle or adhere to a diet plan. Although numerous research efforts have addressed this issue, existing approaches often face key limitations, such as providing only constant caloric output, struggling with multiple food recognition challenges, challenges in image scaling and normalization, and a predominant focus on Western cuisines. In this paper, we propose a tailored solution that specifically targets Bangladeshi street food. We first construct a diverse dataset of popular street foods found across Bangladesh. Then, we develop a refined calorie estimation system by modifying the state-of-the-art vision model YOLOv8. Our modified model achieves superior classification and segmentation results, with only a slight increase in computational complexity compared to the base variant. Coupled with a machine learning regression model, our system achieves an impressive 6.94 mean absolute error (MAE), 11.03 root mean squared error (RMSE), and a 96.0% R^2 score in calorie estimation, making it both highly effective and accurate for real-world food calorie calculations.</li>
</ul>

<h3>Title: Accelerating PDE Solvers with Equation-Recast Neural Operator Preconditioning</h3>
<ul>
<li><strong>Authors: </strong>Qiyun Cheng, Md Hossain Sahadath, Huihua Yang, Shaowu Pan, Wei Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01416">https://arxiv.org/abs/2509.01416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01416">https://arxiv.org/pdf/2509.01416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01416]] Accelerating PDE Solvers with Equation-Recast Neural Operator Preconditioning(https://arxiv.org/abs/2509.01416)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The computational overhead of traditional numerical solvers for partial differential equations (PDEs) remains a critical bottleneck for large-scale parametric studies and design optimization. We introduce a Minimal-Data Parametric Neural Operator Preconditioning (MD-PNOP) framework, which establishes a new paradigm for accelerating parametric PDE solvers while strictly preserving physical constraints. The key idea is to recast the residual from parameter deviation as additional source term, where any trained neural operator can be used to refine the solution in an offline fashion. This directly addresses the fundamental extrapolation limitation of neural operators, enabling extrapolative generalization of any neural operator trained at a single parameter setting across a wide range of configurations without any retraining. The neural operator predictions are then embedded into iterative PDE solvers as improved initial guesses, thereby reducing convergence iterations without sacrificing accuracy. Unlike purely data-driven approaches, MD-PNOP guarantees that the governing equations remain fully enforced, eliminating concerns about loss of physics or interpretability. The framework is architecture-agnostic and is demonstrated using both Deep Operator Networks (DeepONet) and Fourier Neural Operators (FNO) for Boltzmann transport equation solvers in neutron transport applications. We demonstrated that neural operators trained on a single set of constant parameters successfully accelerate solutions with heterogeneous, sinusoidal, and discontinuous parameter distributions. Besides, MD-PNOP consistently achieves ~50% reduction in computational time while maintaining full order fidelity for fixed-source, single-group eigenvalue, and multigroup coupled eigenvalue problems.</li>
</ul>

<h3>Title: On the Alignment of Large Language Models with Global Human Opinion</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Masahiro Kaneko, Chenhui Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01418">https://arxiv.org/abs/2509.01418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01418">https://arxiv.org/pdf/2509.01418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01418]] On the Alignment of Large Language Models with Global Human Opinion(https://arxiv.org/abs/2509.01418)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at this https URL.</li>
</ul>

<h3>Title: InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information</h3>
<ul>
<li><strong>Authors: </strong>Guohui Zhang, Jiangtong Tan, Linjiang Huang, Zhonghang Yuan, Naishan Zheng, Jie Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01421">https://arxiv.org/abs/2509.01421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01421">https://arxiv.org/pdf/2509.01421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01421]] InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information(https://arxiv.org/abs/2509.01421)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have become dominant in visual generation but suffer performance drop when tested on resolutions that differ from the training scale, whether lower or higher. In fact, the key challenge in generating variable-scale images lies in the differing amounts of information across resolutions, which requires information conversion procedures to be varied for generating variable-scaled images. In this paper, we investigate the issues of three critical aspects in DMs for a unified analysis in variable-scaled generation: dilated convolution, attention mechanisms, and initial noise. Specifically, 1) dilated convolution in DMs for the higher-resolution generation loses high-frequency information. 2) Attention for variable-scaled image generation struggles to adjust the information aggregation adaptively. 3) The spatial distribution of information in the initial noise is misaligned with variable-scaled image. To solve the above problems, we propose \textbf{InfoScale}, an information-centric framework for variable-scaled image generation by effectively utilizing information from three aspects correspondingly. For information loss in 1), we introduce Progressive Frequency Compensation module to compensate for high-frequency information lost by dilated convolution in higher-resolution generation. For information aggregation inflexibility in 2), we introduce Adaptive Information Aggregation module to adaptively aggregate information in lower-resolution generation and achieve an effective balance between local and global information in higher-resolution generation. For information distribution misalignment in 3), we design Noise Adaptation module to re-distribute information in initial noise for variable-scaled generation. Our method is plug-and-play for DMs and extensive experiments demonstrate the effectiveness in variable-scaled image generation.</li>
</ul>

<h3>Title: Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction</h3>
<ul>
<li><strong>Authors: </strong>Djamel Eddine Boukhari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01431">https://arxiv.org/abs/2509.01431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01431">https://arxiv.org/pdf/2509.01431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01431]] Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction(https://arxiv.org/abs/2509.01431)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The computational assessment of facial attractiveness, a challenging subjective regression task, is dominated by architectures with a critical trade-off: Convolutional Neural Networks (CNNs) offer efficiency but have limited receptive fields, while Vision Transformers (ViTs) model global context at a quadratic computational cost. To address this, we propose Mamba-CNN, a novel and efficient hybrid architecture. Mamba-CNN integrates a lightweight, Mamba-inspired State Space Model (SSM) gating mechanism into a hierarchical convolutional backbone. This core innovation allows the network to dynamically modulate feature maps and selectively emphasize salient facial features and their long-range spatial relationships, mirroring human holistic perception while maintaining computational efficiency. We conducted extensive experiments on the widely-used SCUT-FBP5500 benchmark, where our model sets a new state-of-the-art. Mamba-CNN achieves a Pearson Correlation (PC) of 0.9187, a Mean Absolute Error (MAE) of 0.2022, and a Root Mean Square Error (RMSE) of 0.2610. Our findings validate the synergistic potential of combining CNNs with selective SSMs and present a powerful new architectural paradigm for nuanced visual understanding tasks.</li>
</ul>

<h3>Title: The Geometry of Nonlinear Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Nikola Milosevic, Nico Scherf</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01432">https://arxiv.org/abs/2509.01432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01432">https://arxiv.org/pdf/2509.01432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01432]] The Geometry of Nonlinear Reinforcement Learning(https://arxiv.org/abs/2509.01432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reward maximization, safe exploration, and intrinsic motivation are often studied as separate objectives in reinforcement learning (RL). We present a unified geometric framework, that views these goals as instances of a single optimization problem on the space of achievable long-term behavior in an environment. Within this framework, classical methods such as policy mirror descent, natural policy gradient, and trust-region algorithms naturally generalize to nonlinear utilities and convex constraints. We illustrate how this perspective captures robustness, safety, exploration, and diversity objectives, and outline open challenges at the interface of geometry and deep RL.</li>
</ul>

<h3>Title: LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT</h3>
<ul>
<li><strong>Authors: </strong>Handi Chen, Jing Deng, Xiuzhe Wu, Zhihan Jiang, Xinchen Zhang, Xianhao Chen, Edith C.H. Ngai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01434">https://arxiv.org/abs/2509.01434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01434">https://arxiv.org/pdf/2509.01434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01434]] LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT(https://arxiv.org/abs/2509.01434)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The expansion of Internet of Things (IoT) devices constantly generates heterogeneous data streams, driving demand for continuous, decentralized intelligence. Federated Lifelong Learning (FLL) provides an ideal solution by incorporating federated and lifelong learning to overcome catastrophic forgetting. The extended lifecycle of FLL in IoT systems increases their vulnerability to persistent attacks, and these risks may be obscured by performance degradation caused by spatial-temporal data heterogeneity. Moreover, this problem is exacerbated by the standard single-server architecture, as its single point of failure makes it difficult to maintain a reliable audit trail for long-term threats. Blockchain provides a tamper-proof foundation for trustworthy FLL systems. Nevertheless, directly applying blockchain to FLL significantly increases computational and retrieval costs with the expansion of the knowledge base, slowing down the training on IoT devices. To address these challenges, we propose LiFeChain, a lightweight blockchain for secure and efficient federated lifelong learning by providing a tamper-resistant ledger with minimal on-chain disclosure and bidirectional verification. To the best of our knowledge, LiFeChain is the first blockchain tailored for FLL. LiFeChain incorporates two complementary mechanisms: the proof-of-model-correlation (PoMC) consensus on the server, which couples learning and unlearning mechanisms to mitigate negative transfer, and segmented zero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates abnormal committee behavior without compromising privacy. LiFeChain is designed as a plug-and-play component that can be seamlessly integrated into existing FLL algorithms. Experimental results demonstrate that LiFeChain not only enhances model performance against two long-term attacks but also sustains high efficiency and scalability.</li>
</ul>

<h3>Title: SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization</h3>
<ul>
<li><strong>Authors: </strong>Artur D√≠az-Juan, Coloma Ballester, Gloria Haro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01439">https://arxiv.org/abs/2509.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01439">https://arxiv.org/pdf/2509.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01439]] SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization(https://arxiv.org/abs/2509.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video summarization aims to extract key shots from longer videos to produce concise and informative summaries. One of its most common applications is in sports, where highlight reels capture the most important moments of a game, along with notable reactions and specific contextual events. Automatic summary generation can support video editors in the sports media industry by reducing the time and effort required to identify key segments. However, the lack of publicly available datasets poses a challenge in developing robust models for sports highlight generation. In this paper, we address this gap by introducing a curated dataset for soccer video summarization, designed to serve as a benchmark for the task. The dataset includes shot boundaries for 237 matches from the Spanish, French, and Italian leagues, using broadcast footage sourced from the SoccerNet dataset. Alongside the dataset, we propose a baseline model specifically designed for this task, which achieves an F1 score of 0.3956 in the test set. Furthermore, we propose a new metric constrained by the length of each target summary, enabling a more objective evaluation of the generated content. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Benchmarking Optimizers for Large Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Andrei Semenov, Matteo Pagliardini, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01440">https://arxiv.org/abs/2509.01440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01440">https://arxiv.org/pdf/2509.01440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01440]] Benchmarking Optimizers for Large Language Model Pretraining(https://arxiv.org/abs/2509.01440)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.</li>
</ul>

<h3>Title: Traces of Image Memorability in Vision Encoders: Activations, Attention Distributions and Autoencoder Losses</h3>
<ul>
<li><strong>Authors: </strong>Ece Takmaz, Albert Gatt, Jakub Dotlacil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01453">https://arxiv.org/abs/2509.01453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01453">https://arxiv.org/pdf/2509.01453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01453]] Traces of Image Memorability in Vision Encoders: Activations, Attention Distributions and Autoencoder Losses(https://arxiv.org/abs/2509.01453)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Images vary in how memorable they are to humans. Inspired by findings from cognitive science and computer vision, this paper explores the correlates of image memorability in pretrained vision encoders, focusing on latent activations, attention distributions, and the uniformity of image patches. We find that these features correlate with memorability to some extent. Additionally, we explore sparse autoencoder loss over the representations of vision transformers as a proxy for memorability, which yields results outperforming past methods using convolutional neural network representations. Our results shed light on the relationship between model-internal features and memorability. They show that some features are informative predictors of what makes images memorable to humans.</li>
</ul>

<h3>Title: Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal</h3>
<ul>
<li><strong>Authors: </strong>Markus Oehri, Giulia Conti, Kaviraj Pather, Alexandre Rossi, Laia Serra, Adrian Parody, Rogvi Johannesen, Aviaja Petersen, Arben Krasniqi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01455">https://arxiv.org/abs/2509.01455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01455">https://arxiv.org/pdf/2509.01455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01455]] Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal(https://arxiv.org/abs/2509.01455)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deployed language models must decide not only what to answer but also when not to answer. We present UniCR, a unified framework that turns heterogeneous uncertainty evidence including sequence likelihoods, self-consistency dispersion, retrieval compatibility, and tool or verifier feedback into a calibrated probability of correctness and then enforces a user-specified error budget via principled refusal. UniCR learns a lightweight calibration head with temperature scaling and proper scoring, supports API-only models through black-box features, and offers distribution-free guarantees using conformal risk control. For long-form generation, we align confidence with semantic fidelity by supervising on atomic factuality scores derived from retrieved evidence, reducing confident hallucinations while preserving coverage. Experiments on short-form QA, code generation with execution tests, and retrieval-augmented long-form QA show consistent improvements in calibration metrics, lower area under the risk-coverage curve, and higher coverage at fixed risk compared to entropy or logit thresholds, post-hoc calibrators, and end-to-end selective baselines. Analyses reveal that evidence contradiction, semantic dispersion, and tool inconsistency are the dominant drivers of abstention, yielding informative user-facing refusal messages. The result is a portable recipe of evidence fusion to calibrated probability to risk-controlled decision that improves trustworthiness without fine-tuning the base model and remains valid under distribution shift.</li>
</ul>

<h3>Title: LLMHoney: A Real-Time SSH Honeypot with Large Language Model-Driven Dynamic Response Generation</h3>
<ul>
<li><strong>Authors: </strong>Pranjay Malhotra</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01463">https://arxiv.org/abs/2509.01463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01463">https://arxiv.org/pdf/2509.01463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01463]] LLMHoney: A Real-Time SSH Honeypot with Large Language Model-Driven Dynamic Response Generation(https://arxiv.org/abs/2509.01463)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Cybersecurity honeypots are deception tools for engaging attackers and gather intelligence, but traditional low or medium-interaction honeypots often rely on static, pre-scripted interactions that can be easily identified by skilled adversaries. This Report presents LLMHoney, an SSH honeypot that leverages Large Language Models (LLMs) to generate realistic, dynamic command outputs in real time. LLMHoney integrates a dictionary-based virtual file system to handle common commands with low latency while using LLMs for novel inputs, achieving a balance between authenticity and performance. We implemented LLMHoney using open-source LLMs and evaluated it on a testbed with 138 representative Linux commands. We report comprehensive metrics including accuracy (exact-match, Cosine Similarity, Jaro-Winkler Similarity, Levenshtein Similarity and BLEU score), response latency and memory overhead. We evaluate LLMHoney using multiple LLM backends ranging from 0.36B to 3.8B parameters, including both open-source models and a proprietary model(Gemini). Our experiments compare 13 different LLM variants; results show that Gemini-2.0 and moderately-sized models Qwen2.5:1.5B and Phi3:3.8B provide the most reliable and accurate responses, with mean latencies around 3 seconds, whereas smaller models often produce incorrect or out-of-character outputs. We also discuss how LLM integration improves honeypot realism and adaptability compared to traditional honeypots, as well as challenges such as occasional hallucinated outputs and increased resource usage. Our findings demonstrate that LLM-driven honeypots are a promising approach to enhance attacker engagement and collect richer threat intelligence.</li>
</ul>

<h3>Title: Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01468">https://arxiv.org/abs/2509.01468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01468">https://arxiv.org/pdf/2509.01468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01468]] Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA(https://arxiv.org/abs/2509.01468)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode vast amounts of world knowledge but remain static once trained, making the timely integration of emerging facts prohibitively expensive via full retraining. Knowledge-editing techniques have thus emerged to inject or overwrite specific facts into LLMs, yet they either over-rely on superficial cues or incur complex, iterative pipelines that collapse under noisy, multi-hop conditions. We introduce Reason-KE, an end-to-end reasoning-chain-based editing framework that steers a pretrained LLM through four structured stages-fact acknowledgment, relevance determination, selective application, and final reasoning-to filter distractors in a single pass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates Qwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop under heavy distraction and <1% when answers are leaked. Our quantitative analysis confirms Reason-KE's resilience and efficiency, establishing a new state-of-the-art for reliable LLM knowledge updates.</li>
</ul>

<h3>Title: Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars</h3>
<ul>
<li><strong>Authors: </strong>Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01469">https://arxiv.org/abs/2509.01469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01469">https://arxiv.org/pdf/2509.01469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01469]] Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars(https://arxiv.org/abs/2509.01469)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel approach for 3D hair reconstruction from single photographs based on a global hair prior combined with local optimization. Capturing strand-based hair geometry from single photographs is challenging due to the variety and geometric complexity of hairstyles and the lack of ground truth training data. Classical reconstruction methods like multi-view stereo only reconstruct the visible hair strands, missing the inner structure of hairstyles and hampering realistic hair simulation. To address this, existing methods leverage hairstyle priors trained on synthetic data. Such data, however, is limited in both quantity and quality since it requires manual work from skilled artists to model the 3D hairstyles and create near-photorealistic renderings. To address this, we propose a novel approach that uses both, real and synthetic data to learn an effective hairstyle prior. Specifically, we train a transformer-based prior model on synthetic data to obtain knowledge of the internal hairstyle geometry and introduce real data in the learning process to model the outer structure. This training scheme is able to model the visible hair strands depicted in an input image, while preserving the general 3D structure of hairstyles. We exploit this prior to create a Gaussian-splatting-based reconstruction method that creates hairstyles from one or more images. Qualitative and quantitative comparisons with existing reconstruction pipelines demonstrate the effectiveness and superior performance of our method for capturing detailed hair orientation, overall silhouette, and backside consistency. For additional results and code, please refer to this https URL.</li>
</ul>

<h3>Title: Privacy-preserving authentication for military 5G networks</h3>
<ul>
<li><strong>Authors: </strong>I.D. Lutz, A.M. Hill, M.C. Valenti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01470">https://arxiv.org/abs/2509.01470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01470">https://arxiv.org/pdf/2509.01470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01470]] Privacy-preserving authentication for military 5G networks(https://arxiv.org/abs/2509.01470)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>As 5G networks gain traction in defense applications, ensuring the privacy and integrity of the Authentication and Key Agreement (AKA) protocol is critical. While 5G AKA improves upon previous generations by concealing subscriber identities, it remains vulnerable to replay-based synchronization and linkability threats under realistic adversary models. This paper provides a unified analysis of the standardized 5G AKA flow, identifying several vulnerabilities and highlighting how each exploits protocol behavior to compromise user privacy. To address these risks, we present five lightweight mitigation strategies. We demonstrate through prototype implementation and testing that these enhancements strengthen resilience against linkability attacks with minimal computational and signaling overhead. Among the solutions studied, those introducing a UE-generated nonce emerge as the most promising, effectively neutralizing the identified tracking and correlation attacks with negligible additional overhead. Integrating this extension as an optional feature to the standard 5G AKA protocol offers a backward-compatible, low-overhead path toward a more privacy-preserving authentication framework for both commercial and military 5G deployments.</li>
</ul>

<h3>Title: Hierarchical Motion Captioning Utilizing External Text Data Source</h3>
<ul>
<li><strong>Authors: </strong>Clayton Leite, Yu Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01471">https://arxiv.org/abs/2509.01471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01471">https://arxiv.org/pdf/2509.01471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01471]] Hierarchical Motion Captioning Utilizing External Text Data Source(https://arxiv.org/abs/2509.01471)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to enhance existing motion captioning methods, which directly map representations of movement to high-level descriptive captions (e.g., ``a person doing jumping jacks"). The existing methods require motion data annotated with high-level descriptions (e.g., ``jumping jacks"). However, such data is rarely available in existing motion-text datasets, which additionally do not include low-level motion descriptions. To address this, we propose a two-step hierarchical approach. First, we employ large language models to create detailed descriptions corresponding to each high-level caption that appears in the motion-text datasets (e.g., ``jumping while synchronizing arm extensions with the opening and closing of legs" for ``jumping jacks"). These refined annotations are used to retrain motion-to-text models to produce captions with low-level details. Second, we introduce a pioneering retrieval-based mechanism. It aligns the detailed low-level captions with candidate high-level captions from additional text data sources, and combine them with motion features to fabricate precise high-level captions. Our methodology is distinctive in its ability to harness knowledge from external text sources to greatly increase motion captioning accuracy, especially for movements not covered in existing motion-text datasets. Experiments on three distinct motion-text datasets (HumanML3D, KIT, and BOTH57M) demonstrate that our method achieves an improvement in average performance (across BLEU-1, BLEU-4, CIDEr, and ROUGE-L) ranging from 6% to 50% compared to the state-of-the-art M2T-Interpretable.</li>
</ul>

<h3>Title: Do Retrieval Augmented Language Models Know When They Don't Know?</h3>
<ul>
<li><strong>Authors: </strong>Youchao Zhou, Heyan Huang, Yicheng Liu, Rui Dai, Xinglin Wang, Xingchen Zhang, Shumin Shi, Yang Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01476">https://arxiv.org/abs/2509.01476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01476">https://arxiv.org/pdf/2509.01476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01476]] Do Retrieval Augmented Language Models Know When They Don't Know?(https://arxiv.org/abs/2509.01476)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing Large Language Models (LLMs) occasionally generate plausible yet factually incorrect responses, known as hallucinations. Researchers are primarily using two approaches to mitigate hallucinations, namely Retrieval Augmented Language Models (RALMs) and refusal post-training. However, current research predominantly emphasizes their individual effectiveness while overlooking the evaluation of the refusal capability of RALMs. In this study, we ask the fundamental question: Do RALMs know when they don't know? Specifically, we ask three questions. First, are RALMs well-calibrated regarding different internal and external knowledge states? We examine the influence of various factors. Contrary to expectations, we find that LLMs exhibit significant \textbf{over-refusal} behavior. Then, how does refusal post-training affect the over-refusal issue? We investigate the Refusal-aware Instruction Tuning and In-Context Fine-tuning methods. Our results show that the over-refusal problem is mitigated by In-context fine-tuning. but magnified by R-tuning. However, we also find that the refusal ability may conflict with the quality of the answer. Finally, we develop a simple yet effective refusal method for refusal post-trained models to improve their overall answer quality in terms of refusal and correct answers. Our study provides a more comprehensive understanding of the influence of important factors on RALM systems.</li>
</ul>

<h3>Title: Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Zhou, Hao Qian, Shikui Tu, Lei Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01486">https://arxiv.org/abs/2509.01486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01486">https://arxiv.org/pdf/2509.01486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01486]] Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number(https://arxiv.org/abs/2509.01486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Structure-based drug design (SBDD), aiming to generate 3D molecules with high binding affinity toward target proteins, is a vital approach in novel drug discovery. Although recent generative models have shown great potential, they suffer from unstable probability dynamics and mismatch between generated molecule size and the protein pockets geometry, resulting in inconsistent quality and off-target effects. We propose PAFlow, a novel target-aware molecular generation model featuring prior interaction guidance and a learnable atom number predictor. PAFlow adopts the efficient flow matching framework to model the generation process and constructs a new form of conditional flow matching for discrete atom types. A protein-ligand interaction predictor is incorporated to guide the vector field toward higher-affinity regions during generation, while an atom number predictor based on protein pocket information is designed to better align generated molecule size with target geometry. Extensive experiments on the CrossDocked2020 benchmark show that PAFlow achieves a new state-of-the-art in binding affinity (up to -8.31 Avg. Vina Score), simultaneously maintains favorable molecular properties.</li>
</ul>

<h3>Title: PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Liu Qifeng, Zhao Dawei, Dong Yabo, Xiao Liang, Wang Juan, Min Chen, Li Fuyang, Jiang Weizhong, Lu Dongming, Nie Yiming</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01487">https://arxiv.org/abs/2509.01487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01487">https://arxiv.org/pdf/2509.01487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01487]] PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds(https://arxiv.org/abs/2509.01487)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D object detection from point clouds plays a critical role in autonomous driving. Currently, the primary methods for point cloud processing are voxel-based and pillarbased approaches. Voxel-based methods offer high accuracy through fine-grained spatial segmentation but suffer from slower inference speeds. Pillar-based methods enhance inference speed but still fall short of voxel-based methods in accuracy. To address these issues, we propose a novel point cloud processing method, PointSlice, which slices point clouds along the horizontal plane and includes a dedicated detection network. The main contributions of PointSlice are: (1) A new point cloud processing technique that converts 3D point clouds into multiple sets of 2D (x-y) data slices. The model only learns 2D data distributions, treating the 3D point cloud as separate batches of 2D data, which reduces the number of model parameters and enhances inference speed; (2) The introduction of a Slice Interaction Network (SIN). To maintain vertical relationships across slices, we incorporate SIN into the 2D backbone network, which improves the model's 3D object perception capability. Extensive experiments demonstrate that PointSlice achieves high detection accuracy and inference speed. On the Waymo dataset, PointSlice is 1.13x faster and has 0.79x fewer parameters than the state-of-the-art voxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On the nuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP. On the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewer parameters and a 1.0 mAP accuracy reduction. The code will be available at this https URL.</li>
</ul>

<h3>Title: A Continuous-Time Consistency Model for 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Eilermann, Ren√© Heesch, Oliver Niggemann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01492">https://arxiv.org/abs/2509.01492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01492">https://arxiv.org/pdf/2509.01492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01492]] A Continuous-Time Consistency Model for 3D Point Cloud Generation(https://arxiv.org/abs/2509.01492)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fast and accurate 3D shape generation from point clouds is essential for applications in robotics, AR/VR, and digital content creation. We introduce ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes directly in point space, without discretized diffusion steps, pre-trained teacher models, or latent-space encodings. The method integrates a TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based geometric loss, enabling stable training on high-dimensional point sets while avoiding expensive Jacobian-vector products. This design supports efficient one- to two-step inference with high geometric fidelity. In contrast to previous approaches that rely on iterative denoising or latent decoders, ConTiCoM-3D employs a time-conditioned neural network operating entirely in continuous time, thereby achieving fast generation. Experiments on the ShapeNet benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art diffusion and latent consistency models in both quality and efficiency, establishing it as a practical framework for scalable 3D shape generation.</li>
</ul>

<h3>Title: MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chao Deng, Xiaosen Li, Xiao Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01498">https://arxiv.org/abs/2509.01498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01498">https://arxiv.org/pdf/2509.01498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01498]] MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation(https://arxiv.org/abs/2509.01498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The nnUNet segmentation framework adeptly adjusts most hyperparameters in training scripts automatically, but it overlooks the tuning of internal hyperparameters within the segmentation network itself, which constrains the model's ability to generalize. Addressing this limitation, this study presents a novel Self-Adaptive Convolution Module that dynamically adjusts the size of the convolution kernels depending on the unique fingerprints of different datasets. This adjustment enables the MSA2-Net, when equipped with this module, to proficiently capture both global and local features within the feature maps. Self-Adaptive Convolution Module is strategically integrated into two key components of the MSA2-Net: the Multi-Scale Convolution Bridge and the Multi-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the ability to refine outputs from various stages of the CSWin Transformer during the skip connections, effectively eliminating redundant data that could potentially impair the decoder's performance. Simultaneously, the MSADecoder, utilizing the module, excels in capturing detailed information of organs varying in size during the decoding phase. This capability ensures that the decoder's output closely reproduces the intricate details within the feature maps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by this advanced architecture, has demonstrated exceptional performance, achieving Dice coefficient scores of 86.49\%, 92.56\%, 93.37\%, and 92.98\% on the Synapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets, respectively. This underscores MSA2-Net's robustness and precision in medical image segmentation tasks across various datasets.</li>
</ul>

<h3>Title: Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal</h3>
<ul>
<li><strong>Authors: </strong>Zhangyue Shi, Zekai Wang, Yuxuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01512">https://arxiv.org/abs/2509.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01512">https://arxiv.org/pdf/2509.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01512]] Unsupervised Identification and Replay-based Detection (UIRD) for New Category Anomaly Detection in ECG Signal(https://arxiv.org/abs/2509.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In clinical practice, automatic analysis of electrocardiogram (ECG) is widely applied to identify irregular heart rhythms and other electrical anomalies of the heart, enabling timely intervention and potentially improving clinical outcomes. However, due to the limited samples in certain types of ECG signals, the class imbalance issues pose a challenge for ECG-based detection. In addition, as the volume of patient data grows, long-term storage of all historical data becomes increasingly burdensome as training samples to recognize new patterns and classify existing ECG signals accurately. Therefore, to enhance the performance of anomaly detection while addressing storage limitations, we propose a pseudo-replay based semi-supervised continual learning framework, which consists of two components: unsupervised identification and replay-based detection. For unsupervised identification, an unsupervised generative adversarial network (GAN)-based framework is integrated to detect novel patterns. Besides, instead of directly storing all historical data, a pseudo replay-based learning strategy is proposed which utilizes a generator to learn the data distribution for each individual task. When a new task arises, the generator synthesizes pseudo data representative of previous learnt classes, enabling the model to detect both the existed patterns and the newly presented anomalies. The effectiveness of the proposed framework is validated in four public ECG datasets, which leverages supervised classification problems for anomaly detection. The experimental results show that the developed approach is very promising in identifying novel anomalies while maintaining good performance on detecting existing ECG signals.</li>
</ul>

<h3>Title: Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health</h3>
<ul>
<li><strong>Authors: </strong>Mingzhi Dai, Weiwei Cai, Xiang Feng, Huiqun Yu, Weibin Guo, Miao Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01526">https://arxiv.org/abs/2509.01526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01526">https://arxiv.org/pdf/2509.01526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01526]] Prediction, Generation of WWTPs microbiome community structures and Clustering of WWTPs various feature attributes using DE-BP model, SiTime-GAN model and DPNG-EPMC ensemble clustering algorithm with modulation of microbial ecosystem health(https://arxiv.org/abs/2509.01526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Microbiomes not only underpin Earth's biogeochemical cycles but also play crucial roles in both engineered and natural ecosystems, such as the soil, wastewater treatment, and the human gut. However, microbiome engineering faces significant obstacles to surmount to deliver the desired improvements in microbiome control. Here, we use the backpropagation neural network (BPNN), optimized through differential evolution (DE-BP), to predict the microbial composition of activated sludge (AS) systems collected from wastewater treatment plants (WWTPs) located worldwide. Furthermore, we introduce a novel clustering algorithm termed Directional Position Nonlinear Emotional Preference Migration Behavior Clustering (DPNG-EPMC). This method is applied to conduct a clustering analysis of WWTPs across various feature attributes. Finally, we employ the Similar Time Generative Adversarial Networks (SiTime-GAN), to synthesize novel microbial compositions and feature attributes data. As a result, we demonstrate that the DE-BP model can provide superior predictions of the microbial composition. Additionally, we show that the DPNG-EPMC can be applied to the analysis of WWTPs under various feature attributes. Finally, we demonstrate that the SiTime-GAN model can generate valuable incremental synthetic data. Our results, obtained through predicting the microbial community and conducting analysis of WWTPs under various feature attributes, develop an understanding of the factors influencing AS communities.</li>
</ul>

<h3>Title: CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01535">https://arxiv.org/abs/2509.01535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01535">https://arxiv.org/pdf/2509.01535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01535]] CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models(https://arxiv.org/abs/2509.01535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. Implementation details can be found at this https URL.</li>
</ul>

<h3>Title: Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Mark, Leonard Galustian, Maximilian P.-P. Kovar, Esther Heid</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01543">https://arxiv.org/abs/2509.01543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01543">https://arxiv.org/pdf/2509.01543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01543]] Feynman-Kac-Flow: Inference Steering of Conditional Flow Matching to an Energy-Tilted Posterior(https://arxiv.org/abs/2509.01543)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Conditional Flow Matching(CFM) represents a fast and high-quality approach to generative modelling, but in many applications it is of interest to steer the generated samples towards precise requirements. While steering approaches like gradient-based guidance, sequential Monte Carlo steering or Feynman-Kac steering are well established for diffusion models, they have not been extended to flow matching approaches yet. In this work, we formulate this requirement as tilting the output with an energy potential. We derive, for the first time, Feynman-Kac steering for CFM. We evaluate our approach on a set of synthetic tasks, including the generation of tilted distributions in a high-dimensional space, which is a particularly challenging case for steering approaches. We then demonstrate the impact of Feynman-Kac steered CFM on the previously unsolved challenge of generated transition states of chemical reactions with the correct chirality, where the reactants or products can have a different handedness, leading to geometric constraints of the viable reaction pathways connecting reactants and products. Code to reproduce this study is avaiable open-source at this https URL.</li>
</ul>

<h3>Title: Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Enneng Yang, Lu Yin, Shiwei Liu, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01548">https://arxiv.org/abs/2509.01548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01548">https://arxiv.org/pdf/2509.01548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01548]] Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing(https://arxiv.org/abs/2509.01548)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Model merging leverages multiple finetuned expert models to construct a multi-task model with low cost, and is gaining increasing attention. However, as a growing number of finetuned models become publicly available, concerns about the safety of model merging have emerged. Unauthorized merging may infringe on developers' rights and risk leaking sensitive personal information. Most existing methods focus on detecting whether a merged model originates from a specific source model, but fail to effectively prevent illegal merging. In this paper, we propose MergeLock, an active protection mechanism that disrupts model parameters to render them unmergeable, thereby directly preventing unauthorized model merging. Specifically, leveraging the inherent symmetry of the attention mechanism in Transformer-based models, we randomly sample two pairs of invertible matrices and apply them to the Query-Key (QK) and Value-Output (VO) branches. This transformation keeps the model's output unchanged while pushing it away from the shared parameter space of other finetuned models. Extensive experiments across both vision and language tasks demonstrate that MergeLock can degrade the performance of merged models by over 95% when a protected model is involved in most cases, demonstrating its effectiveness. Moreover, we further demonstrate that merged models protected by MergeLock cannot be effectively recovered using low-cost restoration methods, further enhancing robustness against unauthorized merging. The code is available at this https URL.</li>
</ul>

<h3>Title: Unified Supervision For Vision-Language Modeling in 3D Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Hao-Chih Lee, Zelong Liu, Hamza Ahmed, Spencer Kim, Sean Huver, Vishwesh Nath, Zahi A. Fayad, Timothy Deyer, Xueyan Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01554">https://arxiv.org/abs/2509.01554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01554">https://arxiv.org/pdf/2509.01554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01554]] Unified Supervision For Vision-Language Modeling in 3D Computed Tomography(https://arxiv.org/abs/2509.01554)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>General-purpose vision-language models (VLMs) have emerged as promising tools in radiology, offering zero-shot capabilities that mitigate the need for large labeled datasets. However, in high-stakes domains like diagnostic radiology, these models often lack the discriminative precision required for reliable clinical use. This challenge is compounded by the scarcity and heterogeneity of publicly available volumetric CT datasets, which vary widely in annotation formats and granularity. To address these limitations, we introduce Uniferum, a volumetric VLM that unifies diverse supervision signals, encoded in classification labels and segmentation masks, into a single training framework. By harmonizing three public 3D CT datasets with distinct annotations, Uniferum achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark by 7% compared to CLIP-based and conventional multi-label convolutional models. The model demonstrates robust out-of-distribution generalization, with observed evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT datasets. Our results highlight the effectiveness of integrating heterogeneous annotations and body segmentation to enhance model performance, setting a new direction for clinically reliable, data-efficient VLMs in 3D medical imaging.</li>
</ul>

<h3>Title: Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Dejia Cai, Yao Ran, Kun Yang, Xinwang Shi, Yingying Zhou, Kexian Wu, Yang Xu, Yi Hu, Xiaowei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01557">https://arxiv.org/abs/2509.01557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01557">https://arxiv.org/pdf/2509.01557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01557]] Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model(https://arxiv.org/abs/2509.01557)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic technique widely used for treating various diseases. However, the success and safety of HIFU treatments depend on real-time monitoring, which is often hindered by interference when using ultrasound to guide HIFU treatment. To address these challenges, we developed HIFU-ILDiff, a novel deep learning-based approach leveraging latent diffusion models to suppress HIFU-induced interference in ultrasound images. The HIFU-ILDiff model employs a Vector Quantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images into a lower-dimensional latent space, followed by a latent diffusion model that iteratively removes interference. The denoised latent vectors are then decoded to reconstruct high-resolution, interference-free ultrasound images. We constructed a comprehensive dataset comprising 18,872 image pairs from in vitro phantoms, ex vivo tissues, and in vivo animal data across multiple imaging modalities and HIFU power levels to train and evaluate the model. Experimental results demonstrate that HIFU-ILDiff significantly outperforms the commonly used Notch Filter method, achieving a Structural Similarity Index (SSIM) of 0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443 and PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally, HIFU-ILDiff achieves real-time processing at 15 frames per second, markedly faster than the Notch Filter's 5 seconds per frame. These findings indicate that HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding images for real-time monitoring during HIFU therapy, which will greatly improve the treatment precision in current clinical applications.</li>
</ul>

<h3>Title: Kwai Keye-VL 1.5 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01563">https://arxiv.org/abs/2509.01563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01563">https://arxiv.org/pdf/2509.01563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01563]] Kwai Keye-VL 1.5 Technical Report(https://arxiv.org/abs/2509.01563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.</li>
</ul>

<h3>Title: Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief</h3>
<ul>
<li><strong>Authors: </strong>Zeguan Xiao, Diyang Dou, Boya Xiong, Yun Chen, Guanhua Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01564">https://arxiv.org/abs/2509.01564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01564">https://arxiv.org/pdf/2509.01564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01564]] Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief(https://arxiv.org/abs/2509.01564)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range.</li>
</ul>

<h3>Title: One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption</h3>
<ul>
<li><strong>Authors: </strong>Maciej Krzysztof Zuziak, Roberto Pellungrini, Salvatore Rinzivillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01587">https://arxiv.org/abs/2509.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01587">https://arxiv.org/pdf/2509.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01587]] One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption(https://arxiv.org/abs/2509.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, explainability</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a widespread and well-adopted paradigm of decentralised learning that allows training one model from multiple sources without the need to transfer data between participating clients directly. Since its inception in 2015, it has been divided into numerous subfields that deal with application-specific issues, such as data heterogeneity or resource allocation. One such sub-field, Clustered Federated Learning (CFL), deals with the problem of clustering the population of clients into separate cohorts to deliver personalised models. Although a few remarkable works have been published in this domain, the problem remains largely unexplored, as its basic assumptions and settings differ slightly from those of standard FL. In this work, we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic algorithm that can automatically detect the earliest suitable moment for clustering. Our algorithm is based on computing the cosine distance between the gradients of the clients and a temperature measure that detects when the federated model starts to converge. We empirically evaluate our methodology by testing various one-shot clustering algorithms for over forty different tasks on five benchmark datasets. Our experiments showcase the good performance of our approach when used to perform CFL in an automated manner without the need to adjust hyperparameters. We also revisit the practical feasibility of CFL algorithms based on the gradients of the clients, providing firm evidence of the high efficiency of density-based clustering methods when used to differentiate between the loss surfaces of neural networks trained on different distributions. Moreover, by inspecting the feasibility of local explanations generated with the help of GradCAM, we can provide more insights into the relationship between personalisation and the explainability of local predictions.</li>
</ul>

<h3>Title: Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Einstein Rivas Pizarro, Wajiha Zaheer, Li Yang, Khalil El-Khatib, Glenn Harvel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01592">https://arxiv.org/abs/2509.01592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01592">https://arxiv.org/pdf/2509.01592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01592]] Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices(https://arxiv.org/abs/2509.01592)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Radiation Detection Systems (RDSs) play a vital role in ensuring public safety across various settings, from nuclear facilities to medical environments. However, these systems are increasingly vulnerable to cyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP floods, botnet attacks, privilege escalation, and distributed denial-of-service (DDoS) attacks. Such threats could compromise the integrity and reliability of radiation measurements, posing significant public health and safety risks. This paper presents a new synthetic radiation dataset and an Intrusion Detection System (IDS) tailored for resource-constrained environments, bringing Machine Learning (ML) predictive capabilities closer to the sensing edge layer of critical infrastructure. Leveraging TinyML techniques, the proposed IDS employs an optimized XGBoost model enhanced with pruning, quantization, feature selection, and sampling. These TinyML techniques significantly reduce the size of the model and computational demands, enabling real-time intrusion detection on low-resource devices while maintaining a reasonable balance between efficiency and accuracy.</li>
</ul>

<h3>Title: O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Chen, Junjie Wang, Lin Liu, Ruihang Chu, Xiaopeng Zhang, Qi Tian, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01596">https://arxiv.org/abs/2509.01596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01596">https://arxiv.org/pdf/2509.01596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01596]] O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing(https://arxiv.org/abs/2509.01596)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently advanced video editing, yet controllable editing remains challenging due to the need for precise manipulation of diverse object properties. Current methods require different control signal for diverse editing tasks, which complicates model design and demands significant training resources. To address this, we propose O-DisCo-Edit, a unified framework that incorporates a novel object distortion control (O-DisCo). This signal, based on random and adaptive noise, flexibly encapsulates a wide range of editing cues within a single representation. Paired with a "copy-form" preservation module for preserving non-edited regions, O-DisCo-Edit enables efficient, high-fidelity editing through an effective training paradigm. Extensive experiments and comprehensive human evaluations consistently demonstrate that O-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods across various video editing tasks. this https URL</li>
</ul>

<h3>Title: Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW</h3>
<ul>
<li><strong>Authors: </strong>Kaitlyn Webb, Prottay Protivash, John Durrell, Daniell Toth, Aleksandra Slavkoviƒá, Daniel Kifer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DS, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01597">https://arxiv.org/abs/2509.01597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01597">https://arxiv.org/pdf/2509.01597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01597]] Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW(https://arxiv.org/abs/2509.01597)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, interpretability</a></li>
<li><strong>Abstract: </strong>Confidentiality for business data is an understudied area of disclosure avoidance, where legacy methods struggle to provide acceptable results. Modern formal privacy techniques designed for person-level data do not provide suitable confidentiality/utility trade-offs due to the highly skewed nature of business data and because extreme outlier records are often important contributors to query answers. In this paper, inspired by Gaussian Differential Privacy, we propose a novel confidentiality framework for business data with a focus on interpretability for policy makers. We propose two query-answering mechanisms and analyze new challenges that arise when noisy query answers are converted into confidentiality-preserving microdata. We evaluate our mechanisms on confidential Quarterly Census of Employment and Wages (QCEW) microdata and a public substitute dataset.</li>
</ul>

<h3>Title: An Efficient Intrusion Detection System for Safeguarding Radiation Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Nathanael Coolidge, Jaime Gonz√°lez Sanz, Li Yang, Khalil El Khatib, Glenn Harvel, Nelson Agbemava, I Putu Susila, Mehmet Yavuz Yagci</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01599">https://arxiv.org/abs/2509.01599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01599">https://arxiv.org/pdf/2509.01599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01599]] An Efficient Intrusion Detection System for Safeguarding Radiation Detection Systems(https://arxiv.org/abs/2509.01599)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Radiation Detection Systems (RDSs) are used to measure and detect abnormal levels of radioactive material in the environment. These systems are used in many applications to mitigate threats posed by high levels of radioactive material. However, these systems lack protection against malicious external attacks to modify the data. The novelty of applying Intrusion Detection Systems (IDS) in RDSs is a crucial element in safeguarding these critical infrastructures. While IDSs are widely used in networking environments to safeguard against various attacks, their application in RDSs is novel. A common attack on RDSs is Denial of Service (DoS), where the attacker aims to overwhelm the system, causing malfunctioning RDSs. This paper proposes an efficient Machine Learning (ML)-based IDS to detect anomalies in radiation data, focusing on DoS attacks. This work explores the use of sampling methods to create a simulated DoS attack based on a real radiation dataset, followed by an evaluation of various ML algorithms, including Random Forest, Support Vector Machine (SVM), logistic regression, and Light Gradient-Boosting Machine (LightGBM), to detect DoS attacks on RDSs. LightGBM is emphasized for its superior accuracy and low computational resource consumption, making it particularly suitable for real-time intrusion detection. Additionally, model optimization and TinyML techniques, including feature selection, parallel execution, and random search methods, are used to improve the efficiency of the proposed IDS. Finally, an optimized and efficient LightGBM-based IDS is developed to achieve accurate intrusion detection for RDSs.</li>
</ul>

<h3>Title: TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization</h3>
<ul>
<li><strong>Authors: </strong>Pedram Fekri, Mehrdad Zadeh, Javad Dargahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01605">https://arxiv.org/abs/2509.01605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01605">https://arxiv.org/pdf/2509.01605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01605]] TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization(https://arxiv.org/abs/2509.01605)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, the emergence of multitask deep learning models has enhanced catheterization procedures by providing tactile and visual perception data through an end-to-end architec- ture. This information is derived from a segmentation and force estimation head, which localizes the catheter in X-ray images and estimates the applied pressure based on its deflection within the image. These stereo vision architectures incorporate a CNN- based encoder-decoder that captures the dependencies between X-ray images from two viewpoints, enabling simultaneous 3D force estimation and stereo segmentation of the catheter. With these tasks in mind, this work approaches the problem from a new perspective. We propose a novel encoder-decoder Vision Transformer model that processes two input X-ray images as separate sequences. Given sequences of X-ray patches from two perspectives, the transformer captures long-range dependencies without the need to gradually expand the receptive field for either image. The embeddings generated by both the encoder and decoder are fed into two shared segmentation heads, while a regression head employs the fused information from the decoder for 3D force estimation. The proposed model is a stereo Vision Transformer capable of simultaneously segmenting the catheter from two angles while estimating the generated forces at its tip in 3D. This model has undergone extensive experiments on synthetic X-ray images with various noise levels and has been compared against state-of-the-art pure segmentation models, vision-based catheter force estimation methods, and a multitask catheter segmentation and force estimation approach. It outperforms existing models, setting a new state-of-the-art in both catheter segmentation and force estimation.</li>
</ul>

<h3>Title: Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply</h3>
<ul>
<li><strong>Authors: </strong>Vivi Nastase, Paola Merlo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01606">https://arxiv.org/abs/2509.01606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01606">https://arxiv.org/pdf/2509.01606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01606]] Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply(https://arxiv.org/abs/2509.01606)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models learn to encode and decode an input text, and produce contextual token embeddings as a side-effect. The mapping from language into the embedding space maps words expressing similar concepts onto points that are close in the space. In practice, the reverse implication is also assumed: words corresponding to close points in this space are similar or related, those that are further are not. Does closeness in the embedding space extend to shared properties for sentence embeddings? We present an investigation of sentence embeddings and show that the geometry of their embedding space is not predictive of their relative performances on a variety of tasks. We compute sentence embeddings in three ways: as averaged token embeddings, as the embedding of the special [CLS] token, and as the embedding of a random token from the sentence. We explore whether there is a correlation between the distance between sentence embedding variations and their performance on linguistic tasks, and whether despite their distances, they do encode the same information in the same manner. The results show that the cosine similarity -- which treats dimensions shallowly -- captures (shallow) commonalities or differences between sentence embeddings, which are not predictive of their performance on specific tasks. Linguistic information is rather encoded in weighted combinations of different dimensions, which are not reflected in the geometry of the sentence embedding space.</li>
</ul>

<h3>Title: Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry</h3>
<ul>
<li><strong>Authors: </strong>Shanshan Wang, Junchao Wu, Fengying Ye, Jingming Yao, Lidia S. Chao, Derek F. Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01620">https://arxiv.org/abs/2509.01620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01620">https://arxiv.org/pdf/2509.01620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01620]] Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry(https://arxiv.org/abs/2509.01620)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of advanced large language models (LLMs) has made AI-generated text indistinguishable from human-written text. Previous work on detecting AI-generated text has made effective progress, but has not involved modern Chinese poetry. Due to the distinctive characteristics of modern Chinese poetry, it is difficult to identify whether a poem originated from humans or AI. The proliferation of AI-generated modern Chinese poetry has significantly disrupted the poetry ecosystem. Based on the urgency of identifying AI-generated poetry in the real Chinese world, this paper proposes a novel benchmark for detecting LLMs-generated modern Chinese poetry. We first construct a high-quality dataset, which includes both 800 poems written by six professional poets and 41,600 poems generated by four mainstream LLMs. Subsequently, we conduct systematic performance assessments of six detectors on this dataset. Experimental results demonstrate that current detectors cannot be used as reliable tools to detect modern Chinese poems generated by LLMs. The most difficult poetic features to detect are intrinsic qualities, especially style. The detection results verify the effectiveness and necessity of our proposed benchmark. Our work lays a foundation for future detection of AI-generated poetry.</li>
</ul>

<h3>Title: Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case</h3>
<ul>
<li><strong>Authors: </strong>Tim Schwabe, Moritz Lange, Laurenz Wiskott, Maribel Acosta</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01621">https://arxiv.org/abs/2509.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01621">https://arxiv.org/pdf/2509.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01621]] Effects of Distributional Biases on Gradient-Based Causal Discovery in the Bivariate Categorical Case(https://arxiv.org/abs/2509.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gradient-based causal discovery shows great potential for deducing causal structure from data in an efficient and scalable way. Those approaches however can be susceptible to distributional biases in the data they are trained on. We identify two such biases: Marginal Distribution Asymmetry, where differences in entropy skew causal learning toward certain factorizations, and Marginal Distribution Shift Asymmetry, where repeated interventions cause faster shifts in some variables than in others. For the bivariate categorical setup with Dirichlet priors, we illustrate how these biases can occur even in controlled synthetic data. To examine their impact on gradient-based methods, we employ two simple models that derive causal factorizations by learning marginal or conditional data distributions - a common strategy in gradient-based causal discovery. We demonstrate how these models can be susceptible to both biases. We additionally show how the biases can be controlled. An empirical evaluation of two related, existing approaches indicates that eliminating competition between possible causal factorizations can make models robust to the presented biases.</li>
</ul>

<h3>Title: Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Natalia Frumkin, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01624">https://arxiv.org/abs/2509.01624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01624">https://arxiv.org/pdf/2509.01624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01624]] Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling(https://arxiv.org/abs/2509.01624)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.</li>
</ul>

<h3>Title: Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP</h3>
<ul>
<li><strong>Authors: </strong>Bingheng Wang, Yichao Gao, Tianchen Sun, Lin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01630">https://arxiv.org/abs/2509.01630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01630">https://arxiv.org/pdf/2509.01630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01630]] Learning to Coordinate: Distributed Meta-Trajectory Optimization Via Differentiable ADMM-DDP(https://arxiv.org/abs/2509.01630)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distributed trajectory optimization via ADMM-DDP is a powerful approach for coordinating multi-agent systems, but it requires extensive tuning of tightly coupled hyperparameters that jointly govern local task performance and global coordination. In this paper, we propose Learning to Coordinate (L2C), a general framework that meta-learns these hyperparameters, modeled by lightweight agent-wise neural networks, to adapt across diverse tasks and agent configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in a distributed manner. It also enables efficient meta-gradient computation by reusing DDP components such as Riccati recursions and feedback gains. These gradients correspond to the optimal solutions of distributed matrix-valued LQR problems, coordinated across agents via an auxiliary ADMM framework that becomes convex under mild assumptions. Training is further accelerated by truncating iterations and meta-learning ADMM penalty parameters optimized for rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a challenging cooperative aerial transport task, L2C generates dynamically feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures quadrotor formations for safe 6-DoF load manipulation in tight spaces, and adapts robustly to varying team sizes and task conditions, while achieving up to $88\%$ faster gradient computation than state-of-the-art methods.</li>
</ul>

<h3>Title: Relative Trajectory Balance is equivalent to Trust-PCL</h3>
<ul>
<li><strong>Authors: </strong>Tristan Deleu, Padideh Nouri, Yoshua Bengio, Doina Precup</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01632">https://arxiv.org/abs/2509.01632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01632">https://arxiv.org/pdf/2509.01632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01632]] Relative Trajectory Balance is equivalent to Trust-PCL(https://arxiv.org/abs/2509.01632)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative modeling has highlighted the importance of Reinforcement Learning (RL) for fine-tuning, with KL-regularized methods in particular proving to be highly effective for both autoregressive and diffusion models. Complementing this line of work, the Relative Trajectory Balance (RTB) objective was recently introduced in the context of Generative Flow Networks (GFlowNets) to serve the same role of improving fine-tuning in sequential generative models. Building on prior work linking GFlowNets and maximum-entropy RL, we establish in this paper an equivalence between RTB and Trust-PCL, an off-policy RL method with KL regularization. This equivalence situates RTB within the broader theoretical landscape of KL-regularized RL, and clarifies its relationship to earlier methods. Leveraging this insight, we revisit an illustrative example from the RTB paper and show that KL-regularized RL methods achieve comparable performance, offering an alternative perspective to what was previously reported.</li>
</ul>

<h3>Title: TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Hind Aljuaid, Areej Alhothali, Ohoud Al-Zamzami, Hussein Assalahi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01640">https://arxiv.org/abs/2509.01640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01640">https://arxiv.org/pdf/2509.01640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01640]] TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring(https://arxiv.org/abs/2509.01640)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Essay writing is a critical component of student assessment, yet manual scoring is labor-intensive and inconsistent. Automated Essay Scoring (AES) offers a promising alternative, but current approaches face limitations. Recent studies have incorporated Graph Neural Networks (GNNs) into AES using static word embeddings that fail to capture contextual meaning, especially for polysemous words. Additionally, many methods rely on holistic scoring, overlooking specific writing aspects such as grammar, vocabulary, and cohesion. To address these challenges, this study proposes TransGAT, a novel approach that integrates fine-tuned Transformer models with GNNs for analytic scoring. TransGAT combines the contextual understanding of Transformers with the relational modeling strength of Graph Attention Networks (GAT). It performs two-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa, and DeBERTaV3) with a separate GAT. In each pair, the first stream generates essay-level predictions, while the second applies GAT to Transformer token embeddings, with edges constructed from syntactic dependencies. The model then fuses predictions from both streams to produce the final analytic score. Experiments on the ELLIPSE dataset show that TransGAT outperforms baseline models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all analytic scoring dimensions. These findings highlight the potential of TransGAT to advance AES systems.</li>
</ul>

<h3>Title: REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Maximilian P. Oppelt, Andreas Foltyn, Nadine R. Lang-Richter, Bjoern M. Eskofier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01642">https://arxiv.org/abs/2509.01642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01642">https://arxiv.org/pdf/2509.01642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01642]] REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization(https://arxiv.org/abs/2509.01642)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Task load detection is essential for optimizing human performance across diverse applications, yet current models often lack generalizability beyond narrow experimental domains. While prior research has focused on individual tasks and limited modalities, there remains a gap in evaluating model robustness and transferability in real-world scenarios. This paper addresses these limitations by introducing a new multimodal dataset that extends established cognitive load detection benchmarks with a real-world gaming application, using the $n$-back test as a scientific foundation. Task load annotations are derived from objective performance, subjective NASA-TLX ratings, and task-level design, enabling a comprehensive evaluation framework. State-of-the-art end-to-end model, including xLSTM, ConvNeXt, and Transformer architectures are systematically trained and evaluated on multiple modalities and application domains to assess their predictive performance and cross-domain generalization. Results demonstrate that multimodal approaches consistently outperform unimodal baselines, with specific modalities and model architectures showing varying impact depending on the application subset. Importantly, models trained on one domain exhibit reduced performance when transferred to novel applications, underscoring remaining challenges for universal cognitive load estimation. These findings provide robust baselines and actionable insights for developing more generalizable cognitive load detection systems, advancing both research and practical implementation in human-computer interaction and adaptive systems.</li>
</ul>

<h3>Title: OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanqing Liu, Xianhang Li, Letian Zhang, Zirui Wang, Zeyu Zheng, Yuyin Zhou, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01644">https://arxiv.org/abs/2509.01644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01644">https://arxiv.org/pdf/2509.01644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01644]] OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning(https://arxiv.org/abs/2509.01644)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.</li>
</ul>

<h3>Title: Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Sachin Goyal, David Lopez-Paz, Kartik Ahuja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01649">https://arxiv.org/abs/2509.01649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01649">https://arxiv.org/pdf/2509.01649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01649]] Distilled Pretraining: A modern lens of Data, In-Context Learning and Test-Time Scaling(https://arxiv.org/abs/2509.01649)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the past year, distillation has seen a renewed prominence in large language model (LLM) pretraining, exemplified by the Llama-3.2 and Gemma model families. While distillation has historically been shown to improve statistical modeling, its effects on new paradigms that are key to modern LLMs, such as test-time scaling and in-context learning, remain underexplored. In this work, we make three main contributions. First, we show that pretraining with distillation yields models that exhibit remarkably better test-time scaling. Second, we observe that this benefit comes with a trade-off: distillation impairs in-context learning capabilities, particularly the one modeled via induction heads. Third, to demystify these findings, we study distilled pretraining in a sandbox of a bigram model, which helps us isolate the common principal factor behind our observations. Finally, using these insights, we shed light on various design choices for pretraining that should help practitioners going forward.</li>
</ul>

<h3>Title: Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhi-Feng Wei, Wenqian Chen, Panos Stinis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01679">https://arxiv.org/abs/2509.01679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01679">https://arxiv.org/pdf/2509.01679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01679]] Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks(https://arxiv.org/abs/2509.01679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Operator learning has emerged as a promising tool for accelerating the solution of partial differential equations (PDEs). The Deep Operator Networks (DeepONets) represent a pioneering framework in this area: the "vanilla" DeepONet is valued for its simplicity and efficiency, while the modified DeepONet achieves higher accuracy at the cost of increased training time. In this work, we propose a series of Transformer-inspired DeepONet variants that introduce bidirectional cross-conditioning between the branch and trunk networks in DeepONet. Query-point information is injected into the branch network and input-function information into the trunk network, enabling dynamic dependencies while preserving the simplicity and efficiency of the "vanilla" DeepONet in a non-intrusive manner. Experiments on four PDE benchmarks -- advection, diffusion-reaction, Burgers', and Korteweg-de Vries equations -- show that for each case, there exists a variant that matches or surpasses the accuracy of the modified DeepONet while offering improved training efficiency. Moreover, the best-performing variant for each equation aligns naturally with the equation's underlying characteristics, suggesting that the effectiveness of cross-conditioning depends on the characteristics of the equation and its underlying physics. To ensure robustness, we validate the effectiveness of our variants through a range of rigorous statistical analyses, among them the Wilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.</li>
</ul>

<h3>Title: GaussianGAN: Real-Time Photorealistic controllable Human Avatars</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Ilyes Lakhal, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01681">https://arxiv.org/abs/2509.01681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01681">https://arxiv.org/pdf/2509.01681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01681]] GaussianGAN: Real-Time Photorealistic controllable Human Avatars(https://arxiv.org/abs/2509.01681)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Photorealistic and controllable human avatars have gained popularity in the research community thanks to rapid advances in neural rendering, providing fast and realistic synthesis tools. However, a limitation of current solutions is the presence of noticeable blurring. To solve this problem, we propose GaussianGAN, an animatable avatar approach developed for photorealistic rendering of people in real-time. We introduce a novel Gaussian splatting densification strategy to build Gaussian points from the surface of cylindrical structures around estimated skeletal limbs. Given the camera calibration, we render an accurate semantic segmentation with our novel view segmentation module. Finally, a UNet generator uses the rendered Gaussian splatting features and the segmentation maps to create photorealistic digital avatars. Our method runs in real-time with a rendering speed of 79 FPS. It outperforms previous methods regarding visual perception and quality, achieving a state-of-the-art results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and 33.39db on the Thuman4 dataset.</li>
</ul>

<h3>Title: Examination of PCA Utilisation for Multilabel Classifier of Multispectral Images</h3>
<ul>
<li><strong>Authors: </strong>Filip Karpowicz, Wiktor Kƒôpi≈Ñski, Bartosz Staszy≈Ñski, Grzegorz Sarwas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01691">https://arxiv.org/abs/2509.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01691">https://arxiv.org/pdf/2509.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01691]] Examination of PCA Utilisation for Multilabel Classifier of Multispectral Images(https://arxiv.org/abs/2509.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper investigates the utility of Principal Component Analysis (PCA) for multi-label classification of multispectral images using ResNet50 and DINOv2, acknowledging the high dimensionality of such data and the associated processing challenges. Multi-label classification, where each image may belong to multiple classes, adds further complexity to feature extraction. Our pipeline includes an optional PCA step that reduces the data to three dimensions before feeding it into a three-layer classifier. The findings demonstrate that the effectiveness of PCA for multi-label multispectral image classification depends strongly on the chosen deep learning architecture and training strategy, opening avenues for future research into self-supervised pre-training and alternative dimensionality reduction approaches.</li>
</ul>

<h3>Title: AmphiKey: A Dual-Mode Secure Authenticated Key Encapsulation Protocol for Smart Grid</h3>
<ul>
<li><strong>Authors: </strong>Kazi Hassan Shakib, Muhammad Asfand Hafeez, Arslan Munir</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01701">https://arxiv.org/abs/2509.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01701">https://arxiv.org/pdf/2509.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01701]] AmphiKey: A Dual-Mode Secure Authenticated Key Encapsulation Protocol for Smart Grid(https://arxiv.org/abs/2509.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>AmphiKey, a dual-mode post-quantum/traditional (PQ/T) hybrid authenticated key exchange mechanism (AKEM) has been designed to secure smart grid communications against both classical and quantum threats. AmphiKey offers two distinct operational modes within a single framework: an Authenticated Mode and a Deniable Mode. The Authenticated Mode employs a blackbox approach, combining ephemeral ML-KEM-768 and X25519 with long-term Raccoon DSA keys to provide forward secrecy and strong, non-repudiable authenticity. This design achieves "OR" confidentiality, where security holds if either of the KEMs is unbroken, and robust "AND" authenticity. For the signature operation, it leverages the 'masking-friendly' Raccoon digital signature (DSA), which is specifically designed for side-channel attack resistance, though this protection is localized to the signing key and does not provide deniability. In contrast, Deniable Mode provides deniable authentication, preserving privacy. The protocol used ML-KEM-768 (AKEM-1), Ephemeral X25519 (AKEM-2), Raccoon-based DSA (Rac) (compared performance to ML-DSA-65), and the Ascon cipher to deliver its security guarantees. Key contributions include providing a flexible protocol with enhanced security, optional deniability, and efficiency adapted to the diverse needs of the smart grid infrastructure. We present a comprehensive performance evaluation on a heterogeneous testbed featuring a powerful server and client (AMD Ryzen 5) and a resource-constrained client (Raspberry Pi). In efficient Deniable mode, the full handshake completes in 0.15 ms on the server and 0.41 ms on the Raspberry Pi client. In contrast, the Authenticated Mode is bottlenecked by the client-side signature generation; the handshake takes 4.8 ms for the Raspberry Pi client to initiate and 0.84 ms for the server to verify.</li>
</ul>

<h3>Title: Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt</h3>
<ul>
<li><strong>Authors: </strong>Anthony Amankwah, Chris Aldrich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01704">https://arxiv.org/abs/2509.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01704">https://arxiv.org/pdf/2509.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01704]] Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt(https://arxiv.org/abs/2509.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate classification of rock sizes is a vital component in geotechnical engineering, mining, and resource management, where precise estimation influences operational efficiency and safety. In this paper, we propose an enhanced deep learning model based on the ConvNeXt architecture, augmented with both self-attention and channel attention mechanisms. Building upon the foundation of ConvNext, our proposed model, termed CNSCA, introduces self-attention to capture long-range spatial dependencies and channel attention to emphasize informative feature channels. This hybrid design enables the model to effectively capture both fine-grained local patterns and broader contextual relationships within rock imagery, leading to improved classification accuracy and robustness. We evaluate our model on a rock size classification dataset and compare it against three strong baseline. The results demonstrate that the incorporation of attention mechanisms significantly enhances the models capability for fine-grained classification tasks involving natural textures like rocks.</li>
</ul>

<h3>Title: Designing a Layered Framework to Secure Data via Improved Multi Stage Lightweight Cryptography in IoT Cloud Systems</h3>
<ul>
<li><strong>Authors: </strong>Hojjat Farshadinia, Ali Barati, Hamid Barati</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01717">https://arxiv.org/abs/2509.01717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01717">https://arxiv.org/pdf/2509.01717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01717]] Designing a Layered Framework to Secure Data via Improved Multi Stage Lightweight Cryptography in IoT Cloud Systems(https://arxiv.org/abs/2509.01717)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel multi-layered hybrid security approach aimed at enhancing lightweight encryption for IoT-Cloud systems. The primary goal is to overcome limitations inherent in conventional solutions such as TPA, Blockchain, ECDSA and ZSS which often fall short in terms of data protection, computational efficiency and scalability. Our proposed method strategically refines and integrates these technologies to address their shortcomings while maximizing their individual strengths. By doing so we create a more reliable and high-performance framework for secure data exchange across heterogeneous environments. The model leverages the combined potential of emerging technologies, particularly Blockchain, IoT and Cloud computing which when effectively coordinated offer significant advancements in security architecture. The proposed framework consists of three core layers: (1) the this http URL Layer which integrates improved versions of Hyperledger Fabric, Enc-Block and a hybrid ECDSA-ZSS scheme to improve encryption speed, scalability and reduce computational cost; (2) the Credential Management Layer independently verifying data integrity and authenticity; and (3) the Time and Auditing Layer designed to reduce traffic overhead and optimize performance across dynamic workloads. Evaluation results highlight that the proposed solution not only strengthens security but also significantly improves execution time, communication efficiency and system responsiveness, offering a robust path forward for next-generation IoT-Cloud infrastructures.</li>
</ul>

<h3>Title: Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection</h3>
<ul>
<li><strong>Authors: </strong>Sara Khan, Mehmed Y√ºksel, Frank Kirchner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01719">https://arxiv.org/abs/2509.01719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01719">https://arxiv.org/pdf/2509.01719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01719]] Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection(https://arxiv.org/abs/2509.01719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wear and tear detection in fleet and shared vehicle systems is a critical challenge, particularly in rental and car-sharing services, where minor damage, such as dents, scratches, and underbody impacts, often goes unnoticed or is detected too late. Currently, manual inspection methods are the default approach but are labour intensive and prone to human error. In contrast, state-of-the-art image-based methods struggle with real-time performance and are less effective at detecting underbody damage due to limited visual access and poor spatial coverage. This work introduces a novel multi-modal architecture based on anomaly detection to address these issues. Sensors such as IMUs and microphones are integrated into a compact device mounted on the vehicle's windshield. This approach supports real-time damage detection while avoiding the need for highly resource-intensive sensors. We developed multiple variants of multi-modal autoencoder-based architectures and evaluated them against unimodal and state-of-the-art methods. Our ensemble pooling multi-modal model achieved the highest performance, with a Receiver Operating Characteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its effectiveness in real-world applications. This approach can also be extended to other applications, such as improving automotive safety - where it can integrate with airbag systems for efficient deployment - and helping autonomous vehicles by complementing other sensors in collision detection.</li>
</ul>

<h3>Title: BM-CL: Bias Mitigation through the lens of Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Lucas Mansilla, Rodrigo Echeveste, Camila Gonzalez, Diego H. Milone, Enzo Ferrante</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01730">https://arxiv.org/abs/2509.01730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01730">https://arxiv.org/pdf/2509.01730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01730]] BM-CL: Bias Mitigation through the lens of Continual Learning(https://arxiv.org/abs/2509.01730)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Biases in machine learning pose significant challenges, particularly when models amplify disparities that affect disadvantaged groups. Traditional bias mitigation techniques often lead to a {\itshape leveling-down effect}, whereby improving outcomes of disadvantaged groups comes at the expense of reduced performance for advantaged groups. This study introduces Bias Mitigation through Continual Learning (BM-CL), a novel framework that leverages the principles of continual learning to address this trade-off. We postulate that mitigating bias is conceptually similar to domain-incremental continual learning, where the model must adjust to changing fairness conditions, improving outcomes for disadvantaged groups without forgetting the knowledge that benefits advantaged groups. Drawing inspiration from techniques such as Learning without Forgetting and Elastic Weight Consolidation, we reinterpret bias mitigation as a continual learning problem. This perspective allows models to incrementally balance fairness objectives, enhancing outcomes for disadvantaged groups while preserving performance for advantaged groups. Experiments on synthetic and real-world image datasets, characterized by diverse sources of bias, demonstrate that the proposed framework mitigates biases while minimizing the loss of original knowledge. Our approach bridges the fields of fairness and continual learning, offering a promising pathway for developing machine learning systems that are both equitable and effective.</li>
</ul>

<h3>Title: Are Enterprises Ready for Quantum-Safe Cybersecurity?</h3>
<ul>
<li><strong>Authors: </strong>Tran Duc Le, Phuc Hao Do, Truong Duy Dinh, Van Dai Pham</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01731">https://arxiv.org/abs/2509.01731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01731">https://arxiv.org/pdf/2509.01731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01731]] Are Enterprises Ready for Quantum-Safe Cybersecurity?(https://arxiv.org/abs/2509.01731)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Quantum computing threatens to undermine classical cryptography by breaking widely deployed encryption and signature schemes. This paper examines enterprise readiness for quantum-safe cybersecurity through three perspectives: (i) the technologist view, assessing the maturity of post-quantum cryptography (PQC) and quantum key distribution (QKD); (ii) the enterprise (CISO/CIO) view, analyzing organizational awareness, risk management, and operational barriers; and (iii) the threat actor view, evaluating the evolving quantum threat and the urgency of migration. Using recent standards (e.g., NIST's 2024 PQC algorithms), industry surveys, and threat intelligence, we synthesize findings via a SWOT analysis to map strengths, weaknesses, opportunities, and threats. Results indicate uneven and generally insufficient preparedness: while PQC standards and niche QKD deployments signal technical progress, fewer than 5\% of enterprises have formal quantum-transition plans, and many underestimate "harvest now, decrypt later" risks. Financial, telecom, and government sectors have begun migration, but most industries remain exploratory or stalled by costs, complexity, and skills gaps. Expert consensus places cryptanalytically relevant quantum computers in the 2030s, yet delayed preparation could leave today's data vulnerable for decades. We recommend immediate steps: establishing crypto-agility, creating quantum transition roadmaps, prioritizing PQC deployment in high-value systems, and upskilling cybersecurity teams. A coordinated, proactive approach is essential to secure current and future digital assets in the quantum era.</li>
</ul>

<h3>Title: BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators</h3>
<ul>
<li><strong>Authors: </strong>Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01742">https://arxiv.org/abs/2509.01742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01742">https://arxiv.org/pdf/2509.01742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01742]] BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators(https://arxiv.org/abs/2509.01742)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that eavesdropping on HBM is difficult -- even for physical attackers -- as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI'20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits. Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fast OMAP accelerator that, for the first time, achieves O(1) + O((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in initialization and query time, respectively, over state-of-the-art OMAPs, including an industry implementation from Facebook.</li>
</ul>

<h3>Title: Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Xinlu Zhang, Na Yan, Yang Su, Yansha Deng, Toktam Mahmoodi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01750">https://arxiv.org/abs/2509.01750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01750">https://arxiv.org/pdf/2509.01750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01750]] Communication-Aware Knowledge Distillation for Federated LLM Fine-Tuning over Wireless Networks(https://arxiv.org/abs/2509.01750)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) for large language models (LLMs) offers a privacy-preserving scheme, enabling clients to collaboratively fine-tune locally deployed LLMs or smaller language models (SLMs) without exchanging raw data. While parameter-sharing methods in traditional FL models solves number of technical challenges, they still incur high communication overhead and struggle with adapting to heterogeneous model architectures. Federated distillation, a framework for mutual knowledge transfer via shared logits, typically offers lower communication overhead than parameter-sharing methods. However, transmitting logits from LLMs remains challenging for bandwidth-limited clients due to their high dimensionality. In this work, we focus on a federated LLM distillation with efficient communication overhead. To achieve this, we first propose an adaptive Top-k logit selection mechanism, dynamically sparsifying logits according to real-time communication conditions. Then to tackle the dimensional inconsistency introduced by the adaptive sparsification, we design an adaptive logits aggregation scheme, effectively alleviating the artificial and uninformative inputs introduced by conventional zero-padding methods. Finally, to enhance the distillation effect, we incorporate LoRA-adapted hidden-layer projection from LLM into the distillation loss, reducing the communication overhead further while providing richer representation. Experimental results demonstrate that our scheme achieves superior performance compared to baseline methods while effectively reducing communication overhead by approximately 50%.</li>
</ul>

<h3>Title: Clinical Metadata Guided Limited-Angle CT Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yu Shi, Shuyi Fan, Changsheng Fang, Shuo Han, Haodong Li, Li Zhou, Bahareh Morovati, Dayang Wang, Hengyong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01752">https://arxiv.org/abs/2509.01752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01752">https://arxiv.org/pdf/2509.01752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01752]] Clinical Metadata Guided Limited-Angle CT Image Reconstruction(https://arxiv.org/abs/2509.01752)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free, transformer</a></li>
<li><strong>Abstract: </strong>Limited-angle computed tomography (LACT) offers improved temporal resolution and reduced radiation dose for cardiac imaging, but suffers from severe artifacts due to truncated projections. To address the ill-posedness of LACT reconstruction, we propose a two-stage diffusion framework guided by structured clinical metadata. In the first stage, a transformer-based diffusion model conditioned exclusively on metadata, including acquisition parameters, patient demographics, and diagnostic impressions, generates coarse anatomical priors from noise. The second stage further refines the images by integrating both the coarse prior and metadata to produce high-fidelity results. Physics-based data consistency is enforced at each sampling step in both stages using an Alternating Direction Method of Multipliers module, ensuring alignment with the measured projections. Extensive experiments on both synthetic and real cardiac CT datasets demonstrate that incorporating metadata significantly improves reconstruction fidelity, particularly under severe angular truncation. Compared to existing metadata-free baselines, our method achieves superior performance in SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of metadata contribute complementary benefits, particularly diagnostic and demographic priors under limited-angle conditions. These findings highlight the dual role of clinical metadata in improving both reconstruction quality and efficiency, supporting their integration into future metadata-guided medical imaging frameworks.</li>
</ul>

<h3>Title: TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Asghari Ilani, Yaser Mike Banad</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01754">https://arxiv.org/abs/2509.01754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01754">https://arxiv.org/pdf/2509.01754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01754]] TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing(https://arxiv.org/abs/2509.01754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks to the structural integrity of additively manufactured components. This paper introduces TransMatch, a novel framework that merges transfer learning and semi-supervised few-shot learning to address the scarcity of labeled AM defect data. By effectively leveraging both labeled and unlabeled novel-class images, TransMatch circumvents the limitations of previous meta-learning approaches. Experimental evaluations on a Surface Defects dataset of 8,284 images demonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimal loss, alongside high precision, recall, and F1-scores for multiple defect classes. These findings underscore its robustness in accurately identifying diverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thus represents a significant leap forward in additive manufacturing defect detection, offering a practical and scalable solution for quality assurance and reliability across a wide range of industrial applications.</li>
</ul>

<h3>Title: chDzDT: Word-level morphology-aware language model for Algerian social media text</h3>
<ul>
<li><strong>Authors: </strong>Abdelkrime Aries</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01772">https://arxiv.org/abs/2509.01772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01772">https://arxiv.org/pdf/2509.01772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01772]] chDzDT: Word-level morphology-aware language model for Algerian social media text(https://arxiv.org/abs/2509.01772)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have substantially advanced natural language processing by providing context-sensitive text representations. However, the Algerian dialect remains under-represented, with few dedicated models available. Processing this dialect is challenging due to its complex morphology, frequent code-switching, multiple scripts, and strong lexical influences from other languages. These characteristics complicate tokenization and reduce the effectiveness of conventional word- or subword-level approaches. To address this gap, we introduce chDzDT, a character-level pre-trained language model tailored for Algerian morphology. Unlike conventional PLMs that rely on token sequences, chDzDT is trained on isolated words. This design allows the model to encode morphological patterns robustly, without depending on token boundaries or standardized orthography. The training corpus draws from diverse sources, including YouTube comments, French, English, and Berber Wikipedia, as well as the Tatoeba project. It covers multiple scripts and linguistic varieties, resulting in a substantial pre-training workload. Our contributions are threefold: (i) a detailed morphological analysis of Algerian dialect using YouTube comments; (ii) the construction of a multilingual Algerian lexicon dataset; and (iii) the development and extensive evaluation of a character-level PLM as a morphology-focused encoder for downstream tasks. The proposed approach demonstrates the potential of character-level modeling for morphologically rich, low-resource dialects and lays a foundation for more inclusive and adaptable NLP systems.</li>
</ul>

<h3>Title: Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs</h3>
<ul>
<li><strong>Authors: </strong>Andong Hua, Kenan Tang, Chenhe Gu, Jindong Gu, Eric Wong, Yao Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01790">https://arxiv.org/abs/2509.01790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01790">https://arxiv.org/pdf/2509.01790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01790]] Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs(https://arxiv.org/abs/2509.01790)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.</li>
</ul>

<h3>Title: E-PhishGen: Unlocking Novel Research in Phishing Email Detection</h3>
<ul>
<li><strong>Authors: </strong>Luca Pajola, Eugenio Caripoti, Simeone Pizzi, Mauro Conti, Stefan Banzer, Giovanni Apruzzese</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01791">https://arxiv.org/abs/2509.01791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01791">https://arxiv.org/pdf/2509.01791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01791]] E-PhishGen: Unlocking Novel Research in Phishing Email Detection(https://arxiv.org/abs/2509.01791)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Every day, our inboxes are flooded with unsolicited emails, ranging between annoying spam to more subtle phishing scams. Unfortunately, despite abundant prior efforts proposing solutions achieving near-perfect accuracy, the reality is that countering malicious emails still remains an unsolved dilemma. This "open problem" paper carries out a critical assessment of scientific works in the context of phishing email detection. First, we focus on the benchmark datasets that have been used to assess the methods proposed in research. We find that most prior work relied on datasets containing emails that -- we argue -- are not representative of current trends, and mostly encompass the English language. Based on this finding, we then re-implement and re-assess a variety of detection methods reliant on machine learning (ML), including large-language models (LLM), and release all of our codebase -- an (unfortunately) uncommon practice in related research. We show that most such methods achieve near-perfect performance when trained and tested on the same dataset -- a result which intrinsically hinders development (how can future research outperform methods that are already near perfect?). To foster the creation of "more challenging benchmarks" that reflect current phishing trends, we propose E-PhishGEN, an LLM-based (and privacy-savvy) framework to generate novel phishing-email datasets. We use our E-PhishGEN to create E-PhishLLM, a novel phishing-email detection dataset containing 16616 emails in three languages. We use E-PhishLLM to test the detectors we considered, showing a much lower performance than that achieved on existing benchmarks -- indicating a larger room for improvement. We also validate the quality of E-PhishLLM with a user study (n=30). To sum up, we show that phishing email detection is still an open problem -- and provide the means to tackle such a problem by future research.</li>
</ul>

<h3>Title: Toward a Unified Benchmark and Taxonomy of Stochastic Environments</h3>
<ul>
<li><strong>Authors: </strong>Aryan Amit Barsainyan, Jing Yu Lim, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01793">https://arxiv.org/abs/2509.01793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01793">https://arxiv.org/pdf/2509.01793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01793]] Toward a Unified Benchmark and Taxonomy of Stochastic Environments(https://arxiv.org/abs/2509.01793)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) agents have achieved strong results on benchmarks such as Atari100k, yet they remain limited in robustness to real-world conditions. Model-Based RL approaches that rely on learned World Models often struggle in environments with true stochasticity and partial observability, despite their theoretical grounding in POMDPs. Current benchmarks rarely capture these challenges, focusing instead on deterministic or overly simplified settings, and the lack of a clear taxonomy of stochasticity further hampers systematic evaluation. To address this gap, we introduce STORI (STOchastic-ataRI), a benchmark that incorporates diverse stochastic effects and enables rigorous assessment of RL methods under varied forms of uncertainty. In addition, we propose a taxonomy of stochasticity in RL environments, providing a unified framework for analyzing and comparing approaches.</li>
</ul>

<h3>Title: A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics</h3>
<ul>
<li><strong>Authors: </strong>Trusting Inekwe, Emmanuel Agu, Winnie Mkandawire, Andres Colubri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01794">https://arxiv.org/abs/2509.01794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01794">https://arxiv.org/pdf/2509.01794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01794]] A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics(https://arxiv.org/abs/2509.01794)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.</li>
</ul>

<h3>Title: Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts</h3>
<ul>
<li><strong>Authors: </strong>Shreyas Tirumala, Nishant Jain, Danny D. Leybzon, Trent D. Buskirk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01814">https://arxiv.org/abs/2509.01814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01814">https://arxiv.org/pdf/2509.01814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01814]] Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts(https://arxiv.org/abs/2509.01814)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models (LLMs) have paved the way for "AI interviewers" that can administer voice-based surveys with respondents in real-time. This position paper reviews emerging evidence to understand when such AI interviewing systems are fit for purpose for collecting data within quantitative and qualitative research contexts. We evaluate the capabilities of AI interviewers as well as current Interactive Voice Response (IVR) systems across two dimensions: input/output performance (i.e., speech recognition, answer recording, emotion handling) and verbal reasoning (i.e., ability to probe, clarify, and handle branching logic). Field studies suggest that AI interviewers already exceed IVR capabilities for both quantitative and qualitative data collection, but real-time transcription error rates, limited emotion detection abilities, and uneven follow-up quality indicate that the utility, use and adoption of current AI interviewer technology may be context-dependent for qualitative data collection efforts.</li>
</ul>

<h3>Title: When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference</h3>
<ul>
<li><strong>Authors: </strong>Wen Ye, Jinbo Liu, Defu Cao, Wei Yang, Yan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01822">https://arxiv.org/abs/2509.01822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01822">https://arxiv.org/pdf/2509.01822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01822]] When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference(https://arxiv.org/abs/2509.01822)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has sparked growing interest in their application to time series analysis tasks. However, their ability to perform complex reasoning over temporal data in real-world application domains remains underexplored. To move toward this goal, a first step is to establish a rigorous benchmark dataset for evaluation. In this work, we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as time-series AI assistants. To ensure both scientific rigor and practical relevance, we surveyed over 20 academic publications and identified 33 real-world task formulations. The benchmark encompasses a broad spectrum of challenges, ranging from constraint-aware forecasting to anomaly detection with threshold calibration: tasks that require compositional reasoning and multi-step time series analysis. The question generator is designed to be dynamic and extensible, supporting continuous expansion as new datasets or task types are introduced. Given the heterogeneous nature of the tasks, we adopt task-specific success criteria and tailored inference-quality metrics to ensure meaningful evaluation for each task. We apply this benchmark to assess eight state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals limitations in current models' ability to assemble complex time series analysis workflows, underscoring the need for specialized methodologies for domain-specific adaptation. Our benchmark is available at this https URL, and the code is available at this https URL.</li>
</ul>

<h3>Title: From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs</h3>
<ul>
<li><strong>Authors: </strong>Saad Ullah, Praneeth Balasubramanian, Wenbo Guo, Amanda Burnett, Hammond Pearce, Christopher Kruegel, Giovanni Vigna, Gianluca Stringhini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01835">https://arxiv.org/abs/2509.01835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01835">https://arxiv.org/pdf/2509.01835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01835]] From CVE Entries to Verifiable Exploits: An Automated Multi-Agent Framework for Reproducing CVEs(https://arxiv.org/abs/2509.01835)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>High-quality datasets of real-world vulnerabilities and their corresponding verifiable exploits are crucial resources in software security research. Yet such resources remain scarce, as their creation demands intensive manual effort and deep security expertise. In this paper, we present CVE-GENIE, an automated, large language model (LLM)-based multi-agent framework designed to reproduce real-world vulnerabilities, provided in Common Vulnerabilities and Exposures (CVE) format, to enable creation of high-quality vulnerability datasets. Given a CVE entry as input, CVE-GENIE gathers the relevant resources of the CVE, automatically reconstructs the vulnerable environment, and (re)produces a verifiable exploit. Our systematic evaluation highlights the efficiency and robustness of CVE-GENIE's design and successfully reproduces approximately 51% (428 of 841) CVEs published in 2024-2025, complete with their verifiable exploits, at an average cost of $2.77 per CVE. Our pipeline offers a robust method to generate reproducible CVE benchmarks, valuable for diverse applications such as fuzzer evaluation, vulnerability patching, and assessing AI's security capabilities.</li>
</ul>

<h3>Title: PractiLight: Practical Light Control Using Foundational Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yotam Erel, Rishabh Dabral, Vladislav Golyanik, Amit H. Bermano, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01837">https://arxiv.org/abs/2509.01837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01837">https://arxiv.org/pdf/2509.01837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01837]] PractiLight: Practical Light Control Using Foundational Diffusion Models(https://arxiv.org/abs/2509.01837)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Light control in generated images is a difficult task, posing specific challenges, spanning over the entire image and frequency spectrum. Most approaches tackle this problem by training on extensive yet domain-specific datasets, limiting the inherent generalization and applicability of the foundational backbones used. Instead, PractiLight is a practical approach, effectively leveraging foundational understanding of recent generative models for the task. Our key insight is that lighting relationships in an image are similar in nature to token interaction in self-attention layers, and hence are best represented there. Based on this and other analyses regarding the importance of early diffusion iterations, PractiLight trains a lightweight LoRA regressor to produce the direct irradiance map for a given image, using a small set of training images. We then employ this regressor to incorporate the desired lighting into the generation process of another image using Classifier Guidance. This careful design generalizes well to diverse conditions and image domains. We demonstrate state-of-the-art performance in terms of quality and control with proven parameter and data efficiency compared to leading works over a wide variety of scenes types. We hope this work affirms that image lighting can feasibly be controlled by tapping into foundational knowledge, enabling practical and general relighting.</li>
</ul>

<h3>Title: Optimizing In-Context Learning for Efficient Full Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Weicao Deng, Sangwoo Park, Min Li, Osvaldo Simeone</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01840">https://arxiv.org/abs/2509.01840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01840">https://arxiv.org/pdf/2509.01840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01840]] Optimizing In-Context Learning for Efficient Full Conformal Prediction(https://arxiv.org/abs/2509.01840)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reliable uncertainty quantification is critical for trustworthy AI. Conformal Prediction (CP) provides prediction sets with distribution-free coverage guarantees, but its two main variants face complementary limitations. Split CP (SCP) suffers from data inefficiency due to dataset partitioning, while full CP (FCP) improves data efficiency at the cost of prohibitive retraining complexity. Recent approaches based on meta-learning or in-context learning (ICL) partially mitigate these drawbacks. However, they rely on training procedures not specifically tailored to CP, which may yield large prediction sets. We introduce an efficient FCP framework, termed enhanced ICL-based FCP (E-ICL+FCP), which employs a permutation-invariant Transformer-based ICL model trained with a CP-aware loss. By simulating the multiple retrained models required by FCP without actual retraining, E-ICL+FCP preserves coverage while markedly reducing both inefficiency and computational overhead. Experiments on synthetic and real tasks demonstrate that E-ICL+FCP attains superior efficiency-coverage trade-offs compared to existing SCP and FCP baselines.</li>
</ul>

<h3>Title: GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping</h3>
<ul>
<li><strong>Authors: </strong>Qifu Wen, Xi Zeng, Zihan Zhou, Shuaijun Liu, Mehdi Hosseinzadeh, Reza Rawassizadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01842">https://arxiv.org/abs/2509.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01842">https://arxiv.org/pdf/2509.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01842]] GradES: Significantly Faster Training in Transformers with Gradient-Based Early Stopping(https://arxiv.org/abs/2509.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Early stopping monitors global validation loss and halts all parameter updates simultaneously, which is computationally costly for large transformers due to the extended time required for validation inference. We propose GradES, a novel gradient-based early stopping approach that operates within transformer components (attention projections and Feed-Forward layer matrices). We found that different components converge at varying rates during fine-tuning. GradES tracks the magnitude of gradients in backpropagation for these matrices during training. When a projection matrix's gradients fall below a convergence threshold $\tau$, we exclude that projection matrix from further updates individually, eliminating costly validation passes while allowing slow converging matrices to continue learning. By strategically freezing parameters when their gradients converge, GradES speeds up training time by 1.57--7.22$\times$ while simultaneously enhancing generalization through early prevention of overfitting, resulting in 1.2% higher average accuracy.</li>
</ul>

<h3>Title: Latent Gene Diffusion for Spatial Transcriptomics Completion</h3>
<ul>
<li><strong>Authors: </strong>Paula C√°rdenas, Leonardo Manrique, Daniela Vega, Daniela Ruiz, Pablo Arbel√°ez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01864">https://arxiv.org/abs/2509.01864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01864">https://arxiv.org/pdf/2509.01864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01864]] Latent Gene Diffusion for Spatial Transcriptomics Completion(https://arxiv.org/abs/2509.01864)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Computer Vision has proven to be a powerful tool for analyzing Spatial Transcriptomics (ST) data. However, current models that predict spatially resolved gene expression from histopathology images suffer from significant limitations due to data dropout. Most existing approaches rely on single-cell RNA sequencing references, making them dependent on alignment quality and external datasets while also risking batch effects and inherited dropout. In this paper, we address these limitations by introducing LGDiST, the first reference-free latent gene diffusion model for ST data dropout. We show that LGDiST outperforms the previous state-of-the-art in gene expression completion, with an average Mean Squared Error that is 18% lower across 26 datasets. Furthermore, we demonstrate that completing ST data with LGDiST improves gene expression prediction performance on six state-of-the-art methods up to 10% in MSE. A key innovation of LGDiST is using context genes previously considered uninformative to build a rich and biologically meaningful genetic latent space. Our experiments show that removing key components of LGDiST, such as the context genes, the ST latent space, and the neighbor conditioning, leads to considerable drops in performance. These findings underscore that the full architecture of LGDiST achieves substantially better performance than any of its isolated components.</li>
</ul>

<h3>Title: Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Komala Subramanyam Cherukuri, Kewei Sha, Zhenhua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01868">https://arxiv.org/abs/2509.01868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01868">https://arxiv.org/pdf/2509.01868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01868]] Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation(https://arxiv.org/abs/2509.01868)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Object detection is crucial for Connected Autonomous Vehicles (CAVs) to perceive their surroundings and make safe driving decisions. Centralized training of object detection models often achieves promising accuracy, fast convergence, and simplified training process, but it falls short in scalability, adaptability, and privacy-preservation. Federated learning (FL), by contrast, enables collaborative, privacy-preserving, and continuous training across naturally distributed CAV fleets. However, deploying FL in real-world CAVs remains challenging due to the substantial computational demands of training and inference, coupled with highly diverse operating conditions. Practical deployment must address three critical factors: (i) heterogeneity from non-IID data distributions, (ii) constrained onboard computing hardware, and (iii) environmental variability such as lighting and weather, alongside systematic evaluation to ensure reliable performance. This work introduces the first holistic deployment-oriented evaluation of FL-based object detection in CAVs, integrating model performance, system-level resource profiling, and environmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8, YOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes datasets, we analyze trade-offs between detection accuracy, computational cost, and resource usage under diverse resolutions, batch sizes, weather and lighting conditions, and dynamic client participation, paving the way for robust FL deployment in CAVs.</li>
</ul>

<h3>Title: Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01873">https://arxiv.org/abs/2509.01873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01873">https://arxiv.org/pdf/2509.01873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01873]] Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction(https://arxiv.org/abs/2509.01873)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern deep learning developments create new opportunities for 3D mapping technology, scene reconstruction pipelines, and virtual reality development. Despite advances in 3D deep learning technology, direct training of deep learning models on 3D data faces challenges due to the high dimensionality inherent in 3D data and the scarcity of labeled datasets. Structure-from-motion (SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust performance when applied to structured indoor environments but often struggle with ambiguous features in unstructured environments. These techniques often struggle to generate detailed geometric representations effective for downstream tasks such as rendering and semantic analysis. Current limitations require the development of 3D representation methods that combine traditional geometric techniques with deep learning capabilities to generate robust geometry-aware deep learning models. The dissertation provides solutions to the fundamental challenges in 3D vision by developing geometric deep learning methods tailored for essential tasks such as camera pose estimation, point cloud registration, depth prediction, and 3D reconstruction. The integration of geometric priors or constraints, such as including depth information, surface normals, and equivariance into deep learning models, enhances both the accuracy and robustness of geometric representations. This study systematically investigates key components of 3D vision, including camera pose estimation, point cloud registration, depth estimation, and high-fidelity 3D reconstruction, demonstrating their effectiveness across real-world applications such as digital cultural heritage preservation and immersive VR/AR environments.</li>
</ul>

<h3>Title: Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function</h3>
<ul>
<li><strong>Authors: </strong>Jason Abohwo, Thomas Mosen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01874">https://arxiv.org/abs/2509.01874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01874">https://arxiv.org/pdf/2509.01874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01874]] Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function(https://arxiv.org/abs/2509.01874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding the inner workings of machine learning models is critical for ensuring their reliability and robustness. Whilst many techniques in mechanistic interpretability focus on activation driven analyses, being able to derive meaningful features directly from the weights of a neural network would provide greater guarantees and more computational efficiency. Existing techniques for analyzing model features through weights suffer from drawbacks such as reduced performance and data inefficiency. In this paper, we introduce Signed Quadratic Shrink (SQS), an activation function designed to allow Gated Linear Units (GLUs) to learn interpretable features without these drawbacks. Our experimental results show that SQS achieves performance competitive with state-of-the-art activation functions whilst enabling weight-based interpretability</li>
</ul>

<h3>Title: HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Shubham Laxmikant Deshmukh, Matthew Wilchek, Feras A. Batarseh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01882">https://arxiv.org/abs/2509.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01882">https://arxiv.org/pdf/2509.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01882]] HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision(https://arxiv.org/abs/2509.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Ongoing advancements in computer vision, particularly in pattern recognition and scene classification, have enabled new applications in environmental monitoring. Deep learning now offers non-contact methods for assessing water quality and detecting contamination, both critical for disaster response and public health protection. This work introduces HydroVision, a deep learning-based scene classification framework that estimates optically active water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and Turbidity from standard Red-Green-Blue (RGB) images of surface water. HydroVision supports early detection of contamination trends and strengthens monitoring by regulatory agencies during external environmental stressors, industrial activities, and force majeure events. The model is trained on more than 500,000 seasonally varied images collected from the United States Geological Survey Hydrologic Imagery Visualization and Information System between 2022 and 2024. This approach leverages widely available RGB imagery as a scalable, cost-effective alternative to traditional multispectral and hyperspectral remote sensing. Four state-of-the-art convolutional neural networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer are evaluated through transfer learning to identify the best-performing architecture. DenseNet121 achieves the highest validation performance, with an R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for real-world water quality monitoring across diverse conditions. While the current model is optimized for well-lit imagery, future work will focus on improving robustness under low-light and obstructed scenarios to expand its operational utility.</li>
</ul>

<h3>Title: Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhimeng Luo, Abhibha Gupta, Adam Frisch, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01885">https://arxiv.org/abs/2509.01885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01885">https://arxiv.org/pdf/2509.01885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01885]] Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning(https://arxiv.org/abs/2509.01885)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The extraction of critical patient information from Electronic Health Records (EHRs) poses significant challenges due to the complexity and unstructured nature of the data. Traditional machine learning approaches often fail to capture pertinent details efficiently, making it difficult for clinicians to utilize these tools effectively in patient care. This paper introduces a novel approach to extracting the OPQRST assessment from EHRs by leveraging the capabilities of Large Language Models (LLMs). We propose to reframe the task from sequence labeling to text generation, enabling the models to provide reasoning steps that mimic a physician's cognitive processes. This approach enhances interpretability and adapts to the limited availability of labeled data in healthcare settings. Furthermore, we address the challenge of evaluating the accuracy of machine-generated text in clinical contexts by proposing a modification to traditional Named Entity Recognition (NER) metrics. This includes the integration of semantic similarity measures, such as the BERT Score, to assess the alignment between generated text and the clinical intent of the original records. Our contributions demonstrate a significant advancement in the use of AI in healthcare, offering a scalable solution that improves the accuracy and usability of information extraction from EHRs, thereby aiding clinicians in making more informed decisions and enhancing patient care outcomes.</li>
</ul>

<h3>Title: Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Miguel Esparza, Archit Gupta, Ali Mostafavi, Kai Yin, Yiming Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01895">https://arxiv.org/abs/2509.01895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01895">https://arxiv.org/pdf/2509.01895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01895]] Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models(https://arxiv.org/abs/2509.01895)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The escalating intensity and frequency of wildfires demand innovative computational methods for rapid and accurate property damage assessment. Traditional methods are often time consuming, while modern computer vision approaches typically require extensive labeled datasets, hindering immediate post-disaster deployment. This research introduces a novel, zero-shot framework leveraging pre-trained vision language models (VLMs) to classify damage from ground-level imagery. We propose and evaluate two pipelines applied to the 2025 Eaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large language model (LLM) approach (Pipeline B), that integrate structured prompts based on specific wildfire damage indicators. A primary scientific contribution of this study is demonstrating the VLMs efficacy in synthesizing information from multiple perspectives to identify nuanced damage, a critical limitation in existing literature. Our findings reveal that while single view assessments struggled to classify affected structures (F1 scores ranging from 0.225 to 0.511), the multi-view analysis yielded dramatic improvements (F1 scores ranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that pipelines with a multi-view image assessment yields statistically significant classification improvements; however, the improvements this research observed between Pipeline A and B were not statistically significant. Thus, future research can explore the potential of LLM prompting in damage assessment. The practical contribution is an immediately deployable, flexible, and interpretable workflow that bypasses the need for supervised training, significantly accelerating triage and prioritization for disaster response practitioners.</li>
</ul>

<h3>Title: Predicting NCAP Safety Ratings: An Analysis of Vehicle Characteristics and ADAS Features Using Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Raunak Kunwar, Aera Kim LeBoulluec (University of Texas at Arlington)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01897">https://arxiv.org/abs/2509.01897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01897">https://arxiv.org/pdf/2509.01897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01897]] Predicting NCAP Safety Ratings: An Analysis of Vehicle Characteristics and ADAS Features Using Machine Learning(https://arxiv.org/abs/2509.01897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vehicle safety assessment is crucial for consumer information and regulatory oversight. The New Car Assessment Program (NCAP) assigns standardized safety ratings, which traditionally emphasize passive safety measures but now include active safety technologies such as Advanced Driver-Assistance Systems (ADAS). It is crucial to understand how these various systems interact empirically. This study explores whether particular ADAS features like Forward Collision Warning, Lane Departure Warning, Crash Imminent Braking, and Blind Spot Detection, together with established vehicle attributes (e.g., Curb Weight, Model Year, Vehicle Type, Drive Train), can reliably predict a vehicle's likelihood of earning the highest (5-star) overall NCAP rating. Using a publicly available dataset derived from NCAP reports that contain approximately 5,128 vehicle variants spanning model years 2011-2025, we compared four different machine learning models: logistic regression, random forest, gradient boosting, and support vector classifier (SVC) using a 5-fold stratified cross-validation approach. The two best-performing algorithms (random forest and gradient boost) were hyperparameter optimized using RandomizedSearchCV. Analysis of feature importance showed that basic vehicle characteristics, specifically curb weight and model year, dominated predictive capability, contributing more than 55% of the feature relevance of the Random Forest model. However, the inclusion of ADAS features also provided meaningful predictive contributions. The optimized Random Forest model achieved robust results on a held-out test set, with an accuracy of 89.18% and a ROC AUC of 0.9586. This research reveals the use of machine learning to analyze large-scale NCAP data and highlights the combined predictive importance of both established vehicle parameters and modern ADAS features to achieve top safety ratings.</li>
</ul>

<h3>Title: DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01898">https://arxiv.org/abs/2509.01898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01898">https://arxiv.org/pdf/2509.01898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01898]] DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective(https://arxiv.org/abs/2509.01898)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Although large scale models achieve significant improvements in performance, the overfitting challenge still frequently undermines their generalization ability. In super resolution tasks on images, diffusion models as representatives of generative models typically adopt large scale architectures. However, few-shot drone-captured infrared training data frequently induces severe overfitting in large-scale architectures. To address this key challenge, our method proposes a new Gaussian quantization representation learning method oriented to diffusion models that alleviates overfitting and enhances robustness. At the same time, an effective monitoring mechanism tracks large scale architectures during training to detect signs of overfitting. By introducing Gaussian quantization representation learning, our method effectively reduces overfitting while maintaining architecture complexity. On this basis, we construct a multi source drone-based infrared image benchmark dataset for detection and use it to emphasize overfitting issues of large scale architectures in few sample, drone-based diverse drone-based image reconstruction scenarios. To verify the efficacy of the method in mitigating overfitting, experiments are conducted on the constructed benchmark. Experimental results demonstrate that our method outperforms existing super resolution approaches and significantly mitigates overfitting of large scale architectures under complex conditions. The code and DroneSR dataset will be available at: this https URL.</li>
</ul>

<h3>Title: Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints</h3>
<ul>
<li><strong>Authors: </strong>Zhimeng Luo, Zhendong Wang, Rui Meng, Diyang Xue, Adam Frisch, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01899">https://arxiv.org/abs/2509.01899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01899">https://arxiv.org/pdf/2509.01899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01899]] Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints(https://arxiv.org/abs/2509.01899)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>A Chief complaint (CC) is the reason for the medical visit as stated in the patient's own words. It helps medical professionals to quickly understand a patient's situation, and also serves as a short summary for medical text mining. However, chief complaint records often take a variety of entering methods, resulting in a wide variation of medical notations, which makes it difficult to standardize across different medical institutions for record keeping or text mining. In this study, we propose a weakly supervised method to automatically extract and link entities in chief complaints in the absence of human annotation. We first adopt a split-and-match algorithm to produce weak annotations, including entity mention spans and class labels, on 1.2 million real-world de-identified and IRB approved chief complaint records. Then we train a BERT-based model with generated weak labels to locate entity mentions in chief complaint text and link them to a pre-defined ontology. We conducted extensive experiments, and the results showed that our Weakly Supervised Entity Extraction and Linking (\ours) method produced superior performance over previous methods without any human annotation.</li>
</ul>

<h3>Title: VISP: Volatility Informed Stochastic Projection for Adaptive Regularization</h3>
<ul>
<li><strong>Authors: </strong>Tanvir Islam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01903">https://arxiv.org/abs/2509.01903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01903">https://arxiv.org/pdf/2509.01903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01903]] VISP: Volatility Informed Stochastic Projection for Adaptive Regularization(https://arxiv.org/abs/2509.01903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose VISP: Volatility Informed Stochastic Projection, an adaptive regularization method that leverages gradient volatility to guide stochastic noise injection in deep neural networks. Unlike conventional techniques that apply uniform noise or fixed dropout rates, VISP dynamically computes volatility from gradient statistics and uses it to scale a stochastic projection matrix. This mechanism selectively regularizes inputs and hidden nodes that exhibit higher gradient volatility while preserving stable representations, thereby mitigating overfitting. Extensive experiments on MNIST, CIFAR-10, and SVHN demonstrate that VISP consistently improves generalization performance over baseline models and fixed-noise alternatives. In addition, detailed analyses of the evolution of volatility, the spectral properties of the projection matrix, and activation distributions reveal that VISP not only stabilizes the internal dynamics of the network but also fosters a more robust feature representation.</li>
</ul>

<h3>Title: RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events</h3>
<ul>
<li><strong>Authors: </strong>Zhenyuan Chen, Chenxi Wang, Ningyu Zhang, Feng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01907">https://arxiv.org/abs/2509.01907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01907">https://arxiv.org/pdf/2509.01907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01907]] RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events(https://arxiv.org/abs/2509.01907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Remote sensing is critical for disaster monitoring, yet existing datasets lack temporal image pairs and detailed textual annotations. While single-snapshot imagery dominates current resources, it fails to capture dynamic disaster impacts over time. To address this gap, we introduce the Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods, wildfires, and more) paired with rich, human-like change captions. By bridging the temporal and semantic divide in remote sensing data, RSCC enables robust training and evaluation of vision-language models for disaster-aware bi-temporal understanding. Our results highlight RSCC's ability to facilitate detailed disaster-related analysis, paving the way for more accurate, interpretable, and scalable vision-language applications in remote sensing. Code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</h3>
<ul>
<li><strong>Authors: </strong>Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01910">https://arxiv.org/abs/2509.01910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01910">https://arxiv.org/pdf/2509.01910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01910]] Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework(https://arxiv.org/abs/2509.01910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Worldwide geo-localization involves determining the exact geographic location of images captured globally, typically guided by geographic cues such as climate, landmarks, and architectural styles. Despite advancements in geo-localization models like GeoCLIP, which leverages images and location alignment via contrastive learning for accurate predictions, the interpretability of these models remains insufficiently explored. Current concept-based interpretability methods fail to align effectively with Geo-alignment image-location embedding objectives, resulting in suboptimal interpretability and performance. To address this gap, we propose a novel framework integrating global geo-localization with concept bottlenecks. Our method inserts a Concept-Aware Alignment Module that jointly projects image and location embeddings onto a shared bank of geographic concepts (e.g., tropical climate, mountain, cathedral) and minimizes a concept-level loss, enhancing alignment in a concept-specific subspace and enabling robust interpretability. To our knowledge, this is the first work to introduce interpretability into geo-localization. Extensive experiments demonstrate that our approach surpasses GeoCLIP in geo-localization accuracy and boosts performance across diverse geospatial prediction tasks, revealing richer semantic insights into geographic decision-making processes.</li>
</ul>

<h3>Title: A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation</h3>
<ul>
<li><strong>Authors: </strong>Seohyun Kim, Junyoung Lee, Jongho Park, Jinhyung Koo, Sungjin Lee, Yeseong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01919">https://arxiv.org/abs/2509.01919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01919">https://arxiv.org/pdf/2509.01919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01919]] A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation(https://arxiv.org/abs/2509.01919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose DiTTO, a novel diffusion-based framework for generating realistic, precisely configurable, and diverse multi-device storage traces. Leveraging advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity continuous traces that capture temporal dynamics and inter-device dependencies with user-defined configurations. Our experimental results demonstrate that DiTTO can generate traces with high fidelity and diversity while aligning closely with guided configurations with only 8% errors.</li>
</ul>

<h3>Title: A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Zhao Wei, Chin Chun Ooi, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01943">https://arxiv.org/abs/2509.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01943">https://arxiv.org/pdf/2509.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01943]] A Continuous Encoding-Based Representation for Efficient Multi-Fidelity Multi-Objective Neural Architecture Search(https://arxiv.org/abs/2509.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Neural architecture search (NAS) is an attractive approach to automate the design of optimized architectures but is constrained by high computational budget, especially when optimizing for multiple, important conflicting objectives. To address this, an adaptive Co-Kriging-assisted multi-fidelity multi-objective NAS algorithm is proposed to further reduce the computational cost of NAS by incorporating a clustering-based local multi-fidelity infill sampling strategy, enabling efficient exploration of the search space for faster convergence. This algorithm is further accelerated by the use of a novel continuous encoding method to represent the connections of nodes in each cell within a generalized cell-based U-Net backbone, thereby decreasing the search dimension (number of variables). Results indicate that the proposed NAS algorithm outperforms previously published state-of-the-art methods under limited computational budget on three numerical benchmarks, a 2D Darcy flow regression problem and a CHASE_DB1 biomedical image segmentation problem. The proposed method is subsequently used to create a wind velocity regression model with application in urban modelling, with the found model able to achieve good prediction with less computational complexity. Further analysis revealed that the NAS algorithm independently identified principles undergirding superior U-Net architectures in other literature, such as the importance of allowing each cell to incorporate information from prior cells.</li>
</ul>

<h3>Title: DRAssist: Dispute Resolution Assistance using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sachin Pawar, Manoj Apte, Girish K. Palshikar, Basit Ali, Nitin Ramrakhiyani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01962">https://arxiv.org/abs/2509.01962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01962">https://arxiv.org/pdf/2509.01962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01962]] DRAssist: Dispute Resolution Assistance using Large Language Models(https://arxiv.org/abs/2509.01962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Disputes between two parties occur in almost all domains such as taxation, insurance, banking, healthcare, etc. The disputes are generally resolved in a specific forum (e.g., consumer court) where facts are presented, points of disagreement are discussed, arguments as well as specific demands of the parties are heard, and finally a human judge resolves the dispute by often favouring one of the two parties. In this paper, we explore the use of large language models (LLMs) as assistants for the human judge to resolve such disputes, as part of our DRAssist system. We focus on disputes from two specific domains -- automobile insurance and domain name disputes. DRAssist identifies certain key structural elements (e.g., facts, aspects or disagreement, arguments) of the disputes and summarizes the unstructured dispute descriptions to produce a structured summary for each dispute. We then explore multiple prompting strategies with multiple LLMs for their ability to assist in resolving the disputes in these domains. In DRAssist, these LLMs are prompted to produce the resolution output at three different levels -- (i) identifying an overall stronger party in a dispute, (ii) decide whether each specific demand of each contesting party can be accepted or not, (iii) evaluate whether each argument by each contesting party is strong or weak. We evaluate the performance of LLMs on all these tasks by comparing them with relevant baselines using suitable evaluation metrics.</li>
</ul>

<h3>Title: 2D Gaussian Splatting with Semantic Alignment for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01964">https://arxiv.org/abs/2509.01964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01964">https://arxiv.org/pdf/2509.01964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01964]] 2D Gaussian Splatting with Semantic Alignment for Image Inpainting(https://arxiv.org/abs/2509.01964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.</li>
</ul>

<h3>Title: Ensemble-Based Event Camera Place Recognition Under Varying Illumination</h3>
<ul>
<li><strong>Authors: </strong>Therese Joseph, Tobias Fischer, Michael Milford</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01968">https://arxiv.org/abs/2509.01968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01968">https://arxiv.org/pdf/2509.01968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01968]] Ensemble-Based Event Camera Place Recognition Under Varying Illumination(https://arxiv.org/abs/2509.01968)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Compared to conventional cameras, event cameras provide a high dynamic range and low latency, offering greater robustness to rapid motion and challenging lighting conditions. Although the potential of event cameras for visual place recognition (VPR) has been established, developing robust VPR frameworks under severe illumination changes remains an open research problem. In this paper, we introduce an ensemble-based approach to event camera place recognition that combines sequence-matched results from multiple event-to-frame reconstructions, VPR feature extractors, and temporal resolutions. Unlike previous event-based ensemble methods, which only utilise temporal resolution, our broader fusion strategy delivers significantly improved robustness under varied lighting conditions (e.g., afternoon, sunset, night), achieving a 57% relative improvement in Recall@1 across day-night transitions. We evaluate our approach on two long-term driving datasets (with 8 km per traverse) without metric subsampling, thereby preserving natural variations in speed and stop duration that influence event density. We also conduct a comprehensive analysis of key design choices, including binning strategies, polarity handling, reconstruction methods, and feature extractors, to identify the most critical components for robust performance. Additionally, we propose a modification to the standard sequence matching framework that enhances performance at longer sequence lengths. To facilitate future research, we will release our codebase and benchmarking framework.</li>
</ul>

<h3>Title: Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems</h3>
<ul>
<li><strong>Authors: </strong>Long Jiang, Yang Yang, Ting Fong May Chui, Morgan Thornwell, Hoshin Vijai Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01972">https://arxiv.org/abs/2509.01972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01972">https://arxiv.org/pdf/2509.01972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01972]] Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems(https://arxiv.org/abs/2509.01972)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Simulating ecohydrological processes is essential for understanding complex environmental systems and guiding sustainable management amid accelerating climate change and human pressures. Process-based models provide physical realism but can suffer from structural rigidity, high computational costs, and complex calibration, while machine learning (ML) methods are efficient and flexible yet often lack interpretability and transferability. We propose a unified three-phase framework that integrates process-based models with ML and progressively embeds them into artificial intelligence (AI) through knowledge distillation. Phase I, behavioral distillation, enhances process models via surrogate learning and model simplification to capture key dynamics at lower computational cost. Phase II, structural distillation, reformulates process equations as modular components within a graph neural network (GNN), enabling multiscale representation and seamless integration with ML models. Phase III, cognitive distillation, embeds expert reasoning and adaptive decision-making into intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture. Demonstrations for the Samish watershed highlight the framework's applicability to ecohydrological modeling, showing that it can reproduce process-based model outputs, improve predictive accuracy, and support scenario-based decision-making. The framework offers a scalable and transferable pathway toward next-generation intelligent ecohydrological modeling systems, with the potential extension to other process-based domains.</li>
</ul>

<h3>Title: Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Quan Dao, Xiaoxiao He, Ligong Han, Ngan Hoai Nguyen, Amin Heyrani Nobar, Faez Ahmed, Han Zhang, Viet Anh Nguyen, Dimitris Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01984">https://arxiv.org/abs/2509.01984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01984">https://arxiv.org/pdf/2509.01984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01984]] Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing(https://arxiv.org/abs/2509.01984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach.</li>
</ul>

<h3>Title: Explaining What Machines See: XAI Strategies in Deep Object Detection Models</h3>
<ul>
<li><strong>Authors: </strong>FatemehSadat Seyedmomeni, Mohammad Ali Keyvanrad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01991">https://arxiv.org/abs/2509.01991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01991">https://arxiv.org/pdf/2509.01991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01991]] Explaining What Machines See: XAI Strategies in Deep Object Detection Models(https://arxiv.org/abs/2509.01991)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>In recent years, deep learning has achieved unprecedented success in various computer vision tasks, particularly in object detection. However, the black-box nature and high complexity of deep neural networks pose significant challenges for interpretability, especially in critical domains such as autonomous driving, medical imaging, and security systems. Explainable Artificial Intelligence (XAI) aims to address this challenge by providing tools and methods to make model decisions more transparent, interpretable, and trust-worthy for humans. This review provides a comprehensive analysis of state-of-the-art explain-ability methods specifically applied to object detection models. The paper be-gins by categorizing existing XAI techniques based on their underlying mechanisms-perturbation-based, gradient-based, backpropagation-based, and graph-based methods. Notable methods such as D-RISE, BODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper investigates their applicability to various object detection architectures, including YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of publication trends from 2022 to mid-2025 shows an accelerating interest in explainable object detection, indicating its increasing importance. The study also explores common datasets and evaluation metrics, and highlights the major challenges associated with model interpretability. By providing a structured taxonomy and a critical assessment of existing methods, this review aims to guide researchers and practitioners in selecting suitable explainability techniques for object detection applications and to foster the development of more interpretable AI systems.</li>
</ul>

<h3>Title: ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Shi, Haibin Wei, Jiang Wang, Xiaowei Xu, Longzhi Du, Taixu Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.01997">https://arxiv.org/abs/2509.01997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.01997">https://arxiv.org/pdf/2509.01997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.01997]] ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting(https://arxiv.org/abs/2509.01997)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Logistical demand-supply forecasting that evaluates the alignment between projected supply and anticipated demand, is essential for the efficiency and quality of on-demand food delivery platforms and serves as a key indicator for scheduling decisions. Future order distribution information, which reflects the distribution of orders in on-demand food delivery, is crucial for the performance of logistical demand-supply forecasting. Current studies utilize spatial-temporal analysis methods to model future order distribution information from serious time slices. However, learning future order distribution in online delivery platform is a time-series-insensitive problem with strong randomness. These approaches often struggle to effectively capture this information while remaining efficient. This paper proposes an innovative spatiotemporal learning model that utilizes only two graphs (ongoing and global) to learn future order distribution information, achieving superior performance compared to traditional spatial-temporal long-series methods. The main contributions are as follows: (1) The introduction of ongoing and global graphs in logistical demand-supply pressure forecasting compared to traditional long time series significantly enhances forecasting performance. (2) An innovative graph learning network framework using adaptive future graph learning and innovative cross attention mechanism (ACA-Net) is proposed to extract future order distribution information, effectively learning a robust future graph that substantially improves logistical demand-supply pressure forecasting outcomes. (3) The effectiveness of the proposed method is validated in real-world production environments.</li>
</ul>

<h3>Title: Palette Aligned Image Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Elad Aharoni, Noy Porat, Dani Lischinski, Ariel Shamir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02000">https://arxiv.org/abs/2509.02000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02000">https://arxiv.org/pdf/2509.02000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02000]] Palette Aligned Image Diffusion(https://arxiv.org/abs/2509.02000)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce the Palette-Adapter, a novel method for conditioning text-to-image diffusion models on a user-specified color palette. While palettes are a compact and intuitive tool widely used in creative workflows, they introduce significant ambiguity and instability when used for conditioning image generation. Our approach addresses this challenge by interpreting palettes as sparse histograms and introducing two scalar control parameters: histogram entropy and palette-to-histogram distance, which allow flexible control over the degree of palette adherence and color variation. We further introduce a negative histogram mechanism that allows users to suppress specific undesired hues, improving adherence to the intended palette under the standard classifier-free guidance mechanism. To ensure broad generalization across the color space, we train on a carefully curated dataset with balanced coverage of rare and common colors. Our method enables stable, semantically coherent generation across a wide range of palettes and prompts. We evaluate our method qualitatively, quantitatively, and through a user study, and show that it consistently outperforms existing approaches in achieving both strong palette adherence and high image quality.</li>
</ul>

<h3>Title: Augmented Shuffle Differential Privacy Protocols for Large-Domain Categorical and Key-Value Data</h3>
<ul>
<li><strong>Authors: </strong>Takao Murakami, Yuichi Sei, Reo Eriguchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02004">https://arxiv.org/abs/2509.02004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02004">https://arxiv.org/pdf/2509.02004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02004]] Augmented Shuffle Differential Privacy Protocols for Large-Domain Categorical and Key-Value Data(https://arxiv.org/abs/2509.02004)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy by introducing a shuffler who randomly shuffles data in a distributed system. However, most shuffle DP protocols are vulnerable to two attacks: collusion attacks by the data collector and users and data poisoning attacks. A recent study addresses this issue by introducing an augmented shuffle DP protocol, where users do not add noise and the shuffler performs random sampling and dummy data addition. However, it focuses on frequency estimation over categorical data with a small domain and cannot be applied to a large domain due to prohibitively high communication and computational costs. In this paper, we fill this gap by introducing a novel augmented shuffle DP protocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME protocol uses a hash function to filter out unpopular items and then accurately calculates frequencies for popular items. To perform this within one round of interaction between users and the shuffler, our protocol carefully communicates within a system using multiple encryption. We also apply our FME protocol to more advanced KV (Key-Value) statistics estimation with an additional technique to reduce bias. For both categorical and KV data, we prove that our protocol provides computational DP, high robustness to the above two attacks, accuracy, and efficiency. We show the effectiveness of our proposals through comparisons with twelve existing protocols.</li>
</ul>

<h3>Title: Second-Order Tensorial Partial Differential Equations on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Aref Einizade, Fragkiskos D. Malliaros, Jhony H. Giraldo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02015">https://arxiv.org/abs/2509.02015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02015">https://arxiv.org/pdf/2509.02015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02015]] Second-Order Tensorial Partial Differential Equations on Graphs(https://arxiv.org/abs/2509.02015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Processing data that lies on multiple interacting (product) graphs is increasingly important in practical applications, yet existing methods are mostly restricted to discrete graph filtering. Tensorial partial differential equations on graphs (TPDEGs) offer a principled framework for modeling such multidomain data in a continuous setting. However, current continuous approaches are limited to first-order derivatives, which tend to dampen high-frequency signals and slow down information propagation. This makes these TPDEGs-based approaches less effective for capturing complex, multi-scale, and heterophilic structures. In this paper, we introduce second-order TPDEGs (So-TPDEGs) and propose the first theoretically grounded framework for second-order continuous product graph neural networks. Our approach leverages the separability of cosine kernels in Cartesian product graphs to implement efficient spectral decomposition, while naturally preserving high-frequency information. We provide rigorous theoretical analyses of stability under graph perturbations and over-smoothing behavior regarding spectral properties. Our theoretical results establish a robust foundation for advancing continuous graph learning across multiple practical domains.</li>
</ul>

<h3>Title: Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings</h3>
<ul>
<li><strong>Authors: </strong>Stanley Mugisha, Rashid Kisitu, Francis Komakech, Excellence Favor</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02018">https://arxiv.org/abs/2509.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02018">https://arxiv.org/pdf/2509.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02018]] Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings(https://arxiv.org/abs/2509.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Preterm birth remains a leading cause of neonatal mortality, disproportionately affecting low-resource settings with limited access to advanced neonatal intensive care units (NICUs).Continuous monitoring of infant behavior, such as sleep/awake states and crying episodes, is critical but relies on manual observation or invasive sensors, which are prone to error, impractical, and can cause skin damage. This paper presents a novel, noninvasive, and automated vision-based framework to address this gap. We introduce an embedded monitoring system that utilizes a quantized MobileNet model deployed on a Raspberry Pi for real-time behavioral state detection. When trained and evaluated on public neonatal image datasets, our system achieves state-of-the-art accuracy (91.8% for sleep detection and 97.7% for crying/normal classification) while maintaining computational efficiency suitable for edge deployment. Through comparative benchmarking, we provide a critical analysis of the trade-offs between model size, inference latency, and diagnostic accuracy. Our findings demonstrate that while larger architectures (e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational cost is prohibitive for real-time edge use. The proposed framework integrates three key innovations: model quantization for memory-efficient inference (68% reduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT communication for clinical alerts. This work conclusively shows that lightweight, optimized models such as the MobileNet offer the most viable foundation for scalable, low-cost, and clinically actionable NICU monitoring systems, paving the way for improved preterm care in resource-constrained environments.</li>
</ul>

<h3>Title: Unsupervised Training of Vision Transformers with Synthetic Negatives</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02024">https://arxiv.org/abs/2509.02024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02024">https://arxiv.org/pdf/2509.02024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02024]] Unsupervised Training of Vision Transformers with Synthetic Negatives(https://arxiv.org/abs/2509.02024)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper does not introduce a novel method per se. Instead, we address the neglected potential of hard negative samples in self-supervised learning. Previous works explored synthetic hard negatives but rarely in the context of vision transformers. We build on this observation and integrate synthetic hard negatives to improve vision transformer representation learning. This simple yet effective technique notably improves the discriminative power of learned representations. Our experiments show performance improvements for both DeiT-S and Swin-T architectures.</li>
</ul>

<h3>Title: See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</h3>
<ul>
<li><strong>Authors: </strong>Halima Bouzidi, Haoyu Liu, Mohammad Al Faruque</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02028">https://arxiv.org/abs/2509.02028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02028">https://arxiv.org/pdf/2509.02028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02028]] See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems(https://arxiv.org/abs/2509.02028)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.</li>
</ul>

<h3>Title: Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02029">https://arxiv.org/abs/2509.02029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02029">https://arxiv.org/pdf/2509.02029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02029]] Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives(https://arxiv.org/abs/2509.02029)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper does not introduce a new method per se. Instead, we build on existing self-supervised learning approaches for vision, drawing inspiration from the adage "fake it till you make it". While contrastive self-supervised learning has achieved remarkable success, it typically relies on vast amounts of real-world data and carefully curated hard negatives. To explore alternatives to these requirements, we investigate two forms of "faking it" in vision transformers. First, we study the potential of generative models for unsupervised representation learning, leveraging synthetic data to augment sample diversity. Second, we examine the feasibility of generating synthetic hard negatives in the representation space, creating diverse and challenging contrasts. Our framework - dubbed Syn2Co - combines both approaches and evaluates whether synthetically enhanced training can lead to more robust and transferable visual representations on DeiT-S and Swin-T architectures. Our findings highlight the promise and limitations of synthetic data in self-supervised learning, offering insights for future work in this direction.</li>
</ul>

<h3>Title: DeepSeek performs better than other Large Language Models in Dental Cases</h3>
<ul>
<li><strong>Authors: </strong>Hexian Zhang, Xinyu Yan, Yanqi Yang, Lijian Jin, Ping Yang, Junwen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02036">https://arxiv.org/abs/2509.02036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02036">https://arxiv.org/pdf/2509.02036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02036]] DeepSeek performs better than other Large Language Models in Dental Cases(https://arxiv.org/abs/2509.02036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent.</li>
</ul>

<h3>Title: Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Guangzeng Han, Weisi Liu, Xiaolei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02040">https://arxiv.org/abs/2509.02040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02040">https://arxiv.org/pdf/2509.02040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02040]] Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation(https://arxiv.org/abs/2509.02040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.</li>
</ul>

<h3>Title: Targeted Physical Evasion Attacks in the Near-Infrared Domain</h3>
<ul>
<li><strong>Authors: </strong>Pascal Zimmer, Simon Lachnit, Alexander Jan Zielinski, Ghassan Karame</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02042">https://arxiv.org/abs/2509.02042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02042">https://arxiv.org/pdf/2509.02042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02042]] Targeted Physical Evasion Attacks in the Near-Infrared Domain(https://arxiv.org/abs/2509.02042)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, segmentation</a></li>
<li><strong>Abstract: </strong>A number of attacks rely on infrared light sources or heat-absorbing material to imperceptibly fool systems into misinterpreting visual input in various image recognition applications. However, almost all existing approaches can only mount untargeted attacks and require heavy optimizations due to the use-case-specific constraints, such as location and shape. In this paper, we propose a novel, stealthy, and cost-effective attack to generate both targeted and untargeted adversarial infrared perturbations. By projecting perturbations from a transparent film onto the target object with an off-the-shelf infrared flashlight, our approach is the first to reliably mount laser-free targeted attacks in the infrared domain. Extensive experiments on traffic signs in the digital and physical domains show that our approach is robust and yields higher attack success rates in various attack scenarios across bright lighting conditions, distances, and angles compared to prior work. Equally important, our attack is highly cost-effective, requiring less than US\$50 and a few tens of seconds for deployment. Finally, we propose a novel segmentation-based detection that thwarts our attack with an F1-score of up to 99%.</li>
</ul>

<h3>Title: Fantastic Pretraining Optimizers and Where to Find Them</h3>
<ul>
<li><strong>Authors: </strong>Kaiyue Wen, David Hall, Tengyu Ma, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02046">https://arxiv.org/abs/2509.02046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02046">https://arxiv.org/pdf/2509.02046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02046]] Fantastic Pretraining Optimizers and Where to Find Them(https://arxiv.org/abs/2509.02046)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.</li>
</ul>

<h3>Title: Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Yi Yin, Guangquan Zhang, Hua Zuo, Jie Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02048">https://arxiv.org/abs/2509.02048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02048">https://arxiv.org/pdf/2509.02048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02048]] Privacy-Utility Trade-off in Data Publication: A Bilevel Optimization Framework with Curvature-Guided Perturbation(https://arxiv.org/abs/2509.02048)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Machine learning models require datasets for effective training, but directly sharing raw data poses significant privacy risk such as membership inference attacks (MIA). To mitigate the risk, privacy-preserving techniques such as data perturbation, generalization, and synthetic data generation are commonly utilized. However, these methods often degrade data accuracy, specificity, and diversity, limiting the performance of downstream tasks and thus reducing data utility. Therefore, striking an optimal balance between privacy preservation and data utility remains a critical challenge. To address this issue, we introduce a novel bilevel optimization framework for the publication of private datasets, where the upper-level task focuses on data utility and the lower-level task focuses on data privacy. In the upper-level task, a discriminator guides the generation process to ensure that perturbed latent variables are mapped to high-quality samples, maintaining fidelity for downstream tasks. In the lower-level task, our framework employs local extrinsic curvature on the data manifold as a quantitative measure of individual vulnerability to MIA, providing a geometric foundation for targeted privacy protection. By perturbing samples toward low-curvature regions, our method effectively suppresses distinctive feature combinations that are vulnerable to MIA. Through alternating optimization of both objectives, we achieve a synergistic balance between privacy and utility. Extensive experimental evaluations demonstrate that our method not only enhances resistance to MIA in downstream tasks but also surpasses existing methods in terms of sample quality and diversity.</li>
</ul>

<h3>Title: Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling</h3>
<ul>
<li><strong>Authors: </strong>Srinivas Anumasa, Barath Chandran.C, Tingting Chen, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02069">https://arxiv.org/abs/2509.02069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02069">https://arxiv.org/pdf/2509.02069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02069]] Data-Dependent Smoothing for Protein Discovery with Walk-Jump Sampling(https://arxiv.org/abs/2509.02069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful class of generative models by learning to iteratively reverse the noising process. Their ability to generate high-quality samples has extended beyond high-dimensional image data to other complex domains such as proteins, where data distributions are typically sparse and unevenly spread. Importantly, the sparsity itself is uneven. Empirically, we observed that while a small fraction of samples lie in dense clusters, the majority occupy regions of varying sparsity across the data space. Existing approaches largely ignore this data-dependent variability. In this work, we introduce a Data-Dependent Smoothing Walk-Jump framework that employs kernel density estimation (KDE) as a preprocessing step to estimate the noise scale $\sigma$ for each data point, followed by training a score model with these data-dependent $\sigma$ values. By incorporating local data geometry into the denoising process, our method accounts for the heterogeneous distribution of protein data. Empirical evaluations demonstrate that our approach yields consistent improvements across multiple metrics, highlighting the importance of data-aware sigma prediction for generative modeling in sparse, high-dimensional settings.</li>
</ul>

<h3>Title: Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Jinbao Tian, Yunqi Xu, Zhou Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02072">https://arxiv.org/abs/2509.02072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02072">https://arxiv.org/pdf/2509.02072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02072]] Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports(https://arxiv.org/abs/2509.02072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The automatic classification of occupational accident reports is a critical research area for enhancing workplace safety and enabling large-scale risk analysis. However, the severe class imbalance inherent in these real-world datasets often compromises the performance of analytical models, particularly for rare but severe incident types, hindering the development of reliable automated systems. To address this challenge, we propose ABEX-RAT, a novel and efficient framework that synergizes generative data augmentation with robust adversarial training. Our approach first employs a twostep abstractive-expansive (ABEX) pipeline, which leverages a large language model to distill core incident semantics and then uses a generative model to create diverse, highquality synthetic samples for underrepresented classes. Subsequently, a lightweight classifier is trained on the augmented data using a computationally efficient random adversarial training (RAT) protocol, which stochastically applies perturbations to enhance model generalization and robustness without significant overhead. Experimental results on the public OSHA dataset demonstrate that our method achieves new state-of-the-art performance, reaching a macro-F1 score of 90.32% and significantly outperforming previous SOTA and fine-tuned large model baselines. Our work validates that this synergistic strategy is a highly effective and efficient alternative to brute-force fine-tuning for specialized, imbalanced classification tasks. The code is publicly available at:this https URL.</li>
</ul>

<h3>Title: How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Elisabetta Rocchetti, Alfio Ferrara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02075">https://arxiv.org/abs/2509.02075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02075">https://arxiv.org/pdf/2509.02075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02075]] How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis(https://arxiv.org/abs/2509.02075)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adhering to explicit length constraints, such as generating text with a precise word count, remains a significant challenge for Large Language Models (LLMs). This study aims at investigating the differences between foundation models and their instruction-tuned counterparts, on length-controlled text generation in English and Italian. We analyze both performance and internal component contributions using Cumulative Weighted Attribution, a metric derived from Direct Logit Attribution. Our findings reveal that instruction-tuning substantially improves length control, primarily by specializing components in deeper model layers. Specifically, attention heads in later layers of IT models show increasingly positive contributions, particularly in English. In Italian, while attention contributions are more attenuated, final-layer MLPs exhibit a stronger positive role, suggesting a compensatory mechanism. These results indicate that instruction-tuning reconfigures later layers for task adherence, with component-level strategies potentially adapting to linguistic context.</li>
</ul>

<h3>Title: Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model</h3>
<ul>
<li><strong>Authors: </strong>Kong Mun Yeen, Rafidah Md Noor, Wahidah Md Shah, Aslinda Hassan, Muhammad Umair Munir</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02076">https://arxiv.org/abs/2509.02076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02076">https://arxiv.org/pdf/2509.02076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02076]] Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model(https://arxiv.org/abs/2509.02076)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper forecasts future Distributed Denial of Service (DDoS) attacks using deep learning models. Although several studies address forecasting DDoS attacks, they remain relatively limited compared to detection-focused research. By studying the current trends and forecasting based on newer and updated datasets, mitigation plans against the attacks can be planned and formulated. The methodology used in this research work conforms to the Cross Industry Standard Process for Data Mining (CRISP-DM) model.</li>
</ul>

<h3>Title: From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Refat Othman, Diaeddin Rimawi, Bruno Rossi, Barbara Russo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02077">https://arxiv.org/abs/2509.02077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02077">https://arxiv.org/pdf/2509.02077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02077]] From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach(https://arxiv.org/abs/2509.02077)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, transformer</a></li>
<li><strong>Abstract: </strong>In the domain of security, vulnerabilities frequently remain undetected even after their exploitation. In this work, vulnerabilities refer to publicly disclosed flaws documented in Common Vulnerabilities and Exposures (CVE) reports. Establishing a connection between attacks and vulnerabilities is essential for enabling timely incident response, as it provides defenders with immediate, actionable insights. However, manually mapping attacks to CVEs is infeasible, thereby motivating the need for automation. This paper evaluates 14 state-of-the-art (SOTA) sentence transformers for automatically identifying vulnerabilities from textual descriptions of attacks. Our results demonstrate that the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior classification performance when using attack Technique descriptions, with an F1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was observed that, on average, 56% of the vulnerabilities identified by the MMPNet model are also represented within the CVE repository in conjunction with an attack, while 61% of the vulnerabilities detected by the model correspond to those cataloged in the CVE repository. A manual inspection of the results revealed the existence of 275 predicted links that were not documented in the MITRE repositories. Consequently, the automation of linking attack techniques to vulnerabilities not only enhances the detection and response capabilities related to software security incidents but also diminishes the duration during which vulnerabilities remain exploitable, thereby contributing to the development of more secure systems.</li>
</ul>

<h3>Title: Performance analysis of common browser extensions for cryptojacking detection</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Tanana</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02083">https://arxiv.org/abs/2509.02083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02083">https://arxiv.org/pdf/2509.02083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02083]] Performance analysis of common browser extensions for cryptojacking detection(https://arxiv.org/abs/2509.02083)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>This paper considers five extensions for Chromium-based browsers in order to determine how effective can browser-based defenses against cryptojacking available to regular users be. We've examined most popular extensions - MinerBlock, AdGuard AdBlocker, Easy Redirect && Prevent Cryptojacking, CoinEater and Miners Shield, which claim to be designed specifically to identify and stop illegal cryptocurrency mining. An empirically confirmed dataset of 373 distinct cryptojacking-infected websites which was assembled during multi-stage procedure, was used to test those extensions. The results showed that all plugins in question had significant performance limits. Easy Redirect and Miners Shield only blocked 6 and 5 websites respectively, while MinerBlock had the greatest detection rate at only 27% (101/373 sites blocked). Most concerningly, despite promises of cryptojacking prevention, AdGuard (which has over 13 million users) and CoinEater were unable to identify any of the compromised websites. These results demonstrate serious flaws in cryptojacking detection products targeted for regular users, since even the best-performing specimen failed to detect 73% of attacks. The obvious difference between advertised capabilities and real performance highlights the urgent need for either accessibility improvements for laboratory-grade detection technologies that show 90%+ efficiency in controlled environment or fundamental upgrades to current commonly used extensions.</li>
</ul>

<h3>Title: Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Juhyeon Lee, Wonduk Seo, Hyunjin An, Seunghyun Lee, Yi Bu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02093">https://arxiv.org/abs/2509.02093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02093">https://arxiv.org/pdf/2509.02093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02093]] Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization(https://arxiv.org/abs/2509.02093)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Automatic prompt optimization has recently emerged as a strategy for improving the quality of prompts used in Large Language Models (LLMs), with the goal of generating more accurate and useful responses. However, most prior work focuses on direct prompt refinement or model fine-tuning, overlooking the potential of leveraging LLMs' inherent reasoning capability to learn from contrasting examples. In this paper, we present Contrastive Reasoning Prompt Optimization (CRPO), a novel framework that formulates prompt optimization as a retrieval augmented reasoning process. Our approach retrieves top k reference prompts from the HelpSteer2 dataset, an open-source collection annotated for helpfulness, correctness, coherence, complexity, and verbosity, and constructs two complementary optimization paradigms: (1) tiered contrastive reasoning, where the LLM compares high, medium, and low quality prompts to refine its own generation through reflective reasoning, and (2) multi-metric contrastive reasoning, where the LLM analyzes the best prompts along each evaluation dimension and integrates their strengths into an optimized prompt. By explicitly contrasting high and low quality exemplars, CRPO enables the model to deduce why certain prompts succeed while others fail, thereby achieving more robust and interpretable optimization. Experimental results on the HelpSteer2 benchmark demonstrate that CRPO significantly outperforms baselines. Our findings highlight the promise of contrastive, retrieval-augmented reasoning for advancing automatic prompt optimization.</li>
</ul>

<h3>Title: JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang, Shengjie Ma, Yinghan Shen, Yuanzhuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02097">https://arxiv.org/abs/2509.02097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02097">https://arxiv.org/pdf/2509.02097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02097]] JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer(https://arxiv.org/abs/2509.02097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the capabilities of large language models (LLMs) is an essential step to ensure the successful application of LLMs across various domains. The current evaluation of LLMs is based on a paradigm that involves querying them with predefined question sets and assessing their outputs. This paradigm offers controllable processes and simplicity, but faces challenges such as limited interaction with targets, insufficient difficulty control, and difficulties in verifying the validity of evaluation results, making it hard to precisely determine the knowledge and capability boundaries of target models. To address these challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic evaluation framework based on a new interviewer-style evaluation paradigm. JudgeAgent employs a comprehensive evaluation approach consisting of benchmark grading, interactive extension, and evaluation feedback. It utilizes knowledge-driven data synthesis and target-adaptive difficulty adjustment methods to conduct extended testing, providing accurate and effective evaluation results. We also introduce a novel insight into validating evaluation methods, demonstrating the effectiveness of JudgeAgent and its dynamic evaluation paradigm through extensive experiments.</li>
</ul>

<h3>Title: A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic Augmentation via Prompt-driven Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Alonso, Sawaiz A. Chaudhry, Juan C. SanMiguel, √Ålvaro Garc√≠a-Mart√≠n, Pablo Ayuso-Albizu, Pablo Carballeira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02099">https://arxiv.org/abs/2509.02099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02099">https://arxiv.org/pdf/2509.02099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02099]] A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic Augmentation via Prompt-driven Diffusion Models(https://arxiv.org/abs/2509.02099)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Pedestrian Attribute Recognition (PAR) is a challenging task as models are required to generalize across numerous attributes in real-world data. Traditional approaches focus on complex methods, yet recognition performance is often constrained by training dataset limitations, particularly the under-representation of certain attributes. In this paper, we propose a data-centric approach to improve PAR by synthetic data augmentation guided by textual descriptions. First, we define a protocol to identify weakly recognized attributes across multiple datasets. Second, we propose a prompt-driven pipeline that leverages diffusion models to generate synthetic pedestrian images while preserving the consistency of PAR datasets. Finally, we derive a strategy to seamlessly incorporate synthetic samples into training data, which considers prompt-based annotation rules and modifies the loss function. Results on popular PAR datasets demonstrate that our approach not only boosts recognition of underrepresented attributes but also improves overall model performance beyond the targeted attributes. Notably, this approach strengthens zero-shot generalization without requiring architectural changes of the model, presenting an efficient and scalable solution to improve the recognition of attributes of pedestrians in the real world.</li>
</ul>

<h3>Title: DivMerge: A divergence-based model merging method for multi-tasking</h3>
<ul>
<li><strong>Authors: </strong>Touayouch Brahim, Fosse Lo√Øc, Damnati G√©raldine, Lecorv√© Gw√©nol√©</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02108">https://arxiv.org/abs/2509.02108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02108">https://arxiv.org/pdf/2509.02108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02108]] DivMerge: A divergence-based model merging method for multi-tasking(https://arxiv.org/abs/2509.02108)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work.</li>
</ul>

<h3>Title: HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis</h3>
<ul>
<li><strong>Authors: </strong>Han Chen, Hanchen Wang, Hongmei Chen, Ying Zhang, Lu Qin, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02113">https://arxiv.org/abs/2509.02113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02113">https://arxiv.org/pdf/2509.02113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02113]] HiGraph: A Large-Scale Hierarchical Graph Dataset for Malware Analysis(https://arxiv.org/abs/2509.02113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The advancement of graph-based malware analysis is critically limited by the absence of large-scale datasets that capture the inherent hierarchical structure of software. Existing methods often oversimplify programs into single level graphs, failing to model the crucial semantic relationship between high-level functional interactions and low-level instruction logic. To bridge this gap, we introduce \dataset, the largest public hierarchical graph dataset for malware analysis, comprising over \textbf{200M} Control Flow Graphs (CFGs) nested within \textbf{595K} Function Call Graphs (FCGs). This two-level representation preserves structural semantics essential for building robust detectors resilient to code obfuscation and malware evolution. We demonstrate HiGraph's utility through a large-scale analysis that reveals distinct structural properties of benign and malicious software, establishing it as a foundational benchmark for the community. The dataset and tools are publicly available at this https URL.</li>
</ul>

<h3>Title: CMRAG: Co-modality-based document retrieval and visual question answering</h3>
<ul>
<li><strong>Authors: </strong>Wang Chen, Guanqiang Qi, Weikang Li, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02123">https://arxiv.org/abs/2509.02123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02123">https://arxiv.org/pdf/2509.02123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02123]] CMRAG: Co-modality-based document retrieval and visual question answering(https://arxiv.org/abs/2509.02123)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has become a core paradigm in document question answering tasks. However, existing methods have limitations when dealing with multimodal documents: one category of methods relies on layout analysis and text extraction, which can only utilize explicit text information and struggle to capture images or unstructured content; the other category treats document segmentation as visual input and directly passes it to visual language models (VLMs) for processing, yet it ignores the semantic advantages of text, leading to suboptimal generation results. This paper proposes co-modality-based RAG (CMRAG), which can simultaneously leverage text and images for efficient retrieval and generation. Specifically, we first perform structured parsing on documents to obtain co-modality representations of text segments and image regions. Subsequently, in response to user queries, we retrieve candidate evidence from text and image channels, respectively, and aggregate the results at the cross-modal retrieval level. Finally, we prompt the VLM to generate the final response based on the co-modality retrieval results. Experiments demonstrate that our method significantly outperforms pure-vision-based RAG in visual document question answering tasks. The findings of this paper show that integrating co-modality information into the RAG framework in a unified manner is an effective approach to improving the performance of complex document visual question-answering (VQA) systems.</li>
</ul>

<h3>Title: Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</h3>
<ul>
<li><strong>Authors: </strong>Jintao Cheng, Weibin Li, Jiehao Luo, Xiaoyu Tang, Zhijian He, Jin Wu, Yao Zou, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02129">https://arxiv.org/abs/2509.02129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02129">https://arxiv.org/pdf/2509.02129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02129]] Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time(https://arxiv.org/abs/2509.02129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition (VPR) has evolved from handcrafted descriptors to deep learning approaches, yet significant challenges remain. Current approaches, including Vision Foundation Models (VFMs) and Multimodal Large Language Models (MLLMs), enhance semantic understanding but suffer from high computational overhead and limited cross-domain transferability when fine-tuned. To address these limitations, we propose a novel zero-shot framework employing Test-Time Scaling (TTS) that leverages MLLMs' vision-language alignment capabilities through Guidance-based methods for direct similarity scoring. Our approach eliminates two-stage processing by employing structured prompts that generate length-controllable JSON outputs. The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enables real-time adaptation without additional training costs, achieving superior generalization across diverse environments. Experimental results demonstrate significant improvements in cross-domain VPR performance with up to 210$\times$ computational efficiency gains.</li>
</ul>

<h3>Title: AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Snehasis Mukhopadhyay, Aryan Kasat, Shivam Dubey, Rahul Karthikeyan, Dhruv Sood, Vinija Jain, Aman Chadha, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02133">https://arxiv.org/abs/2509.02133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02133">https://arxiv.org/pdf/2509.02133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02133]] AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models(https://arxiv.org/abs/2509.02133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at this https URL</li>
</ul>

<h3>Title: Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation</h3>
<ul>
<li><strong>Authors: </strong>Aymene Mohammed Bouayed, Samuel Deslauriers-Gauthier, Adrian Iaccovelli, David Naccache</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02154">https://arxiv.org/abs/2509.02154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02154">https://arxiv.org/pdf/2509.02154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02154]] Conditional-$t^3$VAE: Equitable Latent Space Allocation for Fair Generation(https://arxiv.org/abs/2509.02154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) with global priors mirror the training set's class frequency in latent space, underrepresenting tail classes and reducing generative fairness on imbalanced datasets. While $t^3$VAE improves robustness via heavy-tailed Student's t-distribution priors, it still allocates latent volume proportionally to the class this http URL this work, we address this issue by explicitly enforcing equitable latent space allocation across classes. To this end, we propose Conditional-$t^3$VAE, which defines a per-class \mbox{Student's t} joint prior over latent and output variables, preventing dominance by majority classes. Our model is optimized using a closed-form objective derived from the $\gamma$-power divergence. Moreover, for class-balanced generation, we derive an equal-weight latent mixture of Student's t-distributions. On SVHN-LT, CIFAR100-LT, and CelebA, Conditional-$t^3$VAE consistently achieves lower FID scores than both $t^3$VAE and Gaussian-based VAE baselines, particularly under severe class imbalance. In per-class F1 evaluations, Conditional-$t^3$VAE also outperforms the conditional Gaussian VAE across all highly imbalanced settings. While Gaussian-based models remain competitive under mild imbalance ratio ($\rho \lesssim 3$), our approach substantially improves generative fairness and diversity in more extreme regimes.</li>
</ul>

<h3>Title: SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Asif Mohammed Saad, Umme Niraj Mahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02156">https://arxiv.org/abs/2509.02156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02156">https://arxiv.org/pdf/2509.02156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02156]] SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis(https://arxiv.org/abs/2509.02156)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Hair artifacts in dermoscopic images present significant challenges for accurate skin lesion analysis, potentially obscuring critical diagnostic features in dermatological assessments. This work introduces a fine-tuned SegFormer model augmented with dropout regularization to achieve precise hair mask segmentation. The proposed SegformerWithDropout architecture leverages the MiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2 output classes, incorporating a dropout probability of 0.3 in the segmentation head to prevent overfitting. Training is conducted on a specialized dataset of 500 dermoscopic skin lesion images with fine-grained hair mask annotations, employing 10-fold cross-validation, AdamW optimization with a learning rate of 0.001, and cross-entropy loss. Early stopping is applied based on validation loss, with a patience of 3 epochs and a maximum of 20 epochs per fold. Performance is evaluated using a comprehensive suite of metrics, including Intersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Experimental results from the cross-validation demonstrate robust performance, with average Dice coefficients reaching approximately 0.96 and IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97), and low LPIPS (0.06), highlighting the model's effectiveness in accurate hair artifact segmentation and its potential to enhance preprocessing for downstream skin cancer detection tasks.</li>
</ul>

<h3>Title: Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pablo Ayuso-Albizu, Juan C. SanMiguel, Pablo Carballeira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02161">https://arxiv.org/abs/2509.02161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02161">https://arxiv.org/pdf/2509.02161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02161]] Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models(https://arxiv.org/abs/2509.02161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance.</li>
</ul>

<h3>Title: Omnidirectional Spatial Modeling from Correlated Panoramas</h3>
<ul>
<li><strong>Authors: </strong>Xinshen Zhang, Tongxi Fu, Xu Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02164">https://arxiv.org/abs/2509.02164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02164">https://arxiv.org/pdf/2509.02164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02164]] Omnidirectional Spatial Modeling from Correlated Panoramas(https://arxiv.org/abs/2509.02164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Omnidirectional scene understanding is vital for various downstream applications, such as embodied AI, autonomous driving, and immersive environments, yet remains challenging due to geometric distortion and complex spatial relations in 360¬∞ imagery. Existing omnidirectional methods achieve scene understanding within a single frame while neglecting cross-frame correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the \textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas visual question answering in the holistic 360¬∞ scenes. CFpano consists of over 2700 images together with over 8000 question-answer pairs, and the question types include both multiple choice and open-ended VQA. Building upon our CFpano, we further present \methodname, a multi-modal large language model (MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of tailored reward functions for robust and consistent reasoning with cross-frame correlated panoramas. Benchmark experiments with existing MLLMs are conducted with our CFpano. The experimental results demonstrate that \methodname achieves state-of-the-art performance across both multiple-choice and open-ended VQA tasks, outperforming strong baselines on all major reasoning categories (\textbf{+5.37\%} in overall performance). Our analyses validate the effectiveness of GRPO and establish a new benchmark for panoramic scene understanding.</li>
</ul>

<h3>Title: Avoidance Decoding for Diverse Multi-Branch Story Generation</h3>
<ul>
<li><strong>Authors: </strong>Kyeongman Park, Nakyeong Yang, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02170">https://arxiv.org/abs/2509.02170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02170">https://arxiv.org/pdf/2509.02170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02170]] Avoidance Decoding for Diverse Multi-Branch Story Generation(https://arxiv.org/abs/2509.02170)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often generate repetitive and monotonous outputs, especially in tasks like story generation, due to limited creative diversity when given the same input prompt. To address this challenge, we propose a novel decoding strategy, Avoidance Decoding, that modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories. This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty, which is prioritized in early stages to diversify initial story concepts, and (2) Narrative-level Similarity Penalty, which is increasingly emphasized later to ensure natural yet diverse plot development. Notably, our method achieves up to 2.6 times higher output diversity and reduces repetition by an average of 30% compared to strong baselines, while effectively mitigating text degeneration. Furthermore, we reveal that our method activates a broader range of neurons, demonstrating that it leverages the model's intrinsic creativity.</li>
</ul>

<h3>Title: A Gentle Introduction to Blind signatures: From RSA to Lattice-based Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Aditya Bhardwaj, P√©ter Kutas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02189">https://arxiv.org/abs/2509.02189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02189">https://arxiv.org/pdf/2509.02189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02189]] A Gentle Introduction to Blind signatures: From RSA to Lattice-based Cryptography(https://arxiv.org/abs/2509.02189)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Blind signatures were first introduced by David Chaum. They allow a user to have a message signed by a signer without revealing the message itself. This property is particularly useful in applications such as electronic voting and digital cash, where user anonymity is important. In a blind signature scheme, the user blinds their message before sending it to the signer, who signs the blinded message. The user then unblinds the signed message to obtain a valid signature that can be verified publicly, ensuring that the signer cannot trace the signed message back to the original unblinded version. A good analogy is placing the message inside an envelope and having the envelope signed. Once the envelope is opened, the signature remains valid for the enclosed message, ensuring that the content remains confidential. Such constructions provide anonymity and privacy to the user but given a practical quantum computer, the security of traditional crypto-systems providing such features will be broken. To address this, the development of quantum-resistant cryptographic protocols is essential for maintaining the security of digital transactions and data. Aligning with the same goal, this work aims to thoroughly review the background of lattice-based blind signatures. We start with the foundations of digital signatures in the classical settings and then move on to lattice-based constructions.</li>
</ul>

<h3>Title: FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Anum Afzal, Juraj Vladika, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02198">https://arxiv.org/abs/2509.02198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02198">https://arxiv.org/pdf/2509.02198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02198]] FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain(https://arxiv.org/abs/2509.02198)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models tend to struggle when dealing with specialized domains. While all aspects of evaluation hold importance, factuality is the most critical one. Similarly, reliable fact-checking tools and data sources are essential for hallucination mitigation. We address these issues by providing a comprehensive Fact-checking Benchmark FActBench covering four generation tasks and six state-of-the-art Large Language Models (LLMs) for the Medical domain. We use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT) Prompting and Natural Language Inference (NLI). Our experiments show that the fact-checking scores acquired through the Unanimous Voting of both techniques correlate best with Domain Expert Evaluation.</li>
</ul>

<h3>Title: Baichuan-M2: Scaling Medical Capability with Large Verifier System</h3>
<ul>
<li><strong>Authors: </strong>Baichuan-M2 Team: Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Chenzheng Zhu, Da Pan, Fei Deng, Guangwei Ai, Guosheng Dong, Hongda Zhang, Jinyang Tai, Jixiang Hong, Kai Lu, Linzhuang Sun, Peidong Guo, Qian Ma, Rihui Xin, Shihui Yang, Shusen Zhang, Yichuan Mo, Zheng Liang, Zhishou Zhang, Hengfu Cui, Zuyi Zhu, Xiaochuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02208">https://arxiv.org/abs/2509.02208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02208">https://arxiv.org/pdf/2509.02208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02208]] Baichuan-M2: Scaling Medical Capability with Large Verifier System(https://arxiv.org/abs/2509.02208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.</li>
</ul>

<h3>Title: ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Binqing Wu, Jianlong Huang, Zongjiang Shang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02217">https://arxiv.org/abs/2509.02217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02217">https://arxiv.org/pdf/2509.02217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02217]] ST-Hyper: Learning High-Order Dependencies Across Multiple Spatial-Temporal Scales for Multivariate Time Series Forecasting(https://arxiv.org/abs/2509.02217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In multivariate time series (MTS) forecasting, many deep learning based methods have been proposed for modeling dependencies at multiple spatial (inter-variate) or temporal (intra-variate) scales. However, existing methods may fail to model dependencies across multiple spatial-temporal scales (ST-scales, i.e., scales that jointly consider spatial and temporal scopes). In this work, we propose ST-Hyper to model the high-order dependencies across multiple ST-scales through adaptive hypergraph modeling. Specifically, we introduce a Spatial-Temporal Pyramid Modeling (STPM) module to extract features at multiple ST-scales. Furthermore, we introduce an Adaptive Hypergraph Modeling (AHM) module that learns a sparse hypergraph to capture robust high-order dependencies among features. In addition, we interact with these features through tri-phase hypergraph propagation, which can comprehensively capture multi-scale spatial-temporal dynamics. Experimental results on six real-world MTS datasets demonstrate that ST-Hyper achieves the state-of-the-art performance, outperforming the best baselines with an average MAE reduction of 3.8\% and 6.8\% for long-term and short-term forecasting, respectively.</li>
</ul>

<h3>Title: Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?</h3>
<ul>
<li><strong>Authors: </strong>Jaime Collado-Monta√±ez, L. Alfonso Ure√±a-L√≥pez, Arturo Montejo-R√°ez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02225">https://arxiv.org/abs/2509.02225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02225">https://arxiv.org/pdf/2509.02225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02225]] Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?(https://arxiv.org/abs/2509.02225)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models offer impressive language capabilities but suffer from well-known limitations, including hallucinations, biases, privacy concerns, and high computational costs. These issues are largely driven by the combination of linguistic competence and factual memorization within a single monolithic model. This paper introduces and empirically supports the Fundamental Language Model (FLM) paradigm, which advocates for smaller, linguistically competent models that offload factual retrieval to external tools. We evaluate models ranging from 135M to 32B parameters across three dimensions: linguistic competence, external factual knowledge, and internal factual knowledge. Our findings reveal that while both linguistic competence and factual knowledge improve with scale, internal factual knowledge grows significantly faster, suggesting that model size is more closely tied to memorization than to core language ability. These results support a modular approach to language modeling, where compact, linguistically proficient models serve as the foundation for tool-augmented systems. The FLM paradigm offers a path toward more efficient, interpretable, and sustainable NLP solutions.</li>
</ul>

<h3>Title: Palmistry-Informed Feature Extraction and Analysis using Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Shweta Patil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02248">https://arxiv.org/abs/2509.02248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02248">https://arxiv.org/pdf/2509.02248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02248]] Palmistry-Informed Feature Extraction and Analysis using Machine Learning(https://arxiv.org/abs/2509.02248)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper explores the automated analysis of palmar features using machine learning techniques. We present a computer vision pipeline that extracts key characteristics from palm images, such as principal line structures, texture, and shape metrics. These features are used to train predictive models on a novel dataset curated from annotated palm images. Our approach moves beyond traditional subjective interpretation by providing a data-driven, quantitative framework for studying the correlations between palmar morphology and externally validated traits or conditions. The methodology demonstrates feasibility for applications in digital anthropometry and personalized user analytics, with potential for deployment on mobile platforms. Results indicate that machine learning models can identify complex patterns in palm data, opening avenues for research that intersects cultural practices with computational analysis.</li>
</ul>

<h3>Title: A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Shan, Qishuai Yu, Jiacen Liu, Shaolin Zhang, Wen Shen, Yanxiao Zhao, Tianyi Wang, Xiaolin Qin, Yiheng Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02256">https://arxiv.org/abs/2509.02256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02256">https://arxiv.org/pdf/2509.02256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02256]] A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients(https://arxiv.org/abs/2509.02256)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Neck pain is the primary symptom of cervical spondylosis, yet its underlying mechanisms remain unclear, leading to uncertain treatment outcomes. To address the challenges of multimodal feature fusion caused by imaging differences and spatial mismatches, this paper proposes an Adaptive Bidirectional Pyramid Difference Convolution (ABPDC) module that facilitates multimodal integration by exploiting the advantages of difference convolution in texture extraction and grayscale invariance, and a Feature Pyramid Registration Auxiliary Network (FPRAN) to mitigate structural misalignment. Experiments on the MMCSD dataset demonstrate that the proposed model achieves superior prediction accuracy of postoperative neck pain recovery compared with existing methods, and ablation studies further confirm its effectiveness.</li>
</ul>

<h3>Title: VariAntNet: Learning Decentralized Control of Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Yigal Koifman, Erez Koifman, Eran Iceland, Ariel Barel, Alfred M. Bruckstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02271">https://arxiv.org/abs/2509.02271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02271">https://arxiv.org/pdf/2509.02271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02271]] VariAntNet: Learning Decentralized Control of Multi-Agent Systems(https://arxiv.org/abs/2509.02271)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>A simple multi-agent system can be effectively utilized in disaster response applications, such as firefighting. Such a swarm is required to operate in complex environments with limited local sensing and no reliable inter-agent communication or centralized control. These simple robotic agents, also known as Ant Robots, are defined as anonymous agents that possess limited sensing capabilities, lack a shared coordinate system, and do not communicate explicitly with one another. A key challenge for simple swarms lies in maintaining cohesion and avoiding fragmentation despite limited-range sensing. Recent advances in machine learning offer effective solutions to some of the classical decentralized control challenges. We propose VariAntNet, a deep learning-based decentralized control model designed to facilitate agent swarming and collaborative task execution. VariAntNet includes geometric features extraction from unordered, variable-sized local observations. It incorporates a neural network architecture trained with a novel, differentiable, multi-objective, mathematically justified loss function that promotes swarm cohesiveness by utilizing the properties of the visibility graph Laplacian matrix. VariAntNet is demonstrated on the fundamental multi-agent gathering task, where agents with bearing-only and limited-range sensing must gather at some location. VariAntNet significantly outperforms an existing analytical solution, achieving more than double the convergence rate while maintaining high swarm connectivity across varying swarm sizes. While the analytical solution guarantees cohesion, it is often too slow in practice. In time-critical scenarios, such as emergency response operations where lives are at risk, slower analytical methods are impractical and justify the loss of some agents within the swarm. This paper presents and analyzes this trade-off in detail.</li>
</ul>

<h3>Title: RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Yingrui Ji, Jiansheng Chen, Jingbo Chen, Anzhi Yue, Chenhao Wang, Kai Li, Yao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02273">https://arxiv.org/abs/2509.02273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02273">https://arxiv.org/pdf/2509.02273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02273]] RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing(https://arxiv.org/abs/2509.02273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection represents a critical challenge in remote sensing applications, where reliable identification of novel or anomalous patterns is essential for autonomous monitoring, disaster response, and environmental assessment. Despite remarkable progress in OOD detection for natural images, existing methods and benchmarks remain poorly suited to remote sensing imagery due to data scarcity, complex multi-scale scene structures, and pronounced distribution shifts. To this end, we propose RS-OOD, a novel framework that leverages remote sensing-specific vision-language modeling to enable robust few-shot OOD detection. Our approach introduces three key innovations: spatial feature enhancement that improved scene discrimination, a dual-prompt alignment mechanism that cross-verifies scene context against fine-grained semantics for spatial-semantic consistency, and a confidence-guided self-training loop that dynamically mines pseudo-labels to expand training data without manual annotation. RS-OOD consistently outperforms existing methods across multiple remote sensing benchmarks and enables efficient adaptation with minimal labeled data, demonstrating the critical value of spatial-semantic integration.</li>
</ul>

<h3>Title: SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</h3>
<ul>
<li><strong>Authors: </strong>Pushpendra Dhakara, Prachi Chachodhia, Vaibhav Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02287">https://arxiv.org/abs/2509.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02287">https://arxiv.org/pdf/2509.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02287]] SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images(https://arxiv.org/abs/2509.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unstructured urban environments present unique challenges for scene understanding and generalization due to their complex and diverse layouts. We introduce SynthGenNet, a self-supervised student-teacher architecture designed to enable robust test-time domain generalization using synthetic multi-source imagery. Our contributions include the novel ClassMix++ algorithm, which blends labeled data from various synthetic sources while maintaining semantic integrity, enhancing model adaptability. We further employ Grounded Mask Consistency Loss (GMC), which leverages source ground truth to improve cross-domain prediction consistency and feature alignment. The Pseudo-Label Guided Contrastive Learning (PLGCL) mechanism is integrated into the student network to facilitate domain-invariant feature learning through iterative knowledge distillation from the teacher network. This self-supervised strategy improves prediction accuracy, addresses real-world variability, bridges the sim-to-real domain gap, and reliance on labeled target data, even in complex urban areas. Outcomes show our model outperforms the state-of-the-art (relying on single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on real-world datasets like Indian Driving Dataset (IDD).</li>
</ul>

<h3>Title: Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers</h3>
<ul>
<li><strong>Authors: </strong>Anuj Gautam, Tarun Yadav, Garrett Smith, Kent Seamons, Scott Ruoti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02289">https://arxiv.org/abs/2509.02289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02289">https://arxiv.org/pdf/2509.02289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02289]] Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers(https://arxiv.org/abs/2509.02289)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Password managers provide significant security benefits to users. However, malicious client-side scripts and browser extensions can steal passwords after the manager has autofilled them into the web page. In this paper, we extend prior work by Stock and Johns, showing how password autofill can be hardened to prevent these local attacks. We implement our design in the Firefox browser and conduct experiments demonstrating that our defense successfully protects passwords from XSS attacks and malicious extensions. We also show that our implementation is compatible with 97% of the Alexa top 1000 websites. Next, we generalize our design, creating a second defense that prevents recently discovered local attacks against the FIDO2 protocols. We implement this second defense into Firefox, demonstrating that it protects the FIDO2 protocol against XSS attacks and malicious extensions. This defense is compatible with all websites, though it does require a small change (2-3 lines) to web servers implementing FIDO2.</li>
</ul>

<h3>Title: LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Katharine Kowalyshyn, Matthias Scheutz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02292">https://arxiv.org/abs/2509.02292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02292">https://arxiv.org/pdf/2509.02292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02292]] LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue(https://arxiv.org/abs/2509.02292)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>What if large language models could not only infer human mindsets but also expose every blind spot in team dialogue such as discrepancies in the team members' joint understanding? We present a novel, two-step framework that leverages large language models (LLMs) both as human-style annotators of team dialogues to track the team's shared mental models (SMMs) and as automated discrepancy detectors among individuals' mental states. In the first step, an LLM generates annotations by identifying SMM elements within task-oriented dialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a secondary LLM compares these LLM-derived annotations and human annotations against gold-standard labels to detect and characterize divergences. We define an SMM coherence evaluation framework for this use case and apply it to six CReST dialogues, ultimately producing: (1) a dataset of human and LLM annotations; (2) a reproducible evaluation framework for SMM coherence; and (3) an empirical assessment of LLM-based discrepancy detection. Our results reveal that, although LLMs exhibit apparent coherence on straightforward natural-language annotation tasks, they systematically err in scenarios requiring spatial reasoning or disambiguation of prosodic cues.</li>
</ul>

<h3>Title: Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sapir Esther Yiflach, Yuval Atzmon, Gal Chechik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02295">https://arxiv.org/abs/2509.02295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02295">https://arxiv.org/pdf/2509.02295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02295]] Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation(https://arxiv.org/abs/2509.02295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models can generate stunning visuals, yet they often fail at tasks children find trivial--like placing a dog to the right of a teddy bear rather than to the left. When combinations get more unusual--a giraffe above an airplane--these failures become even more pronounced. Existing methods attempt to fix these spatial reasoning failures through model fine-tuning or test-time optimization with handcrafted losses that are suboptimal. Rather than imposing our assumptions about spatial encoding, we propose learning these objectives directly from the model's internal representations. We introduce Learn-to-Steer, a novel framework that learns data-driven objectives for test-time optimization rather than handcrafting them. Our key insight is to train a lightweight classifier that decodes spatial relationships from the diffusion model's cross-attention maps, then deploy this classifier as a learned loss function during inference. Training such classifiers poses a surprising challenge: they can take shortcuts by detecting linguistic traces rather than learning true spatial patterns. We solve this with a dual-inversion strategy that enforces geometric understanding. Our method dramatically improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to 0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to multiple relations and significantly improves accuracy.</li>
</ul>

<h3>Title: OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds</h3>
<ul>
<li><strong>Authors: </strong>Longrong Yang, Zhixiong Zeng, Yufeng Zhong, Jing Huang, Liming Zheng, Lei Chen, Haibo Qiu, Zequn Qin, Lin Ma, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02322">https://arxiv.org/abs/2509.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02322">https://arxiv.org/pdf/2509.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02322]] OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds(https://arxiv.org/abs/2509.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models are evolving toward multimodal agents capable of proactively executing tasks. Most agent research focuses on GUI or embodied scenarios, which correspond to agents interacting with 2D virtual worlds or 3D real worlds, respectively. However, many complex tasks typically require agents to interleavely interact with these two types of environment. We initially mix GUI and embodied data to train, but find the performance degeneration brought by the data conflict. Further analysis reveals that GUI and embodied data exhibit synergy and conflict at the shallow and deep layers, respectively, which resembles the cerebrum-cerebellum mechanism in the human brain. To this end, we propose a high-performance generalist agent OmniActor, designed from both structural and data perspectives. First, we propose Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data by separating deep-layer parameters, while leverage their synergy by sharing shallow-layer parameters. By successfully leveraging the synergy and eliminating the conflict, OmniActor outperforms agents only trained by GUI or embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces of GUI and embodied tasks, and collect large-scale GUI and embodied data from various sources for training. This significantly improves OmniActor under different scenarios, especially in GUI tasks. The code will be publicly available.</li>
</ul>

<h3>Title: DCPO: Dynamic Clipping Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02333">https://arxiv.org/abs/2509.02333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02333">https://arxiv.org/pdf/2509.02333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02333]] DCPO: Dynamic Clipping Policy Optimization(https://arxiv.org/abs/2509.02333)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.</li>
</ul>

<h3>Title: RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chih-Yu Lai, Yu-Chien Ning, Duane S. Boning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02341">https://arxiv.org/abs/2509.02341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02341">https://arxiv.org/pdf/2509.02341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02341]] RDIT: Residual-based Diffusion Implicit Models for Probabilistic Time Series Forecasting(https://arxiv.org/abs/2509.02341)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Probabilistic Time Series Forecasting (PTSF) plays a critical role in domains requiring accurate and uncertainty-aware predictions for decision-making. However, existing methods offer suboptimal distribution modeling and suffer from a mismatch between training and evaluation metrics. Surprisingly, we found that augmenting a strong point estimator with a zero-mean Gaussian, whose standard deviation matches its training error, can yield state-of-the-art performance in PTSF. In this work, we propose RDIT, a plug-and-play framework that combines point estimation and residual-based conditional diffusion with a bidirectional Mamba network. We theoretically prove that the Continuous Ranked Probability Score (CRPS) can be minimized by adjusting to an optimal standard deviation and then derive algorithms to achieve distribution matching. Evaluations on eight multivariate datasets across varied forecasting horizons demonstrate that RDIT achieves lower CRPS, rapid inference, and improved coverage compared to strong baselines.</li>
</ul>

<h3>Title: Implicit Reasoning in Large Language Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02350">https://arxiv.org/abs/2509.02350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02350">https://arxiv.org/pdf/2509.02350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02350]] Implicit Reasoning in Large Language Models: A Comprehensive Survey(https://arxiv.org/abs/2509.02350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit this http URL maintain a continuously updated project at: this https URL.</li>
</ul>

<h3>Title: Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Alireza Sedighi Moghaddam, Mohammad Reza Mohammadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02351">https://arxiv.org/abs/2509.02351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02351">https://arxiv.org/pdf/2509.02351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02351]] Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels(https://arxiv.org/abs/2509.02351)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Labeled data is a fundamental component in training supervised deep learning models for computer vision tasks. However, the labeling process, especially for ordinal image classification where class boundaries are often ambiguous, is prone to error and noise. Such label noise can significantly degrade the performance and reliability of machine learning models. This paper addresses the problem of detecting and correcting label noise in ordinal image classification tasks. To this end, a novel data-centric method called ORDinal Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy labels. The proposed approach leverages the capabilities of Label Distribution Learning (LDL) to model the inherent ambiguity and uncertainty present in ordinal labels. During training, ORDAC dynamically adjusts the mean and standard deviation of the label distribution for each sample. Rather than discarding potentially noisy samples, this approach aims to correct them and make optimal use of the entire training dataset. The effectiveness of the proposed method is evaluated on benchmark datasets for age estimation (Adience) and disease severity detection (Diabetic Retinopathy) under various asymmetric Gaussian noise scenarios. Results show that ORDAC and its extended versions (ORDAC_C and ORDAC_R) lead to significant improvements in model performance. For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to 0.49. The method also demonstrated its effectiveness in correcting intrinsic noise present in the original datasets. This research indicates that adaptive label correction using label distributions is an effective strategy to enhance the robustness and accuracy of ordinal classification models in the presence of noisy data.</li>
</ul>

<h3>Title: Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology</h3>
<ul>
<li><strong>Authors: </strong>Caterina Fuster-Barcelo, Gonzalo R. Rios-Munoz, Arrate Munoz-Barrutia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02355">https://arxiv.org/abs/2509.02355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02355">https://arxiv.org/pdf/2509.02355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02355]] Scaffolding Collaborative Learning in STEM: A Two-Year Evaluation of a Tool-Integrated Project-Based Methodology(https://arxiv.org/abs/2509.02355)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This study examines the integration of digital collaborative tools and structured peer evaluation in the Machine Learning for Health master's program, through the redesign of a Biomedical Image Processing course over two academic years. The pedagogical framework combines real-time programming with Google Colab, experiment tracking and reporting via Weights & Biases, and rubric-guided peer assessment to foster student engagement, transparency, and fair evaluation. Compared to a pre-intervention cohort, the two implementation years showed increased grade dispersion and higher entropy in final project scores, suggesting improved differentiation and fairness in assessment. The survey results further indicate greater student engagement with the subject and their own learning process. These findings highlight the potential of integrating tool-supported collaboration and structured evaluation mechanisms to enhance both learning outcomes and equity in STEM education.</li>
</ul>

<h3>Title: Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zeren Xiong, Zikun Chen, Zedong Zhang, Xiang Li, Ying Tai, Jian Yang, Jun Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02357">https://arxiv.org/abs/2509.02357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02357">https://arxiv.org/pdf/2509.02357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02357]] Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion(https://arxiv.org/abs/2509.02357)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle a new task of 3D object synthesis, where a 3D model is composited with another object category to create a novel 3D model. However, most existing text/image/3D-to-3D methods struggle to effectively integrate multiple content sources, often resulting in inconsistent textures and inaccurate shapes. To overcome these challenges, we propose a straightforward yet powerful approach, category+3D-to-3D (C33D), for generating novel and structurally coherent 3D models. Our method begins by rendering multi-view images and normal maps from the input 3D model, then generating a novel 2D object using adaptive text-image harmony (ATIH) with the front-view image and a text description from another object category as inputs. To ensure texture consistency, we introduce texture multi-view diffusion, which refines the textures of the remaining multi-view RGB images based on the novel 2D object. For enhanced shape accuracy, we propose shape multi-view diffusion to improve the 2D shapes of both the multi-view RGB images and the normal maps, also conditioned on the novel 2D object. Finally, these outputs are used to reconstruct a complete and novel 3D model. Extensive experiments demonstrate the effectiveness of our method, yielding impressive 3D creations, such as shark(3D)-crocodile(text) in the first row of Fig. 1. A project page is available at: this https URL</li>
</ul>

<h3>Title: Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture</h3>
<ul>
<li><strong>Authors: </strong>Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02359">https://arxiv.org/abs/2509.02359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02359">https://arxiv.org/pdf/2509.02359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02359]] Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture(https://arxiv.org/abs/2509.02359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spatial understanding is essential for Multimodal Large Language Models (MLLMs) to support perception, reasoning, and planning in embodied environments. Despite recent progress, existing studies reveal that MLLMs still struggle with spatial understanding. However, existing research lacks a comprehensive and systematic evaluation of these limitations, often restricted to isolated scenarios, such as single-view or video. In this work, we present a systematic analysis of spatial understanding from both data and architectural perspectives across three representative scenarios: single-view, multi-view, and video. We propose a benchmark named MulSeT (Multi-view Spatial Understanding Tasks), and design a series of experiments to analyze the spatial reasoning capabilities of MLLMs. From the data perspective, the performance of spatial understanding converges quickly as the training data increases, and the upper bound is relatively low, especially for tasks that require spatial imagination. This indicates that merely expanding training data is insufficient to achieve satisfactory performance. From the architectural perspective, we find that spatial understanding relies more heavily on the positional encoding within the visual encoder than within the language model, in both cascaded and native MLLMs. Moreover, we explore reasoning injection and envision future improvements through architectural design to optimize spatial understanding. These insights shed light on the limitations of current MLLMs and suggest new directions for improving spatial reasoning capabilities through data scaling and architectural tuning.</li>
</ul>

<h3>Title: Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Negi, Atul Kr. Ojha, Omnia Zayed, Paul Buitelaar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02363">https://arxiv.org/abs/2509.02363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02363">https://arxiv.org/pdf/2509.02363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02363]] Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models(https://arxiv.org/abs/2509.02363)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>We propose a scalable method for constructing a temporal opinion knowledge base with large language models (LLMs) as automated annotators. Despite the demonstrated utility of time-series opinion analysis of text for downstream applications such as forecasting and trend analysis, existing methodologies underexploit this potential due to the absence of temporally grounded fine-grained annotations. Our approach addresses this gap by integrating well-established opinion mining formulations into a declarative LLM annotation pipeline, enabling structured opinion extraction without manual prompt engineering. We define three data models grounded in sentiment and opinion mining literature, serving as schemas for structured representation. We perform rigorous quantitative evaluation of our pipeline using human-annotated test samples. We carry out the final annotations using two separate LLMs, and inter-annotator agreement is computed label-wise across the fine-grained opinion dimensions, analogous to human annotation protocols. The resulting knowledge base encapsulates time-aligned, structured opinions and is compatible with applications in Retrieval-Augmented Generation (RAG), temporal question answering, and timeline summarisation.</li>
</ul>

<h3>Title: Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Chen, Tara Saba, Xun Deng, Xujie Si, Fan Long</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02372">https://arxiv.org/abs/2509.02372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02372">https://arxiv.org/pdf/2509.02372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02372]] Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs(https://arxiv.org/abs/2509.02372)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become critical to modern software development, but their reliance on internet datasets for training introduces a significant security risk: the absorption and reproduction of malicious content. To evaluate this threat, this paper introduces a scalable, automated audit framework that synthesizes innocuous, developer-style prompts from known scam databases to query production LLMs and determine if they generate code containing harmful URLs. We conducted a large-scale evaluation across four production LLMs (GPT-4o, GPT-4o-mini, Llama-4-Scout, and DeepSeek-V3), and found a systemic vulnerability, with all tested models generating malicious code at a non-negligible rate. On average, 4.2\% of programs generated in our experiments contained malicious URLs. Crucially, this malicious code is often generated in response to benign prompts. We manually validate the prompts which cause all four LLMs to generate malicious code, and resulting in 177 innocuous prompts that trigger all models to produce harmful outputs. These results provide strong empirical evidence that the training data of production LLMs has been successfully poisoned at scale, underscoring the urgent need for more robust defense mechanisms and post-generation safety checks to mitigate the propagation of hidden security threats.</li>
</ul>

<h3>Title: MedDINOv3: How to adapt vision foundation models for medical image segmentation?</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02379">https://arxiv.org/abs/2509.02379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02379">https://arxiv.org/pdf/2509.02379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02379]] MedDINOv3: How to adapt vision foundation models for medical image segmentation?(https://arxiv.org/abs/2509.02379)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce \textbf{MedDINOv3}, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on \textbf{CT-3M}, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at this https URL.</li>
</ul>

<h3>Title: Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems</h3>
<ul>
<li><strong>Authors: </strong>Rye Stahle-Smith, Rasha Karakchi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02387">https://arxiv.org/abs/2509.02387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02387">https://arxiv.org/pdf/2509.02387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02387]] Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems(https://arxiv.org/abs/2509.02387)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The growing use of FPGAs in reconfigurable systems introducessecurity risks through malicious bitstreams that could cause denial-of-service (DoS), data leakage, or covert attacks. We investigated chip-level hardware malicious payload in embedded systems and proposed a supervised machine learning method to detect malicious bitstreams via static byte-level features. Our approach diverges from existing methods by analyzing bitstreams directly at the binary level, enabling real-time detection without requiring access to source code or netlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and re-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset included 122 samples of benign and malicious configurations. The data were vectorized using byte frequency analysis, compressed using TSVD, and balanced using SMOTE to address class imbalance. The evaluated classifiers demonstrated that Random Forest achieved a macro F1-score of 0.97, underscoring the viability of real-time Trojan detection on resource-constrained systems. The final model was serialized and successfully deployed via PYNQ to enable integrated bitstream analysis.</li>
</ul>

<h3>Title: Gaming and Cooperation in Federated Learning: What Can Happen and How to Monitor It</h3>
<ul>
<li><strong>Authors: </strong>Dongseok Kim, Wonjun Jeong, Gisung Oh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02391">https://arxiv.org/abs/2509.02391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02391">https://arxiv.org/pdf/2509.02391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02391]] Gaming and Cooperation in Federated Learning: What Can Happen and How to Monitor It(https://arxiv.org/abs/2509.02391)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>The success of Federated Learning depends on the actions that participants take out of sight. We model Federated Learning not as a mere optimization task but as a strategic system entangled with rules and incentives. From this perspective, we present an analytical framework that makes it possible to clearly identify where behaviors that genuinely improve performance diverge from those that merely target metrics. We introduce two indices that respectively quantify behavioral incentives and collective performance loss, and we use them as the basis for consistently interpreting the impact of operational choices such as rule design, the level of information disclosure, evaluation methods, and aggregator switching. We further summarize thresholds, auto-switch rules, and early warning signals into a checklist that can be applied directly in practice, and we provide both a practical algorithm for allocating limited audit resources and a performance guarantee. Simulations conducted across diverse environments consistently validate the patterns predicted by our framework, and we release all procedures for full reproducibility. While our approach operates most strongly under several assumptions, combining periodic recalibration, randomization, and connectivity-based alarms enables robust application under the variability of real-world operations. We present both design principles and operational guidelines that lower the incentives for metric gaming while sustaining and expanding stable cooperation.</li>
</ul>

<h3>Title: Evaluating Cumulative Spectral Gradient as a Complexity Measure</h3>
<ul>
<li><strong>Authors: </strong>Haji Gul, Abdul Ghani Naim, Ajaz Ahmad Bhat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02399">https://arxiv.org/abs/2509.02399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02399">https://arxiv.org/pdf/2509.02399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02399]] Evaluating Cumulative Spectral Gradient as a Complexity Measure(https://arxiv.org/abs/2509.02399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate estimation of dataset complexity is crucial for evaluating and comparing link prediction models for knowledge graphs (KGs). The Cumulative Spectral Gradient (CSG) metric derived from probabilistic divergence between classes within a spectral clustering framework was proposed as a dataset complexity measure that (1) naturally scales with the number of classes and (2) correlates strongly with downstream classification performance. In this work, we rigorously assess CSG behavior on standard knowledge graph link prediction benchmarks a multi class tail prediction task, using two key parameters governing its computation, M, the number of Monte Carlo sampled points per class, and K, the number of nearest neighbors in the embedding space. Contrary to the original claims, we find that (1) CSG is highly sensitive to the choice of K and therefore does not inherently scale with the number of target classes, and (2) CSG values exhibit weak or no correlation with established performance metrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237, WN18RR, and other standard datasets, we demonstrate that CSG purported stability and generalization predictive power break down in link prediction settings. Our results highlight the need for more robust, classifier agnostic complexity measures in KG link prediction evaluation.</li>
</ul>

<h3>Title: Cache Management for Mixture-of-Experts LLMs -- extended version</h3>
<ul>
<li><strong>Authors: </strong>Spyros Angelopoulos, Loris Marchal, Adrien Obrecht, Bertrand Simon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02408">https://arxiv.org/abs/2509.02408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02408">https://arxiv.org/pdf/2509.02408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02408]] Cache Management for Mixture-of-Experts LLMs -- extended version(https://arxiv.org/abs/2509.02408)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory. In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand. Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.</li>
</ul>

<h3>Title: A Survey: Towards Privacy and Security in Mobile Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Honghui Xu, Kaiyang Li, Wei Chen, Danyang Zheng, Zhiyuan Li, Zhipeng Cai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02411">https://arxiv.org/abs/2509.02411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02411">https://arxiv.org/pdf/2509.02411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02411]] A Survey: Towards Privacy and Security in Mobile Large Language Models(https://arxiv.org/abs/2509.02411)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust, membership infer, federate, large language model</a></li>
<li><strong>Abstract: </strong>Mobile Large Language Models (LLMs) are revolutionizing diverse fields such as healthcare, finance, and education with their ability to perform advanced natural language processing tasks on-the-go. However, the deployment of these models in mobile and edge environments introduces significant challenges related to privacy and security due to their resource-intensive nature and the sensitivity of the data they process. This survey provides a comprehensive overview of privacy and security issues associated with mobile LLMs, systematically categorizing existing solutions such as differential privacy, federated learning, and prompt encryption. Furthermore, we analyze vulnerabilities unique to mobile LLMs, including adversarial attacks, membership inference, and side-channel attacks, offering an in-depth comparison of their effectiveness and limitations. Despite recent advancements, mobile LLMs face unique hurdles in achieving robust security while maintaining efficiency in resource-constrained environments. To bridge this gap, we propose potential applications, discuss open challenges, and suggest future research directions, paving the way for the development of trustworthy, privacy-compliant, and scalable mobile LLM systems.</li>
</ul>

<h3>Title: APEX: Automatic Event Sequence Generation for Android Applications</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Chen, Morris Chang, Witawas Srisa-an, Yong Guan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02412">https://arxiv.org/abs/2509.02412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02412">https://arxiv.org/pdf/2509.02412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02412]] APEX: Automatic Event Sequence Generation for Android Applications(https://arxiv.org/abs/2509.02412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Due to the event driven nature and the versatility of GUI designs in Android programs, it is challenging to generate event sequences with adequate code coverage within a reasonable time. A common approach to handle this issue is to rely on GUI models to generate event sequences. These sequences can be effective in covering GUI states, but inconsistent in exposing program behaviors that require specific inputs. A major obstacle to generate such specific inputs is the lack of a systematic GUI exploration process to accommodate the analysis requirements. In this paper, we introduce Android Path Explorer (APEX), a systematic input generation framework using concolic execution. APEX addresses the limitations of model-based sequence generation by using concolic execution to discover the data dependencies of GUI state transitions. Moreover, concolic execution is also used to prioritize events during the exploration of GUI, which leads to a more robust model and accurate input generation. The key novelty of APEX is that concolic execution is not only used to construct event sequences, but also used to traverse the GUI more systematically. As such, our experimental results show that APEX can be used to generate a set of event sequences that achieve high code coverage, as well as event sequences that reach specific targets.</li>
</ul>

<h3>Title: Enabling decision support over confidential data</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Marangone, Eugenio Nerio Nemmi, Daniele Friolo, Giuseppe Ateniese, Ingo Weber, Claudio Di Ciccio</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02413">https://arxiv.org/abs/2509.02413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02413">https://arxiv.org/pdf/2509.02413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02413]] Enabling decision support over confidential data(https://arxiv.org/abs/2509.02413)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Enabling automated decision-making processes by leveraging data-driven analysis is a core goal of Decision Support Systems (DSSs). In multi-party scenarios where decisions rely on distributed and sensitive data, though, ensuring confidentiality, verifiability, transparency, integrity, and consistency at once remains an open challenge for DSSs. To tackle this multi-faceted problem, we propose the Secure Platform for Automated decision Rules via Trusted Applications (SPARTA) approach. By leveraging Trusted Execution Environments (TEEs) at its core, SPARTA ensures that the decision logic and the data remain protected. To guarantee transparency and consistency of the decision process, SPARTA encodes decision rules into verifiable software objects deployed within TEEs. To maintain the confidentiality of the outcomes while keeping the information integrity, SPARTA employs cryptography techniques on notarized data based on user-definable access policies. Based on experiments conducted on public benchmarks and synthetic data, we find our approach to be practically applicable and scalable.</li>
</ul>

<h3>Title: From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tao Wang, Zhenxuan Zhang, Yuanbo Zhou, Xinlin Zhang, Yuanbin Chen, Tao Tan, Guang Yang, Tong Tong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02419">https://arxiv.org/abs/2509.02419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02419">https://arxiv.org/pdf/2509.02419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02419]] From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation(https://arxiv.org/abs/2509.02419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at this https URL.</li>
</ul>

<h3>Title: Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation</h3>
<ul>
<li><strong>Authors: </strong>Lydia Kin Ching Chau, Zhi Yu, Ruo Wei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02445">https://arxiv.org/abs/2509.02445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02445">https://arxiv.org/pdf/2509.02445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02445]] Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation(https://arxiv.org/abs/2509.02445)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, fair</a></li>
<li><strong>Abstract: </strong>We present a novel framework for real-time virtual makeup try-on that achieves high-fidelity, identity-preserving cosmetic transfer with robust temporal consistency. In live makeup transfer applications, it is critical to synthesize temporally coherent results that accurately replicate fine-grained makeup and preserve user's identity. However, existing methods often struggle to disentangle semitransparent cosmetics from skin tones and other identify features, causing identity shifts and raising fairness concerns. Furthermore, current methods lack real-time capabilities and fail to maintain temporal consistency, limiting practical adoption. To address these challenges, we decouple makeup transfer into two steps: transparent makeup mask extraction and graphics-based mask rendering. After the makeup extraction step, the makeup rendering can be performed in real time, enabling live makeup try-on. Our makeup extraction model trained on pseudo-ground-truth data generated via two complementary methods: a graphics-based rendering pipeline and an unsupervised k-means clustering approach. To further enhance transparency estimation and color fidelity, we propose specialized training objectives, including alpha-weighted reconstruction and lip color losses. Our method achieves robust makeup transfer across diverse poses, expressions, and skin tones while preserving temporal smoothness. Extensive experiments demonstrate that our approach outperforms existing baselines in capturing fine details, maintaining temporal stability, and preserving identity integrity.</li>
</ul>

<h3>Title: An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ali Hamdi, Malak Mohamed, Rokaia Emad, Khaled Shaban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02446">https://arxiv.org/abs/2509.02446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02446">https://arxiv.org/pdf/2509.02446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02446]] An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction(https://arxiv.org/abs/2509.02446)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Social telehealth has made remarkable progress in healthcare by allowing patients to post symptoms and participate in medical consultations remotely. Users frequently post symptoms on social media and online health platforms, creating a huge repository of medical data that can be leveraged for disease classification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along with transformer-based models like BERT, have demonstrated strong capabilities in processing complex medical text. In this study, we evaluate three Arabic medical text preprocessing methods such as summarization, refinement, and Named Entity Recognition (NER) before applying fine-tuned Arabic transformer models (CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a majority voting ensemble that combines predictions from original and preprocessed text representations. This approach achieved the best classification accuracy of 80.56%, thus showing its effectiveness in leveraging various text representations and model predictions to improve the understanding of medical texts. To the best of our knowledge, this is the first work that integrates LLM-based preprocessing with fine-tuned Arabic transformer models and ensemble learning for disease classification in Arabic social telehealth data.</li>
</ul>

<h3>Title: EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling</h3>
<ul>
<li><strong>Authors: </strong>Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02450">https://arxiv.org/abs/2509.02450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02450">https://arxiv.org/pdf/2509.02450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02450]] EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling(https://arxiv.org/abs/2509.02450)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Personality detection from text is commonly performed by analysing users' social media posts. However, existing methods heavily rely on large-scale annotated datasets, making it challenging to obtain high-quality personality labels. Moreover, most studies treat emotion and personality as independent variables, overlooking their interactions. In this paper, we propose a novel self-supervised framework, EmoPerso, which improves personality detection through emotion-aware modelling. EmoPerso first leverages generative mechanisms for synthetic data augmentation and rich representation learning. It then extracts pseudo-labeled emotion features and jointly optimizes them with personality prediction via multi-task learning. A cross-attention module is employed to capture fine-grained interactions between personality traits and the inferred emotional representations. To further refine relational reasoning, EmoPerso adopts a self-taught strategy to enhance the model's reasoning capabilities iteratively. Extensive experiments on two benchmark datasets demonstrate that EmoPerso surpasses state-of-the-art models. The source code is available at this https URL.</li>
</ul>

<h3>Title: RiverScope: High-Resolution River Masking Dataset</h3>
<ul>
<li><strong>Authors: </strong>Rangel Daroya, Taylor Rowley, Jonathan Flores, Elisa Friedmann, Fiona Bennitt, Heejin An, Travis Simmons, Marissa Jean Hughes, Camryn L Kluetmeier, Solomon Kica, J. Daniel V√©lez, Sarah E. Esenther, Thomas E. Howard, Yanqi Ye, Audrey Turcotte, Colin Gleason, Subhransu Maji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02451">https://arxiv.org/abs/2509.02451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02451">https://arxiv.org/pdf/2509.02451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02451]] RiverScope: High-Resolution River Masking Dataset(https://arxiv.org/abs/2509.02451)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Surface water dynamics play a critical role in Earth's climate system, influencing ecosystems, agriculture, disaster resilience, and sustainable development. Yet monitoring rivers and surface water at fine spatial and temporal scales remains challenging -- especially for narrow or sediment-rich rivers that are poorly captured by low-resolution satellite data. To address this, we introduce RiverScope, a high-resolution dataset developed through collaboration between computer science and hydrology experts. RiverScope comprises 1,145 high-resolution images (covering 2,577 square kilometers) with expert-labeled river and surface water masks, requiring over 100 hours of manual annotation. Each image is co-registered with Sentinel-2, SWOT, and the SWOT River Database (SWORD), enabling the evaluation of cost-accuracy trade-offs across sensors -- a key consideration for operational water monitoring. We also establish the first global, high-resolution benchmark for river width estimation, achieving a median error of 7.2 meters -- significantly outperforming existing satellite-derived methods. We extensively evaluate deep networks across multiple architectures (e.g., CNNs and transformers), pretraining strategies (e.g., supervised and self-supervised), and training datasets (e.g., ImageNet and satellite imagery). Our best-performing models combine the benefits of transfer learning with the use of all the multispectral PlanetScope channels via learned adaptors. RiverScope provides a valuable resource for fine-scale and multi-sensor hydrological modeling, supporting climate adaptation and sustainable water management.</li>
</ul>

<h3>Title: Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions</h3>
<ul>
<li><strong>Authors: </strong>Seyedali Mohammadi, Bhaskara Hanuma Vedula, Hemank Lamba, Edward Raff, Ponnurangam Kumaraguru, Francis Ferraro, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02452">https://arxiv.org/abs/2509.02452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02452">https://arxiv.org/pdf/2509.02452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02452]] Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions(https://arxiv.org/abs/2509.02452)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Do LLMs genuinely incorporate external definitions, or do they primarily rely on their parametric knowledge? To address these questions, we conduct controlled experiments across multiple explanation benchmark datasets (general and domain-specific) and label definition conditions, including expert-curated, LLM-generated, perturbed, and swapped definitions. Our results reveal that while explicit label definitions can enhance accuracy and explainability, their integration into an LLM's task-solving processes is neither guaranteed nor consistent, suggesting reliance on internalized representations in many cases. Models often default to their internal representations, particularly in general tasks, whereas domain-specific tasks benefit more from explicit definitions. These findings underscore the need for a deeper understanding of how LLMs process external knowledge alongside their pre-existing capabilities.</li>
</ul>

<h3>Title: Generative Sequential Notification Optimization via Multi-Objective Decision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Borja Ocejo, Ruofan Wang, Ke Liu, Rohit K. Patra, Haotian Shen, David Liu, Yiwen Yuan, Gokulraj Mohanasundaram, Fedor Borisyuk, Prakruthi Prabhakar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02458">https://arxiv.org/abs/2509.02458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02458">https://arxiv.org/pdf/2509.02458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02458]] Generative Sequential Notification Optimization via Multi-Objective Decision Transformers(https://arxiv.org/abs/2509.02458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Notifications are an important communication channel for delivering timely and relevant information. Optimizing their delivery involves addressing complex sequential decision-making challenges under constraints such as message utility and user fatigue. Offline reinforcement learning (RL) methods, such as Conservative Q-Learning (CQL), have been applied to this problem but face practical challenges at scale, including instability, sensitivity to distribution shifts, limited reproducibility, and difficulties with explainability in high-dimensional recommendation settings. We present a Decision Transformer (DT) based framework that reframes policy learning as return-conditioned supervised learning, improving robustness, scalability, and modeling flexibility. Our contributions include a real-world comparison with CQL, a multi-reward design suitable for non-episodic tasks, a quantile regression approach to return-to-go conditioning, and a production-ready system with circular buffer-based sequence processing for near-real-time inference. Extensive offline and online experiments in a deployed notification system show that our approach improves notification utility and overall session activity while minimizing user fatigue. Compared to a multi-objective CQL-based agent, the DT-based approach achieved a +0.72% increase in sessions for notification decision-making at LinkedIn by making notification recommendation more relevant.</li>
</ul>

<h3>Title: GenCompositor: Generative Video Compositing with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02460">https://arxiv.org/abs/2509.02460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02460">https://arxiv.org/pdf/2509.02460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02460]] GenCompositor: Generative Video Compositing with Diffusion Transformer(https://arxiv.org/abs/2509.02460)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.</li>
</ul>

<h3>Title: TeRA: Rethinking Text-driven Realistic 3D Avatar Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanwen Wang, Yiyu Zhuang, Jiawei Zhang, Li Wang, Yifei Zeng, Xun Cao, Xinxin Zuo, Hao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02466">https://arxiv.org/abs/2509.02466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02466">https://arxiv.org/pdf/2509.02466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02466]] TeRA: Rethinking Text-driven Realistic 3D Avatar Generation(https://arxiv.org/abs/2509.02466)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we rethink text-to-avatar generative models by proposing TeRA, a more efficient and effective framework than the previous SDS-based models and general large 3D generative this http URL approach employs a two-stage training strategy for learning a native 3D avatar generative model. Initially, we distill a decoder to derive a structured latent space from a large human reconstruction model. Subsequently, a text-controlled latent diffusion model is trained to generate photorealistic 3D human avatars within this latent space. TeRA enhances the model performance by eliminating slow iterative optimization and enables text-based partial customization through a structured 3D human this http URL have proven our approach's superiority over previous text-to-avatar generative models in subjective and objective evaluation.</li>
</ul>

<h3>Title: Exploring Variational Graph Autoencoders for Distribution Grid Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Syed Zain Abbas, Ehimare Okoyomon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02469">https://arxiv.org/abs/2509.02469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02469">https://arxiv.org/pdf/2509.02469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02469]] Exploring Variational Graph Autoencoders for Distribution Grid Data Generation(https://arxiv.org/abs/2509.02469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>To address the lack of public power system data for machine learning research in energy networks, we investigate the use of variational graph autoencoders (VGAEs) for synthetic distribution grid generation. Using two open-source datasets, ENGAGE and DINGO, we evaluate four decoder variants and compare generated networks against the original grids using structural and spectral metrics. Results indicate that simple decoders fail to capture realistic topologies, while GCN-based approaches achieve strong fidelity on ENGAGE but struggle on the more complex DINGO dataset, producing artifacts such as disconnected components and repeated motifs. These findings highlight both the promise and limitations of VGAEs for grid synthesis, underscoring the need for more expressive generative models and robust evaluation. We release our models and analysis as open source to support benchmarking and accelerate progress in ML-driven power system research.</li>
</ul>

<h3>Title: SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02479">https://arxiv.org/abs/2509.02479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02479">https://arxiv.org/pdf/2509.02479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02479]] SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning(https://arxiv.org/abs/2509.02479)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.</li>
</ul>

<h3>Title: HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction</h3>
<ul>
<li><strong>Authors: </strong>Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02481">https://arxiv.org/abs/2509.02481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02481">https://arxiv.org/pdf/2509.02481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02481]] HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction(https://arxiv.org/abs/2509.02481)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate flood forecasting remains a challenge for water-resource management, as it demands modeling of local, time-varying runoff drivers (e.g., rainfall-induced peaks, baseflow trends) and complex spatial interactions across a river network. Traditional data-driven approaches, such as convolutional networks and sequence-based models, ignore topological information about the region. Graph Neural Networks (GNNs) propagate information exactly along the river network, which is ideal for learning hydrological routing. However, state-of-the-art GNN-based flood prediction models collapse pixels to coarse catchment polygons as the cost of training explodes with graph size and higher resolution. Furthermore, most existing methods treat spatial and temporal dependencies separately, either applying GNNs solely on spatial graphs or transformers purely on temporal sequences, thus failing to simultaneously capture spatiotemporal interactions critical for accurate flood prediction. We introduce a heterogenous basin graph where every land and river pixel is a node connected by physical hydrological flow directions and inter-catchment relationships. We propose HydroGAT, a spatiotemporal network that adaptively learns local temporal importance and the most influential upstream locations. Evaluated in two Midwestern US basins and across five baseline architectures, our model achieves higher NSE (up to 0.97), improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourly discharge prediction, while offering interpretable attention maps that reveal sparse, structured intercatchment influences. To support high-resolution basin-scale training, we develop a distributed data-parallel pipeline that scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer, demonstrating up to 15x speedup across machines. Our code is available at this https URL.</li>
</ul>

<h3>Title: Anisotropic Fourier Features for Positional Encoding in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Nabil Jabareen, Dongsheng Yuan, Dingming Liu, Foo-Wei Ten, S√∂ren Lukassen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02488">https://arxiv.org/abs/2509.02488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02488">https://arxiv.org/pdf/2509.02488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02488]] Anisotropic Fourier Features for Positional Encoding in Medical Imaging(https://arxiv.org/abs/2509.02488)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The adoption of Transformer-based architectures in the medical domain is growing rapidly. In medical imaging, the analysis of complex shapes - such as organs, tissues, or other anatomical structures - combined with the often anisotropic nature of high-dimensional images complicates these adaptations. In this study, we critically examine the role of Positional Encodings (PEs), arguing that commonly used approaches may be suboptimal for the specific challenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have proven effective in vision tasks, but they struggle to preserve Euclidean distances in higher-dimensional spaces. Isotropic Fourier Feature Positional Encodings (IFPEs) have been proposed to better preserve Euclidean distances, but they lack the ability to account for anisotropy in images. To address these limitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE), a generalization of IFPE that incorporates anisotropic, class-specific, and domain-specific spatial dependencies. We systematically benchmark AFPE against commonly used PEs on multi-label classification in chest X-rays, organ classification in CT images, and ejection fraction regression in echocardiography. Our results demonstrate that choosing the correct PE can significantly improve model performance. We show that the optimal PE depends on the shape of the structure of interest and the anisotropy of the data. Finally, our proposed AFPE significantly outperforms state-of-the-art PEs in all tested anisotropic settings. We conclude that, in anisotropic medical images and videos, it is of paramount importance to choose an anisotropic PE that fits the data and the shape of interest.</li>
</ul>

<h3>Title: GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chenglong Wang, Yongyu Mu, Hang Zhou, Yifu Huo, Ziming Zhu, Jiali Zeng, Murun Yang, Bei Li, Tong Xiao, Xiaoyang Hao, Chunliang Zhang, Fandong Meng, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02492">https://arxiv.org/abs/2509.02492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02492">https://arxiv.org/pdf/2509.02492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02492]] GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning(https://arxiv.org/abs/2509.02492)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.</li>
</ul>

<h3>Title: MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</h3>
<ul>
<li><strong>Authors: </strong>Junxi Wu, Jinpeng Wang, Zheng Liu, Bin Chen, Dongjian Hu, Hao Wu, Shu-Tao Xiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02499">https://arxiv.org/abs/2509.02499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02499">https://arxiv.org/pdf/2509.02499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02499]] MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds(https://arxiv.org/abs/2509.02499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at this https URL.</li>
</ul>

<h3>Title: L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Nishant Tanksale, Tanmay Kokate, Darshan Gohad, Sarvadnyaa Barate, Raviraj Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02503">https://arxiv.org/abs/2509.02503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02503">https://arxiv.org/pdf/2509.02503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02503]] L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages(https://arxiv.org/abs/2509.02503)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Semantic evaluation in low-resource languages remains a major challenge in NLP. While sentence transformers have shown strong performance in high-resource settings, their effectiveness in Indic languages is underexplored due to a lack of high-quality benchmarks. To bridge this gap, we introduce L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada, Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000 news articles paired with four headline variants: the original, a semantically similar version, a lexically similar version, and an unrelated one, designed to test fine-grained semantic understanding. The task requires selecting the correct headline from the options using article-headline similarity. We benchmark several sentence transformers, including multilingual and language-specific models, using cosine similarity. Results show that multilingual models consistently perform well, while language-specific models vary in effectiveness. Given the rising use of similarity models in Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a valuable resource for evaluating and improving semantic understanding in such applications. Additionally, the dataset can be repurposed for multiple-choice question answering, headline classification, or other task-specific evaluations of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared publicly at this https URL</li>
</ul>

<h3>Title: Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Erfan Baghaei Potraghloo, Seyedarmin Azizi, Souvik Kundu, Massoud Pedram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02510">https://arxiv.org/abs/2509.02510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02510">https://arxiv.org/pdf/2509.02510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02510]] Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation(https://arxiv.org/abs/2509.02510)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Fitness Movement Recognition with Attention Mechanism and Pre-Trained Feature Extractors</h3>
<ul>
<li><strong>Authors: </strong>Shanjid Hasan Nishat, Srabonti Deb, Mohiuddin Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02511">https://arxiv.org/abs/2509.02511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02511">https://arxiv.org/pdf/2509.02511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02511]] Enhancing Fitness Movement Recognition with Attention Mechanism and Pre-Trained Feature Extractors(https://arxiv.org/abs/2509.02511)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fitness movement recognition, a focused subdomain of human activity recognition (HAR), plays a vital role in health monitoring, rehabilitation, and personalized fitness training by enabling automated exercise classification from video data. However, many existing deep learning approaches rely on computationally intensive 3D models, limiting their feasibility in real-time or resource-constrained settings. In this paper, we present a lightweight and effective framework that integrates pre-trained 2D Convolutional Neural Networks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT) with a Long Short-Term Memory (LSTM) network enhanced by spatial attention. These models efficiently extract spatial features while the LSTM captures temporal dependencies, and the attention mechanism emphasizes informative segments. We evaluate the framework on a curated subset of the UCF101 dataset, achieving a peak accuracy of 93.34\% with the ResNet50-based configuration. Comparative results demonstrate the superiority of our approach over several state-of-the-art HAR systems. The proposed method offers a scalable and real-time-capable solution for fitness activity recognition with broader applications in vision-based health and activity monitoring.</li>
</ul>

<h3>Title: Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mayur Shirke, Amey Shembade, Pavan Thorat, Madhushri Wagh, Raviraj Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02514">https://arxiv.org/abs/2509.02514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02514">https://arxiv.org/pdf/2509.02514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02514]] Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition(https://arxiv.org/abs/2509.02514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English (Hinglish), presents unique challenges due to informal structure, transliteration, and frequent language switching. This study conducts a comparative evaluation of code-mixed fine-tuned models and non-code-mixed multilingual models, along with zero-shot generative large language models (LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained on code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained on non-code-mixed multilingual data). We also assess the performance of Google Gemini in a zero-shot setting using a modified version of the dataset with NER tags removed. All models are tested on a benchmark Hinglish NER dataset using Precision, Recall, and F1-score. Results show that code-mixed models, particularly HingRoBERTa and HingBERT-based fine-tuned models, outperform others - including closed-source LLMs like Google Gemini - due to domain-specific pretraining. Non-code-mixed models perform reasonably but show limited adaptability. Notably, Google Gemini exhibits competitive zero-shot performance, underlining the generalization strength of modern LLMs. This study provides key insights into the effectiveness of specialized versus generalized models for code-mixed NER tasks.</li>
</ul>

<h3>Title: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02522">https://arxiv.org/abs/2509.02522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02522">https://arxiv.org/pdf/2509.02522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02522]] Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR(https://arxiv.org/abs/2509.02522)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at this https URL.</li>
</ul>

<h3>Title: Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.PR, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02528">https://arxiv.org/abs/2509.02528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02528">https://arxiv.org/pdf/2509.02528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02528]] Is RL fine-tuning harder than regression? A PDE learning approach for diffusion models(https://arxiv.org/abs/2509.02528)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study the problem of learning the optimal control policy for fine-tuning a given diffusion process, using general value function approximation. We develop a new class of algorithms by solving a variational inequality problem based on the Hamilton-Jacobi-Bellman (HJB) equations. We prove sharp statistical rates for the learned value function and control policy, depending on the complexity and approximation errors of the function class. In contrast to generic reinforcement learning problems, our approach shows that fine-tuning can be achieved via supervised regression, with faster statistical rate guarantees.</li>
</ul>

<h3>Title: Jointly Reinforcing Diversity and Quality in Language Model Generations</h3>
<ul>
<li><strong>Authors: </strong>Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02534">https://arxiv.org/abs/2509.02534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02534">https://arxiv.org/pdf/2509.02534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02534]] Jointly Reinforcing Diversity and Quality in Language Model Generations(https://arxiv.org/abs/2509.02534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.</li>
</ul>

<h3>Title: Federated learning over physical channels: adaptive algorithms with near-optimal guarantees</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhang, Wenlong Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02538">https://arxiv.org/abs/2509.02538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02538">https://arxiv.org/pdf/2509.02538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02538]] Federated learning over physical channels: adaptive algorithms with near-optimal guarantees(https://arxiv.org/abs/2509.02538)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In federated learning, communication cost can be significantly reduced by transmitting the information over the air through physical channels. In this paper, we propose a new class of adaptive federated stochastic gradient descent (SGD) algorithms that can be implemented over physical channels, taking into account both channel noise and hardware constraints. We establish theoretical guarantees for the proposed algorithms, demonstrating convergence rates that are adaptive to the stochastic gradient noise level. We also demonstrate the practical effectiveness of our algorithms through simulation studies with deep learning models.</li>
</ul>

<h3>Title: Mix-modal Federated Learning for MRI Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guyue Hu, Siyuan Song, Jingpeng Sun, Zhe Jin, Chenglong Li, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02541">https://arxiv.org/abs/2509.02541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02541">https://arxiv.org/pdf/2509.02541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02541]] Mix-modal Federated Learning for MRI Image Segmentation(https://arxiv.org/abs/2509.02541)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, segmentation</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing and treating many diseases, such as brain tumors. Existing MRI image segmentation methods mainly fall into a centralized multimodal paradigm, which is inapplicable in engineering non-centralized mix-modal medical scenarios. In this situation, each distributed client (hospital) processes multiple mixed MRI modalities, and the modality set and image data for each client are diverse, suffering from extensive client-wise modality heterogeneity and data heterogeneity. In this paper, we first formulate non-centralized mix-modal MRI image segmentation as a new paradigm for federated learning (FL) that involves multiple modalities, called mix-modal federated learning (MixMFL). It distinguishes from existing multimodal federating learning (MulMFL) and cross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel modality decoupling and memorizing mix-modal federated learning framework (MDM-MixMFL) for MRI image segmentation, which is characterized by a modality decoupling strategy and a modality memorizing mechanism. Specifically, the modality decoupling strategy disentangles each modality into modality-tailored and modality-shared information. During mix-modal federated updating, corresponding modality encoders undergo tailored and shared updating, respectively. It facilitates stable and adaptive federating aggregation of heterogeneous data and modalities from distributed clients. Besides, the modality memorizing mechanism stores client-shared modality prototypes dynamically refreshed from every modality-tailored encoder to compensate for incomplete modalities in each local client. It further benefits modality aggregation and fusion processes during mixmodal federated learning. Extensive experiments on two public datasets for MRI image segmentation demonstrate the effectiveness and superiority of our methods.</li>
</ul>

<h3>Title: Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery</h3>
<ul>
<li><strong>Authors: </strong>Xinrui Gong, Oliver Hahn, Christoph Reich, Krishnakant Singh, Simone Schaub-Meyer, Daniel Cremers, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02545">https://arxiv.org/abs/2509.02545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02545">https://arxiv.org/pdf/2509.02545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02545]] Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery(https://arxiv.org/abs/2509.02545)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised multi-object discovery (MOD) aims to detect and localize distinct object instances in visual scenes without any form of human supervision. Recent approaches leverage object-centric learning (OCL) and motion cues from video to identify individual objects. However, these approaches use supervision to generate pseudo labels to train the OCL model. We address this limitation with MR-DINOSAUR -- Motion-Refined DINOSAUR -- a minimalistic unsupervised approach that extends the self-supervised pre-trained OCL model, DINOSAUR, to the task of unsupervised multi-object discovery. We generate high-quality unsupervised pseudo labels by retrieving video frames without camera motion for which we perform motion segmentation of unsupervised optical flow. We refine DINOSAUR's slot representations using these pseudo labels and train a slot deactivation module to assign slots to foreground and background. Despite its conceptual simplicity, MR-DINOSAUR achieves strong multi-object discovery results on the TRI-PD and KITTI datasets, outperforming the previous state of the art despite being fully unsupervised.</li>
</ul>

<h3>Title: PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture</h3>
<ul>
<li><strong>Authors: </strong>Fakhraddin Alwajih, Abdellah El Mekki, Hamdy Mubarak, Majd Hawasly, Abubakr Mohamed, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02550">https://arxiv.org/abs/2509.02550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02550">https://arxiv.org/pdf/2509.02550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02550]] PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture(https://arxiv.org/abs/2509.02550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) inherently reflect the vast data distributions they encounter during their pre-training phase. As this data is predominantly sourced from the web, there is a high chance it will be skewed towards high-resourced languages and cultures, such as those of the West. Consequently, LLMs often exhibit a diminished understanding of certain communities, a gap that is particularly evident in their knowledge of Arabic and Islamic cultures. This issue becomes even more pronounced with increasingly under-represented topics. To address this critical challenge, we introduce PalmX 2025, the first shared task designed to benchmark the cultural competence of LLMs in these specific domains. The task is composed of two subtasks featuring multiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General Arabic Culture and General Islamic Culture. These subtasks cover a wide range of topics, including traditions, food, history, religious practices, and language expressions from across 22 Arab countries. The initiative drew considerable interest, with 26 teams registering for Subtask 1 and 19 for Subtask 2, culminating in nine and six valid submissions, respectively. Our findings reveal that task-specific fine-tuning substantially boosts performance over baseline models. The top-performing systems achieved an accuracy of 72.15% on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine-tuning emerged as the predominant and most effective approach among participants, while the utility of data augmentation was found to be domain-dependent.</li>
</ul>

<h3>Title: FastVGGT: Training-Free Acceleration of Visual Geometry Transformer</h3>
<ul>
<li><strong>Authors: </strong>You Shen, Zhipeng Zhang, Yansong Qu, Liujuan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02560">https://arxiv.org/abs/2509.02560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02560">https://arxiv.org/pdf/2509.02560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02560]] FastVGGT: Training-Free Acceleration of Visual Geometry Transformer(https://arxiv.org/abs/2509.02560)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models for 3D vision have recently demonstrated remarkable capabilities in 3D perception. However, scaling these models to long-sequence image inputs remains a significant challenge due to inference-time inefficiency. In this work, we present a detailed analysis of VGGT, a state-of-the-art feed-forward visual geometry model and identify its primary bottleneck. Visualization further reveals a token collapse phenomenon in the attention maps. Motivated by these findings, we explore the potential of token merging in the feed-forward visual geometry model. Owing to the unique architectural and task-specific properties of 3D models, directly applying existing merging techniques proves challenging. To this end, we propose FastVGGT, which, for the first time, leverages token merging in the 3D domain through a training-free mechanism for accelerating VGGT. we devise a unique token partitioning strategy tailored to 3D architectures and tasks, effectively eliminating redundant computation while preserving VGGT's powerful reconstruction capacity. Extensive experiments on multiple 3D geometry benchmarks validate the effectiveness of our approach. Notably, with 1000 input images, FastVGGT achieves a 4x speedup over VGGT while mitigating error accumulation in long-sequence scenarios. These findings underscore the potential of token merging as a principled solution for scalable 3D vision systems. Code is available at: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
