<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-29</h1>
<h3>Title: Random Direct Preference Optimization for Radiography Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Valentin Samokhin, Boris Shirokikh, Mikhail Goncharov, Dmitriy Umerenkov, Maksim Bobrin, Ivan Oseledets, Dmitry Dylov, Mikhail Belyaev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21351">https://arxiv.org/abs/2509.21351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21351">https://arxiv.org/pdf/2509.21351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21351]] Random Direct Preference Optimization for Radiography Report Generation(https://arxiv.org/abs/2509.21351)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Radiography Report Generation (RRG) has gained significant attention in medical image analysis as a promising tool for alleviating the growing workload of radiologists. However, despite numerous advancements, existing methods have yet to achieve the quality required for deployment in real-world clinical settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated remarkable progress in the general domain by adopting training strategies originally designed for Large Language Models (LLMs), such as alignment techniques. In this paper, we introduce a model-agnostic framework to enhance RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages random contrastive sampling to construct training pairs, eliminating the need for reward models or human preference annotations. Experiments on supplementing three state-of-the-art models with our Random DPO show that our method improves clinical performance metrics by up to 5%, without requiring any additional training data.</li>
</ul>

<h3>Title: Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports</h3>
<ul>
<li><strong>Authors: </strong>Razi Mahmood, Diego Machado-Reyes, Joy Wu, Parisa Kaviani, Ken C.L. Wong, Niharika D'Souza, Mannudeep Kalra, Ge Wang, Pingkun Yan, Tanveer Syeda-Mahmood</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21356">https://arxiv.org/abs/2509.21356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21356">https://arxiv.org/pdf/2509.21356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21356]] Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports(https://arxiv.org/abs/2509.21356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the emergence of large-scale vision language models (VLM), it is now possible to produce realistic-looking radiology reports for chest X-ray images. However, their clinical translation has been hampered by the factual errors and hallucinations in the produced descriptions during inference. In this paper, we present a novel phrase-grounded fact-checking model (FC model) that detects errors in findings and their indicated locations in automatically generated chest radiology reports. Specifically, we simulate the errors in reports through a large synthetic dataset derived by perturbing findings and their locations in ground truth reports to form real and fake findings-location pairs with images. A new multi-label cross-modal contrastive regression network is then trained on this dataset. We present results demonstrating the robustness of our method in terms of accuracy of finding veracity prediction and localization on multiple X-ray datasets. We also show its effectiveness for error detection in reports of SOTA report generators on multiple datasets achieving a concordance correlation coefficient of 0.997 with ground truth-based verification, thus pointing to its utility during clinical inference in radiology workflows.</li>
</ul>

<h3>Title: A Novel Differential Feature Learning for Effective Hallucination Detection and Classification</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Wang, Vincent Lee, Yizhen Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21357">https://arxiv.org/abs/2509.21357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21357">https://arxiv.org/pdf/2509.21357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21357]] A Novel Differential Feature Learning for Effective Hallucination Detection and Classification(https://arxiv.org/abs/2509.21357)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model hallucination represents a critical challenge where outputs deviate from factual accuracy due to distributional biases in training data. While recent investigations establish that specific hidden layers exhibit differences between hallucinatory and factual content, the precise localization of hallucination signals within layers remains unclear, limiting the development of efficient detection methods. We propose a dual-model architecture integrating a Projected Fusion (PF) block for adaptive inter-layer feature weighting and a Differential Feature Learning (DFL) mechanism that identifies discriminative features by computing differences between parallel encoders learning complementary representations from identical inputs. Through systematic experiments across HaluEval's question answering, dialogue, and summarization datasets, we demonstrate that hallucination signals concentrate in highly sparse feature subsets, achieving significant accuracy improvements on question answering and dialogue tasks. Notably, our analysis reveals a hierarchical "funnel pattern" where shallow layers exhibit high feature diversity while deep layers demonstrate concentrated usage, enabling detection performance to be maintained with minimal degradation using only 1\% of feature dimensions. These findings suggest that hallucination signals are more concentrated than previously assumed, offering a pathway toward computationally efficient detection systems that could reduce inference costs while maintaining accuracy.</li>
</ul>

<h3>Title: MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Jason Jordan, Mohammadreza Akbari Lor, Peter Koulen, Mei-Ling Shyu, Shu-Ching Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21358">https://arxiv.org/abs/2509.21358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21358">https://arxiv.org/pdf/2509.21358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21358]] MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification(https://arxiv.org/abs/2509.21358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>This study aimed to enhance disease classification accuracy from retinal fundus images by integrating fine-grained image features and global textual context using a novel multimodal deep learning architecture. Existing multimodal large language models (MLLMs) often struggle to capture low-level spatial details critical for diagnosing retinal diseases such as glaucoma, diabetic retinopathy, and retinitis pigmentosa. This model development and validation study was conducted on 1,305 fundus image-text pairs compiled from three public datasets (FIVES, HRF, and StoneRounds), covering acquired and inherited retinal diseases, and evaluated using classification accuracy and F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are patch-wise projected and fused using scaled cross-attention and FiLM-based U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease classification task. MDF-MLLM, with both U-Net and MLLM components fully fine-tuned during training, achieved a significantly higher accuracy of 94%, representing a 56% improvement. Recall and F1-scores improved by as much as 67% and 35% over baseline, respectively. Ablation studies confirmed that the multi-depth fusion approach contributed to substantial gains in spatial reasoning and classification, particularly for inherited diseases with rich clinical text. MDF-MLLM presents a generalizable, interpretable, and modular framework for fundus image classification, outperforming traditional MLLM baselines through multi-scale feature fusion. The architecture holds promise for real-world deployment in clinical decision support systems. Future work will explore synchronized training techniques, a larger pool of diseases for more generalizability, and extending the model for segmentation tasks.</li>
</ul>

<h3>Title: Influence Guided Context Selection for Effective Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21359">https://arxiv.org/abs/2509.21359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21359">https://arxiv.org/pdf/2509.21359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21359]] Influence Guided Context Selection for Effective Retrieval-Augmented Generation(https://arxiv.org/abs/2509.21359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at this https URL.</li>
</ul>

<h3>Title: Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Xingkai Peng, Jun Jiang, Meng Tong, Shuai Li, Weiming Zhang, Nenghai Yu, Kejiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21360">https://arxiv.org/abs/2509.21360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21360">https://arxiv.org/pdf/2509.21360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21360]] Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models(https://arxiv.org/abs/2509.21360)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.</li>
</ul>

<h3>Title: Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Norman Paulsen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21361">https://arxiv.org/abs/2509.21361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21361">https://arxiv.org/pdf/2509.21361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21361]] Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs(https://arxiv.org/abs/2509.21361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.</li>
</ul>

<h3>Title: MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Du, Qingyang Shi, Jiasheng Lu, Yingshan Liang, Xinyu Zhang, Yiran Wang, Peiwu Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21365">https://arxiv.org/abs/2509.21365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21365">https://arxiv.org/pdf/2509.21365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21365]] MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation(https://arxiv.org/abs/2509.21365)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The multimodal relevance metric is usually borrowed from the embedding ability of pretrained contrastive learning models for bimodal data, which is used to evaluate the correlation between cross-modal data (e.g., CLIP). However, the commonly used evaluation metrics are only suitable for the associated analysis between two modalities, which greatly limits the evaluation of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation metric for the relevance of multiple modalities (N modalities, N>=3) via multimodal joint representation for the first time. The ability of multimodal joint representation to integrate multiple modalities into the same latent space can accurately represent different modalities at one scale, providing support for fair relevance scoring. Extensive experiments have shown that MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves as a more reliable metric for evaluating similarity on large-scale multimodal datasets and multimodal model performance evaluation.</li>
</ul>

<h3>Title: Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan</h3>
<ul>
<li><strong>Authors: </strong>Yu-Kai Shih, You-Kai Kang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21367">https://arxiv.org/abs/2509.21367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21367">https://arxiv.org/pdf/2509.21367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21367]] Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan(https://arxiv.org/abs/2509.21367)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function calls, multi-layered linguistic analysis, and guardrails against injections, achieving high contextual awareness and security. Key features include a tiered response strategy, RAG-driven knowledge grounding, and intent decomposition across lexical, semantic, and pragmatic levels. Defense mechanisms include system norms, gatekeepers for intent judgment, and reverse RAG text to prioritize verified data. We also benchmark a GPT-5 variant (released 2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial prompts and 223 benign queries show over 95% accuracy on benign tasks and substantial detection of injection attacks. GPT-5 blocked about 85% of attacks, showing progress yet highlighting the need for layered defenses. Findings emphasize contributions to sustainable tourism, multilingual accessibility, and ethical AI deployment. This work offers a practical framework for deploying secure chatbots in smart tourism and contributes to resilient, trustworthy AI applications.</li>
</ul>

<h3>Title: Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation</h3>
<ul>
<li><strong>Authors: </strong>Yinfeng Yu, Hailong Zhang, Meiling Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21377">https://arxiv.org/abs/2509.21377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21377">https://arxiv.org/pdf/2509.21377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21377]] Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation(https://arxiv.org/abs/2509.21377)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at this https URL.</li>
</ul>

<h3>Title: SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Enrico Cassano, Riccardo Renzulli, Marco Nurisso, Mirko Zaffaroni, Alan Perotti, Marco Grangetto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21379">https://arxiv.org/abs/2509.21379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21379">https://arxiv.org/pdf/2509.21379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21379]] SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders(https://arxiv.org/abs/2509.21379)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Effective concept unlearning in text-to-image diffusion models requires precise localization of concept representations within the model's latent space. While sparse autoencoders successfully reduce neuron polysemanticity (i.e., multiple concepts per neuron) compared to the original network, individual concept representations can still be distributed across multiple latent features, requiring extensive search procedures for concept unlearning. We introduce SAEmnesia, a supervised sparse autoencoder training method that promotes one-to-one concept-neuron mappings through systematic concept labeling, mitigating feature splitting and promoting feature centralization. Our approach learns specialized neurons with significantly stronger concept associations compared to unsupervised baselines. The only computational overhead introduced by SAEmnesia is limited to cross-entropy computation during training. At inference time, this interpretable representation reduces hyperparameter search by 96.67% with respect to current approaches. On the UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the state-of-the-art. In sequential unlearning tasks, we demonstrate superior scalability with a 28.4% improvement in unlearning accuracy for 9-object removal.</li>
</ul>

<h3>Title: The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms</h3>
<ul>
<li><strong>Authors: </strong>Manel Rakez, Thomas Louis, Julien Guillaumin, Foucauld Chamming's, Pierre Fillard, Brice Amadeo, Virginie Rondeau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21383">https://arxiv.org/abs/2509.21383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21383">https://arxiv.org/pdf/2509.21383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21383]] The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms(https://arxiv.org/abs/2509.21383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Risk-adapted breast cancer screening requires robust models that leverage longitudinal imaging data. Most current deep learning models use single or limited prior mammograms and lack adaptation for real-world settings marked by imbalanced outcome distribution and heterogeneous follow-up. We developed LongiMam, an end-to-end deep learning model that integrates both current and up to four prior mammograms. LongiMam combines a convolutional and a recurrent neural network to capture spatial and temporal patterns predictive of breast cancer. The model was trained and evaluated using a large, population-based screening dataset with disproportionate case-to-control ratio typical of clinical screening. Across several scenarios that varied in the number and composition of prior exams, LongiMam consistently improved prediction when prior mammograms were included. The addition of prior and current visits outperformed single-visit models, while priors alone performed less well, highlighting the importance of combining historical and recent information. Subgroup analyses confirmed the model's efficacy across key risk groups, including women with dense breasts and those aged 55 years or older. Moreover, the model performed best in women with observed changes in mammographic density over time. These findings demonstrate that longitudinal modeling enhances breast cancer prediction and support the use of repeated mammograms to refine risk stratification in screening programs. LongiMam is publicly available as open-source software.</li>
</ul>

<h3>Title: Debugging Concept Bottleneck Models through Removal and Retraining</h3>
<ul>
<li><strong>Authors: </strong>Eric Enouen, Sainyam Galhotra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21385">https://arxiv.org/abs/2509.21385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21385">https://arxiv.org/pdf/2509.21385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21385]] Debugging Concept Bottleneck Models through Removal and Retraining(https://arxiv.org/abs/2509.21385)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to predict the final task label, enabling domain experts to not only validate the CBM's predictions, but also intervene on incorrect concepts at test time. However, these interventions fail to address systemic misalignment between the CBM and the expert's reasoning, such as when the model learns shortcuts from biased data. To address this, we present a general interpretable debugging framework for CBMs that follows a two-step process of Removal and Retraining. In the Removal step, experts use concept explanations to identify and remove any undesired concepts. In the Retraining step, we introduce CBDebug, a novel method that leverages the interpretability of CBMs as a bridge for converting concept-level user feedback into sample-level auxiliary labels. These labels are then used to apply supervised bias mitigation and targeted augmentation, reducing the model's reliance on undesired concepts. We evaluate our framework with both real and automated expert feedback, and find that CBDebug significantly outperforms prior retraining methods across multiple CBM architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious correlations.</li>
</ul>

<h3>Title: ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data</h3>
<ul>
<li><strong>Authors: </strong>Anja Sheppard, Tyler Smithline, Andrew Scheffer, David Smith, Advaith V. Sethuraman, Ryan Bird, Sabrina Lin, Katherine A. Skinner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21386">https://arxiv.org/abs/2509.21386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21386">https://arxiv.org/pdf/2509.21386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21386]] ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data(https://arxiv.org/abs/2509.21386)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that detects shipwrecks from multibeam sonar data. Shipwrecks are an important historical marker of maritime history, and can be discovered through manual inspection of bathymetric data. However, this is a time-consuming process and often requires expert analysis. Our proposed tool allows users to automatically preprocess bathymetry data, perform deep learning inference, threshold model outputs, and produce either pixel-wise segmentation masks or bounding boxes of predicted shipwrecks. The backbone of this open-source tool is a deep learning model, which is trained on a variety of shipwreck data from the Great Lakes and the coasts of Ireland. Additionally, we employ synthetic data generation in order to increase the size and diversity of our dataset. We demonstrate superior segmentation performance with our open-source tool and training pipeline as compared to a deep learning-based ArcGIS toolkit and a more classical inverse sinkhole detection method. The open-source tool can be found at this https URL.</li>
</ul>

<h3>Title: Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence</h3>
<ul>
<li><strong>Authors: </strong>Sanish Suwal, Dipkamal Bhusal, Michael Clifford, Nidhi Rastogi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21387">https://arxiv.org/abs/2509.21387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21387">https://arxiv.org/pdf/2509.21387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21387]] Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence(https://arxiv.org/abs/2509.21387)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Prior works have shown that neural networks can be heavily pruned while preserving performance, but the impact of pruning on model interpretability remains unclear. In this work, we investigate how magnitude-based pruning followed by fine-tuning affects both low-level saliency maps and high-level concept representations. Using a ResNet-18 trained on ImageNette, we compare post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG) across pruning levels, evaluating sparsity and faithfulness. We further apply CRAFT-based concept extraction to track changes in semantic coherence of learned concepts. Our results show that light-to-moderate pruning improves saliency-map focus and faithfulness while retaining distinct, semantically meaningful concepts. In contrast, aggressive pruning merges heterogeneous features, reducing saliency map sparsity and concept coherence despite maintaining accuracy. These findings suggest that while pruning can shape internal representations toward more human-aligned attention patterns, excessive pruning undermines interpretability.</li>
</ul>

<h3>Title: Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Devashish Chaudhary, Sutharshan Rajasegarar, Shiva Raj Pokhrel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21389">https://arxiv.org/abs/2509.21389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21389">https://arxiv.org/pdf/2509.21389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21389]] Towards Adapting Federated & Quantum Machine Learning for Network Intrusion Detection: A Survey(https://arxiv.org/abs/2509.21389)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>This survey explores the integration of Federated Learning (FL) with Network Intrusion Detection Systems (NIDS), with particular emphasis on deep learning and quantum machine learning approaches. FL enables collaborative model training across distributed devices while preserving data privacy-a critical requirement in network security contexts where sensitive traffic data cannot be centralized. Our comprehensive analysis systematically examines the full spectrum of FL architectures, deployment strategies, communication protocols, and aggregation methods specifically tailored for intrusion detection. We provide an in-depth investigation of privacy-preserving techniques, model compression approaches, and attack-specific federated solutions for threats including DDoS, MITM, and botnet attacks. The survey further delivers a pioneering exploration of Quantum FL (QFL), discussing quantum feature encoding, quantum machine learning algorithms, and quantum-specific aggregation methods that promise exponential speedups for complex pattern recognition in network traffic. Through rigorous comparative analysis of classical and quantum approaches, identification of research gaps, and evaluation of real-world deployments, we outline a concrete roadmap for industrial adoption and future research directions. This work serves as an authoritative reference for researchers and practitioners seeking to enhance privacy, efficiency, and robustness of federated intrusion detection systems in increasingly complex network environments, while preparing for the quantum-enhanced cybersecurity landscape of tomorrow.</li>
</ul>

<h3>Title: Dynamic Dual-level Defense Routing for Continual Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Chenglei Wang, Xuelin Qian</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21392">https://arxiv.org/abs/2509.21392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21392">https://arxiv.org/pdf/2509.21392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21392]] Dynamic Dual-level Defense Routing for Continual Adversarial Training(https://arxiv.org/abs/2509.21392)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>As adversarial attacks continue to evolve, defense models face the risk of recurrent vulnerabilities, underscoring the importance of continuous adversarial training (CAT). Existing CAT approaches typically balance decision boundaries by either data replay or optimization strategy to constrain shared model parameters. However, due to the diverse and aggressive nature of adversarial examples, these methods suffer from catastrophic forgetting of previous defense knowledge after continual learning. In this paper, we propose a novel framework, called Dual-level Defense Routing or DDeR, that can autonomously select appropriate routers to integrate specific defense experts, thereby adapting to evolving adversarial attacks. Concretely, the first-level defense routing comprises multiple defense experts and routers, with each router dynamically selecting and combining suitable experts to process attacked features. Routers are independently incremented as continuous adversarial training progresses, and their selections are guided by an Adversarial Sentinel Network (ASN) in the second-level defense routing. To compensate for the inability to test due to the independence of routers, we further present a Pseudo-task Substitution Training (PST) strategy, which leverages distributional discrepancy in data to facilitate inter-router communication without storing historical data. Extensive experiments demonstrate that DDeR achieves superior continuous defense performance and classification accuracy compared to existing methods.</li>
</ul>

<h3>Title: Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yi En Chou, Te Hsin Liu, Chao An Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21393">https://arxiv.org/abs/2509.21393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21393">https://arxiv.org/pdf/2509.21393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21393]] Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics(https://arxiv.org/abs/2509.21393)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Physics Informed Neural Networks offer a mesh free framework for solving PDEs but are highly sensitive to loss weight selection. We propose two dimensional analysis based weighting schemes, one based on quantifiable terms, and another also incorporating unquantifiable terms for more balanced training. Benchmarks on heat conduction, convection diffusion, and lid driven cavity flows show that the second scheme consistently improves stability and accuracy over equal weighting. Notably, in high Peclet number convection diffusion, where traditional solvers fail, PINNs with our scheme achieve stable, accurate predictions, highlighting their robustness and generalizability in CFD problems.</li>
</ul>

<h3>Title: Large AI Model-Enabled Generative Semantic Communications for Image Transmission</h3>
<ul>
<li><strong>Authors: </strong>Qiyu Ma, Wanli Ni, Zhijin Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21394">https://arxiv.org/abs/2509.21394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21394">https://arxiv.org/pdf/2509.21394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21394]] Large AI Model-Enabled Generative Semantic Communications for Image Transmission(https://arxiv.org/abs/2509.21394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid development of generative artificial intelligence (AI) has introduced significant opportunities for enhancing the efficiency and accuracy of image transmission within semantic communication systems. Despite these advancements, existing methodologies often neglect the difference in importance of different regions of the image, potentially compromising the reconstruction quality of visually critical content. To address this issue, we introduce an innovative generative semantic communication system that refines semantic granularity by segmenting images into key and non-key regions. Key regions, which contain essential visual information, are processed using an image oriented semantic encoder, while non-key regions are efficiently compressed through an image-to-text modeling approach. Additionally, to mitigate the substantial storage and computational demands posed by large AI models, the proposed system employs a lightweight deployment strategy incorporating model quantization and low-rank adaptation fine-tuning techniques, significantly boosting resource utilization without sacrificing performance. Simulation results demonstrate that the proposed system outperforms traditional methods in terms of both semantic fidelity and visual quality, thereby affirming its effectiveness for image transmission tasks.</li>
</ul>

<h3>Title: Skeleton Sparsification and Densification Scale-Spaces</h3>
<ul>
<li><strong>Authors: </strong>Julia Gierke, Pascal Peter</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21398">https://arxiv.org/abs/2509.21398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21398">https://arxiv.org/pdf/2509.21398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21398]] Skeleton Sparsification and Densification Scale-Spaces(https://arxiv.org/abs/2509.21398)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful shape descriptor that represents binary objects in terms of the centres of maximal inscribed discs. Despite its broad applicability, the medial axis suffers from sensitivity to noise: minor boundary variations can lead to disproportionately large and undesirable expansions of the skeleton. Classical pruning methods mitigate this shortcoming by systematically removing extraneous skeletal branches. This sequential simplification of skeletons resembles the principle of sparsification scale-spaces that embed images into a family of reconstructions from increasingly sparse pixel representations. We combine both worlds by introducing skeletonisation scale-spaces: They leverage sparsification of the medial axis to achieve hierarchical simplification of shapes. Unlike conventional pruning, our framework inherently satisfies key scale-space properties such as hierarchical architecture, controllable simplification, and equivariance to geometric transformations. We provide a rigorous theoretical foundation in both continuous and discrete formulations and extend the concept further with densification. This allows inverse progression from coarse to fine scales and can even reach beyond the original skeleton to produce overcomplete shape representations with relevancy for practical applications. Through proof-of-concept experiments, we demonstrate the effectiveness of our framework for practical tasks including robust skeletonisation, shape compression, and stiffness enhancement for additive manufacturing.</li>
</ul>

<h3>Title: SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiyu Zeng, Siyuan Liang, Liming Lu, Haotian Zhu, Enguang Liu, Jisheng Dang, Yongbin Zhou, Shuchao Pang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21400">https://arxiv.org/abs/2509.21400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21400">https://arxiv.org/pdf/2509.21400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21400]] SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models(https://arxiv.org/abs/2509.21400)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>As the capabilities of Vision Language Models (VLMs) continue to improve, they are increasingly targeted by jailbreak attacks. Existing defense methods face two major limitations: (1) they struggle to ensure safety without compromising the model's utility; and (2) many defense mechanisms significantly reduce the model's inference efficiency. To address these challenges, we propose SafeSteer, a lightweight, inference-time steering framework that effectively defends against diverse jailbreak attacks without modifying model weights. At the core of SafeSteer is the innovative use of Singular Value Decomposition to construct a low-dimensional "safety subspace." By projecting and reconstructing the raw steering vector into this subspace during inference, SafeSteer adaptively removes harmful generation signals while preserving the model's ability to handle benign inputs. The entire process is executed in a single inference pass, introducing negligible overhead. Extensive experiments show that SafeSteer reduces the attack success rate by over 60% and improves accuracy on normal tasks by 1-2%, without introducing significant inference latency. These results demonstrate that robust and practical jailbreak defense can be achieved through simple, efficient inference-time control.</li>
</ul>

<h3>Title: JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Md Jueal Mia, M. Hadi Amini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21401">https://arxiv.org/abs/2509.21401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21401">https://arxiv.org/pdf/2509.21401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21401]] JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation(https://arxiv.org/abs/2509.21401)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have remarkable abilities in generating multimodal reasoning tasks. However, potential misuse or safety alignment concerns of VLMs have increased significantly due to different categories of attack vectors. Among various attack vectors, recent studies have demonstrated that image-based perturbations are particularly effective in generating harmful outputs. In the literature, many existing techniques have been proposed to jailbreak VLMs, leading to unstable performance and visible perturbations. In this study, we propose Jailbreaking with Loss-guided Image Perturbation (JaiLIP), a jailbreaking attack in the image space that minimizes a joint objective combining the mean squared error (MSE) loss between clean and adversarial image with the models harmful-output loss. We evaluate our proposed method on VLMs using standard toxicity metrics from Perspective API and Detoxify. Experimental results demonstrate that our method generates highly effective and imperceptible adversarial images, outperforming existing methods in producing toxicity. Moreover, we have evaluated our method in the transportation domain to demonstrate the attacks practicality beyond toxic text generation in specific domain. Our findings emphasize the practical challenges of image-based jailbreak attacks and the need for efficient defense mechanisms for VLMs.</li>
</ul>

<h3>Title: LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?</h3>
<ul>
<li><strong>Authors: </strong>Rushil Gupta, Jason Hartford, Bang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21403">https://arxiv.org/abs/2509.21403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21403">https://arxiv.org/pdf/2509.21403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21403]] LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?(https://arxiv.org/abs/2509.21403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently been proposed as general-purpose agents for experimental design, with claims that they can perform in-context experimental design. We evaluate this hypothesis using both open- and closed-source instruction-tuned LLMs applied to genetic perturbation and molecular property discovery tasks. We find that LLM-based agents show no sensitivity to experimental feedback: replacing true outcomes with randomly permuted labels has no impact on performance. Across benchmarks, classical methods such as linear bandits and Gaussian process optimization consistently outperform LLM agents. We further propose a simple hybrid method, LLM-guided Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with nearest-neighbor sampling to guide the design of experiments. LLMNN achieves competitive or superior performance across domains without requiring significant in-context adaptation. These results suggest that current open- and closed-source LLMs do not perform in-context experimental design in practice and highlight the need for hybrid frameworks that decouple prior-based reasoning from batch acquisition with updated posteriors.</li>
</ul>

<h3>Title: How Large Language Models Need Symbolism</h3>
<ul>
<li><strong>Authors: </strong>Xiaotie Deng, Hanyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21404">https://arxiv.org/abs/2509.21404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21404">https://arxiv.org/pdf/2509.21404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21404]] How Large Language Models Need Symbolism(https://arxiv.org/abs/2509.21404)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.</li>
</ul>

<h3>Title: Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Zihuan Qiu, Lei Wang, Yang Cao, Runtong Zhang, Bing Su, Yi Xu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21413">https://arxiv.org/abs/2509.21413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21413">https://arxiv.org/pdf/2509.21413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21413]] Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity(https://arxiv.org/abs/2509.21413)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>Data-free continual model merging (DFCMM) aims to fuse independently fine-tuned models into a single backbone that evolves with incoming tasks without accessing task data. This paper formulate two fundamental desiderata for DFCMM: transparency, avoiding interference with earlier tasks, and fidelity, adapting faithfully to each new task. This poses a challenge that existing approaches fail to address: how to bridge data-level desiderata with parameter-space optimization to ensure transparency and fidelity in the absence of task data. To this end, we propose NUFILT (NUll-space FILTering), a data-free framework that directly links these desiderata to optimization. Our key observation is that task vectors approximately align with representation subspaces, providing structural surrogates for enforcing transparency and fidelity. Accordingly, we design a null-space projector that preserves prior responses by filtering out overlapping components of new task vectors, thereby ensuring transparency, and a lightweight LoRA adapter that injects complementary task-specific signals, enabling fidelity in adapting to new tasks. The adapter is trained with a projection-based surrogate loss to retain consistency with previous knowledge while introducing novel directions. This joint filtering-adaptation process allows the backbone to absorb new knowledge while retaining existing behaviors, and the updates are finally fused back in a layer-wise linear fashion without extra parameters or inference cost. Theoretically, we establish approximate subspace alignment guarantees that justify null-space filtering. Empirically, NUFILT achieves state-of-the-art performance with minimal forgetting on both vision and NLP benchmarks, improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing the gap to fine-tuning and reducing computation overhead.</li>
</ul>

<h3>Title: QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jian Liu, Chunshi Wang, Song Guo, Haohan Weng, Zhen Zhou, Zhiqi Li, Jiaao Yu, Yiling Zhu, Jing Xu, Biwen Lei, Zhuo Chen, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21420">https://arxiv.org/abs/2509.21420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21420">https://arxiv.org/pdf/2509.21420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21420]] QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models(https://arxiv.org/abs/2509.21420)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of quadrilateral-dominant meshes is a cornerstone of professional 3D content creation. However, existing generative models generate quad meshes by first generating triangle meshes and then merging triangles into quadrilaterals with some specific rules, which typically produces quad meshes with poor topology. In this paper, we introduce QuadGPT, the first autoregressive framework for generating quadrilateral meshes in an end-to-end manner. QuadGPT formulates this as a sequence prediction paradigm, distinguished by two key innovations: a unified tokenization method to handle mixed topologies of triangles and quadrilaterals, and a specialized Reinforcement Learning fine-tuning method tDPO for better generation quality. Extensive experiments demonstrate that QuadGPT significantly surpasses previous triangle-to-quad conversion pipelines in both geometric accuracy and topological quality. Our work establishes a new benchmark for native quad-mesh generation and showcases the power of combining large-scale autoregressive models with topology-aware RL refinement for creating structured 3D assets.</li>
</ul>

<h3>Title: DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Lan Zhang, Xiaoyong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21433">https://arxiv.org/abs/2509.21433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21433">https://arxiv.org/pdf/2509.21433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21433]] DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation(https://arxiv.org/abs/2509.21433)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted styles and protected visual concepts, raising legal and ethical concerns. Concept erasure has emerged as a safeguard, aiming to selectively suppress such concepts through fine-tuning. However, existing methods do not scale to practical settings where providers must erase multiple and possibly conflicting concepts. The core bottleneck is their reliance on static erasure: a single checkpoint is fine-tuned to remove all target concepts, regardless of the actual erasure needs at inference. This rigid design mismatches real-world usage, where requests vary per generation, leading to degraded erasure success and reduced fidelity for non-target content. We propose DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference. This modular design enables flexible multi-concept erasure, but naive composition causes interference among adapters, especially when many or semantically related concepts are suppressed. To overcome this, we introduce bi-level orthogonality constraints at both the feature and parameter levels, disentangling representation shifts and enforcing orthogonal adapter subspaces. We further develop ErasureBench-H, a new hierarchical benchmark with brand-series-character structure, enabling principled evaluation across semantic granularities and erasure set sizes. Experiments on ErasureBench-H and standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving higher multi-concept erasure fidelity with minimal collateral degradation.</li>
</ul>

<h3>Title: One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sualeha Farid, Jayden Lin, Zean Chen, Shivani Kumar, David Jurgens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21443">https://arxiv.org/abs/2509.21443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21443">https://arxiv.org/pdf/2509.21443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21443]] One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning(https://arxiv.org/abs/2509.21443)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in multilingual and multicultural environments where moral reasoning is essential for generating ethically appropriate responses. Yet, the dominant pretraining of LLMs on English-language data raises critical concerns about their ability to generalize judgments across diverse linguistic and cultural contexts. In this work, we systematically investigate how language mediates moral decision-making in LLMs. We translate two established moral reasoning benchmarks into five culturally and typologically diverse languages, enabling multilingual zero-shot evaluation. Our analysis reveals significant inconsistencies in LLMs' moral judgments across languages, often reflecting cultural misalignment. Through a combination of carefully constructed research questions, we uncover the underlying drivers of these disparities, ranging from disagreements to reasoning strategies employed by LLMs. Finally, through a case study, we link the role of pretraining data in shaping an LLM's moral compass. Through this work, we distill our insights into a structured typology of moral reasoning errors that calls for more culturally-aware AI.</li>
</ul>

<h3>Title: Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope</h3>
<ul>
<li><strong>Authors: </strong>Waleed Esmail, Alexander Kappes, Stuart Russell, Christine Thomas</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.EP, astro-ph.IM, gr-qc</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21446">https://arxiv.org/abs/2509.21446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21446">https://arxiv.org/pdf/2509.21446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21446]] Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope(https://arxiv.org/abs/2509.21446)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce \textit{SeismoGPT}, a transformer-based model for forecasting three-component seismic waveforms in the context of future gravitational wave detectors like the Einstein Telescope. The model is trained in an autoregressive setting and can operate on both single-station and array-based inputs. By learning temporal and spatial dependencies directly from waveform data, SeismoGPT captures realistic ground motion patterns and provides accurate short-term forecasts. Our results show that the model performs well within the immediate prediction window and gradually degrades further ahead, as expected in autoregressive systems. This approach lays the groundwork for data-driven seismic forecasting that could support Newtonian noise mitigation and real-time observatory control.</li>
</ul>

<h3>Title: LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Kumar Gupta, Nirajan Acharya, Pranal Pande</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21450">https://arxiv.org/abs/2509.21450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21450">https://arxiv.org/pdf/2509.21450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21450]] LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5(https://arxiv.org/abs/2509.21450)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Diabetes mellitus is a major global health challenge, affecting over half a billion adults worldwide with prevalence projected to rise. Although the American Diabetes Association (ADA) provides clear diagnostic thresholds, early recognition remains difficult due to vague symptoms, borderline laboratory values, gestational complexity, and the demands of long-term monitoring. Advances in large language models (LLMs) offer opportunities to enhance decision support through structured, interpretable, and patient-friendly outputs. This study evaluates GPT-5, the latest generative pre-trained transformer, using a simulation framework built entirely on synthetic cases aligned with ADA Standards of Care 2025 and inspired by public datasets including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative scenarios were tested: symptom recognition, laboratory interpretation, gestational diabetes screening, remote monitoring, and multimodal complication detection. For each, GPT-5 classified cases, generated clinical rationales, produced patient explanations, and output structured JSON summaries. Results showed strong alignment with ADA-defined criteria, suggesting GPT-5 may function as a dual-purpose tool for clinicians and patients, while underscoring the importance of reproducible evaluation frameworks for responsibly assessing LLMs in healthcare.</li>
</ul>

<h3>Title: VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Abdul Waheed, Zhen Wu, Dareen Alharthi, Seungone Kim, Bhiksha Raj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21451">https://arxiv.org/abs/2509.21451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21451">https://arxiv.org/pdf/2509.21451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21451]] VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding(https://arxiv.org/abs/2509.21451)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.</li>
</ul>

<h3>Title: Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes</h3>
<ul>
<li><strong>Authors: </strong>Guangliang Liu, Bocheng Chen, Xitong Zhang, Kristen Marie Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21456">https://arxiv.org/abs/2509.21456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21456">https://arxiv.org/pdf/2509.21456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21456]] Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes(https://arxiv.org/abs/2509.21456)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Moral alignment has emerged as a widely adopted approach for regulating the behavior of pretrained language models (PLMs), typically through fine-tuning or model editing on curated datasets. However, this process often comes at the cost of degraded downstream task performance. Prior studies commonly aim to achieve a performance trade-off by encouraging PLMs to selectively forget stereotypical knowledge through carefully designed fairness objectives, while preserving their helpfulness. In this short paper, we investigate the underlying mechanisms of the performance trade-off in the context of mitigating gender stereotypes, through the lens of forgetting and the fairness objective. Our analysis reveals the limitations of current fairness objective in achieving trade-off by demonstrating that: (1) downstream task performance is primarily driven by the overall forgetting level; (2) selective forgetting of stereotypes tends to increase overall forgetting; and (3) general solutions for mitigating forgetting are ineffective at reducing overall forgetting and fail to improve downstream task performance.</li>
</ul>

<h3>Title: Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models</h3>
<ul>
<li><strong>Authors: </strong>Khaloud S. AlKhalifah, Malak Mashaabi, Hend Al-Khalifa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21466">https://arxiv.org/abs/2509.21466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21466">https://arxiv.org/pdf/2509.21466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21466]] Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models(https://arxiv.org/abs/2509.21466)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This study investigates the extent to which contemporary Text-to-Image artificial intelligence (AI) models perpetuate gender stereotypes and cultural inaccuracies when generating depictions of professionals in Saudi Arabia. We analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse Saudi professions using neutral prompts. Two trained Saudi annotators evaluated each image on five dimensions: perceived gender, clothing and appearance, background and setting, activities and interactions, and age. A third senior researcher adjudicated whenever the two primary raters disagreed, yielding 10,100 individual judgements. The results reveal a strong gender imbalance, with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\% male, indicating that DALL-E V3 exhibited the strongest overall gender stereotyping. This imbalance was most evident in leadership and technical roles. Moreover, cultural inaccuracies in clothing, settings, and depicted activities were frequently observed across all three models. Counter-stereotypical images often arise from cultural misinterpretations rather than genuinely progressive portrayals. We conclude that current models mirror societal biases embedded in their training data, generated by humans, offering only a limited reflection of the Saudi labour market's gender dynamics and cultural nuances. These findings underscore the urgent need for more diverse training data, fairer algorithms, and culturally sensitive evaluation frameworks to ensure equitable and authentic visual outputs.</li>
</ul>

<h3>Title: Score-based Idempotent Distillation of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shehtab Zaman, Chengyan Liu, Kenneth Chiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21470">https://arxiv.org/abs/2509.21470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21470">https://arxiv.org/pdf/2509.21470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21470]] Score-based Idempotent Distillation of Diffusion Models(https://arxiv.org/abs/2509.21470)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Idempotent generative networks (IGNs) are a new line of generative models based on idempotent mapping to a target manifold. IGNs support both single-and multi-step generation, allowing for a flexible trade-off between computational cost and sample quality. But similar to Generative Adversarial Networks (GANs), conventional IGNs require adversarial training and are prone to training instabilities and mode collapse. Diffusion and score-based models are popular approaches to generative modeling that iteratively transport samples from one distribution, usually a Gaussian, to a target data distribution. These models have gained popularity due to their stable training dynamics and high-fidelity generation quality. However, this stability and quality come at the cost of high computational cost, as the data must be transported incrementally along the entire trajectory. New sampling methods, model distillation, and consistency models have been developed to reduce the sampling cost and even perform one-shot sampling from diffusion models. In this work, we unite diffusion and IGNs by distilling idempotent models from diffusion model scores, called SIGN. Our proposed method is highly stable and does not require adversarial losses. We provide a theoretical analysis of our proposed score-based training methods and empirically show that IGNs can be effectively distilled from a pre-trained diffusion model, enabling faster inference than iterative score-based models. SIGNs can perform multi-step sampling, allowing users to trade off quality for efficiency. These models operate directly on the source domain; they can project corrupted or alternate distributions back onto the target manifold, enabling zero-shot editing of inputs. We validate our models on multiple image datasets, achieving state-of-the-art results for idempotent models on the CIFAR and CelebA datasets.</li>
</ul>

<h3>Title: Are Hallucinations Bad Estimations?</h3>
<ul>
<li><strong>Authors: </strong>Hude Liu, Jerry Yao-Chieh Hu, Jennifer Yuntong Zhang, Zhao Song, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21473">https://arxiv.org/abs/2509.21473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21473">https://arxiv.org/pdf/2509.21473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21473]] Are Hallucinations Bad Estimations?(https://arxiv.org/abs/2509.21473)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.</li>
</ul>

<h3>Title: d2: Improved Techniques for Training Reasoning Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guanghan Wang, Yair Schiff, Gilad Turok, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21474">https://arxiv.org/abs/2509.21474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21474">https://arxiv.org/pdf/2509.21474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21474]] d2: Improved Techniques for Training Reasoning Diffusion Language Models(https://arxiv.org/abs/2509.21474)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).</li>
</ul>

<h3>Title: Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</h3>
<ul>
<li><strong>Authors: </strong>Sen Yang, Burak z, Fei Wu, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21475">https://arxiv.org/abs/2509.21475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21475">https://arxiv.org/pdf/2509.21475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21475]] Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic(https://arxiv.org/abs/2509.21475)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.</li>
</ul>

<h3>Title: VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Hao Wu, Qingsong Wen, Kun Wang, Xian Wu, Xiaomeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21477">https://arxiv.org/abs/2509.21477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21477">https://arxiv.org/pdf/2509.21477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21477]] VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations(https://arxiv.org/abs/2509.21477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing subsurface ocean dynamics, such as vertical velocity fields, from incomplete surface observations poses a critical challenge in Earth science, a field long hampered by the lack of standardized, analysis-ready benchmarks. To systematically address this issue and catalyze research, we first build and release KD48, a high-resolution ocean dynamics benchmark derived from petascale simulations and curated with expert-driven denoising. Building on this benchmark, we introduce VISION, a novel reconstruction paradigm based on Dynamic Prompting designed to tackle the core problem of missing data in real-world observations. The essence of VISION lies in its ability to generate a visual prompt on-the-fly from any available subset of observations, which encodes both data availability and the ocean's physical state. More importantly, we design a State-conditioned Prompting module that efficiently injects this prompt into a universal backbone, endowed with geometry- and scale-aware operators, to guide its adaptive adjustment of computational strategies. This mechanism enables VISION to precisely handle the challenges posed by varying input combinations. Extensive experiments on the KD48 benchmark demonstrate that VISION not only substantially outperforms state-of-the-art models but also exhibits strong generalization under extreme data missing scenarios. By providing a high-quality benchmark and a robust model, our work establishes a solid infrastructure for ocean science research under data uncertainty. Our codes are available at: this https URL.</li>
</ul>

<h3>Title: Learning to Reason with Mixture of Tokens</h3>
<ul>
<li><strong>Authors: </strong>Adit Jain, Brendan Rappazzo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21482">https://arxiv.org/abs/2509.21482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21482">https://arxiv.org/pdf/2509.21482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21482]] Learning to Reason with Mixture of Tokens(https://arxiv.org/abs/2509.21482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.</li>
</ul>

<h3>Title: High-Probability Analysis of Online and Federated Zero-Order Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Arya Akhavan, David Janz, El-Mahdi El-Mhamdi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21484">https://arxiv.org/abs/2509.21484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21484">https://arxiv.org/pdf/2509.21484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21484]] High-Probability Analysis of Online and Federated Zero-Order Optimisation(https://arxiv.org/abs/2509.21484)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>We study distributed learning in the setting of gradient-free zero-order optimization and introduce FedZero, a federated zero-order algorithm that delivers sharp theoretical guarantees. Specifically, FedZero: (1) achieves near-optimal optimization error bounds with high probability in the federated convex setting; and (2) in the single-worker regime-where the problem reduces to the standard zero-order framework, establishes the first high-probability convergence guarantees for convex zero-order optimization, thereby strengthening the classical expectation-based results. At its core, FedZero employs a gradient estimator based on randomization over the $\ell_1$-sphere. To analyze it, we develop new concentration inequalities for Lipschitz functions under the uniform measure on the $\ell_1$-sphere, with explicit constants. These concentration tools are not only central to our high-probability guarantees but may also be of independent interest.</li>
</ul>

<h3>Title: Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Wang, Yu Sun, Hongwei Wang, Baoyu Jing, Xiang Shen, Xin Dong, Zhuolin Hao, Hongyu Xiong, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21486">https://arxiv.org/abs/2509.21486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21486">https://arxiv.org/pdf/2509.21486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21486]] Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation(https://arxiv.org/abs/2509.21486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Short video platforms are evolving rapidly, making the identification of inappropriate content increasingly critical. Existing approaches typically train separate and small classification models for each type of issue, which requires extensive human-labeled data and lacks cross-issue generalization. We propose a reasoning-enhanced multimodal large language model (MLLM) pretraining paradigm for unified inappropriate content detection. To address the distribution gap between short video content and the original pretraining data of MLLMs, as well as the complex issue definitions, we introduce three targeted pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the MLLM's understanding of issue definitions and annotation guidelines; (3) \textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability. Experimental results show that our pretraining approach significantly improves the MLLM's performance in both zero-shot and supervised fine-tuning (SFT) settings. In addition, our pretrained model demonstrates strong generalization capabilities to emergent, previously unseen issues.</li>
</ul>

<h3>Title: Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Alexandru Ioni, Andreea Ioni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21497">https://arxiv.org/abs/2509.21497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21497">https://arxiv.org/pdf/2509.21497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21497]] Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations(https://arxiv.org/abs/2509.21497)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>With the increased interest in artificial intelligence, Machine Learning as a Service provides the infrastructure in the Cloud for easy training, testing, and deploying models. However, these systems have a major privacy issue: uploading sensitive data to the Cloud, especially during training. Therefore, achieving secure Neural Network training has been on many researchers' minds lately. More and more solutions for this problem are built around a main pillar: Functional Encryption (FE). Although these approaches are very interesting and offer a new perspective on ML training over encrypted data, some vulnerabilities do not seem to be taken into consideration. In our paper, we present an attack on neural networks that uses FE for secure training over encrypted data. Our approach uses linear programming to reconstruct the original input, unveiling the previous security promises. To address the attack, we propose two solutions for secure training and inference that involve the client during the computation phase. One approach ensures security without relying on encryption, while the other uses function-hiding inner-product techniques.</li>
</ul>

<h3>Title: SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Arani Roy, Shristi Das Biswas, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21498">https://arxiv.org/abs/2509.21498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21498">https://arxiv.org/pdf/2509.21498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21498]] SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models(https://arxiv.org/abs/2509.21498)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs), lauded for their generative performance, are computationally prohibitive due to their billion-scale parameters and iterative denoising dynamics. Existing efficiency techniques, such as quantization, timestep reduction, or pruning, offer savings in compute, memory, or runtime but are strictly bottlenecked by reliance on fine-tuning or retraining to recover performance. In this work, we introduce SlimDiff, an automated activation-informed structural compression framework that reduces both attention and feedforward dimensionalities in DMs, while being entirely gradient-free. SlimDiff reframes DM compression as a spectral approximation task, where activation covariances across denoising timesteps define low-rank subspaces that guide dynamic pruning under a fixed compression budget. This activation-aware formulation mitigates error accumulation across timesteps by applying module-wise decompositions over functional weight groups: query--key interactions, value--output couplings, and feedforward projections, rather than isolated matrix factorizations, while adaptively allocating sparsity across modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff achieves up to 35\% acceleration and $\sim$100M parameter reduction over baselines, with generation quality on par with uncompressed models without any backpropagation. Crucially, our approach requires only about 500 calibration samples, over 70$\times$ fewer than prior methods. To our knowledge, this is the first closed-form, activation-guided structural compression of DMs that is entirely training-free, providing both theoretical clarity and practical efficiency.</li>
</ul>

<h3>Title: On Code-Induced Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Abdul Waheed, Zhen Wu, Carolyn Ros, Daphne Ippolito</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21499">https://arxiv.org/abs/2509.21499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21499">https://arxiv.org/pdf/2509.21499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21499]] On Code-Induced Reasoning in LLMs(https://arxiv.org/abs/2509.21499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.</li>
</ul>

<h3>Title: Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21500">https://arxiv.org/abs/2509.21500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21500">https://arxiv.org/pdf/2509.21500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21500]] Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training(https://arxiv.org/abs/2509.21500)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement fine-tuning (RFT) often suffers from \emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at this https URL .</li>
</ul>

<h3>Title: Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations</h3>
<ul>
<li><strong>Authors: </strong>Micha Livne</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21511">https://arxiv.org/abs/2509.21511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21511">https://arxiv.org/pdf/2509.21511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21511]] Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations(https://arxiv.org/abs/2509.21511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Learning representations that transfer well to diverse downstream tasks remains a central challenge in representation learning. Existing paradigms -- contrastive learning, self-supervised masking, and denoising auto-encoders -- balance this challenge with different trade-offs. We introduce the {contrastive Mutual Information Machine} (cMIM), a probabilistic framework that extends the Mutual Information Machine (MIM) with a contrastive objective. While MIM maximizes mutual information between inputs and latents and promotes clustering of codes, it falls short on discriminative tasks. cMIM addresses this gap by imposing global discriminative structure while retaining MIM's generative fidelity. Our contributions are threefold. First, we propose cMIM, a contrastive extension of MIM that removes the need for positive data augmentation and is substantially less sensitive to batch size than InfoNCE. Second, we introduce {informative embeddings}, a general technique for extracting enriched features from encoder-decoder models that boosts discriminative performance without additional training and applies broadly beyond MIM. Third, we provide empirical evidence across vision and molecular benchmarks showing that cMIM outperforms MIM and InfoNCE on classification and regression tasks while preserving competitive reconstruction quality. These results position cMIM as a unified framework for representation learning, advancing the goal of models that serve both discriminative and generative applications effectively.</li>
</ul>

<h3>Title: DistillKac: Few-Step Image Generation via Damped Wave Equations</h3>
<ul>
<li><strong>Authors: </strong>Weiqiao Han, Chenlin Meng, Christopher D. Manning, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21513">https://arxiv.org/abs/2509.21513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21513">https://arxiv.org/pdf/2509.21513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21513]] DistillKac: Few-Step Image Generation via Damped Wave Equations(https://arxiv.org/abs/2509.21513)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.</li>
</ul>

<h3>Title: TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongyang He, Xinyuan Song, Yangfan He, Zeyu Zhang, Yanshu Li, Haochen You, Lifan Sun, Wenqiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21526">https://arxiv.org/abs/2509.21526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21526">https://arxiv.org/pdf/2509.21526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21526]] TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning(https://arxiv.org/abs/2509.21526)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce TRiCo, a novel triadic game-theoretic co-training framework that rethinks the structure of semi-supervised learning by incorporating a teacher, two students, and an adversarial generator into a unified training paradigm. Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL as a structured interaction among three roles: (i) two student classifiers trained on frozen, complementary representations, (ii) a meta-learned teacher that adaptively regulates pseudo-label selection and loss balancing via validation-based feedback, and (iii) a non-parametric generator that perturbs embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected based on mutual information rather than confidence, providing a more robust measure of epistemic uncertainty. This triadic interaction is formalized as a Stackelberg game, where the teacher leads strategy optimization and students follow under adversarial perturbations. By addressing key limitations in existing SSL frameworks, such as static view interactions, unreliable pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10, and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art performance in low-label regimes, while remaining architecture-agnostic and compatible with frozen vision backbones.</li>
</ul>

<h3>Title: Preemptive Detection and Steering of LLM Misalignment via Latent Reachability</h3>
<ul>
<li><strong>Authors: </strong>Sathwik Karnik, Somil Bansal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21528">https://arxiv.org/abs/2509.21528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21528">https://arxiv.org/pdf/2509.21528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21528]] Preemptive Detection and Steering of LLM Misalignment via Latent Reachability(https://arxiv.org/abs/2509.21528)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.</li>
</ul>

<h3>Title: Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Dongkyu Cho, Miao Zhang, Rumi Chunara</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21530">https://arxiv.org/abs/2509.21530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21530">https://arxiv.org/pdf/2509.21530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21530]] Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration(https://arxiv.org/abs/2509.21530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Data augmentation is a widely used strategy to improve model robustness and generalization by enriching training datasets with synthetic examples. While large language models (LLMs) have demonstrated strong generative capabilities for this purpose, their applications in high-stakes domains like healthcare present unique challenges due to the risk of generating clinically incorrect or misleading information. In this work, we propose a novel query-based model collaboration framework that integrates expert-level domain knowledge to guide the augmentation process to preserve critical medical information. Experiments on clinical prediction tasks demonstrate that our lightweight collaboration-based approach consistently outperforms existing LLM augmentation methods while improving safety through reduced factual errors. This framework addresses the gap between LLM augmentation potential and the safety requirements of specialized domains.</li>
</ul>

<h3>Title: A circuit for predicting hierarchical structure in-context in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tankred Saanum, Can Demircan, Samuel J. Gershman, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21534">https://arxiv.org/abs/2509.21534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21534">https://arxiv.org/pdf/2509.21534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21534]] A circuit for predicting hierarchical structure in-context in Large Language Models(https://arxiv.org/abs/2509.21534)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at in-context learning, the ability to use information provided as context to improve prediction of future tokens. Induction heads have been argued to play a crucial role for in-context learning in Transformer Language Models. These attention heads make a token attend to successors of past occurrences of the same token in the input. This basic mechanism supports LLMs' ability to copy and predict repeating patterns. However, it is unclear if this same mechanism can support in-context learning of more complex repetitive patterns with hierarchical structure. Natural language is teeming with such cases: The article "the" in English usually prefaces multiple nouns in a text. When predicting which token succeeds a particular instance of "the", we need to integrate further contextual cues from the text to predict the correct noun. If induction heads naively attend to all past instances of successor tokens of "the" in a context-independent manner, they cannot support this level of contextual information integration. In this study, we design a synthetic in-context learning task, where tokens are repeated with hierarchical dependencies. Here, attending uniformly to all successor tokens is not sufficient to accurately predict future tokens. Evaluating a range of LLMs on these token sequences and natural language analogues, we find adaptive induction heads that support prediction by learning what to attend to in-context. Next, we investigate how induction heads themselves learn in-context. We find evidence that learning is supported by attention heads that uncover a set of latent contexts, determining the different token transition relationships. Overall, we not only show that LLMs have induction heads that learn, but offer a complete mechanistic account of how LLMs learn to predict higher-order repetitive patterns in-context.</li>
</ul>

<h3>Title: Agribot: agriculture-specific question answer system</h3>
<ul>
<li><strong>Authors: </strong>Naman Jain, Pranjali Jain, Pratik Kayal, Jayakrishna Sahit, Soham Pachpande, Jayesh Choudhari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21535">https://arxiv.org/abs/2509.21535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21535">https://arxiv.org/pdf/2509.21535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21535]] Agribot: agriculture-specific question answer system(https://arxiv.org/abs/2509.21535)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction</a></li>
<li><strong>Abstract: </strong>India is an agro-based economy and proper information about agricultural practices is the key to optimal agricultural growth and output. In order to answer the queries of the farmer, we have build an agricultural chatbot based on the dataset from Kisan Call Center. This system is robust enough to answer queries related to weather, market rates, plant protection and government schemes. This system is available 24* 7, can be accessed through any electronic device and the information is delivered with the ease of understanding. The system is based on a sentence embedding model which gives an accuracy of 56%. After eliminating synonyms and incorporating entity extraction, the accuracy jumps to 86%. With such a system, farmers can progress towards easier information about farming related practices and hence a better agricultural output. The job of the Call Center workforce would be made easier and the hard work of various such workers can be redirected to a better goal.</li>
</ul>

<h3>Title: Domain-Aware Speaker Diarization On African-Accented English</h3>
<ul>
<li><strong>Authors: </strong>Chibuzor Okocha, Kelechi Ezema, Christan Grant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21554">https://arxiv.org/abs/2509.21554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21554">https://arxiv.org/pdf/2509.21554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21554]] Domain-Aware Speaker Diarization On African-Accented English(https://arxiv.org/abs/2509.21554)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study examines domain effects in speaker diarization for African-accented English. We evaluate multiple production and open systems on general and clinical dialogues under a strict DER protocol that scores overlap. A consistent domain penalty appears for clinical speech and remains significant across models. Error analysis attributes much of this penalty to false alarms and missed detections, aligning with short turns and frequent overlap. We test lightweight domain adaptation by fine-tuning a segmentation module on accent-matched data; it reduces error but does not eliminate the gap. Our contributions include a controlled benchmark across domains, a concise approach to error decomposition and conversation-level profiling, and an adaptation recipe that is easy to reproduce. Results point to overlap-aware segmentation and balanced clinical resources as practical next steps.</li>
</ul>

<h3>Title: Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution</h3>
<ul>
<li><strong>Authors: </strong>Yash Saxena, Raviteja Bommireddy, Ankur Padia, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21557">https://arxiv.org/abs/2509.21557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21557">https://arxiv.org/pdf/2509.21557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21557]] Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution(https://arxiv.org/abs/2509.21557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Trustworthy Large Language Models (LLMs) must cite human-verifiable sources in high-stakes domains such as healthcare, law, academia, and finance, where even small errors can have severe consequences. Practitioners and researchers face a choice: let models generate citations during decoding, or let models draft answers first and then attach appropriate citations. To clarify this choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which produces the answer and citations in one pass, and Post-hoc Citation (P-Cite), which adds or verifies citations after drafting. We conduct a comprehensive evaluation from zero-shot to advanced retrieval-augmented methods across four popular attribution datasets and provide evidence-based recommendations that weigh trade-offs across use cases. Our results show a consistent trade-off between coverage and citation correctness, with retrieval as the main driver of attribution quality in both paradigms. P-Cite methods achieve high coverage with competitive correctness and moderate latency, whereas G-Cite methods prioritize precision at the cost of coverage and speed. We recommend a retrieval-centric, P-Cite-first approach for high-stakes applications, reserving G-Cite for precision-critical settings such as strict claim verification. Our codes and human evaluation results are available at this https URL</li>
</ul>

<h3>Title: X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Reddy Pulakurthi, Jiamian Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Zhiqiang Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21559">https://arxiv.org/abs/2509.21559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21559">https://arxiv.org/pdf/2509.21559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21559]] X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning(https://arxiv.org/abs/2509.21559)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise a retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: this https URL.</li>
</ul>

<h3>Title: Comparative Personalization for Multi-document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Li, Snigdha Chaturvedi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21562">https://arxiv.org/abs/2509.21562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21562">https://arxiv.org/pdf/2509.21562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21562]] Comparative Personalization for Multi-document Summarization(https://arxiv.org/abs/2509.21562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Personalized multi-document summarization (MDS) is essential for meeting individual user preferences of writing style and content focus for summaries. In this paper, we propose that for effective personalization, it is important to identify fine-grained differences between users' preferences by comparing the given user's preferences with other users' this http URL by this, we propose ComPSum, a personalized MDS framework. It first generates a structured analysis of a user by comparing their preferences with other users' preferences. The generated structured analysis is then used to guide the generation of personalized summaries. To evaluate the performance of ComPSum, we propose AuthorMap, a fine-grained reference-free evaluation framework for personalized MDS. It evaluates the personalization of a system based on the authorship attribution between two personalized summaries generated for different users. For robust evaluation of personalized MDS, we construct PerMSum, a personalized MDS dataset in the review and news domain. We evaluate the performance of ComPSum on PerMSum using AuthorMap, showing that it outperforms strong baselines.</li>
</ul>

<h3>Title: No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Junno Yun, Yaar Utku Alalar, Mehmet Akakaya</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21565">https://arxiv.org/abs/2509.21565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21565">https://arxiv.org/pdf/2509.21565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21565]] No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models(https://arxiv.org/abs/2509.21565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Efficient training strategies for large-scale diffusion models have recently emphasized the importance of improving discriminative feature representations in these models. A central line of work in this direction is representation alignment with features obtained from powerful external encoders, which improves the representation quality as assessed through linear probing. Alignment-based approaches show promise but depend on large pretrained encoders, which are computationally expensive to obtain. In this work, we propose an alternative regularization for training, based on promoting the Linear SEParability (LSEP) of intermediate layer representations. LSEP eliminates the need for an auxiliary encoder and representation alignment, while incorporating linear probing directly into the network's learning dynamics rather than treating it as a simple post-hoc evaluation tool. Our results demonstrate substantial improvements in both training efficiency and generation quality on flow-based transformer architectures such as SiTs, achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.</li>
</ul>

<h3>Title: Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms</h3>
<ul>
<li><strong>Authors: </strong>Boyi Chen, Zhangyu Wang, Fabian Deuser, Johann Maximilian Zollner, Martin Werner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21573">https://arxiv.org/abs/2509.21573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21573">https://arxiv.org/pdf/2509.21573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21573]] Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms(https://arxiv.org/abs/2509.21573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate and robust image-based geo-localization at a global scale is challenging due to diverse environments, visually ambiguous scenes, and the lack of distinctive landmarks in many regions. While contrastive learning methods show promising performance by aligning features between street-view images and corresponding locations, they neglect the underlying spatial dependency in the geographic space. As a result, they fail to address the issue of false negatives -- image pairs that are both visually and geographically similar but labeled as negatives, and struggle to effectively distinguish hard negatives, which are visually similar but geographically distant. To address this issue, we propose a novel spatially regularized contrastive learning strategy that integrates a semivariogram, which is a geostatistical tool for modeling how spatial correlation changes with distance. We fit the semivariogram by relating the distance of images in feature space to their geographical distance, capturing the expected visual content in a spatial correlation. With the fitted semivariogram, we define the expected visual dissimilarity at a given spatial distance as reference to identify hard negatives and false negatives. We integrate this strategy into GeoCLIP and evaluate it on the OSV5M dataset, demonstrating that explicitly modeling spatial priors improves image-based geo-localization performance, particularly at finer granularity.</li>
</ul>

<h3>Title: X-Streamer: Unified Human World Modeling with Audiovisual Interaction</h3>
<ul>
<li><strong>Authors: </strong>You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21574">https://arxiv.org/abs/2509.21574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21574">https://arxiv.org/pdf/2509.21574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21574]] X-Streamer: Unified Human World Modeling with Audiovisual Interaction(https://arxiv.org/abs/2509.21574)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.</li>
</ul>

<h3>Title: Interpretable time series analysis with Gumbel dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yiliu Wang, Timothy Doyeon Kim, Eric Shea-Brown, Uygar Smbl</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21578">https://arxiv.org/abs/2509.21578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21578">https://arxiv.org/pdf/2509.21578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21578]] Interpretable time series analysis with Gumbel dynamics(https://arxiv.org/abs/2509.21578)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Switching dynamical systems can model complicated time series data while maintaining interpretability by inferring a finite set of dynamics primitives and explaining different portions of the observed time series with one of these primitives. However, due to the discrete nature of this set, such models struggle to capture smooth, variable-speed transitions, as well as stochastic mixtures of overlapping states, and the inferred dynamics often display spurious rapid switching on real-world datasets. Here, we propose the Gumbel Dynamical Model (GDM). First, by introducing a continuous relaxation of discrete states and a different noise model defined on the relaxed-discrete state space via the Gumbel distribution, GDM expands the set of available state dynamics, allowing the model to approximate smoother and non-stationary ground-truth dynamics more faithfully. Second, the relaxation makes the model fully differentiable, enabling fast and scalable training with standard gradient descent methods. We validate our approach on standard simulation datasets and highlight its ability to model soft, sticky states and transitions in a stochastic setting. Furthermore, we apply our model to two real-world datasets, demonstrating its ability to infer interpretable states in stochastic time series with multiple dynamics, a setting where traditional methods often fail.</li>
</ul>

<h3>Title: From Indexing to Coding: A New Paradigm for Data Availability Sampling</h3>
<ul>
<li><strong>Authors: </strong>Moritz Grundei, Aayush Rajasekaran, Kishori Konwar, Muriel Medard</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21586">https://arxiv.org/abs/2509.21586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21586">https://arxiv.org/pdf/2509.21586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21586]] From Indexing to Coding: A New Paradigm for Data Availability Sampling(https://arxiv.org/abs/2509.21586)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The data availability problem is a central challenge in blockchain systems and lies at the core of the accessibility and scalability issues faced by platforms such as Ethereum. Modern solutions employ several approaches, with data availability sampling (DAS) being the most self-sufficient and minimalistic in its security assumptions. Existing DAS methods typically form cryptographic commitments on codewords of fixed-rate erasure codes, which restrict light nodes to sampling from a predetermined set of coded symbols. In this paper, we introduce a new approach to DAS that modularizes the coding and commitment process by committing to the uncoded data while performing sampling through on-the-fly coding. The resulting samples are significantly more expressive, enabling light nodes to obtain, in concrete implementations, up to multiple orders of magnitude stronger assurances of data availability than from sampling pre-committed symbols from a fixed-rate redundancy code as done in established DAS schemes using Reed Solomon or low density parity check codes. We present a concrete protocol that realizes this paradigm using random linear network coding (RLNC).</li>
</ul>

<h3>Title: It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store</h3>
<ul>
<li><strong>Authors: </strong>Ben Rosenzweig, Valentino Dalla Valle, Giovanni Apruzzese, Aurore Fass</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21590">https://arxiv.org/abs/2509.21590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21590">https://arxiv.org/pdf/2509.21590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21590]] It's not Easy: Applying Supervised Machine Learning to Detect Malicious Extensions in the Chrome Web Store(https://arxiv.org/abs/2509.21590)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Google Chrome is the most popular Web browser. Users can customize it with extensions that enhance their browsing experience. The most well-known marketplace of such extensions is the Chrome Web Store (CWS). Developers can upload their extensions on the CWS, but such extensions are made available to users only after a vetting process carried out by Google itself. Unfortunately, some malicious extensions bypass such checks, putting the security and privacy of downstream browser extension users at risk. Here, we scrutinize the extent to which automated mechanisms reliant on supervised machine learning (ML) can be used to detect malicious extensions on the CWS. To this end, we first collect 7,140 malicious extensions published in 2017--2023. We combine this dataset with 63,598 benign extensions published or updated on the CWS before 2023, and we develop three supervised-ML-based classifiers. We show that, in a "lab setting", our classifiers work well (e.g., 98% accuracy). Then, we collect a more recent set of 35,462 extensions from the CWS, published or last updated in 2023, with unknown ground truth. We were eventually able to identify 68 malicious extensions that bypassed the vetting process of the CWS. However, our classifiers also reported >1k likely malicious extensions. Based on this finding (further supported with empirical evidence), we elucidate, for the first time, a strong concept drift effect on browser extensions. We also show that commercial detectors (e.g., VirusTotal) work poorly to detect known malicious extensions. Altogether, our results highlight that detecting malicious browser extensions is a fundamentally hard problem. This requires additional work both by the research community and by Google itself -- potentially by revising their approaches. In the meantime, we informed Google of our discoveries, and we release our artifacts.</li>
</ul>

<h3>Title: Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sai Varun Kodathala, Rakesh Vunnam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21595">https://arxiv.org/abs/2509.21595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21595">https://arxiv.org/pdf/2509.21595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21595]] Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis(https://arxiv.org/abs/2509.21595)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive comparative analysis of two prominent self-supervised learning architectures for video action recognition: DINOv3, which processes frames independently through spatial feature extraction, and V-JEPA2, which employs joint temporal modeling across video sequences. We evaluate both approaches on the UCF Sports dataset, examining feature quality through multiple dimensions including classification accuracy, clustering performance, intra-class consistency, and inter-class discrimination. Our analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates exceptional discrimination capability (6.16x separation ratio) particularly for pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across all action types with significantly lower performance variance (0.094 vs 0.288). Through action-specific evaluation, we identify that DINOv3's spatial processing architecture excels at static pose recognition but shows degraded performance on motion-dependent actions, whereas V-JEPA2's temporal modeling provides balanced representation quality across diverse action categories. These findings contribute to the understanding of architectural design choices in video analysis systems and provide empirical guidance for selecting appropriate feature extraction methods based on task requirements and reliability constraints.</li>
</ul>

<h3>Title: World's First Authenticated Satellite Pseudorange from Orbit</h3>
<ul>
<li><strong>Authors: </strong>Jason Anderson</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21601">https://arxiv.org/abs/2509.21601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21601">https://arxiv.org/pdf/2509.21601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21601]] World's First Authenticated Satellite Pseudorange from Orbit(https://arxiv.org/abs/2509.21601)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, watermark</a></li>
<li><strong>Abstract: </strong>Cryptographic Ranging Authentication is here! We present initial results on the Pulsar authenticated ranging service broadcast from space with Pulsar-0 utilizing a recording taken at Xona headquarters in Burlingame, CA. No assumptions pertaining to the ownership or leakage of encryption keys are required. This work discusses the Pulsar watermark design and security analysis. We derive the Pulsar watermark's probabilities of missed detection and false alarm, and we discuss the required receiver processing needed to utilize the Pulsar watermark. We present validation results of the Pulsar watermark utilizing the transmissions from orbit. Lastly, we provide results that demonstrate the spoofing detection efficacy with a spoofing scenario that incorporates the authentic transmissions from orbit. Because we make no assumption about the leakage of symmetric encryption keys, this work provides mathematical justification of the watermark's security, and our July 2025 transmissions from orbit, we claim the world's first authenticated satellite pseudorange from orbit.</li>
</ul>

<h3>Title: GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks</h3>
<ul>
<li><strong>Authors: </strong>Tian Yu Yen, Reese E. Jones, Ravi G. Patel</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21605">https://arxiv.org/abs/2509.21605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21605">https://arxiv.org/pdf/2509.21605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21605]] GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks(https://arxiv.org/abs/2509.21605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Operator learning is a recently developed generalization of regression to mappings between functions. It promises to drastically reduce expensive numerical integration of PDEs to fast evaluations of mappings between functional states of a system, i.e., surrogate and reduced-order modeling. Operator learning has already found applications in several areas such as modeling sea ice, combustion, and atmospheric physics. Recent approaches towards integrating uncertainty quantification into the operator models have relied on likelihood based methods to infer parameter distributions from noisy data. However, stochastic operators may yield actions from which a likelihood is difficult or impossible to construct. In this paper, we introduce, GenUQ, a measure-theoretic approach to UQ that avoids constructing a likelihood by introducing a generative hyper-network model that produces parameter distributions consistent with observed data. We demonstrate that GenUQ outperforms other UQ methods in three example problems, recovering a manufactured operator, learning the solution operator to a stochastic elliptic PDE, and modeling the failure location of porous steel under tension.</li>
</ul>

<h3>Title: Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection</h3>
<ul>
<li><strong>Authors: </strong>Seohyeon Cha, Huancheng Chen, Haris Vikalo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21606">https://arxiv.org/abs/2509.21606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21606">https://arxiv.org/pdf/2509.21606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21606]] Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection(https://arxiv.org/abs/2509.21606)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated continual learning (FCL) enables distributed client devices to learn from streaming data across diverse and evolving tasks. A major challenge to continual learning, catastrophic forgetting, is exacerbated in decentralized settings by the data heterogeneity, constrained communication and privacy concerns. We propose Federated gradient Projection-based Continual Learning with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates forgetting by projecting client updates onto the orthogonal complement of the subspace spanned by previously learned representations of the global model. This projection reduces interference with earlier tasks and preserves performance across the task sequence. To further address the challenge of task-agnostic inference, we incorporate a lightweight mechanism that leverages core bases from prior tasks to predict task identity and dynamically adjust the global model's outputs. Extensive experiments across standard FCL benchmarks demonstrate that FedProTIP significantly outperforms state-of-the-art methods in average accuracy, particularly in settings where task identities are a priori unknown.</li>
</ul>

<h3>Title: VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment</h3>
<ul>
<li><strong>Authors: </strong>Md. Mahfuzur Rahman, Kishor Datta Gupta, Marufa Kamal, Fahad Rahman, Sunzida Siddique, Ahmed Rafi Hasan, Mohd Ariful Haque, Roy George</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21609">https://arxiv.org/abs/2509.21609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21609">https://arxiv.org/pdf/2509.21609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21609]] VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment(https://arxiv.org/abs/2509.21609)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Immediate damage assessment is essential after natural catastrophes; yet, conventional hand evaluation techniques are sluggish and perilous. Although satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives of impacted regions, current computer vision methodologies generally yield just classification labels or segmentation masks, so constraining their capacity to deliver a thorough situational comprehension. We introduce the Vision Language Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive, contextually-informed explanations of disaster imagery. VLCE employs a dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset. Both systems utilize external semantic knowledge from ConceptNet and WordNet to expand vocabulary coverage and improve description accuracy. We assess VLCE in comparison to leading vision-language models (LLaVA and QwenVL) utilizing CLIPScore for semantic alignment and InfoMetIC for caption informativeness. Experimental findings indicate that VLCE markedly surpasses baseline models, attaining a maximum of 95.33% on InfoMetIC while preserving competitive semantic alignment. Our dual-architecture system demonstrates significant potential for improving disaster damage assessment by automating the production of actionable, information-dense descriptions from satellite and drone photos.</li>
</ul>

<h3>Title: Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Kong, Cong Yang, Oya Deniz Beyan, Zeyd Boukhers</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21613">https://arxiv.org/abs/2509.21613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21613">https://arxiv.org/pdf/2509.21613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21613]] Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective(https://arxiv.org/abs/2509.21613)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.</li>
</ul>

<h3>Title: LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Marco Paul E. Apolinario, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21617">https://arxiv.org/abs/2509.21617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21617">https://arxiv.org/pdf/2509.21617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21617]] LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning(https://arxiv.org/abs/2509.21617)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>On-device learning is essential for personalization, privacy, and long-term adaptation in resource-constrained environments. Achieving this requires efficient learning, both fine-tuning existing models and continually acquiring new tasks without catastrophic forgetting. Yet both settings are constrained by high memory cost of storing activations during backpropagation. Existing activation compression methods reduce this cost but relying on repeated low-rank decompositions, introducing computational overhead. Also, such methods have not been explored for continual learning. We propose LANCE (Low-rank Activation Compression), a framework that performs one-shot higher-order Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for activation projection. This eliminates repeated decompositions, reducing both memory and computation. Moreover, fixed low-rank subspaces further enable on-device continual learning by allocating tasks to orthogonal subspaces without storing large task-specific matrices. Experiments show that LANCE reduces activation storage up to 250$\times$ while maintaining accuracy comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets, Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive with orthogonal gradient projection methods at a fraction of the memory cost. These results position LANCE as a practical and scalable solution for efficient fine-tuning and continual learning on edge devices.</li>
</ul>

<h3>Title: PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters</h3>
<ul>
<li><strong>Authors: </strong>Krishu K Thapa, Reet Barik, Krishna Teja Chitty-Venkata, Murali Emani, Venkatram Vishwanath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21619">https://arxiv.org/abs/2509.21619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21619">https://arxiv.org/pdf/2509.21619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21619]] PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters(https://arxiv.org/abs/2509.21619)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Training large models ranging from millions to billions of parameters is highly resource-intensive, requiring significant time, compute, and memory. It is observed that most of the learning (higher change in weights) takes place in the earlier stage of the training loop. These changes stabilize as training continues, enabling them to be captured by matrices of a low intrinsic rank. Therefore, we propose an approach to identify such states of partial convergence and dynamically switch from full parameter training to Low-Rank Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that leverages user-defined hyperparameters to determine the switching point and assign a rank specific to each module layer based on its level of convergence. Experimental results show that this approach preserves model accuracy while reducing the number of trainable parameters to 10% of its original size, resulting in a 3x improvement in throughput, and a 1.5x reduction in average training time per epoch while also reducing GPU memory consumption by 20%</li>
</ul>

<h3>Title: OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21623">https://arxiv.org/abs/2509.21623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21623">https://arxiv.org/pdf/2509.21623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21623]] OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule(https://arxiv.org/abs/2509.21623)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.</li>
</ul>

<h3>Title: A Data-driven Typology of Vision Models from Integrated Representational Metrics</h3>
<ul>
<li><strong>Authors: </strong>Jialin Wu, Shreya Saha, Yiqing Bo, Meenakshi Khosla</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21628">https://arxiv.org/abs/2509.21628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21628">https://arxiv.org/pdf/2509.21628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21628]] A Data-driven Typology of Vision Models from Integrated Representational Metrics(https://arxiv.org/abs/2509.21628)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.</li>
</ul>

<h3>Title: Towards Transparent AI: A Survey on Explainable Language Models</h3>
<ul>
<li><strong>Authors: </strong>Avash Palikhe, Zichong Wang, Zhipeng Yin, Rui Guo, Qiang Duan, Jie Yang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21631">https://arxiv.org/abs/2509.21631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21631">https://arxiv.org/pdf/2509.21631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21631]] Towards Transparent AI: A Survey on Explainable Language Models(https://arxiv.org/abs/2509.21631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) have significantly advanced natural language processing and enabled remarkable progress across diverse domains, yet their black-box nature raises critical concerns about the interpretability of their internal mechanisms and decision-making processes. This lack of transparency is particularly problematic for adoption in high-stakes domains, where stakeholders need to understand the rationale behind model outputs to ensure accountability. On the other hand, while explainable artificial intelligence (XAI) methods have been well studied for non-LMs, they face many limitations when applied to LMs due to their complex architectures, considerable training corpora, and broad generalization abilities. Although various surveys have examined XAI in the context of LMs, they often fail to capture the distinct challenges arising from the architectural diversity and evolving capabilities of these models. To bridge this gap, this survey presents a comprehensive review of XAI techniques with a particular emphasis on LMs, organizing them according to their underlying transformer architectures: encoder-only, decoder-only, and encoder-decoder, and analyzing how methods are adapted to each while assessing their respective strengths and limitations. Furthermore, we evaluate these techniques through the dual lenses of plausibility and faithfulness, offering a structured perspective on their effectiveness. Finally, we identify open research challenges and outline promising future directions, aiming to guide ongoing efforts toward the development of robust, transparent, and interpretable XAI methods for LMs.</li>
</ul>

<h3>Title: MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Sharma, Haohuang Wen, Vinod Yegneswaran, Ashish Gehani, Phillip Porras, Zhiqiang Lin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21634">https://arxiv.org/abs/2509.21634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21634">https://arxiv.org/pdf/2509.21634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21634]] MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs(https://arxiv.org/abs/2509.21634)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The evolution toward 6G networks is being accelerated by the Open Radio Access Network (O-RAN) paradigm -- an open, interoperable architecture that enables intelligent, modular applications across public telecom and private enterprise domains. While this openness creates unprecedented opportunities for innovation, it also expands the attack surface, demanding resilient, low-cost, and autonomous security solutions. Legacy defenses remain largely reactive, labor-intensive, and inadequate for the scale and complexity of next-generation systems. Current O-RAN applications focus mainly on network optimization or passive threat detection, with limited capability for closed-loop, automated response. To address this critical gap, we present an agentic AI framework for fully automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM orchestrates security workflows through a modular multi-agent system powered by Large Language Models (LLMs). The framework features a Threat Analysis Agent for real-time data triage, a Threat Classification Agent that uses Retrieval-Augmented Generation (RAG) to map anomalies to specific countermeasures, and a Threat Response Agent that safely operationalizes mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped with robust safety guardrails, MobiLLM provides a blueprint for trustworthy AI-driven network security. Initial evaluations demonstrate that MobiLLM can effectively identify and orchestrate complex mitigation strategies, significantly reducing response latency and showcasing the feasibility of autonomous security operations in 6G.</li>
</ul>

<h3>Title: Understanding and Enhancing Mask-Based Pretraining towards Universal Representations</h3>
<ul>
<li><strong>Authors: </strong>Mingze Dong, Leda Wang, Yuval Kluger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21650">https://arxiv.org/abs/2509.21650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21650">https://arxiv.org/pdf/2509.21650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21650]] Understanding and Enhancing Mask-Based Pretraining towards Universal Representations(https://arxiv.org/abs/2509.21650)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Mask-based pretraining has become a cornerstone of modern large-scale models across language, vision, and recently biology. Despite its empirical success, its role and limits in learning data representations have been unclear. In this work, we show that the behavior of mask-based pretraining can be directly characterized by test risk in high-dimensional minimum-norm ("ridge-less") linear regression, without relying on further model specifications. Further analysis of linear models uncovers several novel aspects of mask-based pretraining. The theoretical framework and its implications have been validated across diverse neural architectures (including MLPs, CNNs, and Transformers) applied to both vision and language tasks. Guided by our theory, we propose an embarrassingly simple yet overlooked pretraining scheme named Randomly Random Mask AutoEncoding (R$^2$MAE), which enforces capturing multi-scale features from data and is able to outperform optimal fixed mask ratio settings in our linear model framework. We implement R$^2$MAE in vision, language, DNA sequence, and single-cell models, where it consistently outperforms standard and more complicated masking schemes, leading to improvements for state-of-the-art models. Our code is available at: this https URL</li>
</ul>

<h3>Title: DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Ren, Wenhao Gao, Lexing Ying, Grant M. Rotskoff, Jiequn Han</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21655">https://arxiv.org/abs/2509.21655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21655">https://arxiv.org/pdf/2509.21655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21655]] DriftLite: Lightweight Drift Control for Inference-Time Scaling of Diffusion Models(https://arxiv.org/abs/2509.21655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study inference-time scaling for diffusion models, where the goal is to adapt a pre-trained model to new target distributions without retraining. Existing guidance-based methods are simple but introduce bias, while particle-based corrections suffer from weight degeneracy and high computational cost. We introduce DriftLite, a lightweight, training-free particle-based approach that steers the inference dynamics on the fly with provably optimal stability control. DriftLite exploits a previously unexplored degree of freedom in the Fokker-Planck equation between the drift and particle potential, and yields two practical instantiations: Variance- and Energy-Controlling Guidance (VCG/ECG) for approximating the optimal drift with minimal overhead. Across Gaussian mixture models, particle systems, and large-scale protein-ligand co-folding problems, DriftLite consistently reduces variance and improves sample quality over pure guidance and sequential Monte Carlo baselines. These results highlight a principled, efficient route toward scalable inference-time adaptation of diffusion models.</li>
</ul>

<h3>Title: RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion</h3>
<ul>
<li><strong>Authors: </strong>Siming Shan, Min Zhu, Youzuo Lin, Lu Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21659">https://arxiv.org/abs/2509.21659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21659">https://arxiv.org/pdf/2509.21659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21659]] RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion(https://arxiv.org/abs/2509.21659)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Partial differential equation (PDE)-governed inverse problems are fundamental across various scientific and engineering applications; yet they face significant challenges due to nonlinearity, ill-posedness, and sensitivity to noise. Here, we introduce a new computational framework, RED-DiffEq, by integrating physics-driven inversion and data-driven learning. RED-DiffEq leverages pretrained diffusion models as a regularization mechanism for PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform inversion problem in geophysics, a challenging seismic imaging technique that seeks to reconstruct high-resolution subsurface velocity models from seismic measurement data. Our method shows enhanced accuracy and robustness compared to conventional methods. Additionally, it exhibits strong generalization ability to more complex velocity models that the diffusion model is not trained on. Our framework can also be directly applied to diverse PDE-governed inverse problems.</li>
</ul>

<h3>Title: MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Afrina Tabassum, Bin Guo, Xiyao Ma, Hoda Eldardiry, Ismini Lourentzou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21662">https://arxiv.org/abs/2509.21662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21662">https://arxiv.org/pdf/2509.21662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21662]] MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning(https://arxiv.org/abs/2509.21662)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Procedural Planning (MPP) aims to generate step-by-step instructions that combine text and images, with the central challenge of preserving object-state consistency across modalities while producing informative plans. Existing approaches often leverage large language models (LLMs) to refine textual steps; however, visual object-state alignment and systematic evaluation are largely underexplored. We present MMPlanner, a zero-shot MPP framework that introduces Object State Reasoning Chain-of-Thought (OSR-CoT) prompting to explicitly model object-state transitions and generate accurate multimodal plans. To assess plan quality, we design LLM-as-a-judge protocols for planning accuracy and cross-modal alignment, and further propose a visual step-reordering task to measure temporal coherence. Experiments on RECIPEPLAN and WIKIPLAN show that MMPlanner achieves state-of-the-art performance, improving textual planning by +6.8%, cross-modal alignment by +11.9%, and visual step ordering by +26.7%</li>
</ul>

<h3>Title: MORPH: Shape-agnostic PDE Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Ayan Biswas, Diane Oyen, Earl Lawrence</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21670">https://arxiv.org/abs/2509.21670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21670">https://arxiv.org/pdf/2509.21670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21670]] MORPH: Shape-agnostic PDE Foundation Models(https://arxiv.org/abs/2509.21670)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.</li>
</ul>

<h3>Title: ReviewScore: Misinformed Peer Review Detection with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyun Ryu, Doohyuk Jang, Hyemin S. Lee, Joonhyun Jeong, Gyeongman Kim, Donghyeon Cho, Gyouk Chu, Minyeong Hwang, Hyeongwon Jang, Changhun Kim, Haechan Kim, Jina Kim, Joowon Kim, Yoonjeon Kim, Kwanhyung Lee, Chanjae Park, Heecheol Yun, Gregor Betz, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21679">https://arxiv.org/abs/2509.21679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21679">https://arxiv.org/pdf/2509.21679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21679]] ReviewScore: Misinformed Peer Review Detection with Large Language Models(https://arxiv.org/abs/2509.21679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either "weaknesses" in a review that contain incorrect premises, or "questions" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation.</li>
</ul>

<h3>Title: Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Kataria, Davood Fattahi, Minxiao Wang, Ran Xiao, Matthew Clark, Timothy Ruchti, Mark Mai, Xiao Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21695">https://arxiv.org/abs/2509.21695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21695">https://arxiv.org/pdf/2509.21695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21695]] Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment(https://arxiv.org/abs/2509.21695)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>High-frequency physiological waveform modality offers deep, real-time insights into patient status. Recently, physiological foundation models based on Photoplethysmography (PPG), such as PPG-GPT, have been shown to predict critical events, including Cardiac Arrest (CA). However, their powerful representation still needs to be leveraged suitably, especially when the downstream data/label is scarce. We offer three orthogonal improvements to improve PPG-only CA systems by using minimal auxiliary information. First, we propose to use time-to-event modeling, either through simple regression to the event onset time or by pursuing fine-grained discrete survival modeling. Second, we encourage the model to learn CA-focused features by making them patient-identity invariant. This is achieved by first training the largest-scale de-identified biometric identification model, referred to as the p-vector, and subsequently using it adversarially to deconfound cues, such as person identity, that may cause overfitting through memorization. Third, we propose regression on the pseudo-lab values generated by pre-trained auxiliary estimator networks. This is crucial since true blood lab measurements, such as lactate, sodium, troponin, and potassium, are collected sparingly. Via zero-shot prediction, the auxiliary networks can enrich cardiac arrest waveform labels and generate pseudo-continuous estimates as targets. Our proposals can independently improve the 24-hour time-averaged AUC from the 0.74 to the 0.78-0.80 range. We primarily improve over longer time horizons with minimal degradation near the event, thus pushing the Early Warning System research. Finally, we pursue multi-task formulation and diagnose it with a high gradient conflict rate among competing losses, which we alleviate via the PCGrad optimization technique.</li>
</ul>

<h3>Title: MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss</h3>
<ul>
<li><strong>Authors: </strong>Jiali Zhang, Thomas S. White, Haoliang Zhang, Wenqing Hu, Donald C. Wunsch II, Jian Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21696">https://arxiv.org/abs/2509.21696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21696">https://arxiv.org/pdf/2509.21696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21696]] MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss(https://arxiv.org/abs/2509.21696)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Infrared imaging has emerged as a robust solution for urban object detection under low-light and adverse weather conditions, offering significant advantages over traditional visible-light cameras. However, challenges such as class imbalance, thermal noise, and computational constraints can significantly hinder model performance in practical settings. To address these issues, we evaluate multiple YOLO variants on the FLIR ADAS V2 dataset, ultimately selecting YOLOv8 as our baseline due to its balanced accuracy and efficiency. Building on this foundation, we present \texttt{MS-YOLO} (\textbf{M}obileNetv4 and \textbf{S}lideLoss based on YOLO), which replaces YOLOv8's CSPDarknet backbone with the more efficient MobileNetV4, reducing computational overhead by \textbf{1.5%} while sustaining high accuracy. In addition, we introduce \emph{SlideLoss}, a novel loss function that dynamically emphasizes under-represented and occluded samples, boosting precision without sacrificing recall. Experiments on the FLIR ADAS V2 benchmark show that \texttt{MS-YOLO} attains competitive mAP and superior precision while operating at only \textbf{6.7 GFLOPs}. These results demonstrate that \texttt{MS-YOLO} effectively addresses the dual challenge of maintaining high detection quality while minimizing computational costs, making it well-suited for real-time edge deployment in urban environments.</li>
</ul>

<h3>Title: GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures</h3>
<ul>
<li><strong>Authors: </strong>Ying Li, Tiejun Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21698">https://arxiv.org/abs/2509.21698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21698">https://arxiv.org/pdf/2509.21698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21698]] GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures(https://arxiv.org/abs/2509.21698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Risk categorization in 10-K risk disclosures matters for oversight and investment, yet no public benchmark evaluates unsupervised topic models for this task. We present GRAB, a finance-specific benchmark with 1.61M sentences from 8,247 filings and span-grounded sentence labels produced without manual annotation by combining FinBERT token attention, YAKE keyphrase signals, and taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy mapping 193 terms to 21 fine-grained types nested under five macro classes; the 21 types guide weak supervision, while evaluation is reported at the macro level. GRAB unifies evaluation with fixed dataset splits and robust metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective Number of Topics. The dataset, labels, and code enable reproducible, standardized comparison across classical, embedding-based, neural, and hybrid topic models on financial disclosures.</li>
</ul>

<h3>Title: Exact Subgraph Isomorphism Network for Predictive Graph Mining</h3>
<ul>
<li><strong>Authors: </strong>Taiga Kojima, Masayuki Karasuyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21699">https://arxiv.org/abs/2509.21699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21699">https://arxiv.org/pdf/2509.21699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21699]] Exact Subgraph Isomorphism Network for Predictive Graph Mining(https://arxiv.org/abs/2509.21699)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In the graph-level prediction task (predict a label for a given graph), the information contained in subgraphs of the input graph plays a key role. In this paper, we propose Exact subgraph Isomorphism Network (EIN), which combines the exact subgraph enumeration, neural network, and a sparse regularization. In general, building a graph-level prediction model achieving high discriminative ability along with interpretability is still a challenging problem. Our combination of the subgraph enumeration and neural network contributes to high discriminative ability about the subgraph structure of the input graph. Further, the sparse regularization in EIN enables us 1) to derive an effective pruning strategy that mitigates computational difficulty of the enumeration while maintaining the prediction performance, and 2) to identify important subgraphs that contributes to high interpretability. We empirically show that EIN has sufficiently high prediction performance compared with standard graph neural network models, and also, we show examples of post-hoc analysis based on the selected subgraphs.</li>
</ul>

<h3>Title: PQFed: A Privacy-Preserving Quality-Controlled Federated Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Yue, Wenbiao Li, Yuzhou Jiang, Anisa Halimi, Roger French, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21704">https://arxiv.org/abs/2509.21704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21704">https://arxiv.org/pdf/2509.21704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21704]] PQFed: A Privacy-Preserving Quality-Controlled Federated Learning Framework(https://arxiv.org/abs/2509.21704)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning enables collaborative model training without sharing raw data, but data heterogeneity consistently challenges the performance of the global model. Traditional optimization methods often rely on collaborative global model training involving all clients, followed by local adaptation to improve individual performance. In this work, we focus on early-stage quality control and propose PQFed, a novel privacy-preserving personalized federated learning framework that designs customized training strategies for each client prior to the federated training process. PQFed extracts representative features from each client's raw data and applies clustering techniques to estimate inter-client dataset similarity. Based on these similarity estimates, the framework implements a client selection strategy that enables each client to collaborate with others who have compatible data distributions. We evaluate PQFed on two benchmark datasets, CIFAR-10 and MNIST, integrated with three existing federated learning algorithms. Experimental results show that PQFed consistently improves the target client's model performance, even with a limited number of participants. We further benchmark PQFed against a baseline cluster-based algorithm, IFCA, and observe that PQFed also achieves better performance in low-participation scenarios. These findings highlight PQFed's scalability and effectiveness in personalized federated learning settings.</li>
</ul>

<h3>Title: Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21710">https://arxiv.org/abs/2509.21710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21710">https://arxiv.org/pdf/2509.21710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21710]] Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval(https://arxiv.org/abs/2509.21710)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.</li>
</ul>

<h3>Title: Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing</h3>
<ul>
<li><strong>Authors: </strong>Bingcan Guo, Eryue Xu, Zhiping Zhang, Tianshi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21712">https://arxiv.org/abs/2509.21712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21712">https://arxiv.org/pdf/2509.21712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21712]] Not My Agent, Not My Boundary? Elicitation of Personal Privacy Boundaries in AI-Delegated Information Sharing(https://arxiv.org/abs/2509.21712)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Aligning AI systems with human privacy preferences requires understanding individuals' nuanced disclosure behaviors beyond general norms. Yet eliciting such boundaries remains challenging due to the context-dependent nature of privacy decisions and the complex trade-offs involved. We present an AI-powered elicitation approach that probes individuals' privacy boundaries through a discriminative task. We conducted a between-subjects study that systematically varied communication roles and delegation conditions, resulting in 1,681 boundary specifications from 169 participants for 61 scenarios. We examined how these contextual factors and individual differences influence the boundary specification. Quantitative results show that communication roles influence individuals' acceptance of detailed and identifiable disclosure, AI delegation and individuals' need for privacy heighten sensitivity to disclosed identifiers, and AI delegation results in less consensus across individuals. Our findings highlight the importance of situating privacy preference elicitation within real-world data flows. We advocate using nuanced privacy boundaries as an alignment goal for future AI systems.</li>
</ul>

<h3>Title: Motion-Aware Transformer for Multi-Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Xu Yang, Gady Agam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21715">https://arxiv.org/abs/2509.21715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21715">https://arxiv.org/pdf/2509.21715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21715]] Motion-Aware Transformer for Multi-Object Tracking(https://arxiv.org/abs/2509.21715)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.</li>
</ul>

<h3>Title: DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining</h3>
<ul>
<li><strong>Authors: </strong>Shuning Sun, Jialang Lu, Xiang Chen, Jichao Wang, Dianjie Lu, Guijuan Zhang, Guangwei Gao, Zhuoran Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21719">https://arxiv.org/abs/2509.21719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21719">https://arxiv.org/pdf/2509.21719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21719]] DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining(https://arxiv.org/abs/2509.21719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Videos captured in the wild often suffer from rain streaks, blur, and noise. In addition, even slight changes in camera pose can amplify cross-frame mismatches and temporal artifacts. Existing methods rely on optical flow or heuristic alignment, which are computationally expensive and less robust. To address these challenges, Lie groups provide a principled way to represent continuous geometric transformations, making them well-suited for enforcing spatial and temporal consistency in video modeling. Building on this insight, we propose DeLiVR, an efficient video deraining method that injects spatiotemporal Lie-group differential biases directly into attention scores of the network. Specifically, the method introduces two complementary components. First, a rotation-bounded Lie relative bias predicts the in-plane angle of each frame using a compact prediction module, where normalized coordinates are rotated and compared with base coordinates to achieve geometry-consistent alignment before feature aggregation. Second, a differential group displacement computes angular differences between adjacent frames to estimate a velocity. This bias computation combines temporal decay and attention masks to focus on inter-frame relationships while precisely matching the direction of rain streaks. Extensive experimental results demonstrate the effectiveness of our method on publicly available benchmarks.</li>
</ul>

<h3>Title: On the Status of Foundation Models for SAR Imagery</h3>
<ul>
<li><strong>Authors: </strong>Nathan Inkawhich</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21722">https://arxiv.org/abs/2509.21722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21722">https://arxiv.org/pdf/2509.21722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21722]] On the Status of Foundation Models for SAR Imagery(https://arxiv.org/abs/2509.21722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work we investigate the viability of foundational AI/ML models for Synthetic Aperture Radar (SAR) object recognition tasks. We are inspired by the tremendous progress being made in the wider community, particularly in the natural image domain where frontier labs are training huge models on web-scale datasets with unprecedented computing budgets. It has become clear that these models, often trained with Self-Supervised Learning (SSL), will transform how we develop AI/ML solutions for object recognition tasks - they can be adapted downstream with very limited labeled data, they are more robust to many forms of distribution shift, and their features are highly transferable out-of-the-box. For these reasons and more, we are motivated to apply this technology to the SAR domain. In our experiments we first run tests with today's most powerful visual foundational models, including DINOv2, DINOv3 and PE-Core and observe their shortcomings at extracting semantically-interesting discriminative SAR target features when used off-the-shelf. We then show that Self-Supervised finetuning of publicly available SSL models with SAR data is a viable path forward by training several AFRL-DINOv2s and setting a new state-of-the-art for SAR foundation models, significantly outperforming today's best SAR-domain model SARATR-X. Our experiments further analyze the performance trade-off of using different backbones with different downstream task-adaptation recipes, and we monitor each model's ability to overcome challenges within the downstream environments (e.g., extended operating conditions and low amounts of labeled data). We hope this work will inform and inspire future SAR foundation model builders, because despite our positive results, we still have a long way to go.</li>
</ul>

<h3>Title: ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation</h3>
<ul>
<li><strong>Authors: </strong>Jiho Kim, Junseong Choi, Woosog Chay, Daeun Kyung, Yeonsu Kwon, Yohan Jo, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21730">https://arxiv.org/abs/2509.21730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21730">https://arxiv.org/pdf/2509.21730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21730]] ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation(https://arxiv.org/abs/2509.21730)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into daily life, there is growing demand for AI assistants that are not only reactive but also proactive and personalized. While recent advances have pushed forward proactivity and personalization individually, their combination remains underexplored. To bridge this gap, we introduce ProPerSim, a new task and simulation framework for developing assistants capable of making timely, personalized recommendations in realistic home scenarios. In our simulation environment, a user agent with a rich persona interacts with the assistant, providing ratings on how well each suggestion aligns with its preferences and context. The assistant's goal is to use these ratings to learn and adapt to achieve higher scores over time. Built on ProPerSim, we propose ProPerAssistant, a retrieval-augmented, preference-aligned assistant that continually learns and adapts through user feedback. Experiments across 32 diverse personas show that ProPerAssistant adapts its strategy and steadily improves user satisfaction, highlighting the promise of uniting proactivity and personalization.</li>
</ul>

<h3>Title: How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?</h3>
<ul>
<li><strong>Authors: </strong>Xiliang Zhu, Shi Zong, David Rossouw</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21732">https://arxiv.org/abs/2509.21732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21732">https://arxiv.org/pdf/2509.21732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21732]] How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?(https://arxiv.org/abs/2509.21732)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying Large Language Models (LLMs) for question answering (QA) over lengthy contexts is a significant challenge. In industrial settings, this process is often hindered by high computational costs and latency, especially when multiple questions must be answered based on the same context. In this work, we explore the capabilities of LLMs to answer multiple questions based on the same conversational context. We conduct extensive experiments and benchmark a range of both proprietary and public models on this challenging task. Our findings highlight that while strong proprietary LLMs like GPT-4o achieve the best overall performance, fine-tuned public LLMs with up to 8 billion parameters can surpass GPT-4o in accuracy, which demonstrates their potential for transparent and cost-effective deployment in real-world applications.</li>
</ul>

<h3>Title: Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</h3>
<ul>
<li><strong>Authors: </strong>Houliang Zhou, Rong Zhou, Yangying Liu, Kanhao Zhao, Li Shen, Brian Y. Chen, Yu Zhang, Lifang He, Alzheimer's Disease Neuroimaging Initiative</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21735">https://arxiv.org/abs/2509.21735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21735">https://arxiv.org/pdf/2509.21735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21735]] Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks(https://arxiv.org/abs/2509.21735)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease (AD) progression is crucial for timely intervention. However, this task remains challenging due to the complex dysfunctions in the spatio-temporal characteristics of underlying brain networks, which are often overlooked by existing methods. To address these limitations, we develop an interpretable spatio-temporal graph neural network framework to predict future AD progression, leveraging dual Stochastic Differential Equations (SDEs) to model the irregularly-sampled longitudinal functional magnetic resonance imaging (fMRI) data. We validate our approach on two independent cohorts, including the Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework effectively learns sparse regional and connective importance probabilities, enabling the identification of key brain circuit abnormalities associated with disease progression. Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal lobule as salient regions, with significant disruptions in the ventral attention, dorsal attention, and default mode networks. These abnormalities correlate strongly with longitudinal AD-related clinical symptoms. Moreover, our interpretability strategy reveals both established and novel neural systems-level and sex-specific biomarkers, offering new insights into the neurobiological mechanisms underlying AD progression. Our findings highlight the potential of spatio-temporal graph-based learning for early, individualized prediction of AD progression, even in the context of irregularly-sampled longitudinal imaging data.</li>
</ul>

<h3>Title: POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ziqing Wang, Yibo Wen, William Pattie, Xiao Luo, Weimin Wu, Jerry Yao-Chieh Hu, Abhishek Pandey, Han Liu, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21737">https://arxiv.org/abs/2509.21737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21737">https://arxiv.org/pdf/2509.21737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21737]] POLO: Preference-Guided Multi-Turn Reinforcement Learning for Lead Optimization(https://arxiv.org/abs/2509.21737)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Lead optimization in drug discovery requires efficiently navigating vast chemical space through iterative cycles to enhance molecular properties while preserving structural similarity to the original lead compound. Despite recent advances, traditional optimization methods struggle with sample efficiency-achieving good optimization performance with limited oracle evaluations. Large Language Models (LLMs) provide a promising approach through their in-context learning and instruction following capabilities, which align naturally with these iterative processes. However, existing LLM-based methods fail to leverage this strength, treating each optimization step independently. To address this, we present POLO (Preference-guided multi-turn Optimization for Lead Optimization), which enables LLMs to learn from complete optimization trajectories rather than isolated steps. At its core, POLO introduces Preference-Guided Policy Optimization (PGPO), a novel reinforcement learning algorithm that extracts learning signals at two complementary levels: trajectory-level optimization reinforces successful strategies, while turn-level preference learning provides dense comparative feedback by ranking intermediate molecules within each trajectory. Through this dual-level learning from intermediate evaluation, POLO achieves superior sample efficiency by fully exploiting each costly oracle call. Extensive experiments demonstrate that POLO achieves 84% average success rate on single-property tasks (2.3x better than baselines) and 50% on multi-property tasks using only 500 oracle evaluations, significantly advancing the state-of-the-art in sample-efficient molecular optimization.</li>
</ul>

<h3>Title: LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mehwish Mehmood, Ivor Spence, Muhammad Fahim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21738">https://arxiv.org/abs/2509.21738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21738">https://arxiv.org/pdf/2509.21738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21738]] LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation(https://arxiv.org/abs/2509.21738)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Lightweight retinal vessel segmentation is important for the early diagnosis of vision-threatening and systemic diseases, especially in a real-world clinical environment with limited computational resources. Although segmentation methods based on deep learning are improving, existing models are still facing challenges of small vessel segmentation and high computational costs. To address these challenges, we proposed a new vascular segmentation network, LFA-Net, which incorporates a newly designed attention module, LiteFusion-Attention. This attention module incorporates residual learning connections, Vision Mamba-inspired dynamics, and modulation-based attention, enabling the model to capture local and global context efficiently and in a lightweight manner. LFA-Net offers high performance with 0.11 million parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for resource-constrained environments. We validated our proposed model on DRIVE, STARE, and CHASE_DB with outstanding performance in terms of dice scores of 83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%, respectively. The code of LFA-Net is available online this https URL.</li>
</ul>

<h3>Title: Self-Speculative Biased Decoding for Faster Live Translation</h3>
<ul>
<li><strong>Authors: </strong>Linxiao Zeng, Haoyun Deng, Kangyuan Shu, Shizhen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21740">https://arxiv.org/abs/2509.21740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21740">https://arxiv.org/pdf/2509.21740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21740]] Self-Speculative Biased Decoding for Faster Live Translation(https://arxiv.org/abs/2509.21740)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated impressive capabilities in various text generation tasks. However, it remains challenging to use them off-the-shelf in streaming applications (such as live translation), where the output must continually update as the input context expands, while still maintaining a reasonable computational cost to meet the latency requirement. In this work, we reexamine the re-translation approach to simultaneous translation and propose Self-Speculative Biased Decoding, a novel inference paradigm designed to avoid repeatedly generating output from scratch for a consistently growing input stream. We propose using the most recent output as a draft for the current growing input context. During the verification stage, the output will be biased towards the draft token for a higher draft acceptance rate. This strategy not only minimizes flickering that might distract users but also leads to higher speedups. Conventional decoding may take charge from the point of divergence after draft verification and continue until the end condition is met. Unlike existing speculative decoding strategies, our approach eliminates the need for draft computations, making it a model-agnostic and plug-and-play solution for accelerating latency-sensitive streaming applications. Experimental results on simultaneous text-to-text re-translation demonstrate that our approach achieves up to 1.7x speedup compared to conventional auto-regressive re-translation without compromising quality. Additionally, it significantly reduces flickering by 80% by incorporating the display-only mask-k technique.</li>
</ul>

<h3>Title: HyperCore: Coreset Selection under Noise via Hypersphere Models</h3>
<ul>
<li><strong>Authors: </strong>Brian B. Moser, Arundhati S. Shanbhag, Tobias C. Nauen, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21746">https://arxiv.org/abs/2509.21746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21746">https://arxiv.org/pdf/2509.21746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21746]] HyperCore: Coreset Selection under Noise via Hypersphere Models(https://arxiv.org/abs/2509.21746)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The goal of coreset selection methods is to identify representative subsets of datasets for efficient model training. Yet, existing methods often ignore the possibility of annotation errors and require fixed pruning ratios, making them impractical in real-world settings. We present HyperCore, a robust and adaptive coreset selection framework designed explicitly for noisy environments. HyperCore leverages lightweight hypersphere models learned per class, embedding in-class samples close to a hypersphere center while naturally segregating out-of-class samples based on their distance. By using Youden's J statistic, HyperCore can adaptively select pruning thresholds, enabling automatic, noise-aware data pruning without hyperparameter tuning. Our experiments reveal that HyperCore consistently surpasses state-of-the-art coreset selection methods, especially under noisy and low-data regimes. HyperCore effectively discards mislabeled and ambiguous points, yielding compact yet highly informative subsets suitable for scalable and noise-free learning.</li>
</ul>

<h3>Title: Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qing Zhu, Wangdong Guo, Qirong Mao, Xiaohua Huang, Xiuyan Shao, Wenming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21747">https://arxiv.org/abs/2509.21747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21747">https://arxiv.org/pdf/2509.21747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21747]] Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition(https://arxiv.org/abs/2509.21747)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Group-level emotion recognition (GER) aims to identify holistic emotions within a scene involving multiple individuals. Current existed methods underestimate the importance of visual scene contextual information in modeling individual relationships. Furthermore, they overlook the crucial role of semantic information from emotional labels for complete understanding of emotions. To address this limitation, we propose a novel framework that incorporates visual scene context and label-guided semantic information to improve GER performance. It involves the visual context encoding module that leverages multi-scale scene information to diversely encode individual relationships. Complementarily, the emotion semantic encoding module utilizes group-level emotion labels to prompt a large language model to generate nuanced emotion lexicons. These lexicons, in conjunction with the emotion labels, are then subsequently refined into comprehensive semantic representations through the utilization of a structured emotion tree. Finally, similarity-aware interaction is proposed to align and integrate visual and semantic information, thereby generating enhanced group-level emotion representations and subsequently improving the performance of GER. Experiments on three widely adopted GER datasets demonstrate that our proposed method achieves competitive performance compared to state-of-the-art methods.</li>
</ul>

<h3>Title: SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection</h3>
<ul>
<li><strong>Authors: </strong>Brian B. Moser, Tobias C. Nauen, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Joachim Folz, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21748">https://arxiv.org/abs/2509.21748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21748">https://arxiv.org/pdf/2509.21748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21748]] SubZeroCore: A Submodular Approach with Zero Training for Coreset Selection(https://arxiv.org/abs/2509.21748)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The goal of coreset selection is to identify representative subsets of datasets for efficient model training. Yet, existing approaches paradoxically require expensive training-based signals, e.g., gradients, decision boundary estimates or forgetting counts, computed over the entire dataset prior to pruning, which undermines their very purpose by requiring training on samples they aim to avoid. We introduce SubZeroCore, a novel, training-free coreset selection method that integrates submodular coverage and density into a single, unified objective. To achieve this, we introduce a sampling strategy based on a closed-form solution to optimally balance these objectives, guided by a single hyperparameter that explicitly controls the desired coverage for local density measures. Despite no training, extensive evaluations show that SubZeroCore matches training-based baselines and significantly outperforms them at high pruning rates, while dramatically reducing computational overhead. SubZeroCore also demonstrates superior robustness to label noise, highlighting its practical effectiveness and scalability for real-world scenarios.</li>
</ul>

<h3>Title: Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhen Xiong, Yujun Cai, Zhecheng Li, Junsong Yuan, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21749">https://arxiv.org/abs/2509.21749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21749">https://arxiv.org/pdf/2509.21749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21749]] Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models(https://arxiv.org/abs/2509.21749)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q\&A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce Thinking-with-Sound (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively think with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct MELD-Hard1k, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain $24.73\%$ absolute accuracy, with improvements scaling consistently up to $36.61\%$ for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.</li>
</ul>

<h3>Title: KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Da Chang, Xi Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21750">https://arxiv.org/abs/2509.21750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21750">https://arxiv.org/pdf/2509.21750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21750]] KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields(https://arxiv.org/abs/2509.21750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.</li>
</ul>

<h3>Title: UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Lan Chen, Yuchao Gu, Qi Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21760">https://arxiv.org/abs/2509.21760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21760">https://arxiv.org/pdf/2509.21760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21760]] UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models(https://arxiv.org/abs/2509.21760)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at this https URL.</li>
</ul>

<h3>Title: Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang, Stephen Wang, Yueming Jin, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21761">https://arxiv.org/abs/2509.21761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21761">https://arxiv.org/pdf/2509.21761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21761]] Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models(https://arxiv.org/abs/2509.21761)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks through data poisoning, yet the internal mechanisms governing these attacks remain a black box. Previous research on interpretability for LLM safety tends to focus on alignment, jailbreak, and hallucination, but overlooks backdoor mechanisms, making it difficult to understand and fully eliminate the backdoor threat. In this paper, aiming to bridge this gap, we explore the interpretable mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a tripartite causal analysis framework. We first introduce the Backdoor Probe that proves the existence of learnable backdoor features encoded within the representations. Building on this insight, we further develop Backdoor Attention Head Attribution (BAHA), efficiently pinpointing the specific attention heads responsible for processing these features. Our primary experiments reveals these heads are relatively sparse; ablating a minimal \textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these findings to construct the Backdoor Vector derived from these attributed heads as a master controller for the backdoor. Through only \textbf{1-point} intervention on \textbf{single} representation, the vector can either boost ASR up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)} on triggered inputs. In conclusion, our work pioneers the exploration of mechanistic interpretability in LLM backdoors, demonstrating a powerful method for backdoor control and revealing actionable insights for the community.</li>
</ul>

<h3>Title: PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Daiki Chiba, Hiroki Nakano, Takashi Koide</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21772">https://arxiv.org/abs/2509.21772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21772">https://arxiv.org/pdf/2509.21772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21772]] PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation(https://arxiv.org/abs/2509.21772)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks are a significant societal threat, disproportionately harming vulnerable populations and eroding trust in essential digital services. Current defenses are often reactive, failing against modern evasive tactics like cloaking that conceal malicious content. To address this, we introduce PhishLumos, an adaptive multi-agent system that proactively mitigates entire attack campaigns. It confronts a core cybersecurity imbalance: attackers can easily scale operations, while defense remains an intensive expert task. Instead of being blocked by evasion, PhishLumos treats it as a critical signal to investigate the underlying infrastructure. Its Large Language Model (LLM)-powered agents uncover shared hosting, certificates, and domain registration patterns. On real-world data, our system identified 100% of campaigns in the median case, over a week before their confirmation by cybersecurity experts. PhishLumos demonstrates a practical shift from reactive URL blocking to proactive campaign mitigation, protecting users before they are harmed and making the digital world safer for all.</li>
</ul>

<h3>Title: Training-Free Multimodal Deepfake Detection via Graph Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Liu, Fei Wang, Kun Li, Yiqi Nie, Junjie Chen, Yanyan Wei, Zhangling Duan, Zhaohong Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21774">https://arxiv.org/abs/2509.21774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21774">https://arxiv.org/pdf/2509.21774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21774]] Training-Free Multimodal Deepfake Detection via Graph Reasoning(https://arxiv.org/abs/2509.21774)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal deepfake detection (MDD) aims to uncover manipulations across visual, textual, and auditory modalities, thereby reinforcing the reliability of modern information systems. Although large vision-language models (LVLMs) exhibit strong multimodal reasoning, their effectiveness in MDD is limited by challenges in capturing subtle forgery cues, resolving cross-modal inconsistencies, and performing task-aligned retrieval. To this end, we propose Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a training-free framework for MDD. GASP-ICL employs a pipeline to preserve semantic relevance while injecting task-aware knowledge into LVLMs. We leverage an MDD-adapted feature extractor to retrieve aligned image-text pairs and build a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer (GSTAS) to capture cross-sample relations and propagate query-aligned signals, producing discriminative exemplars. This enables precise selection of semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust MDD. Experiments on four forgery types show that GASP-ICL surpasses strong baselines, delivering gains without LVLM fine-tuning.</li>
</ul>

<h3>Title: SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Vianne R. Gao, Chen Xue, Marc Versage, Xie Zhou, Zhongruo Wang, Chao Li, Yeon Seonwoo, Nan Chen, Zhen Ge, Gourab Kundu, Weiqi Zhang, Tian Wang, Qingjun Cui, Trishul Chilimbi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21777">https://arxiv.org/abs/2509.21777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21777">https://arxiv.org/pdf/2509.21777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21777]] SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation(https://arxiv.org/abs/2509.21777)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>The dominant retrieve-then-rank pipeline in large-scale recommender systems suffers from mis-calibration and engineering overhead due to its architectural split and differing optimization objectives. While recent generative sequence models have shown promise in unifying retrieval and ranking by auto-regressively generating ranked items, existing solutions typically address either personalized search or query-free recommendation, often exhibiting performance trade-offs when attempting to unify both. We introduce \textit{SynerGen}, a novel generative recommender model that bridges this critical gap by providing a single generative backbone for both personalized search and recommendation, while simultaneously excelling at retrieval and ranking tasks. Trained on behavioral sequences, our decoder-only Transformer leverages joint optimization with InfoNCE for retrieval and a hybrid pointwise-pairwise loss for ranking, allowing semantic signals from search to improve recommendation and vice versa. We also propose a novel time-aware rotary positional embedding to effectively incorporate time information into the attention mechanism. \textit{SynerGen} achieves significant improvements on widely adopted recommendation and search benchmarks compared to strong generative recommender and joint search and recommendation baselines. This work demonstrates the viability of a single generative foundation model for industrial-scale unified information access.</li>
</ul>

<h3>Title: Beyond Formula Complexity: Effective Information Criterion Improves Performance and Interpretability for Symbolic Regression</h3>
<ul>
<li><strong>Authors: </strong>Zihan Yu, Guanren Wang, Jingtao Ding, Huandong Wang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21780">https://arxiv.org/abs/2509.21780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21780">https://arxiv.org/pdf/2509.21780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21780]] Beyond Formula Complexity: Effective Information Criterion Improves Performance and Interpretability for Symbolic Regression(https://arxiv.org/abs/2509.21780)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Symbolic regression discovers accurate and interpretable formulas to describe given data, thereby providing scientific insights for domain experts and promoting scientific discovery. However, existing symbolic regression methods often use complexity metrics as a proxy for interoperability, which only considers the size of the formula but ignores its internal mathematical structure. Therefore, while they can discover formulas with compact forms, the discovered formulas often have structures that are difficult to analyze or interpret mathematically. In this work, inspired by the observation that physical formulas are typically numerically stable under limited calculation precision, we propose the Effective Information Criterion (EIC). It treats formulas as information processing systems with specific internal structures and identifies the unreasonable structure in them by the loss of significant digits or the amplification of rounding noise as data flows through the system. We find that this criterion reveals the gap between the structural rationality of models discovered by existing symbolic regression algorithms and real-world physical formulas. Combining EIC with various search-based symbolic regression algorithms improves their performance on the Pareto frontier and reduces the irrational structure in the results. Combining EIC with generative-based algorithms reduces the number of samples required for pre-training, improving sample efficiency by 2~4 times. Finally, for different formulas with similar accuracy and complexity, EIC shows a 70.2% agreement with 108 human experts' preferences for formula interpretability, demonstrating that EIC, by measuring the unreasonable structures in formulas, actually reflects the formula's interpretability.</li>
</ul>

<h3>Title: Lattice-Based Dynamic $k$-times Anonymous Authentication</h3>
<ul>
<li><strong>Authors: </strong>Junjie Song, Jinguang Han, Man Ho Au, Rupeng Yang, Chao Sun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21786">https://arxiv.org/abs/2509.21786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21786">https://arxiv.org/pdf/2509.21786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21786]] Lattice-Based Dynamic $k$-times Anonymous Authentication(https://arxiv.org/abs/2509.21786)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>With the development of Internet, privacy has become a close concern of users. Anonymous authentication plays an important role in privacy-preserving systems. $k$-times anonymous authentication ($k$-TAA) scheme allows members of a group to be authenticated anonymously by application providers up to $k$ times. Considering quantum computing attacks, lattice-based $k$-TAA was introduced. However, existing schemes do not support dynamically granting and revoking users. In this paper, we construct the first lattice-based dynamic $k$-TAA, which offers limited times anonymous authentication, dynamic member management, and post-quantum security. We present a concrete construction, and reduce its security to standard complexity assumptions. Notably, compared with existing lattice-based $k$-TAA, our scheme is efficient in terms of communication cost.</li>
</ul>

<h3>Title: DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images</h3>
<ul>
<li><strong>Authors: </strong>Dwip Dalal, Gautam Vashishtha, Anku Ranui, Aishwarya Reganti, Parth Patwa, Mohd Sarique, Chandan Gupta, Keshav Nath, Viswanatha Reddy, Vinija Jain, Aman Chadha, Amitava Das, Amit Sheth, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21787">https://arxiv.org/abs/2509.21787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21787">https://arxiv.org/pdf/2509.21787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21787]] DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images(https://arxiv.org/abs/2509.21787)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, diffusion</a></li>
<li><strong>Abstract: </strong>The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.</li>
</ul>

<h3>Title: LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE</h3>
<ul>
<li><strong>Authors: </strong>Yu Shang, Lei Jin, Yiding Ma, Xin Zhang, Chen Gao, Wei Wu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21790">https://arxiv.org/abs/2509.21790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21790">https://arxiv.org/pdf/2509.21790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21790]] LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE(https://arxiv.org/abs/2509.21790)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Han Yuan, Yue Zhao, Li Zhang, Wuqiong Luo, Zheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21791">https://arxiv.org/abs/2509.21791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21791">https://arxiv.org/pdf/2509.21791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21791]] Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference(https://arxiv.org/abs/2509.21791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions.</li>
</ul>

<h3>Title: FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21792">https://arxiv.org/abs/2509.21792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21792">https://arxiv.org/pdf/2509.21792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21792]] FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning(https://arxiv.org/abs/2509.21792)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at this https URL.</li>
</ul>

<h3>Title: Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection</h3>
<ul>
<li><strong>Authors: </strong>Kourosh Kakhi, Abbas Khosravi, Roohallah Alizadehsani, U. Rajendra Acharyab</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21794">https://arxiv.org/abs/2509.21794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21794">https://arxiv.org/pdf/2509.21794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21794]] Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection(https://arxiv.org/abs/2509.21794)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Fatigue detection using physiological signals is critical in domains such as transportation, healthcare, and performance monitoring. While most studies focus on single modalities, this work examines statistical relationships between signal pairs to improve classification robustness. Using the DROZY dataset, we extracted features from ECG, EMG, EOG, and EEG across 15 signal combinations and evaluated them with Decision Tree, Random Forest, Logistic Regression, and XGBoost. Results show that XGBoost with the EMG EEG combination achieved the best performance. SHAP analysis highlighted ECG EOG correlation as a key feature, and multi signal models consistently outperformed single signal ones. These findings demonstrate that feature level fusion of physiological signals enhances accuracy, interpretability, and practical applicability of fatigue monitoring systems.</li>
</ul>

<h3>Title: MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation</h3>
<ul>
<li><strong>Authors: </strong>Yu Shang, Yangcheng Yu, Xin Zhang, Xin Jin, Haisheng Su, Wei Wu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21797">https://arxiv.org/abs/2509.21797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21797">https://arxiv.org/pdf/2509.21797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21797]] MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation(https://arxiv.org/abs/2509.21797)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: this https URL.</li>
</ul>

<h3>Title: Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21798">https://arxiv.org/abs/2509.21798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21798">https://arxiv.org/pdf/2509.21798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21798]] Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment(https://arxiv.org/abs/2509.21798)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are crucial for aligning large language models (LLMs) with diverse cultures. Consequently, evaluating their cultural awareness is essential for further advancing global alignment of LLMs. However, existing RM evaluations fall short in assessing cultural awareness due to the scarcity of culturally relevant evaluation datasets. To fill this gap, we propose Cultural Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs reveals their deficiencies in modeling cultural awareness and demonstrates a positive correlation between performance on CARB and downstream multilingual cultural alignment tasks. Further analysis identifies the spurious correlations within culture-aware reward modeling, wherein RM's scoring relies predominantly on surface-level features rather than authentic cultural nuance understanding. To address these, we propose Think-as-Locals to elicit deeper culturally grounded reasoning from generative RMs via reinforcement learning from verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate preference judgments and high-quality structured evaluation criteria generation. Experimental results validate its efficacy in mitigating spurious features interference and advancing culture-aware reward modeling.</li>
</ul>

<h3>Title: Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies</h3>
<ul>
<li><strong>Authors: </strong>Qianen Zhang, Satoshi Nakamura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21801">https://arxiv.org/abs/2509.21801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21801">https://arxiv.org/pdf/2509.21801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21801]] Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies(https://arxiv.org/abs/2509.21801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional encoder-decoder policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION and PRONOMINALIZATION, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We implement these actions in a decoder-only large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and latency, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese and English-German benchmarks show that our framework consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower delay (measured by Average Lagging) compared to reference translations and salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the best overall balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.</li>
</ul>

<h3>Title: ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Bohao Zhao, Jingtao Ding, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21802">https://arxiv.org/abs/2509.21802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21802">https://arxiv.org/pdf/2509.21802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21802]] ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations(https://arxiv.org/abs/2509.21802)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately forecasting chaotic systems, prevalent in domains such as weather prediction and fluid dynamics, remains a significant scientific challenge. The inherent sensitivity of these systems to initial conditions, coupled with a scarcity of observational data, severely constrains traditional modeling approaches. Since these models are typically trained for a specific system, they lack the generalization capacity necessary for real-world applications, which demand robust zero-shot or few-shot forecasting on novel or data-limited scenarios. To overcome this generalization barrier, we propose ChaosNexus, a foundation model pre-trained on a diverse corpus of chaotic dynamics. ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented with Mixture-of-Experts layers, to capture both universal patterns and system-specific behaviors. The model demonstrates state-of-the-art zero-shot generalization across both synthetic and real-world benchmarks. On a large-scale testbed comprising over 9,000 synthetic chaotic systems, it improves the fidelity of long-term attractor statistics by more than 40% compared to the leading baseline. This robust performance extends to real-world applications with exceptional data efficiency. For instance, in 5-day global weather forecasting, ChaosNexus achieves a competitive zero-shot mean error below 1 degree, a result that further improves with few-shot fine-tuning. Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding principle for scientific foundation models: cross-system generalization stems from the diversity of training systems, rather than sheer data volume.</li>
</ul>

<h3>Title: Towards Minimal Causal Representations for Human Multimodal Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Menghua Jiang, Yuncheng Jiang, Haifeng Hu, Sijie Mai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21805">https://arxiv.org/abs/2509.21805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21805">https://arxiv.org/pdf/2509.21805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21805]] Towards Minimal Causal Representations for Human Multimodal Language Understanding(https://arxiv.org/abs/2509.21805)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Human Multimodal Language Understanding (MLU) aims to infer human intentions by integrating related cues from heterogeneous modalities. Existing works predominantly follow a ``learning to attend" paradigm, which maximizes mutual information between data and labels to enhance predictive performance. However, such methods are vulnerable to unintended dataset biases, causing models to conflate statistical shortcuts with genuine causal features and resulting in degraded out-of-distribution (OOD) generalization. To alleviate this issue, we introduce a Causal Multimodal Information Bottleneck (CaMIB) model that leverages causal principles rather than traditional likelihood. Concretely, we first applies the information bottleneck to filter unimodal inputs, removing task-irrelevant noise. A parameterized mask generator then disentangles the fused multimodal representation into causal and shortcut subrepresentations. To ensure global consistency of causal features, we incorporate an instrumental variable constraint, and further adopt backdoor adjustment by randomly recombining causal and shortcut features to stabilize causal estimation. Extensive experiments on multimodal sentiment analysis, humor detection, and sarcasm detection, along with OOD test sets, demonstrate the effectiveness of CaMIB. Theoretical and empirical analyses further highlight its interpretability and soundness.</li>
</ul>

<h3>Title: Scaling Laws for Neural Material Models</h3>
<ul>
<li><strong>Authors: </strong>Akshay Trikha, Kyle Chu, Advait Gosai, Parker Szachta, Eric Weiner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21811">https://arxiv.org/abs/2509.21811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21811">https://arxiv.org/pdf/2509.21811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21811]] Scaling Laws for Neural Material Models(https://arxiv.org/abs/2509.21811)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Predicting material properties is crucial for designing better batteries, semiconductors, and medical devices. Deep learning helps scientists quickly find promising materials by predicting their energy, forces, and stresses. Companies scale capacities of deep learning models in multiple domains, such as language modeling, and invest many millions of dollars into such models. Our team analyzes how scaling training data (giving models more information to learn from), model sizes (giving models more capacity to learn patterns), and compute (giving models more computational resources) for neural networks affects their performance for material property prediction. In particular, we trained both transformer and EquiformerV2 neural networks to predict material properties. We find empirical scaling laws for these models: we can predict how increasing each of the three hyperparameters (training data, model size, and compute) affects predictive performance. In particular, the loss $L$ can be measured with a power law relationship $L = \alpha \cdot N^{-\beta}$, where $\alpha$ and $\beta$ are constants while $N$ is the relevant hyperparameter. We also incorporate command-line arguments for changing training settings such as the amount of epochs, maximum learning rate, and whether mixed precision is enabled. Future work could entail further investigating scaling laws for other neural network models in this domain, such as GemNet and fully connected networks, to assess how they compare to the models we trained.</li>
</ul>

<h3>Title: Can LLMs Solve and Generate Linguistic Olympiad Puzzles?</h3>
<ul>
<li><strong>Authors: </strong>Neh Majmudar, Elena Filatova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21820">https://arxiv.org/abs/2509.21820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21820">https://arxiv.org/pdf/2509.21820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21820]] Can LLMs Solve and Generate Linguistic Olympiad Puzzles?(https://arxiv.org/abs/2509.21820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a combination of novel and exciting tasks: the solution and generation of linguistic puzzles. We focus on puzzles used in Linguistic Olympiads for high school students. We first extend the existing benchmark for the task of solving linguistic puzzles. We explore the use of Large Language Models (LLMs), including recent state-of-the-art models such as OpenAI's o1, for solving linguistic puzzles, analyzing their performance across various linguistic topics. We demonstrate that LLMs outperform humans on most puzzles types, except for those centered on writing systems, and for the understudied languages. We use the insights from puzzle-solving experiments to direct the novel task of puzzle generation. We believe that automating puzzle generation, even for relatively simple puzzles, holds promise for expanding interest in linguistics and introducing the field to a broader audience. This finding highlights the importance of linguistic puzzle generation as a research task: such puzzles can not only promote linguistics but also support the dissemination of knowledge about rare and understudied languages.</li>
</ul>

<h3>Title: SoK: Potentials and Challenges of Large Language Models for Reverse Engineering</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Hu, Zhiwei Fu, Shaocong Xie, Steven H. H. Ding, Philippe Charland</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21821">https://arxiv.org/abs/2509.21821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21821">https://arxiv.org/pdf/2509.21821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21821]] SoK: Potentials and Challenges of Large Language Models for Reverse Engineering(https://arxiv.org/abs/2509.21821)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reverse Engineering (RE) is central to software security, enabling tasks such as vulnerability discovery and malware analysis, but it remains labor-intensive and requires substantial expertise. Earlier advances in deep learning start to automate parts of RE, particularly for malware detection and vulnerability classification. More recently, a rapidly growing body of work has applied Large Language Models (LLMs) to similar purposes. Their role compared to prior machine learning remains unclear, since some efforts simply adapt existing pipelines with minimal change while others seek to exploit broader reasoning and generative abilities. These differences, combined with varied problem definitions, methods, and evaluation practices, limit comparability, reproducibility, and cumulative progress. This paper systematizes the field by reviewing 44 research papers, including peer-reviewed publications and preprints, and 18 additional open-source projects that apply LLMs in RE. We propose a taxonomy that organizes existing work by objective, target, method, evaluation strategy, and data scale. Our analysis identifies strengths and limitations, highlights reproducibility and evaluation gaps, and examines emerging risks. We conclude with open challenges and future research directions that aim to guide more coherent and security-relevant applications of LLMs in RE.</li>
</ul>

<h3>Title: ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Lin, Xiaohan Wang, Jie Cao, Jiajun Chai, Guojun Yin, Wei Lin, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21826">https://arxiv.org/abs/2509.21826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21826">https://arxiv.org/pdf/2509.21826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21826]] ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models(https://arxiv.org/abs/2509.21826)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.</li>
</ul>

<h3>Title: Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>The Viet Bui, Tien Mai, Hong Thanh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21828">https://arxiv.org/abs/2509.21828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21828">https://arxiv.org/pdf/2509.21828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21828]] Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2509.21828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.</li>
</ul>

<h3>Title: On the Complexity Theory of Masked Discrete Diffusion: From $\mathrm{poly}(1/)$ to Nearly $$-Free</h3>
<ul>
<li><strong>Authors: </strong>Xunpeng Huang, Yingyu Lin, Nishant Jain, Kaibo Wang, Difan Zou, Yian Ma, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21835">https://arxiv.org/abs/2509.21835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21835">https://arxiv.org/pdf/2509.21835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21835]] On the Complexity Theory of Masked Discrete Diffusion: From $\mathrm{poly}(1/)$ to Nearly $$-Free(https://arxiv.org/abs/2509.21835)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study masked discrete diffusion -- a flexible paradigm for text generation in which tokens are progressively corrupted by special mask symbols before being denoised. Although this approach has demonstrated strong empirical performance, its theoretical complexity in high-dimensional settings remains insufficiently understood. Existing analyses largely focus on uniform discrete diffusion, and more recent attempts addressing masked diffusion either (1) overlook widely used Euler samplers, (2) impose restrictive bounded-score assumptions, or (3) fail to showcase the advantages of masked discrete diffusion over its uniform counterpart. To address this gap, we show that Euler samplers can achieve $\epsilon$-accuracy in total variation (TV) with $\tilde{O}(d^{2}\epsilon^{-3/2})$ discrete score evaluations, thereby providing the first rigorous analysis of typical Euler sampler in masked discrete diffusion. We then propose a Mask-Aware Truncated Uniformization (MATU) approach that both removes bounded-score assumptions and preserves unbiased discrete score approximation. By exploiting the property that each token can be unmasked at most once, MATU attains a nearly $\epsilon$-free complexity of $O(d\,\ln d\cdot (1-\epsilon^2))$. This result surpasses existing uniformization methods under uniform discrete diffusion, eliminating the $\ln(1/\epsilon)$ factor and substantially speeding up convergence. Our findings not only provide a rigorous theoretical foundation for masked discrete diffusion, showcasing its practical advantages over uniform diffusion for text generation, but also pave the way for future efforts to analyze diffusion-based language models developed under masking paradigm.</li>
</ul>

<h3>Title: Semantic Agreement Enables Efficient Open-Ended LLM Cascades</h3>
<ul>
<li><strong>Authors: </strong>Duncan Soiffer, Steven Kolawole, Virginia Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21837">https://arxiv.org/abs/2509.21837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21837">https://arxiv.org/pdf/2509.21837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21837]] Semantic Agreement Enables Efficient Open-Ended LLM Cascades(https://arxiv.org/abs/2509.21837)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cascade systems route computational requests to smaller models when possible and defer to larger models only when necessary, offering a promising approach to balance cost and quality in LLM deployment. However, they face a fundamental challenge in open-ended text generation: determining output reliability when generation quality lies on a continuous spectrum, often with multiple valid responses. To address this, we propose semantic agreement -- meaning-level consensus between ensemble outputs -- as a training-free signal for reliable deferral. We show that when diverse model outputs agree semantically, their consensus is a stronger reliability signal than token-level confidence. Evaluated from 500M to 70B-parameter models, we find that semantic cascades match or surpass target-model quality at 40% of the cost and reduce latency by up to 60%. Our method requires no model internals, works across black-box APIs, and remains robust to model updates, making it a practical baseline for real-world LLM deployment.</li>
</ul>

<h3>Title: DiTraj: training-free trajectory control for video diffusion transformer</h3>
<ul>
<li><strong>Authors: </strong>Cheng Lei, Jiayu Zhang, Yue Ma, Xinyu Wang, Long Chen, Liang Tang, Yiqiang Yan, Fei Su, Zhicheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21839">https://arxiv.org/abs/2509.21839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21839">https://arxiv.org/pdf/2509.21839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21839]] DiTraj: training-free trajectory control for video diffusion transformer(https://arxiv.org/abs/2509.21839)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.</li>
</ul>

<h3>Title: SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingkai Guo, Chaitali Chakrabarti, Deliang Fan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21843">https://arxiv.org/abs/2509.21843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21843">https://arxiv.org/pdf/2509.21843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21843]] SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models(https://arxiv.org/abs/2509.21843)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs' catastrophic accuracy degradation. However, existing BFA methods typically focus on either integer or floating-point models separately, limiting attack flexibility. Moreover, in floating-point models, random bit flips often cause perturbed parameters to extreme values (e.g., flipping in exponent bit), making it not stealthy and leading to numerical runtime error (e.g., invalid tensor values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution. It is achieved through iterative searching and ranking through our defined parameter sensitivity metric, ImpactScore, which combines gradient sensitivity and perturbation range constrained by the benign layer-wise weight distribution. A novel lightweight SKIP searching algorithm is also proposed to greatly reduce searching complexity, which leads to successful SBFA searching taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma models, with only one single bit flip, SBFA successfully degrades accuracy to below random levels on MMLU and SST-2 in both BF16 and INT8 data formats. Remarkably, flipping a single bit out of billions of parameters reveals a severe security concern of SOTA LLM models.</li>
</ul>

<h3>Title: A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design</h3>
<ul>
<li><strong>Authors: </strong>Zichen Zhang, Kunlong Zhang, Hongwei Ruan, Yiming Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21845">https://arxiv.org/abs/2509.21845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21845">https://arxiv.org/pdf/2509.21845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21845]] A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design(https://arxiv.org/abs/2509.21845)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models have advanced the field of question answering, but multi-hop reasoning, where answers require combining evidence across multiple passages, remains difficult. This paper presents a comprehensive evaluation of retrieval strategies for multi-hop question answering within a retrieval-augmented generation framework. We compare cosine similarity, maximal marginal relevance, and a hybrid method that integrates dense embeddings with lexical overlap and re-ranking. To further improve retrieval, we adapt the EfficientRAG pipeline for query optimization, introducing token labeling and iterative refinement while maintaining efficiency. Experiments on the HotpotQA dataset show that the hybrid approach substantially outperforms baseline methods, achieving a relative improvement of 50 percent in exact match and 47 percent in F1 score compared to cosine similarity. Error analysis reveals that hybrid retrieval improves entity recall and evidence complementarity, while remaining limited in handling distractors and temporal reasoning. Overall, the results suggest that hybrid retrieval-augmented generation provides a practical zero-shot solution for multi-hop question answering, balancing accuracy, efficiency, and interpretability.</li>
</ul>

<h3>Title: Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms</h3>
<ul>
<li><strong>Authors: </strong>Rohan Deb, Qiaobo Li, Mayank Shrivastava, Arindam Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21847">https://arxiv.org/abs/2509.21847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21847">https://arxiv.org/pdf/2509.21847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21847]] Beyond Johnson-Lindenstrauss: Uniform Bounds for Sketched Bilinear Forms(https://arxiv.org/abs/2509.21847)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Uniform bounds on sketched inner products of vectors or matrices underpin several important computational and statistical results in machine learning and randomized algorithms, including the Johnson-Lindenstrauss (J-L) lemma, the Restricted Isometry Property (RIP), randomized sketching, and approximate linear algebra. However, many modern analyses involve *sketched bilinear forms*, for which existing uniform bounds either do not apply or are not sharp on general sets. In this work, we develop a general framework to analyze such sketched bilinear forms and derive uniform bounds in terms of geometric complexities of the associated sets. Our approach relies on generic chaining and introduces new techniques for handling suprema over pairs of sets. We further extend these results to the setting where the bilinear form involves a sum of $T$ independent sketching matrices and show that the deviation scales as $\sqrt{T}$. This unified analysis recovers known results such as the J-L lemma as special cases, while extending RIP-type guarantees. Additionally, we obtain improved convergence bounds for sketched Federated Learning algorithms where such cross terms arise naturally due to sketched gradient compression, and design sketched variants of bandit algorithms with sharper regret bounds that depend on the geometric complexity of the action and parameter sets, rather than the ambient dimension.</li>
</ul>

<h3>Title: Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21848">https://arxiv.org/abs/2509.21848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21848">https://arxiv.org/pdf/2509.21848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21848]] Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration(https://arxiv.org/abs/2509.21848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at this https URL.</li>
</ul>

<h3>Title: Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Liu, Ziyang Zhou, Yilin Li, Haiyang Zhang, Yangbin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21849">https://arxiv.org/abs/2509.21849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21849">https://arxiv.org/pdf/2509.21849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21849]] Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models(https://arxiv.org/abs/2509.21849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Empathetic response generation is a crucial task for creating more human-like and supportive conversational agents. However, existing methods face a core trade-off between the analytical depth of specialized models and the generative fluency of Large Language Models (LLMs). To address this, we propose TRACE, Task-decomposed Reasoning for Affective Communication and Empathy, a novel framework that models empathy as a structured cognitive process by decomposing the task into a pipeline for analysis and synthesis. By building a comprehensive understanding before generation, TRACE unites deep analysis with expressive generation. Experimental results show that our framework significantly outperforms strong baselines in both automatic and LLM-based evaluations, confirming that our structured decomposition is a promising paradigm for creating more capable and interpretable empathetic agents. Our code is available at this https URL.</li>
</ul>

<h3>Title: KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Junhao Chen, Yu Huang, Siyuan Li, Rui Yao, Hanqian Li, Hanyu Zhang, Jungang Li, Jian Chen, Bowen Wang, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21856">https://arxiv.org/abs/2509.21856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21856">https://arxiv.org/pdf/2509.21856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21856]] KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues(https://arxiv.org/abs/2509.21856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application paradigm of Large Language Models (LLMs) in knowledge-intensive domains. However, existing benchmarks are limited to single-turn dialogue, while multi-turn dialogue benchmarks typically assess other orthogonal capabilities rather than knowledge-intensive factuality. To bridge this critical gap, we introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields, including medicine, finance, and law. To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories given logically progressive question sequences. The factual capability and information delivery efficiency of the \textit{final-turn} answer are then evaluated using a human-validated automated pipeline. Our experiments reveal that multi-turn contexts degrade performance: factual capability declines due to the contextual noise from self-generated histories, while information efficiency drops as models become more verbose with increasing dialogue length. We then investigate mitigation strategies, demonstrating that retrieval-augmented generation (RAG) can effectively alleviate and even reverse this factual degradation. These findings underscore the importance of our benchmark in evaluating and enhancing the conversational factual capabilities of LLMs in real-world knowledge-intensive applications. Code is available at \href{this https URL}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.</li>
</ul>

<h3>Title: Deepfakes: we need to re-think the concept of "real" images</h3>
<ul>
<li><strong>Authors: </strong>Janis Keuper, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21864">https://arxiv.org/abs/2509.21864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21864">https://arxiv.org/pdf/2509.21864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21864]] Deepfakes: we need to re-think the concept of "real" images(https://arxiv.org/abs/2509.21864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The wide availability and low usability barrier of modern image generation models has triggered the reasonable fear of criminal misconduct and negative social implications. The machine learning community has been engaging this problem with an extensive series of publications proposing algorithmic solutions for the detection of "fake", e.g. entirely generated or partially manipulated images. While there is undoubtedly some progress towards technical solutions of the problem, we argue that current and prior work is focusing too much on generative algorithms and "fake" data-samples, neglecting a clear definition and data collection of "real" images. The fundamental question "what is a real image?" might appear to be quite philosophical, but our analysis shows that the development and evaluation of basically all current "fake"-detection methods is relying on only a few, quite old low-resolution datasets of "real" images like ImageNet. However, the technology for the acquisition of "real" images, aka taking photos, has drastically evolved over the last decade: Today, over 90% of all photographs are produced by smartphones which typically use algorithms to compute an image from multiple inputs (over time) from multiple sensors. Based on the fact that these image formation algorithms are typically neural network architectures which are closely related to "fake"-image generators, we state the position that today, we need to re-think the concept of "real" images. The purpose of this position paper is to raise the awareness of the current shortcomings in this active field of research and to trigger an open discussion whether the detection of "fake" images is a sound objective at all. At the very least, we need a clear technical definition of "real" images and new benchmark datasets.</li>
</ul>

<h3>Title: Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding</h3>
<ul>
<li><strong>Authors: </strong>Seong-Woong Shim, Myunsoo Kim, Jae Hyeon Cho, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21865">https://arxiv.org/abs/2509.21865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21865">https://arxiv.org/pdf/2509.21865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21865]] Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding(https://arxiv.org/abs/2509.21865)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is a framework for grounding Large Language Models (LLMs) in external, up-to-date information. However, recent advancements in context window size allow LLMs to process inputs of up to 128K tokens or more, offering an alternative strategy: supplying the full document context directly to the model, rather than relying on RAG to retrieve a subset of contexts. Nevertheless, this emerging alternative strategy has notable limitations: (i) it is token-inefficient to handle large and potentially redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon; and (iii) under limited model capacity, it amplifies distraction, ultimately degrading LLM output quality. In this paper, we propose LDAR (Learning Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve contexts in a way that mitigates interference from distracting passages, thereby achieving significantly higher performance with reduced token usage compared to long-context approaches. Extensive experiments across diverse LLM architectures and six knowledge-intensive benchmarks demonstrate the effectiveness and robustness of our approach, highlighting the importance of balancing the trade-off between information coverage and distraction.</li>
</ul>

<h3>Title: Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations</h3>
<ul>
<li><strong>Authors: </strong>Guanzhi Deng, Mingyang Liu, Dapeng Wu, Yinqiao Li, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21870">https://arxiv.org/abs/2509.21870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21870">https://arxiv.org/pdf/2509.21870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21870]] Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations(https://arxiv.org/abs/2509.21870)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning.</li>
</ul>

<h3>Title: Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao, Tao Gui, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21871">https://arxiv.org/abs/2509.21871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21871">https://arxiv.org/pdf/2509.21871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21871]] Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization(https://arxiv.org/abs/2509.21871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) are well suited to image aesthetic assessment, as they can capture high-level aesthetic features leveraging their cross-modal understanding capacity. However, the scarcity of multimodal aesthetic reasoning data and the inherently subjective nature of aesthetic judgment make it difficult for MLLMs to generate accurate aesthetic judgments with interpretable rationales. To this end, we propose Aes-R1, a comprehensive aesthetic reasoning framework with reinforcement learning (RL). Concretely, Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality chain-of-thought aesthetic reasoning data used for cold-start. After teaching the model to generate structured explanations prior to scoring, we then employ the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that jointly optimizes absolute score regression and relative ranking order, improving both per-image accuracy and cross-image preference judgments. Aes-R1 enables MLLMs to generate grounded explanations alongside faithful scores, thereby enhancing aesthetic scoring and reasoning in a unified framework. Extensive experiments demonstrate that Aes-R1 improves the backbone's average PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar size. More ablation studies validate Aes-R1's robust generalization under limited supervision and in out-of-distribution scenarios.</li>
</ul>

<h3>Title: Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifei Peng, Yaoli Liu, Enbo Xia, Yu Jin, Wang-Zhou Dai, Zhong Ren, Yao-Xiang Ding, Kun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21874">https://arxiv.org/abs/2509.21874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21874">https://arxiv.org/pdf/2509.21874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21874]] Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models(https://arxiv.org/abs/2509.21874)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP) and Multimodal Large Language Models (MLLMs) for abductive logical rule induction. The task involves both discovering logical facts and inducing logical rules from a small number of unstructured textual or visual inputs, which still remain challenging when solely relying on ILP, due to the requirement of specified background knowledge and high computational cost, or MLLMs, due to the appearance of perceptual hallucinations. Based on the key observation that MLLMs could propose structure-correct rules even under hallucinations, our approach automatically builds ILP tasks with pruned search spaces based on the rule structure proposals from MLLMs, and utilizes ILP system to output rules built upon rectified logical facts and formal inductive reasoning. Its effectiveness is verified through challenging logical induction benchmarks, as well as a potential application of our approach, namely text-to-image customized generation with rule induction. Our code and data are released at this https URL.</li>
</ul>

<h3>Title: LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals</h3>
<ul>
<li><strong>Authors: </strong>Min-Hsuan Yeh, Yixuan Li, Tanwi Mallick</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21875">https://arxiv.org/abs/2509.21875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21875">https://arxiv.org/pdf/2509.21875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21875]] LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals(https://arxiv.org/abs/2509.21875)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality.</li>
</ul>

<h3>Title: Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness</h3>
<ul>
<li><strong>Authors: </strong>Chaoyang Luo, Yan Zou, Nanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21879">https://arxiv.org/abs/2509.21879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21879">https://arxiv.org/pdf/2509.21879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21879]] Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness(https://arxiv.org/abs/2509.21879)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Despite neural ordinary differential equations (Neural ODEs) exhibiting intrinsic robustness under input perturbations due to their dynamical systems nature, recent approaches often involve imposing Lyapunov-based stability conditions to provide formal robustness guarantees. However, a fundamental challenge remains: the tension between robustness and accuracy, primarily stemming from the difficulty in imposing appropriate stability conditions. To address this, we propose an adaptive stable learning framework named Zubov-Net, which innovatively reformulates Zubov's equation into a consistency characterization between regions of attraction (RoAs) and prescribed RoAs (PRoAs). Building on this consistency, we introduce a new paradigm for actively controlling the geometry of RoAs by directly optimizing PRoAs to reconcile accuracy and robustness. Our approach is realized through tripartite losses (consistency, classification, and separation losses) and a parallel boundary sampling algorithm that co-optimizes the Neural ODE and the Lyapunov function. To enhance the discriminativity of Lyapunov functions, we design an input-attention-based convex neural network via a softmax attention mechanism that focuses on equilibrium-relevant features and also serves as weight normalization to maintain training stability in deep architectures. Theoretically, we prove that minimizing the tripartite loss guarantees consistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping PRoAs. Moreover, we establish stochastic convex separability with tighter probability bounds and fewer dimensionality requirements to justify the convex design in Lyapunov functions. Experimentally, Zubov-Net maintains high classification accuracy while significantly improving robustness against various stochastic noises and adversarial attacks.</li>
</ul>

<h3>Title: No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Long V. Le, Myeongho Jeon, Kim Vu, Viet Lai, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21880">https://arxiv.org/abs/2509.21880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21880">https://arxiv.org/pdf/2509.21880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21880]] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping(https://arxiv.org/abs/2509.21880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.</li>
</ul>

<h3>Title: Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Aaron Tu, Weihao Xuan, Heli Qi, Xu Huang, Qingcheng Zeng, Shayan Talaei, Yijia Xiao, Peng Xia, Xiangru Tang, Yuchen Zhuang, Bing Hu, Hanqun Cao, Wenqi Shi, Tianang Leng, Rui Yang, Yingjian Chen, Ziqi Wang, Irene Li, Nan Liu, Huaxiu Yao, Li Erran Li, Ge Liu, Amin Saberi, Naoto Yokoya, Jure Leskovec, Yejin Choi, Fang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21882">https://arxiv.org/abs/2509.21882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21882">https://arxiv.org/pdf/2509.21882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21882]] Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards(https://arxiv.org/abs/2509.21882)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) is a practical and scalable approach to enhancing large language models in areas such as math, code, and other structured tasks. Two questions motivate this paper: how much of the reported gains survive under strictly parity-controlled evaluation, and whether RLVR is cost-free or exacts a measurable tax. We argue that progress is real, but gains are often overstated due to three forces - an RLVR tax, evaluation pitfalls, and data contamination. Using a partial-prompt contamination audit and matched-budget reproductions across base and RL models, we show that several headline gaps shrink or vanish under clean, parity-controlled evaluation. We then propose a tax-aware training and evaluation protocol that co-optimizes accuracy, grounding, and calibrated abstention and standardizes budgeting and provenance checks. Applied to recent RLVR setups, this protocol yields more reliable estimates of reasoning gains and, in several cases, revises prior conclusions. Our position is constructive: RLVR is valuable and industry-ready; we advocate keeping its practical benefits while prioritizing reliability, safety, and measurement.</li>
</ul>

<h3>Title: You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors</h3>
<ul>
<li><strong>Authors: </strong>Bochuan Cao, Changjiang Li, Yuanpu Cao, Yameng Ge, Ting Wang, Jinghui Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21884">https://arxiv.org/abs/2509.21884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21884">https://arxiv.org/pdf/2509.21884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21884]] You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors(https://arxiv.org/abs/2509.21884)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely adopted across various applications, leveraging customized system prompts for diverse tasks. Facing potential system prompt leakage risks, model developers have implemented strategies to prevent leakage, primarily by disabling LLMs from repeating their context when encountering known attack patterns. However, it remains vulnerable to new and unforeseen prompt-leaking techniques. In this paper, we first introduce a simple yet effective prompt leaking attack to reveal such risks. Our attack is capable of extracting system prompts from various LLM-based application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our findings further inspire us to search for a fundamental solution to the problems by having no system prompt in the context. To this end, we propose SysVec, a novel method that encodes system prompts as internal representation vectors rather than raw text. By doing so, SysVec minimizes the risk of unauthorized disclosure while preserving the LLM's core language capabilities. Remarkably, this approach not only enhances security but also improves the model's general instruction-following abilities. Experimental results demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.</li>
</ul>

<h3>Title: StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing</h3>
<ul>
<li><strong>Authors: </strong>Liyang Chen, Tianze Zhou, Xu He, Boshi Tang, Zhiyong Wu, Yang Huang, Yang Wu, Zhongqian Sun, Wei Yang, Helen Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21887">https://arxiv.org/abs/2509.21887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21887">https://arxiv.org/pdf/2509.21887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21887]] StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing(https://arxiv.org/abs/2509.21887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at this https URL.</li>
</ul>

<h3>Title: Drag4D: Align Your Motion with Text-Driven 3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Minjun Kang, Inkyu Shin, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21888">https://arxiv.org/abs/2509.21888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21888">https://arxiv.org/pdf/2509.21888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21888]] Drag4D: Align Your Motion with Text-Driven 3D Scene Generation(https://arxiv.org/abs/2509.21888)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.</li>
</ul>

<h3>Title: Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Naibin Gu, Zhenyu Zhang, Yuchen Feng, Yilong Chen, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21892">https://arxiv.org/abs/2509.21892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21892">https://arxiv.org/pdf/2509.21892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21892]] Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts(https://arxiv.org/abs/2509.21892)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2-3$\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.</li>
</ul>

<h3>Title: Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21893">https://arxiv.org/abs/2509.21893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21893">https://arxiv.org/pdf/2509.21893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21893]] Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers(https://arxiv.org/abs/2509.21893)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Text-to-video and image-to-video generation have made rapid progress in visual quality, but they remain limited in controlling the precise timing of motion. In contrast, audio provides temporal cues aligned with video motion, making it a promising condition for temporally controlled video generation. However, existing audio-to-video (A2V) models struggle with fine-grained synchronization due to indirect conditioning mechanisms or limited temporal modeling capacity. We present Syncphony, which generates 380x640 resolution, 24fps videos synchronized with diverse audio inputs. Our approach builds upon a pre-trained video backbone and incorporates two key components to improve synchronization: (1) Motion-aware Loss, which emphasizes learning at high-motion regions; (2) Audio Sync Guidance, which guides the full model using a visually aligned off-sync model without audio layers to better exploit audio cues at inference while maintaining visual quality. To evaluate synchronization, we propose CycleSync, a video-to-audio-based metric that measures the amount of motion cues in the generated video to reconstruct the original audio. Experiments on AVSync15 and The Greatest Hits datasets demonstrate that Syncphony outperforms existing methods in both synchronization accuracy and visual quality. Project page is available at: this https URL</li>
</ul>

<h3>Title: LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Liu (1), Yizhou Yang (1), Jinwen Li (2), Jun Tao (1), Ruoyu Li (1), Xiangkun Wang (1), Min Zhu (1), Junlong Cheng (1) ((1) College of Computer Science, Sichuan University, China, (2) School of Computer Science and Technology, Xinjiang University, China)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21894">https://arxiv.org/abs/2509.21894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21894">https://arxiv.org/pdf/2509.21894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21894]] LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation(https://arxiv.org/abs/2509.21894)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Remote Sensing Change Detection (RSCD) typically identifies changes in land cover or surface conditions by analyzing multi-temporal images. Currently, most deep learning-based methods primarily focus on learning unimodal visual information, while neglecting the rich semantic information provided by multimodal data such as text. To address this limitation, we propose a novel Language-Guided Change Detection model (LG-CD). This model leverages natural language prompts to direct the network's attention to regions of interest, significantly improving the accuracy and robustness of change detection. Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature extractor to capture multi-scale pyramid features from high-resolution to low-resolution across bi-temporal remote sensing images. Subsequently, multi-layer adapters are employed to fine-tune the model for downstream tasks, ensuring its effectiveness in remote sensing change detection. Additionally, we design a Text Fusion Attention Module (TFAM) to align visual and textual information, enabling the model to focus on target change regions using text prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented, which deeply integrates visual and semantic information through a cross-attention mechanism to produce highly accurate change detection masks. Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate that LG-CD consistently outperforms state-of-the-art change detection methods. Furthermore, our approach provides new insights into achieving generalized change detection by leveraging multimodal information.</li>
</ul>

<h3>Title: TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Qihang Wang, Yaxiong Wang, Lechao Cheng, Zhun Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21905">https://arxiv.org/abs/2509.21905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21905">https://arxiv.org/pdf/2509.21905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21905]] TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation(https://arxiv.org/abs/2509.21905)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper explores image editing under the joint control of text and drag interactions. While recent advances in text-driven and drag-driven editing have achieved remarkable progress, they suffer from complementary limitations: text-driven methods excel in texture manipulation but lack precise spatial control, whereas drag-driven approaches primarily modify shape and structure without fine-grained texture guidance. To address these limitations, we propose a unified diffusion-based framework for joint drag-text image editing, integrating the strengths of both paradigms. Our framework introduces two key innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising, dynamically balancing the influence of drag and text conditions during denoising. Notably, our model supports flexible editing modes - operating with text-only, drag-only, or combined conditions - while maintaining strong performance in each setting. Extensive quantitative and qualitative experiments demonstrate that our method not only achieves high-fidelity joint editing but also matches or surpasses the performance of specialized text-only or drag-only approaches, establishing a versatile and generalizable solution for controllable image manipulation. Code will be made publicly available to reproduce all results presented in this work.</li>
</ul>

<h3>Title: A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kemal Sami Karaca, Bahaeddin Eravc</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21907">https://arxiv.org/abs/2509.21907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21907">https://arxiv.org/pdf/2509.21907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21907]] A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs(https://arxiv.org/abs/2509.21907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the qualitative intent of citations is essential for a comprehensive assessment of academic research, a task that poses unique challenges for agglutinative languages like Turkish. This paper introduces a systematic methodology and a foundational dataset to address this problem. We first present a new, publicly available dataset of Turkish citation intents, created with a purpose-built annotation tool. We then evaluate the performance of standard In-Context Learning (ICL) with Large Language Models (LLMs), demonstrating that its effectiveness is limited by inconsistent results caused by manually designed prompts. To address this core limitation, we introduce a programmable classification pipeline built on the DSPy framework, which automates prompt optimization systematically. For final classification, we employ a stacked generalization ensemble to aggregate outputs from multiple optimized models, ensuring stable and reliable predictions. This ensemble, with an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%. Ultimately, this study provides the Turkish NLP community and the broader academic circles with a foundational dataset and a robust classification framework paving the way for future qualitative citation studies.</li>
</ul>

<h3>Title: AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yun Wang, Zhaojun Ding, Xuansheng Wu, Siyue Sun, Ninghao Liu, Xiaoming Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21910">https://arxiv.org/abs/2509.21910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21910">https://arxiv.org/pdf/2509.21910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21910]] AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition(https://arxiv.org/abs/2509.21910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.</li>
</ul>

<h3>Title: Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Wan, Yidong Ouyang, Liyan Xie, Fang Fang, Hongyuan Zha, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21912">https://arxiv.org/abs/2509.21912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21912">https://arxiv.org/pdf/2509.21912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21912]] Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching(https://arxiv.org/abs/2509.21912)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Guidance provides a simple and effective framework for posterior sampling by steering the generation process towards the desired distribution. When modeling discrete data, existing approaches mostly focus on guidance with the first-order Taylor approximation to improve the sampling efficiency. However, such an approximation is inappropriate in discrete state spaces since the approximation error could be large. A novel guidance framework for discrete data is proposed to address this problem: We derive the exact transition rate for the desired distribution given a learned discrete flow matching model, leading to guidance that only requires a single forward pass in each sampling step, significantly improving efficiency. This unified novel framework is general enough, encompassing existing guidance methods as special cases, and it can also be seamlessly applied to the masked diffusion model. We demonstrate the effectiveness of our proposed guidance on energy-guided simulations and preference alignment on text-to-image generation and multimodal understanding tasks. The code is available through this https URL.</li>
</ul>

<h3>Title: Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Boying Li, Chang Liu, Petter Kysti, Mattias hman, Devashish Singha Roy, Sofia Plazzi, Hamam Mokayed, Olle Hagner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21916">https://arxiv.org/abs/2509.21916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21916">https://arxiv.org/pdf/2509.21916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21916]] Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning(https://arxiv.org/abs/2509.21916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aside from common challenges in remote sensing like small, sparse targets and computation cost limitations, detecting vehicles from UAV images in the Nordic regions faces strong visibility challenges and domain shifts caused by diverse levels of snow coverage. Although annotated data are expensive, unannotated data is cheaper to obtain by simply flying the drones. In this work, we proposed a sideload-CL-adaptation framework that enables the use of unannotated data to improve vehicle detection using lightweight models. Specifically, we propose to train a CNN-based representation extractor through contrastive learning on the unannotated data in the pretraining stage, and then sideload it to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust sideload-CL-adaptation, we conducted extensive experiments to compare various fusion methods and granularity. Our proposed sideload-CL-adaptation model improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD dataset.</li>
</ul>

<h3>Title: Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects</h3>
<ul>
<li><strong>Authors: </strong>Fumin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21923">https://arxiv.org/abs/2509.21923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21923">https://arxiv.org/pdf/2509.21923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21923]] Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects(https://arxiv.org/abs/2509.21923)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Interpretability is one of the considerations when applying machine learning to high-stakes fields such as healthcare that involve matters of life safety. Generalized Additive Models (GAMs) enhance interpretability by visualizing shape functions. Nevertheless, to preserve interpretability, GAMs omit higher-order interaction effects (beyond pairwise interactions), which imposes significant constraints on their predictive performance. We observe that Curve Ergodic Set Regression (CESR), a multiplicative model, naturally enables the visualization of its shape functions and simultaneously incorporates both interactions among all features and individual feature effects. Nevertheless, CESR fails to demonstrate superior performance compared to GAMs. We introduce Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an additive part to disentangle the intertwined coefficients of its interactive and independent terms, thus effectively broadening the hypothesis space. The model is composed of a multiplicative part and an additive part, whose shape functions can both be naturally visualized, thereby assisting users in interpreting how features participate in the decision-making process. Consequently, MACMs constitute an improvement over both CESR and GAMs. The experimental results indicate that neural network-based MACMs significantly outperform both CESR and the current state-of-the-art GAMs in terms of predictive performance.</li>
</ul>

<h3>Title: Generation Properties of Stochastic Interpolation under Finite Training Set</h3>
<ul>
<li><strong>Authors: </strong>Yunchen Li, Shaohui Lin, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21925">https://arxiv.org/abs/2509.21925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21925">https://arxiv.org/pdf/2509.21925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21925]] Generation Properties of Stochastic Interpolation under Finite Training Set(https://arxiv.org/abs/2509.21925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the theoretical behavior of generative models under finite training populations. Within the stochastic interpolation generative framework, we derive closed-form expressions for the optimal velocity field and score function when only a finite number of training samples are available. We demonstrate that, under some regularity conditions, the deterministic generative process exactly recovers the training samples, while the stochastic generative process manifests as training samples with added Gaussian noise. Beyond the idealized setting, we consider model estimation errors and introduce formal definitions of underfitting and overfitting specific to generative models. Our theoretical analysis reveals that, in the presence of estimation errors, the stochastic generation process effectively produces convex combinations of training samples corrupted by a mixture of uniform and Gaussian noise. Experiments on generation tasks and downstream tasks such as classification support our theory.</li>
</ul>

<h3>Title: PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhang, Bowen Wang, Hong Liu, Yuta Nakashima, Hajime Nagahara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21926">https://arxiv.org/abs/2509.21926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21926">https://arxiv.org/pdf/2509.21926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21926]] PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning(https://arxiv.org/abs/2509.21926)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Visual In-Context Learning (VICL) uses input-output image pairs, referred to as in-context pairs (or examples), as prompts alongside query images to guide models in performing diverse vision tasks. However, VICL often suffers from over-reliance on a single in-context pair, which can lead to biased and unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual In-Context Learning (PANICL), a general training-free framework that mitigates this issue by leveraging multiple in-context pairs. PANICL smooths assignment scores across pairs, reducing bias without requiring additional training. Extensive experiments on a variety of tasks, including foreground segmentation, single object detection, colorization, multi-object segmentation, and keypoint detection, demonstrate consistent improvements over strong baselines. Moreover, PANICL exhibits strong robustness to domain shifts, including dataset-level shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and generalizes well to other VICL models such as SegGPT, Painter, and LVM, highlighting its versatility and broad applicability.</li>
</ul>

<h3>Title: SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Wang, Haiyue Zhu, Haoren Guo, Abdullah Al Mamun, Cheng Xiang, Tong Heng Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21927">https://arxiv.org/abs/2509.21927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21927">https://arxiv.org/pdf/2509.21927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21927]] SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference(https://arxiv.org/abs/2509.21927)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent 6D pose estimation methods demonstrate notable performance but still face some practical limitations. For instance, many of them rely heavily on sensor depth, which may fail with challenging surface conditions, such as transparent or highly reflective materials. In the meantime, RGB-based solutions provide less robust matching performance in low-light and texture-less scenes due to the lack of geometry information. Motivated by these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB image as a reference, eliminating the need for costly depth sensors, multi-view image acquisition, or training view synthesis models and neural fields. This enables SingRef6D to remain robust and capable even under resource-limited settings where depth or dense templates are unavailable. Our framework incorporates two key innovations. First, we propose a token-scaler-based fine-tuning mechanism with a novel optimization loss on top of Depth-Anything v2 to enhance its ability to predict accurate depth, even for challenging surfaces. Our results show a 14.41% improvement (in $\delta_{1.05}$) on REAL275 depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second, benefiting from depth availability, we introduce a depth-aware matching process that effectively integrates spatial relationships within LoFTR, enabling our system to handle matching for challenging materials and lighting conditions. Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light datasets show that our approach surpasses state-of-the-art methods, achieving a 6.1% improvement in average recall.</li>
</ul>

<h3>Title: DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Wang, Changhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21930">https://arxiv.org/abs/2509.21930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21930">https://arxiv.org/pdf/2509.21930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21930]] DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation(https://arxiv.org/abs/2509.21930)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.</li>
</ul>

<h3>Title: SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation</h3>
<ul>
<li><strong>Authors: </strong>Haotian Tan, Hiroki Ouchi, Sakriani Sakti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21932">https://arxiv.org/abs/2509.21932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21932">https://arxiv.org/pdf/2509.21932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21932]] SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation(https://arxiv.org/abs/2509.21932)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How to make human-interpreter-like read/write decisions for simultaneous speech translation (SimulST) systems? Current state-of-the-art systems formulate SimulST as a multi-turn dialogue task, requiring specialized interleaved training data and relying on computationally expensive large language model (LLM) inference for decision-making. In this paper, we propose SimulSense, a novel framework for SimulST that mimics human interpreters by continuously reading input speech and triggering write decisions to produce translation when a new sense unit is perceived. Experiments against two state-of-the-art baseline systems demonstrate that our proposed method achieves a superior quality-latency tradeoff and substantially improved real-time efficiency, where its decision-making is up to 9.6x faster than the baselines.</li>
</ul>

<h3>Title: Why Chain of Thought Fails in Clinical Text Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jiageng Wu, Kevin Xie, Bowen Gu, Nils Krger, Kueiyu Joshua Lin, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21933">https://arxiv.org/abs/2509.21933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21933">https://arxiv.org/pdf/2509.21933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21933]] Why Chain of Thought Fails in Clinical Text Understanding(https://arxiv.org/abs/2509.21933)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly being applied to clinical care, a domain where both accuracy and transparent reasoning are critical for safe and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits step-by-step reasoning, has demonstrated improvements in performance and interpretability across a wide range of tasks. However, its effectiveness in clinical contexts remains largely unexplored, particularly in the context of electronic health records (EHRs), the primary source of clinical documentation, which are often lengthy, fragmented, and noisy. In this work, we present the first large-scale systematic study of CoT for clinical text understanding. We assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9 languages and 8 task types. Contrary to prior findings in other domains, we observe that 86.3\% of models suffer consistent performance degradation in the CoT setting. More capable models remain relatively robust, while weaker ones suffer substantial declines. To better characterize these effects, we perform fine-grained analyses of reasoning length, medical concept alignment, and error profiles, leveraging both LLM-as-a-judge evaluation and clinical expert evaluation. Our results uncover systematic patterns in when and why CoT fails in clinical contexts, which highlight a critical paradox: CoT enhances interpretability but may undermine reliability in clinical text tasks. This work provides an empirical basis for clinical reasoning strategies of LLMs, highlighting the need for transparent and trustworthy approaches.</li>
</ul>

<h3>Title: Statistical Advantage of Softmax Attention: Insights from Single-Location Regression</h3>
<ul>
<li><strong>Authors: </strong>O. Duranthon, P. Marion, C. Boyer, B. Loureiro, L. Zdeborov</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21936">https://arxiv.org/abs/2509.21936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21936">https://arxiv.org/pdf/2509.21936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21936]] Statistical Advantage of Softmax Attention: Insights from Single-Location Regression(https://arxiv.org/abs/2509.21936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.</li>
</ul>

<h3>Title: SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet</h3>
<ul>
<li><strong>Authors: </strong>Woosung Joung, Daewon Chae, Jinkyu Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21938">https://arxiv.org/abs/2509.21938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21938">https://arxiv.org/pdf/2509.21938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21938]] SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet(https://arxiv.org/abs/2509.21938)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt-a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings-for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., "a human playing guitar" for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., cat playing guitar). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code is available at this https URL.</li>
</ul>

<h3>Title: Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xianghua Zeng, Hao Peng, Angsheng Li, Yicheng Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21942">https://arxiv.org/abs/2509.21942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21942">https://arxiv.org/pdf/2509.21942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21942]] Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning(https://arxiv.org/abs/2509.21942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative methods have shown promising potential for modeling trajectories from offline reinforcement learning (RL) datasets, and hierarchical diffusion has been introduced to mitigate variance accumulation and computational challenges in long-horizon planning tasks. However, existing approaches typically assume a fixed two-layer diffusion hierarchy with a single predefined temporal scale, which limits adaptability to diverse downstream tasks and reduces flexibility in decision making. In this work, we propose SIHD, a novel Structural Information-based Hierarchical Diffusion framework for effective and stable offline policy learning in long-horizon environments with sparse rewards. Specifically, we analyze structural information embedded in offline trajectories to construct the diffusion hierarchy adaptively, enabling flexible trajectory modeling across multiple temporal scales. Rather than relying on reward predictions from localized sub-trajectories, we quantify the structural information gain of each state community and use it as a conditioning signal within the corresponding diffusion layer. To reduce overreliance on offline datasets, we introduce a structural entropy regularizer that encourages exploration of underrepresented states while avoiding extrapolation errors from distributional shifts. Extensive evaluations on challenging offline RL tasks show that SIHD significantly outperforms state-of-the-art baselines in decision-making performance and demonstrates superior generalization across diverse scenarios.</li>
</ul>

<h3>Title: Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration</h3>
<ul>
<li><strong>Authors: </strong>Kasidit Sermsri, Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21946">https://arxiv.org/abs/2509.21946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21946">https://arxiv.org/pdf/2509.21946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21946]] Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration(https://arxiv.org/abs/2509.21946)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape - marked by indirect language, polarized figures, and entangled sentiment and stance - LLMs often display systematic biases such as sentiment leakage and favoritism toward entities. These biases undermine fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without requiring fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance and reduce bias. We also release the first high-quality Thai political stance dataset, annotated with stance, sentiment, rationales, and bias markers across diverse entities and events. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, enhances zero-shot generalization, and improves fairness across multiple LLMs. This work highlights the importance of culturally grounded debiasing techniques for underrepresented languages.</li>
</ul>

<h3>Title: Active Attacks: Red-teaming LLMs via Adaptive Environments</h3>
<ul>
<li><strong>Authors: </strong>Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, Minsu Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21947">https://arxiv.org/abs/2509.21947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21947">https://arxiv.org/pdf/2509.21947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21947]] Active Attacks: Red-teaming LLMs via Adaptive Environments(https://arxiv.org/abs/2509.21947)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>We address the challenge of generating diverse attack prompts for large language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual content) and are used for safety fine-tuning. Rather than relying on manual prompt engineering, attacker LLMs can be trained with reinforcement learning (RL) to automatically generate such prompts using only a toxicity classifier as a reward. However, capturing a wide range of harmful behaviors is a significant challenge that requires explicit diversity objectives. Existing diversity-seeking RL methods often collapse to limited modes: once high-reward prompts are found, exploration of new regions is discouraged. Inspired by the active learning paradigm that encourages adaptive exploration, we introduce \textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its attacks as the victim evolves. By periodically safety fine-tuning the victim LLM with collected attack prompts, rewards in exploited regions diminish, which forces the attacker to seek unexplored vulnerabilities. This process naturally induces an easy-to-hard exploration curriculum, where the attacker progresses beyond easy modes toward increasingly difficult ones. As a result, Active Attacks uncovers a wide range of local attack modes step by step, and their combination achieves wide coverage of the multi-mode distribution. Active Attacks, a simple plug-and-play module that seamlessly integrates into existing RL objectives, unexpectedly outperformed prior RL-based methods -- including GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a relative gain greater than $400\ \times$) with only a 6% increase in computation. Our code is publicly available \href{this https URL}{here}.</li>
</ul>

<h3>Title: Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach</h3>
<ul>
<li><strong>Authors: </strong>Daiqing Wu, Dongbao Yang, Sicheng Zhao, Can Ma, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21950">https://arxiv.org/abs/2509.21950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21950">https://arxiv.org/pdf/2509.21950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21950]] Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach(https://arxiv.org/abs/2509.21950)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Project page: this https URL.</li>
</ul>

<h3>Title: PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data</h3>
<ul>
<li><strong>Authors: </strong>Zhe Zhu, Le Wan, Rui Xu, Yiheng Zhang, Honghua Chen, Zhiyang Dou, Cheng Lin, Yuan Liu, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21965">https://arxiv.org/abs/2509.21965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21965">https://arxiv.org/pdf/2509.21965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21965]] PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data(https://arxiv.org/abs/2509.21965)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segmenting 3D objects into parts is a long-standing challenge in computer vision. To overcome taxonomy constraints and generalize to unseen 3D objects, recent works turn to open-world part segmentation. These approaches typically transfer supervision from 2D foundation models, such as SAM, by lifting multi-view masks into 3D. However, this indirect paradigm fails to capture intrinsic geometry, leading to surface-only understanding, uncontrolled decomposition, and limited generalization. We present PartSAM, the first promptable part segmentation model trained natively on large-scale 3D data. Following the design philosophy of SAM, PartSAM employs an encoder-decoder architecture in which a triplane-based dual-branch encoder produces spatially structured tokens for scalable part-aware representation learning. To enable large-scale supervision, we further introduce a model-in-the-loop annotation pipeline that curates over five million 3D shape-part pairs from online assets, providing diverse and fine-grained labels. This combination of scalable architecture and diverse 3D data yields emergent open-world capabilities: with a single prompt, PartSAM achieves highly accurate part identification, and in a Segment-Every-Part mode, it automatically decomposes shapes into both surface and internal structures. Extensive experiments show that PartSAM outperforms state-of-the-art methods by large margins across multiple benchmarks, marking a decisive step toward foundation models for 3D part understanding. Our code and model will be released soon.</li>
</ul>

<h3>Title: No-Reference Image Contrast Assessment with Customized EfficientNet-B0</h3>
<ul>
<li><strong>Authors: </strong>Javad Hassannataj Joloudari, Bita Mesbahzadeh, Omid Zare, Emrah Arslan, Roohallah Alizadehsani, Hossein Moosaei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21967">https://arxiv.org/abs/2509.21967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21967">https://arxiv.org/pdf/2509.21967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21967]] No-Reference Image Contrast Assessment with Customized EfficientNet-B0(https://arxiv.org/abs/2509.21967)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image contrast was a fundamental factor in visual perception and played a vital role in overall image quality. However, most no reference image quality assessment NR IQA models struggled to accurately evaluate contrast distortions under diverse real world conditions. In this study, we proposed a deep learning based framework for blind contrast quality assessment by customizing and fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and MobileNetV2, for perceptual Mean Opinion Score, along with an additional model built on a Siamese network, which indicated a limited ability to capture perceptual contrast distortions. Each model is modified with a contrast-aware regression head and trained end to end using targeted data augmentations on two benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic contrast distortions. Performance is evaluated using Pearson Linear Correlation Coefficient and Spearman Rank Order Correlation Coefficient, which assess the alignment between predicted and human rated scores. Among these three models, our customized EfficientNet B0 model achieved state-of-the-art performance with PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369 on CID2013, surpassing traditional methods and outperforming other deep baselines. These results highlighted the models robustness and effectiveness in capturing perceptual contrast distortion. Overall, the proposed method demonstrated that contrast aware adaptation of lightweight pre trained networks can yield a high performing, scalable solution for no reference contrast quality assessment suitable for real time and resource constrained applications.</li>
</ul>

<h3>Title: GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction</h3>
<ul>
<li><strong>Authors: </strong>Feng Jiang, Amina Mollaysa, Hehuan Ma, Tommaso Mansi, Junzhou Huang, Mangal Prakash, Rui Liao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21971">https://arxiv.org/abs/2509.21971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21971">https://arxiv.org/pdf/2509.21971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21971]] GRAM-TDI: adaptive multimodal representation learning for drug target interaction prediction(https://arxiv.org/abs/2509.21971)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Drug target interaction (DTI) prediction is a cornerstone of computational drug discovery, enabling rational design, repurposing, and mechanistic insights. While deep learning has advanced DTI modeling, existing approaches primarily rely on SMILES protein pairs and fail to exploit the rich multimodal information available for small molecules and proteins. We introduce GRAMDTI, a pretraining framework that integrates multimodal molecular and protein inputs into unified representations. GRAMDTI extends volume based contrastive learning to four modalities, capturing higher-order semantic alignment beyond conventional pairwise approaches. To handle modality informativeness, we propose adaptive modality dropout, dynamically regulating each modality's contribution during pre-training. Additionally, IC50 activity measurements, when available, are incorporated as weak supervision to ground representations in biologically meaningful interaction strengths. Experiments on four publicly available datasets demonstrate that GRAMDTI consistently outperforms state of the art baselines. Our results highlight the benefits of higher order multimodal alignment, adaptive modality utilization, and auxiliary supervision for robust and generalizable DTI prediction.</li>
</ul>

<h3>Title: Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li, Yuxiang Cai, Zhonggen Su, Zhaojun Liu, Jianwei Yin, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21976">https://arxiv.org/abs/2509.21976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21976">https://arxiv.org/pdf/2509.21976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21976]] Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning(https://arxiv.org/abs/2509.21976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This "reason first, then act" process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at this http URL.</li>
</ul>

<h3>Title: MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation</h3>
<ul>
<li><strong>Authors: </strong>Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21978">https://arxiv.org/abs/2509.21978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21978">https://arxiv.org/pdf/2509.21978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21978]] MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation(https://arxiv.org/abs/2509.21978)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.</li>
</ul>

<h3>Title: Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zikun Guo, Xinyue Xu, Pei Xiang, Shu Yang, Xin Han, Di Wang, Lijie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21979">https://arxiv.org/abs/2509.21979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21979">https://arxiv.org/pdf/2509.21979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21979]] Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models(https://arxiv.org/abs/2509.21979)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Vision language models(VLMs) are increasingly integrated into clinical workflows, but they often exhibit sycophantic behavior prioritizing alignment with user phrasing social cues or perceived authority over evidence based reasoning. This study evaluate clinical sycophancy in medical visual question answering through a novel clinically grounded benchmark. We propose a medical sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by different type organ system and modality. Using psychologically motivated pressure templates including various sycophancy. In our adversarial experiments on various VLMs, we found that these models are generally vulnerable, exhibiting significant variations in the occurrence of adversarial responses, with weak correlations to the model accuracy or size. Imitation and expert provided corrections were found to be the most effective triggers, suggesting that the models possess a bias mechanism independent of visual evidence. To address this, we propose Visual Information Purification for Evidence based Response (VIPER) a lightweight mitigation strategy that filters non evidentiary content for example social pressures and then generates constrained evidence first answers. This framework reduces sycophancy by an average amount outperforming baselines while maintaining interpretability. Our benchmark analysis and mitigation framework lay the groundwork for robust deployment of medical VLMs in real world clinician interactions emphasizing the need for evidence anchored defenses.</li>
</ul>

<h3>Title: Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Wang, Baiyu Chen, Kun Yan, Hongjing Piao, Hao Xue, Flora D. Salim, Yuanchun Shi, Yuntao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21980">https://arxiv.org/abs/2509.21980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21980">https://arxiv.org/pdf/2509.21980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21980]] Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm(https://arxiv.org/abs/2509.21980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rise in popularity of smart glasses, users' attention has been integrated into Vision-Language Models (VLMs) to streamline multi-modal querying in daily scenarios. However, leveraging gaze data to model users' attention may introduce ambiguity challenges: (1) users' verbal questions become ambiguous by using pronouns or skipping context, (2) humans' gaze patterns can be noisy and exhibit complex spatiotemporal relationships with their spoken questions. Previous works only consider single image as visual modality input, failing to capture the dynamic nature of the user's attention. In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal gaze information to enhance the model's effectiveness in real-world applications. Initially, we analyzed hundreds of querying samples with the gaze modality to demonstrate the noisy nature of users' gaze patterns. We then utilized GPT-4o to design an automatic data synthesis pipeline to generate the GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process to handle noisy gaze patterns. Finally, we designed a heatmap module to incorporate gaze information into cutting-edge VLMs while preserving their pretrained knowledge. We evaluated GLARIFY using a hold-out test set. Experiments demonstrate that GLARIFY significantly outperforms baselines. By robustly aligning VLMs with human attention, GLARIFY paves the way for a usable and intuitive interaction paradigm with a visual assistant.</li>
</ul>

<h3>Title: From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Weili Guan, Jun Yu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21984">https://arxiv.org/abs/2509.21984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21984">https://arxiv.org/pdf/2509.21984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21984]] From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs(https://arxiv.org/abs/2509.21984)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have achieved remarkable success across a wide range of multimodal tasks, yet their robustness to spatial variations remains insufficiently understood. In this work, we present a systematic study of the spatial bias of LVLMs, focusing on how models respond when identical key visual information is placed at different locations within an image. Through a carefully designed probing dataset, we demonstrate that current LVLMs often produce inconsistent outputs under such spatial shifts, revealing a fundamental limitation in their spatial-semantic understanding. Further analysis shows that this phenomenon originates not from the vision encoder, which reliably perceives and interprets visual content across positions, but from the unbalanced design of position embeddings in the language model component. In particular, the widely adopted position embedding strategies, such as RoPE, introduce imbalance during cross-modal interaction, leading image tokens at different positions to exert unequal influence on semantic understanding. To mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple yet effective mechanism that assigns identical position embeddings to all image tokens, promoting a more balanced integration of visual information. Extensive experiments show that BaPA enhances the spatial robustness of LVLMs without retraining and further boosts their performance across diverse multimodal benchmarks when combined with lightweight fine-tuning. Further analysis of information flow reveals that BaPA yields balanced attention, enabling more holistic visual understanding.</li>
</ul>

<h3>Title: Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation</h3>
<ul>
<li><strong>Authors: </strong>Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21989">https://arxiv.org/abs/2509.21989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21989">https://arxiv.org/pdf/2509.21989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21989]] Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation(https://arxiv.org/abs/2509.21989)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:this https URL</li>
</ul>

<h3>Title: WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM</h3>
<ul>
<li><strong>Authors: </strong>Changli Tang, Qinfan Xiao, Ke Mei, Tianyi Wang, Fengyun Rao, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21990">https://arxiv.org/abs/2509.21990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21990">https://arxiv.org/pdf/2509.21990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21990]] WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM(https://arxiv.org/abs/2509.21990)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\textbf{u}nified \& \textbf{v}ersatile \textbf{a}udio-\textbf{v}isual \textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code, checkpoints, and data will be released.</li>
</ul>

<h3>Title: DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints</h3>
<ul>
<li><strong>Authors: </strong>Sungmin Woo, Sangyoun Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21992">https://arxiv.org/abs/2509.21992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21992">https://arxiv.org/pdf/2509.21992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21992]] DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints(https://arxiv.org/abs/2509.21992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth-from-Focus (DFF) enables precise depth estimation by analyzing focus cues across a stack of images captured at varying focal lengths. While recent learning-based approaches have advanced this field, they often struggle in complex scenes with fine textures or abrupt depth changes, where focus cues may become ambiguous or misleading. We present DualFocus, a novel DFF framework that leverages the focal stack's unique gradient patterns induced by focus variation, jointly modeling focus changes over spatial and focal dimensions. Our approach introduces a variational formulation with dual constraints tailored to DFF: spatial constraints exploit gradient pattern changes across focus levels to distinguish true depth edges from texture artifacts, while focal constraints enforce unimodal, monotonic focus probabilities aligned with physical focus behavior. These inductive biases improve robustness and accuracy in challenging regions. Comprehensive experiments on four public datasets demonstrate that DualFocus consistently outperforms state-of-the-art methods in both depth accuracy and perceptual quality.</li>
</ul>

<h3>Title: Rate-Distortion Optimized Communication for Collaborative Perception</h3>
<ul>
<li><strong>Authors: </strong>Genjia Liu, Anning Hu, Yue Hu, Wenjun Zhang, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21994">https://arxiv.org/abs/2509.21994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21994">https://arxiv.org/pdf/2509.21994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21994]] Rate-Distortion Optimized Communication for Collaborative Perception(https://arxiv.org/abs/2509.21994)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Collaborative perception emphasizes enhancing environmental understanding by enabling multiple agents to share visual information with limited bandwidth resources. While prior work has explored the empirical trade-off between task performance and communication volume, a significant gap remains in the theoretical foundation. To fill this gap, we draw on information theory and introduce a pragmatic rate-distortion theory for multi-agent collaboration, specifically formulated to analyze performance-communication trade-off in goal-oriented multi-agent systems. This theory concretizes two key conditions for designing optimal communication strategies: supplying pragmatically relevant information and transmitting redundancy-less messages. Guided by these two conditions, we propose RDcomm, a communication-efficient collaborative perception framework that introduces two key innovations: i) task entropy discrete coding, which assigns features with task-relevant codeword-lengths to maximize the efficiency in supplying pragmatic information; ii) mutual-information-driven message selection, which utilizes mutual information neural estimation to approach the optimal redundancy-less condition. Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing communication volume by up to 108 times. The code will be released.</li>
</ul>

<h3>Title: FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration</h3>
<ul>
<li><strong>Authors: </strong>Muxi Chen, Zhaohua Zhang, Chenchen Zhao, Mingyang Chen, Wenyu Jiang, Tianwen Jiang, Jianhuan Zhuo, Yu Tang, Qiuyong Xiao, Jihong Zhang, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21995">https://arxiv.org/abs/2509.21995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21995">https://arxiv.org/pdf/2509.21995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21995]] FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration(https://arxiv.org/abs/2509.21995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at this https URL</li>
</ul>

<h3>Title: Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors</h3>
<ul>
<li><strong>Authors: </strong>Youxu Shi, Suorong Yang, Dong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21997">https://arxiv.org/abs/2509.21997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21997">https://arxiv.org/pdf/2509.21997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21997]] Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors(https://arxiv.org/abs/2509.21997)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet they remain highly susceptible to hallucinations, producing content that is fluent but inconsistent with visual evidence. Such hallucinations, spanning objects, attributes, and relations, persist even in larger models, while existing mitigation approaches often require additional finetuning, handcrafted priors, or trade-offs that compromise informativeness and scalability. To address this limitation, we propose a training-free, self-supervised method for hallucination mitigation. Our approach introduces a novel hallucination amplification mechanism: a caption is projected into the visual space via a text-to-image model to reveal implicit hallucination signals, serving as a negative anchor, while the original image provides a positive anchor. Leveraging these dual anchors, we edit decoder hidden states by pulling representations toward faithful semantics and pushing them away from hallucination directions. This correction requires no human priors or additional training costs, ensuring both effectiveness and efficiency. Extensive experiments across multiple benchmarks show that our method significantly reduces hallucinations at the object, attribute, and relation levels while largely preserving recall and caption richness, e.g., achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR. Furthermore, results on diverse architectures, including LLaVA-NEXT-7B, Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture generalization. More importantly, when applied to hallucination-free captions, our method introduces almost no side effects, underscoring its robustness and practical plug-and-play applicability. The implementation will be publicly available.</li>
</ul>

<h3>Title: Black-Box Hallucination Detection via Consistency Under the Uncertain Expression</h3>
<ul>
<li><strong>Authors: </strong>Seongho Joo, Kyungmin Min, Jahyun Koo, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21999">https://arxiv.org/abs/2509.21999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21999">https://arxiv.org/pdf/2509.21999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21999]] Black-Box Hallucination Detection via Consistency Under the Uncertain Expression(https://arxiv.org/abs/2509.21999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the great advancement of Language modeling in recent days, Large Language Models (LLMs) such as GPT3 are notorious for generating non-factual responses, so-called "hallucination" problems. Existing methods for detecting and alleviating this hallucination problem require external resources or the internal state of LLMs, such as the output probability of each token. Given the LLM's restricted external API availability and the limited scope of external resources, there is an urgent demand to establish the Black-Box approach as the cornerstone for effective hallucination detection. In this work, we propose a simple black-box hallucination detection metric after the investigation of the behavior of LLMs under expression of uncertainty. Our comprehensive analysis reveals that LLMs generate consistent responses when they present factual responses while non-consistent responses vice versa. Based on the analysis, we propose an efficient black-box hallucination detection metric with the expression of uncertainty. The experiment demonstrates that our metric is more predictive of the factuality in model responses than baselines that use internal knowledge of LLMs.</li>
</ul>

<h3>Title: Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jin, Qitan Shi, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22007">https://arxiv.org/abs/2509.22007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22007">https://arxiv.org/pdf/2509.22007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22007]] Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models(https://arxiv.org/abs/2509.22007)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) is widely used to improve conditional fidelity in diffusion models, but its impact on sampling dynamics remains poorly understood. Prior studies, often restricted to unimodal conditional distributions or simplified cases, provide only a partial picture. We analyze CFG under multimodal conditionals and show that the sampling process unfolds in three successive stages. In the Direction Shift stage, guidance accelerates movement toward the weighted mean, introducing initialization bias and norm growth. In the Mode Separation stage, local dynamics remain largely neutral, but the inherited bias suppresses weaker modes, reducing global diversity. In the Concentration stage, guidance amplifies within-mode contraction, diminishing fine-grained variability. This unified view explains a widely observed phenomenon: stronger guidance improves semantic alignment but inevitably reduces diversity. Experiments support these predictions, showing that early strong guidance erodes global diversity, while late strong guidance suppresses fine-grained variation. Moreover, our theory naturally suggests a time-varying guidance schedule, and empirical results confirm that it consistently improves both quality and diversity.</li>
</ul>

<h3>Title: Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yajie Qi, Wei Wei, Lin Li, Lijun Zhang, Zhidong Gao, Da Wang, Huizhong Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22008">https://arxiv.org/abs/2509.22008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22008">https://arxiv.org/pdf/2509.22008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22008]] Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning(https://arxiv.org/abs/2509.22008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Real-world decision-making tasks typically occur in complex and open environments, posing significant challenges to reinforcement learning (RL) agents' exploration efficiency and long-horizon planning capabilities. A promising approach is LLM-enhanced RL, which leverages the rich prior knowledge and strong planning capabilities of LLMs to guide RL agents in efficient exploration. However, existing methods mostly rely on frequent and costly LLM invocations and suffer from limited performance due to the semantic mismatch. In this paper, we introduce a Structured Goal-guided Reinforcement Learning (SGRL) method that integrates a structured goal planner and a goal-conditioned action pruner to guide RL agents toward efficient exploration. Specifically, the structured goal planner utilizes LLMs to generate a reusable, structured function for goal generation, in which goals are prioritized. Furthermore, by utilizing LLMs to determine goals' priority weights, it dynamically generates forward-looking goals to guide the agent's policy toward more promising decision-making trajectories. The goal-conditioned action pruner employs an action masking mechanism that filters out actions misaligned with the current goal, thereby constraining the RL agent to select goal-consistent policies. We evaluate the proposed method on Crafter and Craftax-Classic, and experimental results demonstrate that SGRL achieves superior performance compared to existing state-of-the-art methods.</li>
</ul>

<h3>Title: Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics</h3>
<ul>
<li><strong>Authors: </strong>Saurav Jha, Stefan K. Ehrlich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22014">https://arxiv.org/abs/2509.22014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22014">https://arxiv.org/pdf/2509.22014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22014]] Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics(https://arxiv.org/abs/2509.22014)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.</li>
</ul>

<h3>Title: EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Yuki Sakai, Ryosuke Furuta, Juichun Yen, Yoichi Sato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22019">https://arxiv.org/abs/2509.22019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22019">https://arxiv.org/pdf/2509.22019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22019]] EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking(https://arxiv.org/abs/2509.22019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Analyzing instructional interactions between an instructor and a learner who are co-present in the same physical space is a critical problem for educational support and skill transfer. Yet such face-to-face instructional scenes have not been systematically studied in computer vision. We identify two key reasons: i) the lack of suitable datasets and ii) limited analytical techniques. To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification. Using this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models. Since face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner. Accordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text. This evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes. In experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</li>
</ul>

<h3>Title: Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Shilei Cao, Hehai Lin, Jiashun Cheng, Yang Liu, Guowen Li, Xuehe Wang, Juepeng Zheng, Haoyuan Liang, Meng Jin, Chengwei Qin, Hong Cheng, Haohuan Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22020">https://arxiv.org/abs/2509.22020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22020">https://arxiv.org/pdf/2509.22020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22020]] Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models(https://arxiv.org/abs/2509.22020)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>While recent advances in machine learning have equipped Weather Foundation Models (WFMs) with substantial generalization capabilities across diverse downstream tasks, the escalating computational requirements associated with their expanding scale increasingly hinder practical deployment. Current Parameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or language tasks, fail to address the unique challenges of weather downstream tasks, such as variable heterogeneity, resolution diversity, and spatiotemporal coverage variations, leading to suboptimal performance when applied to WFMs. To bridge this gap, we introduce WeatherPEFT, a novel PEFT framework for WFMs incorporating two synergistic innovations. First, during the forward pass, Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embedding weights within the encoder to the input tokens of the pre-trained backbone via internal and external pattern extraction, enabling context-aware feature recalibration for specific downstream tasks. Furthermore, during backpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not only leverages Fisher information to identify and update the most task-critical parameters, thereby preserving invariant pre-trained knowledge, but also introduces randomness to stabilize the selection. We demonstrate the effectiveness and efficiency of WeatherPEFT on three downstream tasks, where existing PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFT achieves performance parity with Full-Tuning using fewer trainable parameters. The code of this work will be released.</li>
</ul>

<h3>Title: Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions</h3>
<ul>
<li><strong>Authors: </strong>Marc Damie, Florian Hahn, Andreas Peter, Jan Ramon</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22022">https://arxiv.org/abs/2509.22022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22022">https://arxiv.org/pdf/2509.22022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22022]] Eliminating Exponential Key Growth in PRG-Based Distributed Point Functions(https://arxiv.org/abs/2509.22022)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Distributed Point Functions (DPFs) enable sharing secret point functions across multiple parties, supporting privacy-preserving technologies such as Private Information Retrieval, and anonymous communications. While 2-party PRG-based schemes with logarithmic key sizes have been known for a decade, extending these solutions to multi-party settings has proven challenging. In particular, PRG-based multi-party DPFs have historically struggled with practicality due to key sizes growing exponentially with the number of parties and the field size. Our work addresses this efficiency bottleneck by optimizing the PRG-based multi-party DPF scheme of Boyle et al. (EUROCRYPT'15). By leveraging the honest-majority assumption, we eliminate the exponential factor present in this scheme. Our construction is the first PRG-based multi-party DPF scheme with practical key sizes, and provides key up to 3x smaller than the best known multi-party DPF. This work demonstrates that with careful optimization, PRG-based multi-party DPFs can achieve practical performances, and even obtain top performances.</li>
</ul>

<h3>Title: Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22023">https://arxiv.org/abs/2509.22023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22023">https://arxiv.org/pdf/2509.22023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22023]] Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error(https://arxiv.org/abs/2509.22023)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.</li>
</ul>

<h3>Title: Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer</h3>
<ul>
<li><strong>Authors: </strong>Zhihua Zhong, Xuanyang Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22038">https://arxiv.org/abs/2509.22038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22038">https://arxiv.org/pdf/2509.22038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22038]] Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer(https://arxiv.org/abs/2509.22038)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Latent space is one of the key concepts in generative AI, offering powerful means for creative exploration through vector manipulation. However, diffusion models like Stable Diffusion lack the intuitive latent vector control found in GANs, limiting their flexibility for artistic expression. This paper introduces \workname, a framework for integrating customizable latent space operations into the diffusion process. By enabling direct manipulation of conceptual and spatial representations, this approach expands creative possibilities in generative art. We demonstrate the potential of this framework through two artworks, \textit{Infinitepedia} and \textit{Latent Motion}, highlighting its use in conceptual blending and dynamic motion generation. Our findings reveal latent space structures with semantic and meaningless regions, offering insights into the geometry of diffusion models and paving the way for further explorations of latent space.</li>
</ul>

<h3>Title: "Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors</h3>
<ul>
<li><strong>Authors: </strong>Yue Liu, Yanjie Zhao, Yunbo Lyu, Ting Zhang, Haoyu Wang, David Lo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22040">https://arxiv.org/abs/2509.22040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22040">https://arxiv.org/pdf/2509.22040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22040]] "Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors(https://arxiv.org/abs/2509.22040)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Agentic AI coding editors driven by large language models have recently become more popular due to their ability to improve developer productivity during software development. Modern editors such as Cursor are designed not just for code completion, but also with more system privileges for complex coding tasks (e.g., run commands in the terminal, access development environments, and interact with external systems). While this brings us closer to the "fully automated programming" dream, it also raises new security concerns. In this study, we present the first empirical analysis of prompt injection attacks targeting these high-privilege agentic AI coding editors. We show how attackers can remotely exploit these systems by poisoning external development resources with malicious instructions, effectively hijacking AI agents to run malicious commands, turning "your AI" into "attacker's shell". To perform this analysis, we implement AIShellJack, an automated testing framework for assessing prompt injection vulnerabilities in agentic AI coding editors. AIShellJack contains 314 unique attack payloads that cover 70 techniques from the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale evaluation on GitHub Copilot and Cursor, and our evaluation results show that attack success rates can reach as high as 84% for executing malicious commands. Moreover, these attacks are proven effective across a wide range of objectives, ranging from initial access and system discovery to credential theft and data exfiltration.</li>
</ul>

<h3>Title: BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Ding, Muyun Jiang, Weibang Jiang, Shuailei Zhang, Xinliang Zhou, Chenyu Liu, Shanglin Li, Yong Li, Cuntai Guan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22050">https://arxiv.org/abs/2509.22050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22050">https://arxiv.org/pdf/2509.22050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22050]] BrainPro: Towards Large-scale Brain State-aware EEG Representation Learning(https://arxiv.org/abs/2509.22050)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) is a non-invasive technique for recording brain electrical activity, widely used in brain-computer interface (BCI) and healthcare. Recent EEG foundation models trained on large-scale datasets have shown improved performance and generalizability over traditional decoding methods, yet significant challenges remain. Existing models often fail to explicitly capture channel-to-channel and region-to-region interactions, which are critical sources of information inherently encoded in EEG signals. Due to varying channel configurations across datasets, they either approximate spatial structure with self-attention or restrict training to a limited set of common channels, sacrificing flexibility and effectiveness. Moreover, although EEG datasets reflect diverse brain states such as emotion, motor, and others, current models rarely learn state-aware representations during self-supervised pre-training. To address these gaps, we propose BrainPro, a large EEG model that introduces a retrieval-based spatial learning block to flexibly capture channel- and region-level interactions across varying electrode layouts, and a brain state-decoupling block that enables state-aware representation learning through parallel encoders with decoupling and region-aware reconstruction losses. This design allows BrainPro to adapt seamlessly to diverse tasks and hardware settings. Pre-trained on an extensive EEG corpus, BrainPro achieves state-of-the-art performance and robust generalization across nine public BCI datasets. Our codes and the pre-trained weights will be released.</li>
</ul>

<h3>Title: Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity</h3>
<ul>
<li><strong>Authors: </strong>Ping Chen, Xiang Liu, Zhaoxiang Liu, Zezhou Chen, Xingpeng Zhang, Huan Hu, Zipeng Wang, Kai Wang, Shuming Shi, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22054">https://arxiv.org/abs/2509.22054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22054">https://arxiv.org/pdf/2509.22054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22054]] Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity(https://arxiv.org/abs/2509.22054)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.</li>
</ul>

<h3>Title: RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media</h3>
<ul>
<li><strong>Authors: </strong>Yudong Li, Yufei Sun, Yuhan Yao, Peiru Yang, Wanyue Li, Jiajun Zou, Yongfeng Huang, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22055">https://arxiv.org/abs/2509.22055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22055">https://arxiv.org/pdf/2509.22055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22055]] RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media(https://arxiv.org/abs/2509.22055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) has led to widespread AI-Generated Text (AIGT) on social media platforms, creating unique challenges where content dynamics are driven by user engagement and evolve over time. However, existing datasets mainly depict static AIGT detection. In this work, we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social media AIGT analysis. This dataset is sourced from Xiaohongshu platform, containing user engagement metrics (e.g., likes, comments) and timestamps spanning from the pre-LLM period to July 2025, which enables research into the temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection Framework (PLAD), an interpretable approach that leverages psycholinguistic features. Our experiments show that PLAD achieves superior detection performance and provides insights into the signatures distinguishing human and AI-generated content. More importantly, it reveals the complex relationship between these linguistic features and social media engagement. The dataset is available at this https URL.</li>
</ul>

<h3>Title: High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22063">https://arxiv.org/abs/2509.22063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22063">https://arxiv.org/pdf/2509.22063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22063]] High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling(https://arxiv.org/abs/2509.22063)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.</li>
</ul>

<h3>Title: The Rogue Scalpel: Activation Steering Compromises LLM Safety</h3>
<ul>
<li><strong>Authors: </strong>Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22067">https://arxiv.org/abs/2509.22067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22067">https://arxiv.org/pdf/2509.22067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22067]] The Rogue Scalpel: Activation Steering Compromises LLM Safety(https://arxiv.org/abs/2509.22067)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability</a></li>
<li><strong>Abstract: </strong>Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.</li>
</ul>

<h3>Title: SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Inzamamul Alam, Md Tanvir Islam, Simon S. Woo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22070">https://arxiv.org/abs/2509.22070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22070">https://arxiv.org/pdf/2509.22070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22070]] SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection(https://arxiv.org/abs/2509.22070)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The increasing realism of content generated by GANs and diffusion models has made deepfake detection significantly more challenging. Existing approaches often focus solely on spatial or frequency-domain features, limiting their generalization to unseen manipulations. We propose the Spectral Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)} decomposes features into a local spatial branch for capturing texture-level anomalies and a global spectral branch that employs Fast Fourier Transform to model periodic inconsistencies. This dual-domain formulation allows SpecXNet to jointly exploit localized detail and global structural coherence, which are critical for distinguishing authentic from manipulated images. We also introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically fuses spatial and spectral features in a content-aware manner. Built atop a modified XceptionNet backbone, we embed the DDFC and DFA modules within a separable convolution block. Extensive experiments on multiple deepfake benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly under cross-dataset and unseen manipulation scenarios, while maintaining real-time feasibility. Our results highlight the effectiveness of unified spatial-spectral learning for robust and generalizable deepfake detection. To ensure reproducibility, we released the full code on \href{this https URL}{\textcolor{blue}{\textbf{GitHub}}}.</li>
</ul>

<h3>Title: Fine-tuning Done Right in Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Wanli Yang, Fei Sun, Rui Tang, Hongyu Zang, Du Su, Qi Cao, Jingang Wang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22072">https://arxiv.org/abs/2509.22072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22072">https://arxiv.org/pdf/2509.22072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22072]] Fine-tuning Done Right in Model Editing(https://arxiv.org/abs/2509.22072)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning, a foundational method for adapting large language models, has long been considered ineffective for model editing. Here, we challenge this belief, arguing that the reported failure arises not from the inherent limitation of fine-tuning itself, but from adapting it to the sequential nature of the editing task, a single-pass depth-first pipeline that optimizes each sample to convergence before moving on. While intuitive, this depth-first pipeline coupled with sample-wise updating over-optimizes each edit and induces interference across edits. Our controlled experiments reveal that simply restoring fine-tuning to the standard breadth-first (i.e., epoch-based) pipeline with mini-batch optimization substantially improves its effectiveness for model editing. Moreover, fine-tuning in editing also suffers from suboptimal tuning parameter locations inherited from prior methods. Through systematic analysis of tuning locations, we derive LocFT-BF, a simple and effective localized editing method built on the restored fine-tuning framework. Extensive experiments across diverse LLMs and datasets demonstrate that LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x beyond prior practice, without sacrificing general capabilities. By clarifying a long-standing misconception and introducing a principled localized tuning strategy, we advance fine-tuning from an underestimated baseline to a leading method for model editing, establishing a solid foundation for future research.</li>
</ul>

<h3>Title: COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning</h3>
<ul>
<li><strong>Authors: </strong>Dmitriy Shopkhoev, Denis Makhov, Magauiya Zhussip, Ammar Ali, Stamatios Lefkimmiatis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22075">https://arxiv.org/abs/2509.22075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22075">https://arxiv.org/pdf/2509.22075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22075]] COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning(https://arxiv.org/abs/2509.22075)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment.</li>
</ul>

<h3>Title: Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Li Xia, Zheng Liu, Sili Huang, Wei Tang, Xuan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22082">https://arxiv.org/abs/2509.22082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22082">https://arxiv.org/pdf/2509.22082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22082]] Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning(https://arxiv.org/abs/2509.22082)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) preserves privacy by keeping raw data local, yet Gradient Inversion Attacks (GIAs) pose significant threats. In FedAVG multi-step scenarios, attackers observe only aggregated gradients, making data reconstruction challenging. Existing surrogate model methods like SME assume linear parameter trajectories, but we demonstrate this severely underestimates SGD's nonlinear complexity, fundamentally limiting attack effectiveness. We propose Non-Linear Surrogate Model Extension (NL-SME), the first method to introduce nonlinear parametric trajectory modeling for GIAs. Our approach replaces linear interpolation with learnable quadratic Bzier curves that capture SGD's curved characteristics through control points, combined with regularization and dvec scaling mechanisms for enhanced expressiveness. Extensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME significantly outperforms baselines across all metrics, achieving order-of-magnitude improvements in cosine similarity loss while maintaining computational this http URL work exposes heightened privacy vulnerabilities in FL's multi-step update paradigm and offers novel perspectives for developing robust defense strategies.</li>
</ul>

<h3>Title: S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models</h3>
<ul>
<li><strong>Authors: </strong>Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22099">https://arxiv.org/abs/2509.22099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22099">https://arxiv.org/pdf/2509.22099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22099]] S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models(https://arxiv.org/abs/2509.22099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.</li>
</ul>

<h3>Title: Think Right, Not More: Test-Time Scaling for Numerical Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Primakov Chungkham, V Venktesh, Vinay Setty, Avishek Anand</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22101">https://arxiv.org/abs/2509.22101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22101">https://arxiv.org/pdf/2509.22101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22101]] Think Right, Not More: Test-Time Scaling for Numerical Claim Verification(https://arxiv.org/abs/2509.22101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at this https URL</li>
</ul>

<h3>Title: Reinforcement Learning for Durable Algorithmic Recourse</h3>
<ul>
<li><strong>Authors: </strong>Marina Ceccon, Alessandro Fabris, Goran Radanovi, Asia J. Biega, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22102">https://arxiv.org/abs/2509.22102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22102">https://arxiv.org/pdf/2509.22102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22102]] Reinforcement Learning for Durable Algorithmic Recourse(https://arxiv.org/abs/2509.22102)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention has been given to the temporal dynamics of recourse--particularly in competitive, resource-constrained settings where recommendations shape future applicant pools. In this work, we present a novel time-aware framework for algorithmic recourse, explicitly modeling how candidate populations adapt in response to recommendations. Additionally, we introduce a novel reinforcement learning (RL)-based recourse algorithm that captures the evolving dynamics of the environment to generate recommendations that are both feasible and valid. We design our recommendations to be durable, supporting validity over a predefined time horizon T. This durability allows individuals to confidently reapply after taking time to implement the suggested changes. Through extensive experiments in complex simulation environments, we show that our approach substantially outperforms existing baselines, offering a superior balance between feasibility and long-term validity. Together, these results underscore the importance of incorporating temporal and behavioral dynamics into the design of practical recourse systems.</li>
</ul>

<h3>Title: Large Material Gaussian Model for Relightable 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingrui Ye, Lingting Zhu, Runze Zhang, Zeyu Hu, Yingda Yin, Lanjiong Li, Lequan Yu, Qingmin Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22112">https://arxiv.org/abs/2509.22112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22112">https://arxiv.org/pdf/2509.22112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22112]] Large Material Gaussian Model for Relightable 3D Generation(https://arxiv.org/abs/2509.22112)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.</li>
</ul>

<h3>Title: Countering adversarial evasion in regression analysis</h3>
<ul>
<li><strong>Authors: </strong>David Benfield, Phan Tu Vuong, Alain Zemkoho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22113">https://arxiv.org/abs/2509.22113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22113">https://arxiv.org/pdf/2509.22113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22113]] Countering adversarial evasion in regression analysis(https://arxiv.org/abs/2509.22113)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Adversarial machine learning challenges the assumption that the underlying distribution remains consistent throughout the training and implementation of a prediction model. In particular, adversarial evasion considers scenarios where adversaries adapt their data to influence particular outcomes from established prediction models, such scenarios arise in applications such as spam email filtering, malware detection and fake-image generation, where security methods must be actively updated to keep up with the ever-improving generation of malicious data. Game theoretic models have been shown to be effective at modelling these scenarios and hence training resilient predictors against such adversaries. Recent advancements in the use of pessimistic bilevel optimsiation which remove assumptions about the convexity and uniqueness of the adversary's optimal strategy have proved to be particularly effective at mitigating threats to classifiers due to its ability to capture the antagonistic nature of the adversary. However, this formulation has not yet been adapted to regression scenarios. This article serves to propose a pessimistic bilevel optimisation program for regression scenarios which makes no assumptions on the convexity or uniqueness of the adversary's solutions.</li>
</ul>

<h3>Title: Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chao Wang, Tao Yang, Hongtao Tian, Yunsheng Shi, Qiyao Ma, Xiaotao Liu, Ting Yao, Wenbo Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22115">https://arxiv.org/abs/2509.22115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22115">https://arxiv.org/pdf/2509.22115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22115]] Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization(https://arxiv.org/abs/2509.22115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.</li>
</ul>

<h3>Title: Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM</h3>
<ul>
<li><strong>Authors: </strong>Xiao Chi, Wenlin Zhong, Yiquan Wu, Wei Wang, Kun Kuang, Fei Wu, Minghui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22119">https://arxiv.org/abs/2509.22119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22119">https://arxiv.org/pdf/2509.22119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22119]] Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM(https://arxiv.org/abs/2509.22119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Legal Article Prediction (LAP) is a critical task in legal text classification, leveraging natural language processing (NLP) techniques to automatically predict relevant legal articles based on the fact descriptions of cases. As a foundational step in legal decision-making, LAP plays a pivotal role in determining subsequent judgments, such as charges and penalties. Despite its importance, existing methods face significant challenges in addressing the complexities of LAP. Supervised classification models (SCMs), such as CNN and BERT, struggle to fully capture intricate fact patterns due to their inherent limitations. Conversely, large language models (LLMs), while excelling in generative tasks, perform suboptimally in predictive scenarios due to the abstract and ID-based nature of legal articles. Furthermore, the diversity of legal systems across jurisdictions exacerbates the issue, as most approaches are tailored to specific countries and lack broader applicability. To address these limitations, we propose Uni-LAP, a universal framework for legal article prediction that integrates the strengths of SCMs and LLMs through tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel Top-K loss function to generate accurate candidate articles, while the LLM employs syllogism-inspired reasoning to refine the final predictions. We evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical results demonstrate that our approach consistently outperforms existing baselines, showcasing its effectiveness and generalizability.</li>
</ul>

<h3>Title: Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jeong Eul Kwon, Joo Heung Yoon, Hyo Kyung Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22121">https://arxiv.org/abs/2509.22121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22121">https://arxiv.org/pdf/2509.22121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22121]] Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models(https://arxiv.org/abs/2509.22121)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Irregular sampling and high missingness are intrinsic challenges in modeling time series derived from electronic health records (EHRs),where clinical variables are measured at uneven intervals depending on workflow and intervention timing. To address this, we propose VITAL, a variable-aware, large language model (LLM) based framework tailored for learning from irregularly sampled physiological time series. VITAL differentiates between two distinct types of clinical variables: vital signs, which are frequently recorded and exhibit temporal patterns, and laboratory tests, which are measured sporadically and lack temporal structure. It reprograms vital signs into the language space, enabling the LLM to capture temporal context and reason over missing values through explicit encoding. In contrast, laboratory variables are embedded either using representative summary values or a learnable [Not measured] token, depending on their availability. Extensive evaluations on the benchmark datasets from the PhysioNet demonstrate that VITAL outperforms state of the art methods designed for irregular time series. Furthermore, it maintains robust performance under high levels of missingness, which is prevalent in real world clinical scenarios where key variables are often unavailable.</li>
</ul>

<h3>Title: Multilingual Vision-Language Models, A Survey</h3>
<ul>
<li><strong>Authors: </strong>Andrei-Alexandru Manea, Jindich Libovick</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22123">https://arxiv.org/abs/2509.22123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22123">https://arxiv.org/pdf/2509.22123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22123]] Multilingual Vision-Language Models, A Survey(https://arxiv.org/abs/2509.22123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This survey examines multilingual vision-language models that process text and images across languages. We review 31 models and 21 benchmarks, spanning encoder-only and generative architectures, and identify a key tension between language neutrality (consistent cross-lingual representations) and cultural awareness (adaptation to cultural contexts). Current training methods favor neutrality through contrastive learning, while cultural awareness depends on diverse data. Two-thirds of evaluation benchmarks use translation-based approaches prioritizing semantic consistency, though recent work incorporates culturally grounded content. We find discrepancies in cross-lingual capabilities and gaps between training objectives and evaluation goals.</li>
</ul>

<h3>Title: FoodSEM: Large Language Model Specialized in Food Named-Entity Linking</h3>
<ul>
<li><strong>Authors: </strong>Ana Gjorgjevikj, Matej Martinc, Gjorgjina Cenikj, Sao Deroski, Barbara Koroui Seljak, Tome Eftimov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22125">https://arxiv.org/abs/2509.22125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22125">https://arxiv.org/pdf/2509.22125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22125]] FoodSEM: Large Language Model Specialized in Food Named-Entity Linking(https://arxiv.org/abs/2509.22125)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source large language model (LLM) for named-entity linking (NEL) to food-related ontologies. To the best of our knowledge, food NEL is a task that cannot be accurately solved by state-of-the-art general-purpose (large) language models or custom domain-specific models/systems. Through an instruction-response (IR) scenario, FoodSEM links food-related entities mentioned in a text to several ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM model achieves state-of-the-art performance compared to related models/systems, with F1 scores even reaching 98% on some ontologies and datasets. The presented comparative analyses against zero-shot, one-shot, and few-shot LLM prompting baselines further highlight FoodSEM's superior performance over its non-fine-tuned version. By making FoodSEM and its related resources publicly available, the main contributions of this article include (1) publishing a food-annotated corpora into an IR format suitable for LLM fine-tuning/evaluation, (2) publishing a robust model to advance the semantic understanding of text in the food domain, and (3) providing a strong baseline on food NEL for future benchmarking.</li>
</ul>

<h3>Title: Guidance Watermarking for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Enoal Gesny, Eva Giboulot, Teddy Furon, Vivien Chappelier</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22126">https://arxiv.org/abs/2509.22126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22126">https://arxiv.org/pdf/2509.22126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22126]] Guidance Watermarking for Diffusion Models(https://arxiv.org/abs/2509.22126)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel watermarking method for diffusion models. It is based on guiding the diffusion process using the gradient computed from any off-the-shelf watermark decoder. The gradient computation encompasses different image augmentations, increasing robustness to attacks against which the decoder was not originally robust, without retraining or fine-tuning. Our method effectively convert any \textit{post-hoc} watermarking scheme into an in-generation embedding along the diffusion process. We show that this approach is complementary to watermarking techniques modifying the variational autoencoder at the end of the diffusion process. We validate the methods on different diffusion models and detectors. The watermarking guidance does not significantly alter the generated image for a given seed and prompt, preserving both the diversity and quality of generation.</li>
</ul>

<h3>Title: R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, Han Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22131">https://arxiv.org/abs/2509.22131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22131">https://arxiv.org/pdf/2509.22131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22131]] R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning(https://arxiv.org/abs/2509.22131)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: this https URL</li>
</ul>

<h3>Title: Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shijing Hu, Jingyang Li, Zhihui Lu, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22134">https://arxiv.org/abs/2509.22134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22134">https://arxiv.org/pdf/2509.22134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22134]] Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding(https://arxiv.org/abs/2509.22134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path, while decoding follows a tree policy that re-ranks and verifies multiple branches. This draft policy misalignment limits achievable speedups. We introduce Group Tree Optimization (GTO), which aligns training with the decoding-time tree policy through two components: (i) Draft Tree Reward, a sampling-free objective equal to the expected acceptance length of the draft tree under the target model, directly measuring decoding performance; (ii) Group-based Draft Policy Training, a stable optimization scheme that contrasts trees from the current and a frozen reference draft model, forming debiased group-standardized advantages and applying a PPO-style surrogate along the longest accepted sequence for robust updates. We further prove that increasing our Draft Tree Reward provably improves acceptance length and speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B, DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By bridging draft policy misalignment, GTO offers a practical, general solution for efficient LLM inference.</li>
</ul>

<h3>Title: REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Jiang, Jin Yuan, Hua Yuan, Yao Zhang, Yong Rui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22139">https://arxiv.org/abs/2509.22139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22139">https://arxiv.org/pdf/2509.22139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22139]] REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation(https://arxiv.org/abs/2509.22139)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Conditional image generation models have achieved remarkable results by leveraging text-based control to generate customized images. However, the high resource demands of these models and the scarcity of well-annotated data have hindered their deployment on edge devices, leading to enormous costs and privacy concerns, especially when user data is sent to a third party. To overcome these challenges, we propose Refine-Control, a semi-supervised distillation framework. Specifically, we improve the performance of the student model by introducing a tri-level knowledge fusion loss to transfer different levels of knowledge. To enhance generalization and alleviate dataset scarcity, we introduce a semi-supervised distillation method utilizing both labeled and unlabeled data. Our experiments reveal that Refine-Control achieves significant reductions in computational cost and latency, while maintaining high-fidelity generation capabilities and controllability, as quantified by comparative metrics.</li>
</ul>

<h3>Title: NFDI4DS Shared Tasks for Scholarly Document Processing</h3>
<ul>
<li><strong>Authors: </strong>Raia Abu Ahmad, Rana Abdulla, Tilahun Abedissa Taffa, Soeren Auer, Hamed Babaei Giglou, Ekaterina Borisova, Zongxiong Chen, Stefan Dietze, Jennifer DSouza, Mayra Elwes, Genet-Asefa Gesese, Shufan Jiang, Ekaterina Kutafina, Philipp Mayr, Georg Rehm, Sameer Sadruddin, Sonja Schimmler, Daniel Schneider, Kanishka Silva, Sharmila Upadhyaya, Ricardo Usbeck</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22141">https://arxiv.org/abs/2509.22141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22141">https://arxiv.org/pdf/2509.22141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22141]] NFDI4DS Shared Tasks for Scholarly Document Processing(https://arxiv.org/abs/2509.22141)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Shared tasks are powerful tools for advancing research through community-based standardised evaluation. As such, they play a key role in promoting findable, accessible, interoperable, and reusable (FAIR), as well as transparent and reproducible research practices. This paper presents an updated overview of twelve shared tasks developed and hosted under the German National Research Data Infrastructure for Data Science and Artificial Intelligence (NFDI4DS) consortium, covering a diverse set of challenges in scholarly document processing. Hosted at leading venues, the tasks foster methodological innovations and contribute open-access datasets, models, and tools for the broader research community, which are integrated into the consortium's research data infrastructure.</li>
</ul>

<h3>Title: The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost</h3>
<ul>
<li><strong>Authors: </strong>Johnnatan Messias, Christof Ferreira Torres</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22143">https://arxiv.org/abs/2509.22143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22143">https://arxiv.org/pdf/2509.22143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22143]] The Express Lane to Spam and Centralization: An Empirical Analysis of Arbitrum's Timeboost(https://arxiv.org/abs/2509.22143)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>DeFi applications are vulnerable to MEV, where specialized actors profit by reordering or inserting transactions. To mitigate latency races and internalize MEV revenue, Arbitrum introduced Timeboost, an auction-based transaction sequencing mechanism that grants short-term priority access to an express lane. In this paper we present the first large-scale empirical study of Timeboost, analyzing over 11.5 million express lane transactions and 151 thousand auctions between April and July 2025. Our results reveal five main findings. First, express lane control is highly centralized, with two entities winning more than 90% of auctions. Second, while express lane access provides earlier inclusion, profitable MEV opportunities cluster at the end of blocks, limiting the value of priority access. Third, approximately 22% of time-boosted transactions are reverted, indicating that the Timeboost does not effectively mitigate spam. Fourth, secondary markets for reselling express lane rights have collapsed due to poor execution reliability and unsustainable economics. Finally, auction competition declined over time, leading to steadily reduced revenue for the Arbitrum DAO. Taken together, these findings show that Timeboost fails to deliver on its stated goals of fairness, decentralization, and spam reduction. Instead, it reinforces centralization and narrows adoption, highlighting the limitations of auction-based ordering as a mechanism for fair transaction sequencing in rollups.</li>
</ul>

<h3>Title: Mixture of Detectors: A Compact View of Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Lekkala, Yadagiri Annepaka, Arun Kumar Challa, Samatha Reddy Machireddy, Partha Pakray, Chukhu Chunka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22147">https://arxiv.org/abs/2509.22147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22147">https://arxiv.org/pdf/2509.22147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22147]] Mixture of Detectors: A Compact View of Machine-Generated Text Detection(https://arxiv.org/abs/2509.22147)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are gearing up to surpass human creativity. The veracity of the statement needs careful consideration. In recent developments, critical questions arise regarding the authenticity of human work and the preservation of their creativity and innovative abilities. This paper investigates such issues. This paper addresses machine-generated text detection across several scenarios, including document-level binary and multiclass classification or generator attribution, sentence-level segmentation to differentiate between human-AI collaborative text, and adversarial attacks aimed at reducing the detectability of machine-generated text. We introduce a new work called BMAS English: an English language dataset for binary classification of human and machine text, for multiclass classification, which not only identifies machine-generated text but can also try to determine its generator, and Adversarial attack addressing where it is a common act for the mitigation of detection, and Sentence-level segmentation, for predicting the boundaries between human and machine-generated text. We believe that this paper will address previous work in Machine-Generated Text Detection (MGTD) in a more meaningful way.</li>
</ul>

<h3>Title: Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Tian, Weigang Li, Junwei Hu, Chunhua Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22150">https://arxiv.org/abs/2509.22150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22150">https://arxiv.org/pdf/2509.22150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22150]] Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions(https://arxiv.org/abs/2509.22150)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Classification tasks in 3D point clouds often assume that class events \replaced{are }{follow }independent and identically distributed (IID), although this assumption destroys the correlation between classes. This \replaced{study }{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph \textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for non-independent and identically distributed 3D point cloud data, \replaced{which }{the strategy } achieves knowledge transfer of class correlations through knowledge distillation by constructing a loss function based on joint graph entropy. First\deleted{ly}, we employ joint graphs to capture add{the }hidden relationships between classes\replaced{ and}{,} implement knowledge distillation to train our model by calculating the entropy of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds \deleted{that is }invariant to spatial transformations, we construct \replaced{S}{s}iamese structures and develop two frameworks, self-knowledge distillation and teacher-knowledge distillation, to facilitate information transfer between different transformation forms of the same data. \replaced{In addition}{ Additionally}, we use the above framework to achieve knowledge transfer between point clouds and their corrupted forms, and increase the robustness against corruption of model. Extensive experiments on ScanObject, ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy can achieve competitive results.</li>
</ul>

<h3>Title: Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Zhou Xu, Guyue Li, Zhe Peng, Aiqun Hu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22154">https://arxiv.org/abs/2509.22154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22154">https://arxiv.org/pdf/2509.22154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22154]] Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting(https://arxiv.org/abs/2509.22154)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Radio frequency fingerprint (RFF) is a promising device identification technology, with recent research shifting from robustness to security due to growing concerns over vulnerabilities. To date, while the security of RFF against basic spoofing such as MAC address tampering has been validated, its resilience to advanced mimicry remains unknown. To address this gap, we propose a collusion-driven impersonation attack that achieves RF-level mimicry, successfully breaking RFF identification systems across diverse environments. Specifically, the attacker synchronizes with a colluding receiver to match the centralized logarithmic power spectrum (CLPS) of the legitimate transmitter; once the colluder deems the CLPS identical, the victim receiver will also accept the forged fingerprint, completing RF-level spoofing. Given that the distribution of CLPS features is relatively concentrated and has a clear underlying structure, we design a spoofed signal generation network that integrates a variational autoencoder (VAE) with a multi-objective loss function to enhance the similarity and deceptive capability of the generated samples. We carry out extensive simulations, validating cross-channel attacks in environments that incorporate standard channel variations including additive white Gaussian noise (AWGN), multipath fading, and Doppler shift. The results indicate that the proposed attack scheme essentially maintains a success rate of over 95% under different channel conditions, revealing the effectiveness of this attack.</li>
</ul>

<h3>Title: Context Parametrization with Compositional Adapters</h3>
<ul>
<li><strong>Authors: </strong>Josip Juki, Martin Tutek, Jan najder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22158">https://arxiv.org/abs/2509.22158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22158">https://arxiv.org/pdf/2509.22158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22158]] Context Parametrization with Compositional Adapters(https://arxiv.org/abs/2509.22158)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often seamlessly adapt to new tasks through in-context learning (ICL) or supervised fine-tuning (SFT). However, both of these approaches face key limitations: ICL is inefficient when handling many demonstrations, and SFT incurs training overhead while sacrificing flexibility. Mapping instructions or demonstrations from context directly into adapter parameters offers an appealing alternative. While prior work explored generating adapters based on a single input context, it has overlooked the need to integrate multiple chunks of information. To address this gap, we introduce CompAs, a meta-learning framework that translates context into adapter parameters with a compositional structure. Adapters generated this way can be merged algebraically, enabling instructions, demonstrations, or retrieved passages to be seamlessly combined without reprocessing long prompts. Critically, this approach yields three benefits: lower inference cost, robustness to long-context instability, and establishes a principled solution when input exceeds the model's context window. Furthermore, CompAs encodes information into adapter parameters in a reversible manner, enabling recovery of input context through a decoder, facilitating safety and security. Empirical results on diverse multiple-choice and extractive question answering tasks show that CompAs outperforms ICL and prior generator-based methods, especially when scaling to more inputs. Our work establishes composable adapter generation as a practical and efficient alternative for scaling LLM deployment.</li>
</ul>

<h3>Title: Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shirin Alanova, Kristina Kazistova, Ekaterina Galaeva, Alina Kostromina, Vladimir Smirnov, Redko Dmitry, Alexey Dontsov, Maxim Zhelnin, Evgeny Burnaev, Egor Shvetsov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22166">https://arxiv.org/abs/2509.22166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22166">https://arxiv.org/pdf/2509.22166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22166]] Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs(https://arxiv.org/abs/2509.22166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The demand for efficient large language model (LLM) inference has intensified the focus on sparsification techniques. While semi-structured (N:M) pruning is well-established for weights, its application to activation pruning remains underexplored despite its potential for dynamic, input-adaptive compression and reductions in I/O overhead. This work presents a comprehensive analysis of methods for post-training N:M activation pruning in LLMs. Across multiple LLMs, we demonstrate that pruning activations enables superior preservation of generative capabilities compared to weight pruning at equivalent sparsity levels. We evaluate lightweight, plug-and-play error mitigation techniques and pruning criteria, establishing strong hardware-friendly baselines that require minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's standard 2:4, showing that the 16:32 pattern achieves performance nearly on par with unstructured sparsity. However, considering the trade-off between flexibility and hardware implementation complexity, we focus on the 8:16 pattern as a superior candidate. Our findings provide both effective practical methods for activation pruning and a motivation for future hardware to support more flexible sparsity patterns. Our code is available this https URL .</li>
</ul>

<h3>Title: Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead</h3>
<ul>
<li><strong>Authors: </strong>Durgesh Kalwar, Mayank Baranwal, Harshad Khadilkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22174">https://arxiv.org/abs/2509.22174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22174">https://arxiv.org/pdf/2509.22174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22174]] Efficiency Boost in Decentralized Optimization: Reimagining Neighborhood Aggregation with Minimal Overhead(https://arxiv.org/abs/2509.22174)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In today's data-sensitive landscape, distributed learning emerges as a vital tool, not only fortifying privacy measures but also streamlining computational operations. This becomes especially crucial within fully decentralized infrastructures where local processing is imperative due to the absence of centralized aggregation. Here, we introduce DYNAWEIGHT, a novel framework to information aggregation in multi-agent networks. DYNAWEIGHT offers substantial acceleration in decentralized learning with minimal additional communication and memory overhead. Unlike traditional static weight assignments, such as Metropolis weights, DYNAWEIGHT dynamically allocates weights to neighboring servers based on their relative losses on local datasets. Consequently, it favors servers possessing diverse information, particularly in scenarios of substantial data heterogeneity. Our experiments on various datasets MNIST, CIFAR10, and CIFAR100 incorporating various server counts and graph topologies, demonstrate notable enhancements in training speeds. Notably, DYNAWEIGHT functions as an aggregation scheme compatible with any underlying server-level optimization algorithm, underscoring its versatility and potential for widespread integration.</li>
</ul>

<h3>Title: When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Kevin El-Haddad, Cline Hudelot, Pierre Colombo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22193">https://arxiv.org/abs/2509.22193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22193">https://arxiv.org/pdf/2509.22193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22193]] When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance(https://arxiv.org/abs/2509.22193)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.</li>
</ul>

<h3>Title: The Outputs of Large Language Models are Meaningless</h3>
<ul>
<li><strong>Authors: </strong>Anandi Hattiangadi, Anders J. Schoubye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22206">https://arxiv.org/abs/2509.22206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22206">https://arxiv.org/pdf/2509.22206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22206]] The Outputs of Large Language Models are Meaningless(https://arxiv.org/abs/2509.22206)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we offer a simple argument for the conclusion that the outputs of large language models (LLMs) are meaningless. Our argument is based on two key premises: (a) that certain kinds of intentions are needed in order for LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have the right kinds of intentions. We defend this argument from various types of responses, for example, the semantic externalist argument that deference can be assumed to take the place of intentions and the semantic internalist argument that meanings can be defined purely in terms of intrinsic relations between concepts, such as conceptual roles. We conclude the paper by discussing why, even if our argument is sound, the outputs of LLMs nevertheless seem meaningful and can be used to acquire true beliefs and even knowledge.</li>
</ul>

<h3>Title: Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation</h3>
<ul>
<li><strong>Authors: </strong>Tiago Fernandes Tavares</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22211">https://arxiv.org/abs/2509.22211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22211">https://arxiv.org/pdf/2509.22211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22211]] Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation(https://arxiv.org/abs/2509.22211)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Unsupervised analysis of text corpora is challenging, especially in data-scarce domains where traditional topic models struggle. While these models offer a solution, they typically describe clusters with lists of keywords that require significant manual effort to interpret and often lack semantic coherence. To address this critical interpretability gap, we introduce Recursive Thematic Partitioning (RTP), a novel framework that leverages Large Language Models (LLMs) to interactively build a binary tree. Each node in the tree is a natural language question that semantically partitions the data, resulting in a fully interpretable taxonomy where the logic of each cluster is explicit. Our experiments demonstrate that RTP's question-driven hierarchy is more interpretable than the keyword-based topics from a strong baseline like BERTopic. Furthermore, we establish the quantitative utility of these clusters by showing they serve as powerful features in downstream classification tasks, particularly when the data's underlying themes correlate with the task labels. RTP introduces a new paradigm for data exploration, shifting the focus from statistical pattern discovery to knowledge-driven thematic analysis. Furthermore, we demonstrate that the thematic paths from the RTP tree can serve as structured, controllable prompts for generative models. This transforms our analytical framework into a powerful tool for synthesis, enabling the consistent imitation of specific characteristics discovered in the source corpus.</li>
</ul>

<h3>Title: Accuracy-First Rnyi Differential Privacy and Post-Processing Immunity</h3>
<ul>
<li><strong>Authors: </strong>Ossi Ris, Antti Koskela, Antti Honkela</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22213">https://arxiv.org/abs/2509.22213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22213">https://arxiv.org/pdf/2509.22213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22213]] Accuracy-First Rnyi Differential Privacy and Post-Processing Immunity(https://arxiv.org/abs/2509.22213)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The accuracy-first perspective of differential privacy addresses an important shortcoming by allowing a data analyst to adaptively adjust the quantitative privacy bound instead of sticking to a predetermined bound. Existing works on the accuracy-first perspective have neglected an important property of differential privacy known as post-processing immunity, which ensures that an adversary is not able to weaken the privacy guarantee by post-processing. We address this gap by determining which existing definitions in the accuracy-first perspective have post-processing immunity, and which do not. The only definition with post-processing immunity, pure ex-post privacy, lacks useful tools for practical problems, such as an ex-post analogue of the Gaussian mechanism, and an algorithm to check if accuracy on separate private validation set is high enough. To address this, we propose a new definition based on Rnyi differential privacy that has post-processing immunity, and we develop basic theory and tools needed for practical applications. We demonstrate the practicality of our theory with an application to synthetic data generation, where our algorithm successfully adjusts the privacy bound until an accuracy threshold is met on a private validation dataset.</li>
</ul>

<h3>Title: Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking</h3>
<ul>
<li><strong>Authors: </strong>Stefan Marksteiner, Mikael Sjdin, Marjan Sirjani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22215">https://arxiv.org/abs/2509.22215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22215">https://arxiv.org/pdf/2509.22215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22215]] Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking(https://arxiv.org/abs/2509.22215)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cyber-physical systems are part of industrial systems and critical infrastructure. Therefore, they should be examined in a comprehensive manner to verify their correctness and security. At the same time, the complexity of such systems demands such examinations to be systematic and, if possible, automated for efficiency and accuracy. A method that can be useful in this context is model checking. However, this requires a model that faithfully represents the behavior of the examined system. Obtaining such a model is not trivial, as many of these systems can be examined only in black box settings due to, e.g., long supply chains or secrecy. We therefore utilize active black box learning techniques to infer behavioral models in the form of Mealy machines of such systems and translate them into a form that can be evaluated using a model checker. To this end, we will investigate a cyber-physical systems as a black box using its external communication interface. We first annotate the model with propositions by mapping context information from the respective protocol to the model using Context-based Proposition Maps (CPMs). We gain annotated Mealy machines that resemble Kripke structures. We then formally define a template, to transfer the structures model checker-compatible format. We further define generic security properties based on basic security requirements. Due to the used CPMs, we can instantiate these properties with a meaningful context to check a specific protocol, which makes the approach flexible and scalable. The gained model can be easily altered to introduce non-deterministic behavior (like timeouts) or faults and examined if the properties still. Lastly, we demonstrate the versatility of the approach by providing case studies of different communication protocols (NFC and UDS), checked with the same tool chain and the same security properties.</li>
</ul>

<h3>Title: StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Song, Linhao Zhang, Chuhan Wu, Aiwei Liu, Wei Jia, Houfeng Wang, Xiao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22220">https://arxiv.org/abs/2509.22220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22220">https://arxiv.org/pdf/2509.22220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22220]] StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs(https://arxiv.org/abs/2509.22220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks.</li>
</ul>

<h3>Title: Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Zishan Ahmad, Saisubramaniam Gopalakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22224">https://arxiv.org/abs/2509.22224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22224">https://arxiv.org/pdf/2509.22224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22224]] Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data(https://arxiv.org/abs/2509.22224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite their remarkable capabilities, rely on singular, pre-dominant reasoning paradigms, hindering their performance on intricate problems that demand diverse cognitive strategies. To address this, we introduce Composite Reasoning (CR), a novel reasoning approach empowering LLMs to dynamically explore and combine multiple reasoning styles like deductive, inductive, and abductive for more nuanced problem-solving. Evaluated on scientific and medical question-answering benchmarks, our approach outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while demonstrating superior sample efficiency and adequate token usage. Notably, CR adaptively emphasizes domain-appropriate reasoning styles. It prioritizes abductive and deductive reasoning for medical question answering, but shifts to causal, deductive, and inductive methods for scientific reasoning. Our findings highlight that by cultivating internal reasoning style diversity, LLMs acquire more robust, adaptive, and efficient problem-solving abilities.</li>
</ul>

<h3>Title: Polysemous Language Gaussian Splatting via Matching-based Mask Lifting</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Ding, Xinpeng Liu, Zhiyi Pan, Shiqiang Long, Ge Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22225">https://arxiv.org/abs/2509.22225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22225">https://arxiv.org/pdf/2509.22225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22225]] Polysemous Language Gaussian Splatting via Matching-based Mask Lifting(https://arxiv.org/abs/2509.22225)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.</li>
</ul>

<h3>Title: UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jun He, Yi Lin, Zilong Huang, Jiacong Yin, Junyan Ye, Yuchuan Zhou, Weijia Li, Xiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22228">https://arxiv.org/abs/2509.22228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22228">https://arxiv.org/pdf/2509.22228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22228]] UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective(https://arxiv.org/abs/2509.22228)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Urban development impacts over half of the global population, making human-centered understanding of its structural and perceptual changes essential for sustainable development. While Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various domains, existing benchmarks that explore their performance in urban environments remain limited, lacking systematic exploration of temporal evolution and subjective perception of urban environment that aligns with human perception. To address these limitations, we propose UrbanFeel, a comprehensive benchmark designed to evaluate the performance of MLLMs in urban development understanding and subjective environmental perception. UrbanFeel comprises 14.3K carefully constructed visual questions spanning three cognitively progressive dimensions: Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception. We collect multi-temporal single-view and panoramic street-view images from 11 representative cities worldwide, and generate high-quality question-answer pairs through a hybrid pipeline of spatial clustering, rule-based generation, model-assisted prompting, and manual annotation. Through extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5 Pro achieves the best overall performance, with its accuracy approaching human expert levels and narrowing the average gap to just 1.5\%. Most models perform well on tasks grounded in scene understanding. In particular, some models even surpass human annotators in pixel-level change detection. However, performance drops notably in tasks requiring temporal reasoning over urban development. Additionally, in the subjective perception dimension, several models reach human-level or even higher consistency in evaluating dimension such as beautiful and safety.</li>
</ul>

<h3>Title: A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jiaping Yu, Muli Yang, Jiapeng Ji, Jiexi Yan, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22229">https://arxiv.org/abs/2509.22229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22229">https://arxiv.org/pdf/2509.22229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22229]] A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation(https://arxiv.org/abs/2509.22229)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic challenge of adapting a source-trained model to a target domain without access to the source data, driven by concerns over privacy and cost. Existing SFUDA methods either exploit only the source model's predictions or fine-tune large multimodal models, yet both neglect complementary insights and the latent structure of target data. In this paper, we propose the Experts Cooperative Learning (EXCL). EXCL contains the Dual Experts framework and Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts framework places a frozen source-domain model (augmented with Conv-Adapter) and a pretrained vision-language model (with a trainable text prompt) on equal footing to mine consensus knowledge from unlabeled target samples. To effectively train these plug-in modules under purely unsupervised conditions, we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that (1) collaboratively retrieves pseudo-source and complex target samples, (2) separately fine-tunes each expert on its respective sample set, and (3) enforces learning object consistency via a shared learning result. Extensive experiments on four benchmark datasets demonstrate that our approach matches state-of-the-art performance.</li>
</ul>

<h3>Title: Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Cimpean, Nicole Orzan, Catholijn Jonker, Pieter Libin, Ann Now</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22232">https://arxiv.org/abs/2509.22232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22232">https://arxiv.org/pdf/2509.22232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22232]] Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making(https://arxiv.org/abs/2509.22232)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Equity in real-world sequential decision problems can be enforced using fairness-aware methods. Therefore, we require algorithms that can make suitable and transparent trade-offs between performance and the desired fairness notions. As the desired performance-fairness trade-off is hard to specify a priori, we propose a framework where multiple trade-offs can be explored. Insights provided by the reinforcement learning algorithm regarding the obtainable performance-fairness trade-offs can then guide stakeholders in selecting the most appropriate policy. To capture fairness, we propose an extended Markov decision process, $f$MDP, that explicitly encodes individuals and groups. Given this $f$MDP, we formalise fairness notions in the context of sequential decision problems and formulate a fairness framework that computes fairness measures over time. We evaluate our framework in two scenarios with distinct fairness requirements: job hiring, where strong teams must be composed while treating applicants equally, and fraud detection, where fraudulent transactions must be detected while ensuring the burden on customers is fairly distributed. We show that our framework learns policies that are more fair across multiple scenarios, with only minor loss in performance reward. Moreover, we observe that group and individual fairness notions do not necessarily imply one another, highlighting the benefit of our framework in settings where both fairness types are desired. Finally, we provide guidelines on how to apply this framework across different problem settings.</li>
</ul>

<h3>Title: FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding</h3>
<ul>
<li><strong>Authors: </strong>Haorui Chen, Chengze Li, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22237">https://arxiv.org/abs/2509.22237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22237">https://arxiv.org/pdf/2509.22237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22237]] FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding(https://arxiv.org/abs/2509.22237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has given rise to a novel software development paradigm known as "vibe coding," where users interact with coding agents through high-level natural language. However, existing evaluation benchmarks for code generation inadequately assess an agent's vibe coding capabilities. Existing benchmarks are misaligned, as they either require code-level specifications or focus narrowly on issue-solving, neglecting the critical scenario of feature implementation within the vibe coding paradiam. To address this gap, we propose FeatBench, a novel benchmark for vibe coding that focuses on feature implementation. Our benchmark is distinguished by several key features: 1. Pure Natural Language Prompts. Task inputs consist solely of abstract natural language descriptions, devoid of any code or structural hints. 2. A Rigorous & Evolving Data Collection Process. FeatBench is built on a multi-level filtering pipeline to ensure quality and a fully automated pipeline to evolve the benchmark, mitigating data contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass (F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent regressions. 4. Diverse Application Domains. The benchmark includes repositories from diverse domains to ensure it reflects real-world scenarios. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. Our evaluation reveals that feature implementation within the vibe coding paradigm is a significant challenge, with the highest success rate of only 29.94%. Our analysis also reveals a tendency for "aggressive implementation," a strategy that paradoxically leads to both critical failures and superior software design. We release FeatBench, our automated collection pipeline, and all experimental results to facilitate further community research.</li>
</ul>

<h3>Title: FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yuan Ge, Saihan Chen, Jingqi Xiao, Xiaoqian Liu, Tong Xiao, Yan Xiang, Zhengtao Yu, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22243">https://arxiv.org/abs/2509.22243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22243">https://arxiv.org/pdf/2509.22243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22243]] FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction(https://arxiv.org/abs/2509.22243)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling real-time spoken dialogue systems. However, benchmarking and modeling these models remains a fundamental challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human spoken interaction that explicitly incorporates model interruption in emergency scenarios. FLEXI systematically evaluates the latency, quality, and conversational effectiveness of real-time dialogue through six diverse human-LLM interaction scenarios, revealing significant gaps between open source and commercial models in emergency awareness, turn terminating, and interaction latency. Finally, we suggest that next token-pair prediction offers a promising path toward achieving truly seamless and human-like full-duplex interaction.</li>
</ul>

<h3>Title: FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22244">https://arxiv.org/abs/2509.22244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22244">https://arxiv.org/pdf/2509.22244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22244]] FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing(https://arxiv.org/abs/2509.22244)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Hu, Huihao Jing, Haochen Shi, Haoran Li, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22250">https://arxiv.org/abs/2509.22250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22250">https://arxiv.org/pdf/2509.22250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22250]] Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance(https://arxiv.org/abs/2509.22250)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety. However, existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic protection, failing to ensure safety for the nuanced and complex behaviors of modern LLM systems. To address this problem, we solve LLM safety from legal compliance perspectives, named safety compliance. In this work, we posit relevant established legal frameworks as safety standards for defining and measuring safety compliance, including the EU AI Act and GDPR, which serve as core legal frameworks for AI safety and data security in Europe. To bridge the gap between LLM safety and legal compliance, we first develop a new benchmark for safety compliance by generating realistic LLM safety scenarios seeded with legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization (GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively aligns LLMs with legal standards to mitigate safety risks. Our comprehensive experiments demonstrate that the Compliance Reasoner achieves superior performance on the new benchmark, with average improvements of +10.45% for the EU AI Act and +11.85% for GDPR.</li>
</ul>

<h3>Title: Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22251">https://arxiv.org/abs/2509.22251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22251">https://arxiv.org/pdf/2509.22251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22251]] Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs(https://arxiv.org/abs/2509.22251)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at this https URL.</li>
</ul>

<h3>Title: Secure and Efficient Access Control for Computer-Use Agents via Context Space</h3>
<ul>
<li><strong>Authors: </strong>Haochen Gong, Chenxiao Li, Rui Chang, Wenbo Shen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22256">https://arxiv.org/abs/2509.22256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22256">https://arxiv.org/pdf/2509.22256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22256]] Secure and Efficient Access Control for Computer-Use Agents via Context Space(https://arxiv.org/abs/2509.22256)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.</li>
</ul>

<h3>Title: Wavelet-Induced Rotary Encodings: RoPE Meets Graphs</h3>
<ul>
<li><strong>Authors: </strong>Isaac Reid, Arijit Sehanobish, Cedrik Hfs, Bruno Mlodozeniec, Leonhard Vulpius, Federico Barbero, Adrian Weller, Krzysztof Choromanski, Richard E. Turner, Petar Velikovi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22259">https://arxiv.org/abs/2509.22259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22259">https://arxiv.org/pdf/2509.22259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22259]] Wavelet-Induced Rotary Encodings: RoPE Meets Graphs(https://arxiv.org/abs/2509.22259)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to graph-structured data. We demonstrate that WIRE is more general than RoPE, recovering the latter in the special case of grid graphs. WIRE also enjoys a host of desirable theoretical properties, including equivariance under node ordering permutation, compatibility with linear attention, and (under select assumptions) asymptotic dependence on graph resistive distance. We test WIRE on a range of synthetic and real-world tasks, including identifying monochromatic subgraphs, semantic segmentation of point clouds, and more standard graph benchmarks. We find it to be effective in settings where the underlying graph structure is important.</li>
</ul>

<h3>Title: UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data</h3>
<ul>
<li><strong>Authors: </strong>Yujian Yuan, Changjie Wu, Xinyuan Chang, Sijin Wang, Hang Zhang, Shiyi Liang, Shuang Zeng, Mu Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22262">https://arxiv.org/abs/2509.22262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22262">https://arxiv.org/pdf/2509.22262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22262]] UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data(https://arxiv.org/abs/2509.22262)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large-scale map construction is foundational for critical applications such as autonomous driving and navigation systems. Traditional large-scale map construction approaches mainly rely on costly and inefficient special data collection vehicles and labor-intensive annotation processes. While existing satellite-based methods have demonstrated promising potential in enhancing the efficiency and coverage of map construction, they exhibit two major limitations: (1) inherent drawbacks of satellite data (e.g., occlusions, outdatedness) and (2) inefficient vectorization from perception-based methods, resulting in discontinuous and rough roads that require extensive post-processing. This paper presents a novel generative framework, UniMapGen, for large-scale map construction, offering three key innovations: (1) representing lane lines as \textbf{discrete sequence} and establishing an iterative strategy to generate more complete and smooth map vectors than traditional perception-based methods. (2) proposing a flexible architecture that supports \textbf{multi-modal} inputs, enabling dynamic selection among BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3) developing a \textbf{state update} strategy for global continuity and consistency of the constructed large-scale map. UniMapGen achieves state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen can infer occluded roads and predict roads missing from dataset annotations. Our code will be released.</li>
</ul>

<h3>Title: Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Nakyeong Yang, Dong-Kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, Meeyoung Cha</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22263">https://arxiv.org/abs/2509.22263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22263">https://arxiv.org/pdf/2509.22263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22263]] Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning(https://arxiv.org/abs/2509.22263)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models trained on web-scale data can memorize private or sensitive knowledge, raising significant privacy risks. Although some unlearning methods mitigate these risks, they remain vulnerable to "relearning" during subsequent training, allowing a substantial portion of forgotten knowledge to resurface. In this paper, we show that widely used unlearning methods cause shallow alignment: instead of faithfully erasing target knowledge, they generate spurious unlearning neurons that amplify negative influence to hide it. To overcome this limitation, we introduce Ssiuu, a new class of unlearning methods that employs attribution-guided regularization to prevent spurious negative influence and faithfully remove target knowledge. Experimental results confirm that our method reliably erases target knowledge and outperforms strong baselines across two practical retraining scenarios: (1) adversarial injection of private data, and (2) benign attack using an instruction-following benchmark. Our findings highlight the necessity of robust and faithful unlearning methods for safe deployment of language models.</li>
</ul>

<h3>Title: Towards a more realistic evaluation of machine learning models for bearing fault diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Joo Paulo Vieira, Victor Afonso Bauler, Rodrigo Kobashikawa Rosa, Danilo Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22267">https://arxiv.org/abs/2509.22267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22267">https://arxiv.org/pdf/2509.22267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22267]] Towards a more realistic evaluation of machine learning models for bearing fault diagnosis(https://arxiv.org/abs/2509.22267)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reliable detection of bearing faults is essential for maintaining the safety and operational efficiency of rotating machinery. While recent advances in machine learning (ML), particularly deep learning, have shown strong performance in controlled settings, many studies fail to generalize to real-world applications due to methodological flaws, most notably data leakage. This paper investigates the issue of data leakage in vibration-based bearing fault diagnosis and its impact on model evaluation. We demonstrate that common dataset partitioning strategies, such as segment-wise and condition-wise splits, introduce spurious correlations that inflate performance metrics. To address this, we propose a rigorous, leakage-free evaluation methodology centered on bearing-wise data partitioning, ensuring no overlap between the physical components used for training and testing. Additionally, we reformulate the classification task as a multi-label problem, enabling the detection of co-occurring fault types and the use of prevalence-independent metrics such as Macro AUROC. Beyond preventing leakage, we also examine the effect of dataset diversity on generalization, showing that the number of unique training bearings is a decisive factor for achieving robust performance. We evaluate our methodology on three widely adopted datasets: CWRU, Paderborn University (PU), and University of Ottawa (UORED-VAFCLS). This study highlights the importance of leakage-aware evaluation protocols and provides practical guidelines for dataset partitioning, model selection, and validation, fostering the development of more trustworthy ML systems for industrial fault diagnosis applications.</li>
</ul>

<h3>Title: Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach</h3>
<ul>
<li><strong>Authors: </strong>Nassim Walha, Sebastian G. Gruber, Thomas Decker, Yinchong Yang, Alireza Javanmardi, Eyke Hllermeier, Florian Buettner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22272">https://arxiv.org/abs/2509.22272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22272">https://arxiv.org/pdf/2509.22272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22272]] Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach(https://arxiv.org/abs/2509.22272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly integrated in diverse applications, obtaining reliable measures of their predictive uncertainty has become critically important. A precise distinction between aleatoric uncertainty, arising from inherent ambiguities within input data, and epistemic uncertainty, originating exclusively from model limitations, is essential to effectively address each uncertainty source. In this paper, we introduce Spectral Uncertainty, a novel approach to quantifying and decomposing uncertainties in LLMs. Leveraging the Von Neumann entropy from quantum information theory, Spectral Uncertainty provides a rigorous theoretical foundation for separating total uncertainty into distinct aleatoric and epistemic components. Unlike existing baseline methods, our approach incorporates a fine-grained representation of semantic similarity, enabling nuanced differentiation among various semantic interpretations in model responses. Empirical evaluations demonstrate that Spectral Uncertainty outperforms state-of-the-art methods in estimating both aleatoric and total uncertainty across diverse models and benchmark datasets.</li>
</ul>

<h3>Title: A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Snchez, Ghada Elbez, Veit Hagenmeyer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22280">https://arxiv.org/abs/2509.22280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22280">https://arxiv.org/pdf/2509.22280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22280]] A Global Analysis of Cyber Threats to the Energy Sector: "Currents of Conflict" from a Geopolitical Perspective(https://arxiv.org/abs/2509.22280)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, generative</a></li>
<li><strong>Abstract: </strong>The escalating frequency and sophistication of cyber threats increased the need for their comprehensive understanding. This paper explores the intersection of geopolitical dynamics, cyber threat intelligence analysis, and advanced detection technologies, with a focus on the energy domain. We leverage generative artificial intelligence to extract and structure information from raw cyber threat descriptions, enabling enhanced analysis. By conducting a geopolitical comparison of threat actor origins and target regions across multiple databases, we provide insights into trends within the general threat landscape. Additionally, we evaluate the effectiveness of cybersecurity tools -- with particular emphasis on learning-based techniques -- in detecting indicators of compromise for energy-targeted attacks. This analysis yields new insights, providing actionable information to researchers, policy makers, and cybersecurity professionals.</li>
</ul>

<h3>Title: Conditional Denoising Diffusion Autoencoders for Wireless Semantic Communications</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Letafati, Samad Ali, Matti Latva-aho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22282">https://arxiv.org/abs/2509.22282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22282">https://arxiv.org/pdf/2509.22282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22282]] Conditional Denoising Diffusion Autoencoders for Wireless Semantic Communications(https://arxiv.org/abs/2509.22282)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Semantic communication (SemCom) systems aim to learn the mapping from low-dimensional semantics to high-dimensional ground-truth. While this is more akin to a "domain translation" problem, existing frameworks typically emphasize on channel-adaptive neural encoding-decoding schemes, lacking full exploration of signal distribution. Moreover, such methods so far have employed autoencoder-based architectures, where the encoding is tightly coupled to a matched decoder, causing scalability issues in practice. To address these gaps, diffusion autoencoder models are proposed for wireless SemCom. The goal is to learn a "semantic-to-clean" mapping, from the semantic space to the ground-truth probability distribution. A neural encoder at semantic transmitter extracts the high-level semantics, and a conditional diffusion model (CDiff) at the semantic receiver exploits the source distribution for signal-space denoising, while the received semantic latents are incorporated as the conditioning input to "steer" the decoding process towards the semantics intended by the transmitter. It is analytically proved that the proposed decoder model is a consistent estimator of the ground-truth data. Furthermore, extensive simulations over CIFAR-10 and MNIST datasets are provided along with design insights, highlighting the performance compared to legacy autoencoders and variational autoencoders (VAE). Simulations are further extended to the multi-user SemCom, identifying the dominating factors in a more realistic setup.</li>
</ul>

<h3>Title: Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Mayank Jobanputra, Ji-Ung Lee, Soyoung Oh, Isabel Valera, Vera Demberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22291">https://arxiv.org/abs/2509.22291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22291">https://arxiv.org/pdf/2509.22291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22291]] Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?(https://arxiv.org/abs/2509.22291)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, explainability</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) models often replicate or amplify social bias from training data, raising concerns about fairness. At the same time, their black-box nature makes it difficult for users to recognize biased predictions and for developers to effectively mitigate them. While some studies suggest that input-based explanations can help detect and mitigate bias, others question their reliability in ensuring fairness. Existing research on explainability in fair NLP has been predominantly qualitative, with limited large-scale quantitative analysis. In this work, we conduct the first systematic study of the relationship between explainability and fairness in hate speech detection, focusing on both encoder- and decoder-only models. We examine three key dimensions: (1) identifying biased predictions, (2) selecting fair models, and (3) mitigating bias during model training. Our findings show that input-based explanations can effectively detect biased predictions and serve as useful supervision for reducing bias during training, but they are unreliable for selecting fair models among candidates.</li>
</ul>

<h3>Title: Jailbreaking on Text-to-Video Models via Scene Splitting Strategy</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Lee, Haon Park, Doehyeon Lee, Bumsub Ham, Suhyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22292">https://arxiv.org/abs/2509.22292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22292">https://arxiv.org/pdf/2509.22292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22292]] Jailbreaking on Text-to-Video Models via Scene Splitting Strategy(https://arxiv.org/abs/2509.22292)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.</li>
</ul>

<h3>Title: Aurora: Towards Universal Generative Multimodal Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Xingjian Wu, Jianxin Jin, Wanghui Qiu, Peng Chen, Yang Shu, Bin Yang, Chenjuan Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22295">https://arxiv.org/abs/2509.22295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22295">https://arxiv.org/pdf/2509.22295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22295]] Aurora: Towards Universal Generative Multimodal Time Series Forecasting(https://arxiv.org/abs/2509.22295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Corss-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corrsponding text or image modalities, thus possessing strong Cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.</li>
</ul>

<h3>Title: HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space</h3>
<ul>
<li><strong>Authors: </strong>Ke Li, Zheng Yang, Zhongbin Zhou, Feng Xue, Zhonglin Jiang, Wenxiao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22299">https://arxiv.org/abs/2509.22299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22299">https://arxiv.org/pdf/2509.22299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22299]] HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space(https://arxiv.org/abs/2509.22299)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of 20% ~ 25% in most models, while also reducing FLOPs nearly by 20%. The code can be found at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22300">https://arxiv.org/abs/2509.22300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22300">https://arxiv.org/pdf/2509.22300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22300]] HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models(https://arxiv.org/abs/2509.22300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity.</li>
</ul>

<h3>Title: SoDaDE: Solvent Data-Driven Embeddings with Small Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Kitso Gibberd, Jose Pablo Folch, Antonio Del Rio Chanona</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22302">https://arxiv.org/abs/2509.22302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22302">https://arxiv.org/pdf/2509.22302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22302]] SoDaDE: Solvent Data-Driven Embeddings with Small Transformer Models(https://arxiv.org/abs/2509.22302)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Computational representations have become crucial in unlocking the recent growth of machine learning algorithms for chemistry. Initially hand-designed, machine learning has shown that meaningful representations can be learnt from data. Chemical datasets are limited and so the representations learnt from data are generic, being trained on broad datasets which contain shallow information on many different molecule types. For example, generic fingerprints lack physical context specific to solvents. However, the use of harmful solvents is a leading climate-related issue in the chemical industry, and there is a surge of interest in green solvent replacement. To empower this research, we propose a new solvent representation scheme by developing Solvent Data Driven Embeddings (SoDaDE). SoDaDE uses a small transformer model and solvent property dataset to create a fingerprint for solvents. To showcase their effectiveness, we use SoDaDE to predict yields on a recently published dataset, outperforming previous representations. We demonstrate through this paper that data-driven fingerprints can be made with small datasets and set-up a workflow that can be explored for other applications.</li>
</ul>

<h3>Title: Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jinpeng Lu, Linghan Cai, Yinda Chen, Guo Tang, Songhan Jiang, Haoyuan Shi, Zhiwei Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22307">https://arxiv.org/abs/2509.22307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22307">https://arxiv.org/pdf/2509.22307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22307]] Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation(https://arxiv.org/abs/2509.22307)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Lightweight 3D medical image segmentation remains constrained by a fundamental "efficiency / robustness conflict", particularly when processing complex anatomical structures and heterogeneous modalities. In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods. Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). For each 3D image, we invoke a "glance-and-focus" principle, where PWA rapidly retrieves multi-scale information, and JLC ensures robust local feature extraction with minimal parameters, significantly enhancing the model's ability to operate with low computational budget. Followed by an extension of the dual-stream architecture that incorporates modal interaction into the multi-scale image-retrieval process, VeloxSeg efficiently models heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer (SDKT) via Gram matrices injects the texture prior extracted by a self-supervised network into the segmentation network, yielding stronger representations than baselines at no extra inference cost. Experimental results on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement, alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available at this https URL.</li>
</ul>

<h3>Title: NIFTY: a Non-Local Image Flow Matching for Texture Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Pierrick Chatillon, Julien Rabin, David Tschumperl</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22318">https://arxiv.org/abs/2509.22318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22318">https://arxiv.org/pdf/2509.22318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22318]] NIFTY: a Non-Local Image Flow Matching for Texture Synthesis(https://arxiv.org/abs/2509.22318)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of exemplar-based texture synthesis. We introduce NIFTY, a hybrid framework that combines recent insights on diffusion models trained with convolutional neural networks, and classical patch-based texture optimization techniques. NIFTY is a non-parametric flow-matching model built on non-local patch matching, which avoids the need for neural network training while alleviating common shortcomings of patch-based methods, such as poor initialization or visual artifacts. Experimental results demonstrate the effectiveness of the proposed approach compared to representative methods from the literature. Code is available at this https URL</li>
</ul>

<h3>Title: Distributed Associative Memory via Online Convex Optimization</h3>
<ul>
<li><strong>Authors: </strong>Bowen Wang, Matteo Zecchin, Osvaldo Simeone</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22321">https://arxiv.org/abs/2509.22321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22321">https://arxiv.org/pdf/2509.22321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22321]] Distributed Associative Memory via Online Convex Optimization(https://arxiv.org/abs/2509.22321)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>An associative memory (AM) enables cue-response recall, and associative memorization has recently been noted to underlie the operation of modern neural architectures such as Transformers. This work addresses a distributed setting where agents maintain a local AM to recall their own associations as well as selective information from others. Specifically, we introduce a distributed online gradient descent method that optimizes local AMs at different agents through communication over routing trees. Our theoretical analysis establishes sublinear regret guarantees, and experiments demonstrate that the proposed protocol consistently outperforms existing online optimization baselines.</li>
</ul>

<h3>Title: RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22323">https://arxiv.org/abs/2509.22323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22323">https://arxiv.org/pdf/2509.22323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22323]] RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer(https://arxiv.org/abs/2509.22323)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.</li>
</ul>

<h3>Title: Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Felix Vossel, Till Mossakowski, Bjrn Gehrke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22338">https://arxiv.org/abs/2509.22338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22338">https://arxiv.org/pdf/2509.22338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22338]] Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs(https://arxiv.org/abs/2509.22338)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.</li>
</ul>

<h3>Title: CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process</h3>
<ul>
<li><strong>Authors: </strong>Arman Akbari, Jian Gao, Yifei Zou, Mei Yang, Jinru Duan, Dmitrii Torbunov, Yanzhi Wang, Yihui Ren, Xuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22339">https://arxiv.org/abs/2509.22339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22339">https://arxiv.org/pdf/2509.22339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22339]] CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process(https://arxiv.org/abs/2509.22339)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85\% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19\%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence.</li>
</ul>

<h3>Title: Transformers Can Learn Connectivity in Some Graphs but Not Others</h3>
<ul>
<li><strong>Authors: </strong>Amit Roy, Abulhair Saparov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22343">https://arxiv.org/abs/2509.22343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22343">https://arxiv.org/pdf/2509.22343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22343]] Transformers Can Learn Connectivity in Some Graphs but Not Others(https://arxiv.org/abs/2509.22343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on "grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.</li>
</ul>

<h3>Title: The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling</h3>
<ul>
<li><strong>Authors: </strong>Sophie Spliethoff, Sanne Hoeken, Silke Schwandt, Sina Zarrie, zge Alaam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22345">https://arxiv.org/abs/2509.22345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22345">https://arxiv.org/pdf/2509.22345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22345]] The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling(https://arxiv.org/abs/2509.22345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we aim at the application of Natural Language Processing (NLP) techniques to historical research endeavors, particularly addressing the study of religious invectives in the context of the Protestant Reformation in Tudor England. We outline a workflow spanning from raw data, through pre-processing and data selection, to an iterative annotation process. As a result, we introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English (EModE) sentences, which are enriched with expert annotations regarding invective language throughout 16th-century England. Subsequently, we assess and compare the performance of fine-tuned BERT-based models and zero-shot prompted instruction-tuned large language models (LLMs), which highlights the superiority of models pre-trained on historical data and fine-tuned to invective detection.</li>
</ul>

<h3>Title: SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Marie Brockschmidt, Maresa Schrder, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22352">https://arxiv.org/abs/2509.22352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22352">https://arxiv.org/pdf/2509.22352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22352]] SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis(https://arxiv.org/abs/2509.22352)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Survival analysis is a cornerstone of clinical research by modeling time-to-event outcomes such as metastasis, disease relapse, or patient death. Unlike standard tabular data, survival data often come with incomplete event information due to dropout, or loss to follow-up. This poses unique challenges for synthetic data generation, where it is crucial for clinical research to faithfully reproduce both the event-time distribution and the censoring mechanism. In this paper, we propose SurvDiff, an end-to-end diffusion model specifically designed for generating synthetic data in survival analysis. SurvDiff is tailored to capture the data-generating mechanism by jointly generating mixed-type covariates, event times, and right-censoring, guided by a survival-tailored loss function. The loss encodes the time-to-event structure and directly optimizes for downstream survival tasks, which ensures that SurvDiff (i) reproduces realistic event-time distributions and (ii) preserves the censoring mechanism. Across multiple datasets, we show that \survdiff consistently outperforms state-of-the-art generative baselines in both distributional fidelity and downstream evaluation metrics across multiple medical datasets. To the best of our knowledge, SurvDiff is the first diffusion model explicitly designed for generating synthetic survival data.</li>
</ul>

<h3>Title: Conversational Implicatures: Modelling Relevance Theory Probabilistically</h3>
<ul>
<li><strong>Authors: </strong>Christoph Unger, Hendrik Buschmeier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22354">https://arxiv.org/abs/2509.22354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22354">https://arxiv.org/pdf/2509.22354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22354]] Conversational Implicatures: Modelling Relevance Theory Probabilistically(https://arxiv.org/abs/2509.22354)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Recent advances in Bayesian probability theory and its application to cognitive science in combination with the development of a new generation of computational tools and methods for probabilistic computation have led to a 'probabilistic turn' in pragmatics and semantics. In particular, the framework of Rational Speech Act theory has been developed to model broadly Gricean accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple reference games and covering ever more complex communicative exchanges such as verbal syllogistic reasoning. This paper explores in which way a similar Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber & Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication of implicit meaning by ways of (conversational) implicatures.</li>
</ul>

<h3>Title: Stochastic activations</h3>
<ul>
<li><strong>Authors: </strong>Maria Lomeli, Matthijs Douze, Gergely Szilvasy, Loic Cabannes, Jade Copet, Sainbayar Sukhbaatar, Jason Weston, Gabriel Synnaeve, Pierre-Emmanuel Mazar, Herv Jgou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22358">https://arxiv.org/abs/2509.22358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22358">https://arxiv.org/pdf/2509.22358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22358]] Stochastic activations(https://arxiv.org/abs/2509.22358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways: (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function. (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.</li>
</ul>

<h3>Title: CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22360">https://arxiv.org/abs/2509.22360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22360">https://arxiv.org/pdf/2509.22360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22360]] CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models(https://arxiv.org/abs/2509.22360)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( this https URL). Code is available at (this https URL).</li>
</ul>

<h3>Title: Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Max Malyi, Jonathan Shek, Andre Biscaya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22366">https://arxiv.org/abs/2509.22366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22366">https://arxiv.org/pdf/2509.22366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22366]] Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models(https://arxiv.org/abs/2509.22366)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A wealth of operational intelligence is locked within the unstructured free-text of wind turbine maintenance logs, a resource largely inaccessible to traditional quantitative reliability analysis. While machine learning has been applied to this data, existing approaches typically stop at classification, categorising text into predefined labels. This paper addresses the gap in leveraging modern large language models (LLMs) for more complex reasoning tasks. We introduce an exploratory framework that uses LLMs to move beyond classification and perform deep semantic analysis. We apply this framework to a large industrial dataset to execute four analytical workflows: failure mode identification, causal chain inference, comparative site analysis, and data quality auditing. The results demonstrate that LLMs can function as powerful "reliability co-pilots," moving beyond labelling to synthesise textual information and generate actionable, expert-level hypotheses. This work contributes a novel and reproducible methodology for using LLMs as a reasoning tool, offering a new pathway to enhance operational intelligence in the wind energy sector by unlocking insights previously obscured in unstructured data.</li>
</ul>

<h3>Title: What Is The Political Content in LLMs' Pre- and Post-Training Data?</h3>
<ul>
<li><strong>Authors: </strong>Tanise Ceron, Dmitry Nikolaev, Dominik Stammbach, Debora Nozza</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22367">https://arxiv.org/abs/2509.22367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22367">https://arxiv.org/pdf/2509.22367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22367]] What Is The Political Content in LLMs' Pre- and Post-Training Data?(https://arxiv.org/abs/2509.22367)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to generate politically biased text, yet how such biases arise remains unclear. A crucial step toward answering this question is the analysis of training data, whose political content remains largely underexplored in current LLM research. To address this gap, we present in this paper an analysis of the pre- and post-training corpora of OLMO2, the largest fully open-source model released together with its complete dataset. From these corpora, we draw large random samples, automatically annotate documents for political orientation, and analyze their source domains and content. We then assess how political content in the training data correlates with models' stance on specific policy issues. Our analysis shows that left-leaning documents predominate across datasets, with pre-training corpora containing significantly more politically engaged content than post-training data. We also find that left- and right-leaning documents frame similar topics through distinct values and sources of legitimacy. Finally, the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues. These findings underscore the need to integrate political content analysis into future data curation pipelines as well as in-depth documentation of filtering strategies for transparency.</li>
</ul>

<h3>Title: Role-Aware Multi-modal federated learning system for detecting phishing webpages</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Imran Khan, Martin White, Natalia Beloff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22369">https://arxiv.org/abs/2509.22369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22369">https://arxiv.org/pdf/2509.22369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22369]] Role-Aware Multi-modal federated learning system for detecting phishing webpages(https://arxiv.org/abs/2509.22369)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>We present a federated, multi-modal phishing website detector that supports URL, HTML, and IMAGE inputs without binding clients to a fixed modality at inference: any client can invoke any modality head trained elsewhere. Methodologically, we propose role-aware bucket aggregation on top of FedProx, inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling separate aggregation of modality-specific parameters to isolate cross-embedding conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc 97.5% with FPR 2.4% across two data types; on the image subset (ablation) it attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc 96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results indicate that bucket aggregation with hard-gated experts enables stable federated training under strict privacy, while improving the usability and flexibility of multi-modal phishing detection.</li>
</ul>

<h3>Title: Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results</h3>
<ul>
<li><strong>Authors: </strong>Yasmina Kheddache, Marc Lalonde</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22377">https://arxiv.org/abs/2509.22377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22377">https://arxiv.org/pdf/2509.22377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22377]] Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results(https://arxiv.org/abs/2509.22377)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The proliferation of disinformation, particularly in multimodal contexts combining text and images, presents a significant challenge across digital platforms. This study investigates the potential of large multimodal models (LMMs) in detecting and mitigating false information. We propose to approach multimodal disinformation detection by leveraging the advanced capabilities of the GPT-4o model. Our contributions include: (1) the development of an optimized prompt incorporating advanced prompt engineering techniques to ensure precise and consistent evaluations; (2) the implementation of a structured framework for multimodal analysis, including a preprocessing methodology for images and text to comply with the model's token limitations; (3) the definition of six specific evaluation criteria that enable a fine-grained classification of content, complemented by a self-assessment mechanism based on confidence levels; (4) a comprehensive performance analysis of the model across multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench, and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation detection; (5) an investigation of prediction variability through repeated testing, evaluating the stability and reliability of the model's classifications; and (6) the introduction of confidence-level and variability-based evaluation methods. These contributions provide a robust and reproducible methodological framework for automated multimodal disinformation analysis.</li>
</ul>

<h3>Title: Enhancing Credit Risk Prediction: A Meta-Learning Framework Integrating Baseline Models, LASSO, and ECOC for Superior Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Haibo Wang, Lutfu S. Sua, Jun Huang, Figen Balo, Burak Dolar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22381">https://arxiv.org/abs/2509.22381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22381">https://arxiv.org/pdf/2509.22381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22381]] Enhancing Credit Risk Prediction: A Meta-Learning Framework Integrating Baseline Models, LASSO, and ECOC for Superior Accuracy(https://arxiv.org/abs/2509.22381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Effective credit risk management is fundamental to financial decision-making, necessitating robust models for default probability prediction and financial entity classification. Traditional machine learning approaches face significant challenges when confronted with high-dimensional data, limited interpretability, rare event detection, and multi-class imbalance problems in risk assessment. This research proposes a comprehensive meta-learning framework that synthesizes multiple complementary models: supervised learning algorithms, including XGBoost, Random Forest, Support Vector Machine, and Decision Tree; unsupervised methods such as K-Nearest Neighbors; deep learning architectures like Multilayer Perceptron; alongside LASSO regularization for feature selection and dimensionality reduction; and Error-Correcting Output Codes as a meta-classifier for handling imbalanced multi-class problems. We implement Permutation Feature Importance analysis for each prediction class across all constituent models to enhance model transparency. Our framework aims to optimize predictive performance while providing a more holistic approach to credit risk assessment. This research contributes to the development of more accurate and reliable computational models for strategic financial decision support by addressing three fundamental challenges in credit risk modeling. The empirical validation of our approach involves an analysis of the Corporate Credit Ratings dataset with credit ratings for 2,029 publicly listed US companies. Results demonstrate that our meta-learning framework significantly enhances the accuracy of financial entity classification regarding credit rating migrations (upgrades and downgrades) and default probability estimation.</li>
</ul>

<h3>Title: GPT-4 for Occlusion Order Recovery</h3>
<ul>
<li><strong>Authors: </strong>Kaziwa Saleh, Zhyar Rzgar K Rostam, Sndor Sznsi, Zoltn Vmossy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22383">https://arxiv.org/abs/2509.22383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22383">https://arxiv.org/pdf/2509.22383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22383]] GPT-4 for Occlusion Order Recovery(https://arxiv.org/abs/2509.22383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Occlusion remains a significant challenge for current vision models to robustly interpret complex and dense real-world images and scenes. To address this limitation and to enable accurate prediction of the occlusion order relationship between objects, we propose leveraging the advanced capability of a pre-trained GPT-4 model to deduce the order. By providing a specifically designed prompt along with the input image, GPT-4 can analyze the image and generate order predictions. The response can then be parsed to construct an occlusion matrix which can be utilized in assisting with other occlusion handling tasks and image understanding. We report the results of evaluating the model on COCOA and InstaOrder datasets. The results show that by using semantic context, visual patterns, and commonsense knowledge, the model can produce more accurate order predictions. Unlike baseline methods, the model can reason about occlusion relationships in a zero-shot fashion, which requires no annotated training data and can easily be integrated into occlusion handling frameworks.</li>
</ul>

<h3>Title: (Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation for Interpretable Classification</h3>
<ul>
<li><strong>Authors: </strong>Luca Bergamin, Roberto Confalonieri, Fabio Aiolli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22384">https://arxiv.org/abs/2509.22384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22384">https://arxiv.org/pdf/2509.22384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22384]] (Sometimes) Less is More: Mitigating the Complexity of Rule-based Representation for Interpretable Classification(https://arxiv.org/abs/2509.22384)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks are widely used in practical applications of AI, however, their inner structure and complexity made them generally not easily interpretable. Model transparency and interpretability are key requirements for multiple scenarios where high performance is not enough to adopt the proposed solution. In this work, a differentiable approximation of $L_0$ regularization is adapted into a logic-based neural network, the Multi-layer Logical Perceptron (MLLP), to study its efficacy in reducing the complexity of its discrete interpretable version, the Concept Rule Set (CRS), while retaining its performance. The results are compared to alternative heuristics like Random Binarization of the network weights, to determine if better results can be achieved when using a less-noisy technique that sparsifies the network based on the loss function instead of a random distribution. The trade-off between the CRS complexity and its performance is discussed.</li>
</ul>

<h3>Title: SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly</h3>
<ul>
<li><strong>Authors: </strong>Narada Maugin, Tristan Cazenave</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22387">https://arxiv.org/abs/2509.22387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22387">https://arxiv.org/pdf/2509.22387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22387]] SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly(https://arxiv.org/abs/2509.22387)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.</li>
</ul>

<h3>Title: Text Adversarial Attacks with Dynamic Outputs</h3>
<ul>
<li><strong>Authors: </strong>Wenqiang Wang, Siyuan Liang, Xiao Yan, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22393">https://arxiv.org/abs/2509.22393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22393">https://arxiv.org/pdf/2509.22393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22393]] Text Adversarial Attacks with Dynamic Outputs(https://arxiv.org/abs/2509.22393)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Text adversarial attack methods are typically designed for static scenarios with fixed numbers of output labels and a predefined label space, relying on extensive querying of the victim model (query-based attacks) or the surrogate model (transfer-based attacks). To address this gap, we introduce the Textual Dynamic Outputs Attack (TDOA) method, which employs a clustering-based surrogate model training approach to convert the dynamic-output scenario into a static single-output scenario. To improve attack effectiveness, we propose the farthest-label targeted attack strategy, which selects adversarial vectors that deviate most from the model's coarse-grained labels, thereby maximizing disruption. We extensively evaluate TDOA on four datasets and eight victim models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting adversarial examples and its strong potential to compromise large language models with limited access. With a single query per text, TDOA achieves a maximum attack success rate of 50.81\%. Additionally, we find that TDOA also achieves state-of-the-art performance in conventional static output scenarios, reaching a maximum ASR of 82.68\%. Meanwhile, by conceptualizing translation tasks as classification problems with unbounded output spaces, we extend the TDOA framework to generative settings, surpassing prior results by up to 0.64 RDBLEU and 0.62 RDchrF.</li>
</ul>

<h3>Title: Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Luca Bergamin, Giovanna Maria Dimitri, Fabio Aiolli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22399">https://arxiv.org/abs/2509.22399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22399">https://arxiv.org/pdf/2509.22399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22399]] Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks(https://arxiv.org/abs/2509.22399)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is a fundamental task in medical image analysis, aiding medical decision-making by helping radiologists distinguish objects in an image. Research in this field has been driven by deep learning applications, which have the potential to scale these systems even in the presence of noise and artifacts. However, these systems are not yet perfected. We argue that performance can be improved by incorporating common medical knowledge into the segmentation model's loss function. To this end, we introduce Logic Tensor Networks (LTNs) to encode medical background knowledge using first-order logic (FOL) rules. The encoded rules span from constraints on the shape of the produced segmentation, to relationships between different segmented areas. We apply LTNs in an end-to-end framework with a SwinUNETR for semantic segmentation. We evaluate our method on the task of segmenting the hippocampus in brain MRI scans. Our experiments show that LTNs improve the baseline segmentation performance, especially when training data is scarce. Despite being in its preliminary stages, we argue that neurosymbolic methods are general enough to be adapted and applied to other medical semantic segmentation tasks.</li>
</ul>

<h3>Title: Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Zhong, Yimin Zhou, Zhiqi Zhang, Junhao Li, Yi Sun, Bin Chen, Shu-Tao Xia, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22400">https://arxiv.org/abs/2509.22400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22400">https://arxiv.org/pdf/2509.22400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22400]] Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models(https://arxiv.org/abs/2509.22400)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid progress of visual autoregressive (VAR) models has brought new opportunities for text-to-image generation, but also heightened safety concerns. Existing concept erasure techniques, primarily designed for diffusion models, fail to generalize to VARs due to their next-scale token prediction paradigm. In this paper, we first propose a novel VAR Erasure framework VARE that enables stable concept erasure in VAR models by leveraging auxiliary visual tokens to reduce fine-tuning intensity. Building upon this, we introduce S-VARE, a novel and effective concept erasure method designed for VAR, which incorporates a filtered cross entropy loss to precisely identify and minimally adjust unsafe visual tokens, along with a preservation loss to maintain semantic fidelity, addressing the issues such as language drift and reduced diversity introduce by nave fine-tuning. Extensive experiments demonstrate that our approach achieves surgical concept erasure while preserving generation quality, thereby closing the safety gap in autoregressive text-to-image generation by earlier methods.</li>
</ul>

<h3>Title: MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fanjin Meng, Yuan Yuan, Jingtao Ding, Jie Feng, Chonghua Han, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22403">https://arxiv.org/abs/2509.22403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22403">https://arxiv.org/pdf/2509.22403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22403]] MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning(https://arxiv.org/abs/2509.22403)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLM's reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at this https URL.</li>
</ul>

<h3>Title: RAU: Reference-based Anatomical Understanding with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Li, Yikang Liu, Jiaqi Guo, Lin Zhao, Zheyuan Zhang, Xiao Chen, Boris Mailhe, Ankush Mukherjee, Terrence Chen, Shanhui Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22404">https://arxiv.org/abs/2509.22404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22404">https://arxiv.org/pdf/2509.22404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22404]] RAU: Reference-based Anatomical Understanding with Vision Language Models(https://arxiv.org/abs/2509.22404)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Anatomical understanding through deep learning is critical for automatic report generation, intra-operative navigation, and organ localization in medical imaging; however, its progress is constrained by the scarcity of expert-labeled data. A promising remedy is to leverage an annotated reference image to guide the interpretation of an unlabeled target. Although recent vision-language models (VLMs) exhibit non-trivial visual reasoning, their reference-based understanding and fine-grained localization remain limited. We introduce RAU, a framework for reference-based anatomical understanding with VLMs. We first show that a VLM learns to identify anatomical regions through relative spatial reasoning between reference and target images, trained on a moderately sized dataset. We validate this capability through visual question answering (VQA) and bounding box prediction. Next, we demonstrate that the VLM-derived spatial cues can be seamlessly integrated with the fine-grained segmentation capability of SAM2, enabling localization and pixel-level segmentation of small anatomical regions, such as vessel segments. Across two in-distribution and two out-of-distribution datasets, RAU consistently outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding more accurate segmentations and more reliable localization. More importantly, its strong generalization ability makes it scalable to out-of-distribution datasets, a property crucial for medical image applications. To the best of our knowledge, RAU is the first to explore the capability of VLMs for reference-based identification, localization, and segmentation of anatomical structures in medical images. Its promising performance highlights the potential of VLM-driven approaches for anatomical understanding in automated clinical workflows.</li>
</ul>

<h3>Title: Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators</h3>
<ul>
<li><strong>Authors: </strong>Xiao Xue, Marco F.P. ten Eikelder, Mingyang Gao, Xiaoyuan Cheng, Yiming Yang, Yi He, Shuo Wang, Sibo Cheng, Yukun Hu, Peter V. Coveney</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CG, physics.comp-ph, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22411">https://arxiv.org/abs/2509.22411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22411">https://arxiv.org/pdf/2509.22411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22411]] Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators(https://arxiv.org/abs/2509.22411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a powerful framework for capturing complex flow behaviour by describing the evolution of single-particle distribution functions (PDFs). Despite its success, solving the LBE numerically remains computationally intensive due to strict time-step restrictions imposed by collision kernels. Here, we introduce a physics-informed neural operator framework for the LBE that enables prediction over large time horizons without step-by-step integration, effectively bypassing the need to explicitly solve the collision kernel. We incorporate intrinsic moment-matching constraints of the LBE, along with global equivariance of the full distribution field, enabling the model to capture the complex dynamics of the underlying kinetic system. Our framework is discretization-invariant, enabling models trained on coarse lattices to generalise to finer ones (kinetic super-resolution). In addition, it is agnostic to the specific form of the underlying collision model, which makes it naturally applicable across different kinetic datasets regardless of the governing dynamics. Our results demonstrate robustness across complex flow scenarios, including von Karman vortex shedding, ligament breakup, and bubble adhesion. This establishes a new data-driven pathway for modelling kinetic systems.</li>
</ul>

<h3>Title: LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Song Fei, Tian Ye, Lujia Wang, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22414">https://arxiv.org/abs/2509.22414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22414">https://arxiv.org/pdf/2509.22414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22414]] LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer(https://arxiv.org/abs/2509.22414)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.</li>
</ul>

<h3>Title: Explaining multimodal LLMs via intra-modal token interactions</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Liang, Ruoyu Chen, Xianghao Jiao, Siyuan Liang, Shiming Liu, Qunli Zhang, Zheng Hu, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22415">https://arxiv.org/abs/2509.22415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22415">https://arxiv.org/pdf/2509.22415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22415]] Explaining multimodal LLMs via intra-modal token interactions(https://arxiv.org/abs/2509.22415)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.</li>
</ul>

<h3>Title: Privacy Mechanism Design based on Empirical Distributions</h3>
<ul>
<li><strong>Authors: </strong>Leonhard Grosse, Sara Saeidian, Mikael Skoglund, Tobias J. Oechtering</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22428">https://arxiv.org/abs/2509.22428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22428">https://arxiv.org/pdf/2509.22428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22428]] Privacy Mechanism Design based on Empirical Distributions(https://arxiv.org/abs/2509.22428)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Pointwise maximal leakage (PML) is a per-outcome privacy measure based on threat models from quantitative information flow. Privacy guarantees with PML rely on knowledge about the distribution that generated the private data. In this work, we propose a framework for PML privacy assessment and mechanism design with empirical estimates of this data-generating distribution. By extending the PML framework to consider sets of data-generating distributions, we arrive at bounds on the worst-case leakage within a given set. We use these bounds alongside large-deviation bounds from the literature to provide a method for obtaining distribution-independent $(\varepsilon,\delta)$-PML guarantees when the data-generating distribution is estimated from available data samples. We provide an optimal binary mechanism, and show that mechanism design with this type of uncertainty about the data-generating distribution reduces to a linearly constrained convex program. Further, we show that optimal mechanisms designed for a distribution estimate can be used. Finally, we apply these tools to leakage assessment of the Laplace mechanism and the Gaussian mechanism for binary private data, and numerically show that the presented approach to mechanism design can yield significant utility increase compared to local differential privacy, while retaining similar privacy guarantees.</li>
</ul>

<h3>Title: Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ziheng Chi, Yifan Hou, Chenxi Pang, Shaobo Cui, Mubashara Akhtar, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22437">https://arxiv.org/abs/2509.22437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22437">https://arxiv.org/pdf/2509.22437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22437]] Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding(https://arxiv.org/abs/2509.22437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams. To address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning. We use Chimera to measure the presence of three types of shortcuts in visual question answering: (1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns; (2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly. These findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.</li>
</ul>

<h3>Title: U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bohan Huang, Qianyun Bao, Haoyuan Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22444">https://arxiv.org/abs/2509.22444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22444">https://arxiv.org/pdf/2509.22444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22444]] U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation(https://arxiv.org/abs/2509.22444)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation faces significant challenges in preserving fine-grained details and precise boundaries due to complex anatomical structures and pathological regions. These challenges primarily stem from two key limitations of conventional U-Net architectures: (1) their simple skip connections ignore the encoder-decoder semantic gap between various features, and (2) they lack the capability for multi-scale feature extraction in deep layers. To address these challenges, we propose the U-Net with Multi-scale Adaptive KAN (U-MAN), a novel architecture that enhances the emerging Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN). Our PAGF module replaces the simple skip connection, using attention to fuse features from the encoder and decoder. The MAN module enables the network to adaptively process features at multiple scales, improving its ability to segment objects of various sizes. Experiments on three public datasets (BUSI, GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods, particularly in defining accurate boundaries and preserving fine details.</li>
</ul>

<h3>Title: Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Peter Shaw, James Cohan, Jacob Eisenstein, Kristina Toutanova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22445">https://arxiv.org/abs/2509.22445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22445">https://arxiv.org/pdf/2509.22445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22445]] Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers(https://arxiv.org/abs/2509.22445)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Minimum Description Length (MDL) principle offers a formal framework for applying Occam's razor in machine learning. However, its application to neural networks such as Transformers is challenging due to the lack of a principled, universal measure for model complexity. This paper introduces the theoretical notion of asymptotically optimal description length objectives, grounded in the theory of Kolmogorov complexity. We establish that a minimizer of such an objective achieves optimal compression, for any dataset, up to an additive constant, in the limit as model resource bounds increase. We prove that asymptotically optimal objectives exist for Transformers, building on a new demonstration of their computational universality. We further show that such objectives can be tractable and differentiable by constructing and analyzing a variational objective based on an adaptive Gaussian mixture prior. Our empirical analysis shows that this variational objective selects for a low-complexity solution with strong generalization on an algorithmic task, but standard optimizers fail to find such solutions from a random initialization, highlighting key optimization challenges. More broadly, by providing a theoretical framework for identifying description length objectives with strong asymptotic guarantees, we outline a potential path towards training neural networks that achieve greater compression and generalization.</li>
</ul>

<h3>Title: Detecting (Un)answerability in Large Language Models with Linear Directions</h3>
<ul>
<li><strong>Authors: </strong>Maor Juliet Lavi, Tova Milo, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22449">https://arxiv.org/abs/2509.22449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22449">https://arxiv.org/pdf/2509.22449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22449]] Detecting (Un)answerability in Large Language Models with Linear Directions(https://arxiv.org/abs/2509.22449)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection, focusing on extractive question answering (QA) where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach for identifying a direction in the model's activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the model's abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four extractive QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Moreover, the obtained directions extend beyond extractive QA to unanswerability that stems from factors, such as lack of scientific consensus and subjectivity. Last, causal interventions show that adding or ablating the directions effectively controls the abstention behavior of the model.</li>
</ul>

<h3>Title: SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zixian Zhao, Xingchen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22450">https://arxiv.org/abs/2509.22450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22450">https://arxiv.org/pdf/2509.22450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22450]] SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion(https://arxiv.org/abs/2509.22450)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Visible and infrared image fusion (VIF) has gained significant attention in recent years due to its wide application in tasks such as scene segmentation and object detection. VIF methods can be broadly classified into traditional VIF methods and application-oriented VIF methods. Traditional methods focus solely on improving the quality of fused images, while application-oriented VIF methods additionally consider the performance of downstream tasks on fused images by introducing task-specific loss terms during training. However, compared to traditional methods, application-oriented VIF methods require datasets labeled for downstream tasks (e.g., semantic segmentation or object detection), making data acquisition labor-intensive and time-consuming. To address this issue, we propose a self-supervised training framework for segmentation-oriented VIF methods (SSVIF). Leveraging the consistency between feature-level fusion-based segmentation and pixel-level fusion-based segmentation, we introduce a novel self-supervised task-cross-segmentation consistency-that enables the fusion model to learn high-level semantic features without the supervision of segmentation labels. Additionally, we design a two-stage training strategy and a dynamic weight adjustment method for effective joint learning within our self-supervised framework. Extensive experiments on public datasets demonstrate the effectiveness of our proposed SSVIF. Remarkably, although trained only on unlabeled visible-infrared image pairs, our SSVIF outperforms traditional VIF methods and rivals supervised segmentation-oriented ones. Our code will be released upon acceptance.</li>
</ul>

<h3>Title: Overclocking Electrostatic Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Daniil Shlenskii, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22454">https://arxiv.org/abs/2509.22454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22454">https://arxiv.org/pdf/2509.22454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22454]] Overclocking Electrostatic Generative Models(https://arxiv.org/abs/2509.22454)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Electrostatic generative models such as PFGM++ have recently emerged as a powerful framework, achieving state-of-the-art performance in image synthesis. PFGM++ operates in an extended data space with auxiliary dimensionality $D$, recovering the diffusion model framework as $D\to\infty$, while yielding superior empirical results for finite $D$. Like diffusion models, PFGM++ relies on expensive ODE simulations to generate samples, making it computationally costly. To address this, we propose Inverse Poisson Flow Matching (IPFM), a novel distillation framework that accelerates electrostatic generative models across all values of $D$. Our IPFM reformulates distillation as an inverse problem: learning a generator whose induced electrostatic field matches that of the teacher. We derive a tractable training objective for this problem and show that, as $D \to \infty$, our IPFM closely recovers Score Identity Distillation (SiD), a recent method for distilling diffusion models. Empirically, our IPFM produces distilled generators that achieve near-teacher or even superior sample quality using only a few function evaluations. Moreover, we observe that distillation converges faster for finite $D$ than in the $D \to \infty$ (diffusion) limit, which is consistent with prior findings that finite-$D$ PFGM++ models exhibit more favorable optimization and sampling properties.</li>
</ul>

<h3>Title: Nonlinear Optimization with GPU-Accelerated Neural Network Constraints</h3>
<ul>
<li><strong>Authors: </strong>Robert Parker, Oscar Dowson, Nicole LoGiudice, Manuel Garcia, Russell Bent</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22462">https://arxiv.org/abs/2509.22462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22462">https://arxiv.org/pdf/2509.22462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22462]] Nonlinear Optimization with GPU-Accelerated Neural Network Constraints(https://arxiv.org/abs/2509.22462)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We propose a reduced-space formulation for optimizing over trained neural networks where the network's outputs and derivatives are evaluated on a GPU. To do this, we treat the neural network as a "gray box" where intermediate variables and constraints are not exposed to the optimization solver. Compared to the full-space formulation, in which intermediate variables and constraints are exposed to the optimization solver, the reduced-space formulation leads to faster solves and fewer iterations in an interior point method. We demonstrate the benefits of this method on two optimization problems: Adversarial generation for a classifier trained on MNIST images and security-constrained optimal power flow with transient feasibility enforced using a neural network surrogate.</li>
</ul>

<h3>Title: IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Bei Li, Jiahao Liu, Junhao Ruan, Kechen Jiao, Hongyin Tang, Jingang Wang, Xiao Tong, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22463">https://arxiv.org/abs/2509.22463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22463">https://arxiv.org/pdf/2509.22463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22463]] IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method(https://arxiv.org/abs/2509.22463)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>High-order numerical methods enhance Transformer performance in tasks like NLP and CV, but introduce a performance-efficiency trade-off due to increased computational overhead. Our analysis reveals that conventional efficiency techniques, such as distillation, can be detrimental to the performance of these models, exemplified by PCformer. To explore more optimizable ODE-based Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit \textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies high-order methods using an iterative implicit Euler approach. This simplification not only leads to superior performance but also facilitates model compression compared to PCformer. To enhance inference efficiency, we introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware \textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD allows users to effectively balance the performance-efficiency trade-off. On lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET, significantly cuts inference overhead by 55\% while retaining 99.4\% of the original task accuracy. Moreover, the most efficient IIET variant achieves an average performance gain exceeding 1.6\% over vanilla Transformer with comparable speed.</li>
</ul>

<h3>Title: Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Boshra Ariguib, Mathias Niepert, Andrei Manolache</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22468">https://arxiv.org/abs/2509.22468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22468">https://arxiv.org/pdf/2509.22468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22468]] Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining(https://arxiv.org/abs/2509.22468)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>High-quality molecular representations are essential for property prediction and molecular design, yet large labeled datasets remain scarce. While self-supervised pretraining on molecular graphs has shown promise, many existing approaches either depend on hand-crafted augmentations or complex generative objectives, and often rely solely on 2D topology, leaving valuable 3D structural information underutilized. To address this gap, we introduce C-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework that integrates 2D graphs with ensembles of 3D conformers. C-FREE learns molecular representations by predicting subgraph embeddings from their complementary neighborhoods in the latent space, using fixed-radius ego-nets as modeling units across different conformers. This design allows us to integrate both geometric and topological information within a hybrid Graph Neural Network (GNN)-Transformer backbone, without negatives, positional encodings, or expensive pre-processing. Pretraining on the GEOM dataset, which provides rich 3D conformational diversity, C-FREE achieves state-of-the-art results on MoleculeNet, surpassing contrastive, generative, and other multimodal self-supervised methods. Fine-tuning across datasets with diverse sizes and molecule types further demonstrates that pretraining transfers effectively to new chemical domains, highlighting the importance of 3D-informed molecular representations.</li>
</ul>

<h3>Title: Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve Grel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22472">https://arxiv.org/abs/2509.22472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22472">https://arxiv.org/pdf/2509.22472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22472]] Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning(https://arxiv.org/abs/2509.22472)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.</li>
</ul>

<h3>Title: Bzier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chen Li, Meilong Xu, Xiaoling Hu, Weimin Lyu, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22476">https://arxiv.org/abs/2509.22476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22476">https://arxiv.org/pdf/2509.22476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22476]] Bzier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation(https://arxiv.org/abs/2509.22476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, Bzier Meets Diffusion, for cross-domain image generation. First, we introduce a Bzier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance.</li>
</ul>

<h3>Title: Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Hang Li, Kaiqi Yang, Yucheng Chu, Hui Liu, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22480">https://arxiv.org/abs/2509.22480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22480">https://arxiv.org/pdf/2509.22480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22480]] Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving(https://arxiv.org/abs/2509.22480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.</li>
</ul>

<h3>Title: OFMU: Optimization-Driven Framework for Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Sadia Asif, Mohammad Mohammadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22483">https://arxiv.org/abs/2509.22483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22483">https://arxiv.org/pdf/2509.22483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22483]] OFMU: Optimization-Driven Framework for Machine Unlearning(https://arxiv.org/abs/2509.22483)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models deployed in sensitive applications increasingly require the ability to unlearn specific knowledge, such as user requests, copyrighted materials, or outdated information, without retraining from scratch to ensure regulatory compliance, user privacy, and safety. This task, known as machine unlearning, aims to remove the influence of targeted data (forgetting) while maintaining performance on the remaining data (retention). A common approach is to formulate this as a multi-objective problem and reduce it to a single-objective problem via scalarization, where forgetting and retention losses are combined using a weighted sum. However, this often results in unstable training dynamics and degraded model utility due to conflicting gradient directions. To address these challenges, we propose OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting while preserving retention through a hierarchical structure. Our method enforces forgetting via an inner maximization step that incorporates a similarity-aware penalty to decorrelate the gradients of the forget and retention objectives, and restores utility through an outer minimization step. To ensure scalability, we develop a two-loop algorithm with provable convergence guarantees under both convex and non-convex regimes. We further provide a rigorous theoretical analysis of convergence rates and show that our approach achieves better trade-offs between forgetting efficacy and model utility compared to prior methods. Extensive experiments across vision and language benchmarks demonstrate that OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility.</li>
</ul>

<h3>Title: A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches</h3>
<ul>
<li><strong>Authors: </strong>Samuele Punzo, Silvia Giulia Galfr, Francesco Massafra, Alessandro Maglione, Corrado Priami, Alina Srbu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22484">https://arxiv.org/abs/2509.22484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22484">https://arxiv.org/pdf/2509.22484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22484]] A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches(https://arxiv.org/abs/2509.22484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a machine learning pipeline for biomarker discovery in Multiple Sclerosis (MS), integrating eight publicly available microarray datasets from Peripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we trained an XGBoost classifier optimized via Bayesian search. SHapley Additive exPlanations (SHAP) were used to identify key features for model prediction, indicating thus possible biomarkers. These were compared with genes identified through classical Differential Expression Analysis (DEA). Our comparison revealed both overlapping and unique biomarkers between SHAP and DEA, suggesting complementary strengths. Enrichment analysis confirmed the biological relevance of SHAP-selected genes, linking them to pathways such as sphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr virus infection all known to be associated with MS. This study highlights the value of combining explainable AI (xAI) with traditional statistical methods to gain deeper insights into disease mechanism.</li>
</ul>

<h3>Title: Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22496">https://arxiv.org/abs/2509.22496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22496">https://arxiv.org/pdf/2509.22496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22496]] Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation(https://arxiv.org/abs/2509.22496)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at this https URL.</li>
</ul>

<h3>Title: Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise</h3>
<ul>
<li><strong>Authors: </strong>Juan Ramirez, Simon Lacoste-Julien</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22500">https://arxiv.org/abs/2509.22500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22500">https://arxiv.org/pdf/2509.22500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22500]] Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise(https://arxiv.org/abs/2509.22500)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Constrained optimization is a powerful framework for enforcing requirements on neural networks. These constrained deep learning problems are typically solved using first-order methods on their min-max Lagrangian formulation, but such approaches often suffer from oscillations and can fail to find all local solutions. While the Augmented Lagrangian method (ALM) addresses these issues, practitioners often favor dual optimistic ascent schemes (PI control) on the standard Lagrangian, which perform well empirically but lack formal guarantees. In this paper, we establish a previously unknown equivalence between these approaches: dual optimistic ascent on the Lagrangian is equivalent to gradient descent-ascent on the Augmented Lagrangian. This finding allows us to transfer the robust theoretical guarantees of the ALM to the dual optimistic setting, proving it converges linearly to all local solutions. Furthermore, the equivalence provides principled guidance for tuning the optimism hyper-parameter. Our work closes a critical gap between the empirical success of dual optimistic methods and their theoretical foundation.</li>
</ul>

<h3>Title: Representing LLMs in Prompt Semantic Task Space</h3>
<ul>
<li><strong>Authors: </strong>Idan Kashani, Avi Mendelson, Yaniv Nemcovsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22506">https://arxiv.org/abs/2509.22506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22506">https://arxiv.org/pdf/2509.22506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22506]] Representing LLMs in Prompt Semantic Task Space(https://arxiv.org/abs/2509.22506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive results over various tasks, and ever-expanding public repositories contain an abundance of pre-trained models. Therefore, identifying the best-performing LLM for a given task is a significant challenge. Previous works have suggested learning LLM representations to address this. However, these approaches present limited scalability and require costly retraining to encompass additional models and datasets. Moreover, the produced representation utilizes distinct spaces that cannot be easily interpreted. This work presents an efficient, training-free approach to representing LLMs as linear operators within the prompts' semantic task space, thus providing a highly interpretable representation of the models' application. Our method utilizes closed-form computation of geometrical properties and ensures exceptional scalability and real-time adaptability to dynamically expanding repositories. We demonstrate our approach on success prediction and model selection tasks, achieving competitive or state-of-the-art results with notable performance in out-of-sample scenarios.</li>
</ul>

<h3>Title: Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Zahid Iqbal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22507">https://arxiv.org/abs/2509.22507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22507">https://arxiv.org/pdf/2509.22507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22507]] Adaptive Dual-Mode Distillation with Incentive Schemes for Scalable, Heterogeneous Federated Learning on Non-IID Data(https://arxiv.org/abs/2509.22507)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising decentralized learning (DL) approach that enables the use of distributed data without compromising user privacy. However, FL poses several key challenges. First, it is frequently assumed that every client can train the same machine learning models, however, not all clients are able to meet this assumption because of differences in their business needs and computational resources. Second, statistical heterogeneity (a.k.a. non-IID data) poses a major challenge in FL, which can lead to lower global model performance. Third, while addressing these challenges, there is a need for a cost-effective incentive mechanism to encourage clients to participate in FL training. In response to these challenges, we propose several methodologies: DL-SH, which facilitates efficient, privacy-preserving, and communication-efficient learning in the context of statistical heterogeneity; DL-MH, designed to manage fully heterogeneous models while tackling statistical disparities; and I-DL-MH, an incentive-based extension of DL-MH that promotes client engagement in federated learning training by providing incentives within this complex federated learning framework. Comprehensive experiments were carried out to assess the performance and scalability of the proposed approaches across a range of complex experimental settings. This involved utilizing various model architectures, in diverse data distributions, including IID and several non-IID scenarios, as well as multiple datasets. Experimental results demonstrate that the proposed approaches significantly enhance accuracy and decrease communication costs while effectively addressing statistical heterogeneity and model heterogeneity in comparison to existing state-of-the-art approaches and baselines, with DL-SH improving global model accuracy by 153%, and I-DL-MH achieving a 225% improvement under non-IID conditions.</li>
</ul>

<h3>Title: We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong</h3>
<ul>
<li><strong>Authors: </strong>Gautam Siddharth Kashyap, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22510">https://arxiv.org/abs/2509.22510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22510">https://arxiv.org/pdf/2509.22510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22510]] We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong(https://arxiv.org/abs/2509.22510)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Alignment of Large Language Models (LLMs) along multiple objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe and reliable deployment. Prior work has used steering vector-small control signals injected into hidden states-to guide LLM outputs, typically via one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single alignment objective can inadvertently overwrite representations learned for other objectives, leading to catastrophic forgetting. More recent approaches extend steering vectors via one-to-many (1-to-N) Transformer decoders. While this alleviates catastrophic forgetting, naive multi-branch designs optimize each objective independently, which can cause inference fragmentation-outputs across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In Stage I, post-attention hidden states of the Transformer layer are computed once to form a shared representation. In Stage II, this representation is cloned into parallel branches and steered via a policy-reference mechanism, enabling objective-specific control while maintaining cross-objective consistency. Empirical evaluations on Alpaca, BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared to a naive 1-to-N baseline, while remaining competitive with state-of-the-art methods.</li>
</ul>

<h3>Title: JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation</h3>
<ul>
<li><strong>Authors: </strong>Guillem Capellera, Luis Ferraz, Antonio Rubio, Alexandre Alahi, Antonio Agudo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22522">https://arxiv.org/abs/2509.22522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22522">https://arxiv.org/pdf/2509.22522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22522]] JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation(https://arxiv.org/abs/2509.22522)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems.</li>
</ul>

<h3>Title: EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model</h3>
<ul>
<li><strong>Authors: </strong>Andrii Litvynchuk, Ivan Livinsky, Anand Ravi, Nima Kalantari, Andrii Tsarov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22527">https://arxiv.org/abs/2509.22527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22527">https://arxiv.org/pdf/2509.22527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22527]] EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model(https://arxiv.org/abs/2509.22527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation (MDE) plays a pivotal role in various computer vision applications, such as robotics, augmented reality, and autonomous driving. Despite recent advancements, existing methods often fail to meet key requirements for 3D reconstruction and view synthesis, including geometric consistency, fine details, robustness to real-world challenges like reflective surfaces, and efficiency for edge devices. To address these challenges, we introduce a novel MDE system, called EfficientDepth, which combines a transformer architecture with a lightweight convolutional decoder, as well as a bimodal density head that allows the network to estimate detailed depth maps. We train our model on a combination of labeled synthetic and real images, as well as pseudo-labeled real images, generated using a high-performing MDE method. Furthermore, we employ a multi-stage optimization strategy to improve training efficiency and produce models that emphasize geometric consistency and fine detail. Finally, in addition to commonly used objectives, we introduce a loss function based on LPIPS to encourage the network to produce detailed depth maps. Experimental results demonstrate that EfficientDepth achieves performance comparable to or better than existing state-of-the-art models, with significantly reduced computational resources.</li>
</ul>

<h3>Title: InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Yuan Xie, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22536">https://arxiv.org/abs/2509.22536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22536">https://arxiv.org/pdf/2509.22536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22536]] InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models(https://arxiv.org/abs/2509.22536)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.</li>
</ul>

<h3>Title: Category Discovery: An Open-World Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhenqi He, Yuanpei Liu, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22542">https://arxiv.org/abs/2509.22542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22542">https://arxiv.org/pdf/2509.22542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22542]] Category Discovery: An Open-World Perspective(https://arxiv.org/abs/2509.22542)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Category discovery (CD) is an emerging open-world learning task, which aims at automatically categorizing unlabelled data containing instances from unseen classes, given some labelled data from seen classes. This task has attracted significant attention over the years and leads to a rich body of literature trying to address the problem from different perspectives. In this survey, we provide a comprehensive review of the literature, and offer detailed analysis and in-depth discussion on different methods. Firstly, we introduce a taxonomy for the literature by considering two base settings, namely novel category discovery (NCD) and generalized category discovery (GCD), and several derived settings that are designed to address the extra challenges in different real-world application scenarios, including continual category discovery, skewed data distribution, federated category discovery, etc. Secondly, for each setting, we offer a detailed analysis of the methods encompassing three fundamental components, representation learning, label assignment, and estimation of class number. Thirdly, we benchmark all the methods and distill key insights showing that large-scale pretrained backbones, hierarchical and auxiliary cues, and curriculum-style training are all beneficial for category discovery, while challenges remain in the design of label assignment, the estimation of class numbers, and scaling to complex multi-object this http URL, we discuss the key insights from the literature so far and point out promising future research directions. We compile a living survey of the category discovery literature at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mahdi Hemmatyar, Mahdi Jafari, Mohammad Amin Yousefi, Mohammad Reza Nemati, Mobin Azadani, Hamid Reza Rastad, Amirmohammad Akbari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22544">https://arxiv.org/abs/2509.22544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22544">https://arxiv.org/pdf/2509.22544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22544]] HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection(https://arxiv.org/abs/2509.22544)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) is crucial for intelligent surveillance, but a significant challenge lies in identifying complex anomalies, which are events defined by intricate relationships and temporal dependencies among multiple entities rather than by isolated actions. While self-supervised learning (SSL) methods effectively model low-level spatiotemporal patterns, they often struggle to grasp the semantic meaning of these interactions. Conversely, large language models (LLMs) offer powerful contextual reasoning but are computationally expensive for frame-by-frame analysis and lack fine-grained spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal analyzer with LLM validator. The SSL module is built upon an nnFormer backbone which is a transformer-based model for image segmentation. It is trained with multiple proxy tasks, learns from video frames to identify those suspected of anomaly. The selected frames are then forwarded to the LLM, which enriches the analysis with semantic context by applying structured, rule-based reasoning to validate the presence of anomalies. Experiments on the challenging ComplexVAD dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming existing baselines by 12.5% while reducing LLM computation. We release our interaction anomaly taxonomy, adaptive thresholding protocol, and code to facilitate future research in complex VAD scenarios.</li>
</ul>

<h3>Title: JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22548">https://arxiv.org/abs/2509.22548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22548">https://arxiv.org/pdf/2509.22548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22548]] JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation(https://arxiv.org/abs/2509.22548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: this https URL.</li>
</ul>

<h3>Title: Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois Grolleau, Emily Alsentzer, Jonathan H. Chen, Stephen P. Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22565">https://arxiv.org/abs/2509.22565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22565">https://arxiv.org/pdf/2509.22565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22565]] Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation(https://arxiv.org/abs/2509.22565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Asynchronous patient-clinician messaging via EHR portals is a growing source of clinician workload, prompting interest in large language models (LLMs) to assist with draft responses. However, LLM outputs may contain clinical inaccuracies, omissions, or tone mismatches, making robust evaluation essential. Our contributions are threefold: (1) we introduce a clinically grounded error ontology comprising 5 domains and 59 granular error codes, developed through inductive coding and expert adjudication; (2) we develop a retrieval-augmented evaluation pipeline (RAEC) that leverages semantically similar historical message-response pairs to improve judgment quality; and (3) we provide a two-stage prompting architecture using DSPy to enable scalable, interpretable, and hierarchical error detection. Our approach assesses the quality of drafts both in isolation and with reference to similar past message-response pairs retrieved from institutional archives. Using a two-stage DSPy pipeline, we compared baseline and reference-enhanced evaluations on over 1,500 patient messages. Retrieval context improved error identification in domains such as clinical completeness and workflow appropriateness. Human validation on 100 messages demonstrated superior agreement (concordance = 50% vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs. baseline, supporting the use of our RAEC pipeline as AI guardrails for patient messaging.</li>
</ul>

<h3>Title: From Parameters to Behavior: Unsupervised Compression of the Policy Space</h3>
<ul>
<li><strong>Authors: </strong>Davide Tenedini, Riccardo Zamboni, Mirco Mutti, Marcello Restelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22566">https://arxiv.org/abs/2509.22566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22566">https://arxiv.org/pdf/2509.22566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22566]] From Parameters to Behavior: Unsupervised Compression of the Policy Space(https://arxiv.org/abs/2509.22566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $\Theta$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $\Theta$ into a low-dimensional latent space $\mathcal{Z}$. We train a generative model $g:\mathcal{Z}\to\Theta$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment's complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\mathcal{Z}$.</li>
</ul>

<h3>Title: The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?</h3>
<ul>
<li><strong>Authors: </strong>Guannan Lai, Da-Wei Zhou, Xin Yang, Han-Jia Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22580">https://arxiv.org/abs/2509.22580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22580">https://arxiv.org/pdf/2509.22580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22580]] The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?(https://arxiv.org/abs/2509.22580)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Class Incremental Learning (CIL) requires models to continuously learn new classes without forgetting previously learned ones, while maintaining stable performance across all possible class sequences. In real-world settings, the order in which classes arrive is diverse and unpredictable, and model performance can vary substantially across different sequences. Yet mainstream evaluation protocols calculate mean and variance from only a small set of randomly sampled sequences. Our theoretical analysis and empirical results demonstrate that this sampling strategy fails to capture the full performance range, resulting in biased mean estimates and a severe underestimation of the true variance in the performance distribution. We therefore contend that a robust CIL evaluation protocol should accurately characterize and estimate the entire performance distribution. To this end, we introduce the concept of extreme sequences and provide theoretical justification for their crucial role in the reliable evaluation of CIL. Moreover, we observe a consistent positive correlation between inter-task similarity and model performance, a relation that can be leveraged to guide the search for extreme sequences. Building on these insights, we propose EDGE (Extreme case-based Distribution and Generalization Evaluation), an evaluation protocol that adaptively identifies and samples extreme class sequences using inter-task similarity, offering a closer approximation of the ground-truth performance distribution. Extensive experiments demonstrate that EDGE effectively captures performance extremes and yields more accurate estimates of distributional boundaries, providing actionable insights for model selection and robustness checking. Our code is available at this https URL.</li>
</ul>

<h3>Title: ArabJobs: A Multinational Corpus of Arabic Job Ads</h3>
<ul>
<li><strong>Authors: </strong>Mo El-Haj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22589">https://arxiv.org/abs/2509.22589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22589">https://arxiv.org/pdf/2509.22589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22589]] ArabJobs: A Multinational Corpus of Arabic Job Ads(https://arxiv.org/abs/2509.22589)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: this https URL.</li>
</ul>

<h3>Title: Transport Based Mean Flows for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22592">https://arxiv.org/abs/2509.22592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22592">https://arxiv.org/pdf/2509.22592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22592]] Transport Based Mean Flows for Generative Modeling(https://arxiv.org/abs/2509.22592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow-matching generative models have emerged as a powerful paradigm for continuous data generation, achieving state-of-the-art results across domains such as images, 3D shapes, and point clouds. Despite their success, these models suffer from slow inference due to the requirement of numerous sequential sampling steps. Recent work has sought to accelerate inference by reducing the number of sampling steps. In particular, Mean Flows offer a one-step generation approach that delivers substantial speedups while retaining strong generative performance. Yet, in many continuous domains, Mean Flows fail to faithfully approximate the behavior of the original multi-step flow-matching process. In this work, we address this limitation by incorporating optimal transport-based sampling strategies into the Mean Flow framework, enabling one-step generators that better preserve the fidelity and diversity of the original multi-step flow process. Experiments on controlled low-dimensional settings and on high-dimensional tasks such as image generation, image-to-image translation, and point cloud generation demonstrate that our approach achieves superior inference accuracy in one-step generative modeling.</li>
</ul>

<h3>Title: Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yasmine Omri, Connor Ding, Tsachy Weissman, Thierry Tambe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22615">https://arxiv.org/abs/2509.22615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22615">https://arxiv.org/pdf/2509.22615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22615]] Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting(https://arxiv.org/abs/2509.22615)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.</li>
</ul>

<h3>Title: LongLive: Real-time Interactive Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22622">https://arxiv.org/abs/2509.22622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22622">https://arxiv.org/pdf/2509.22622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22622]] LongLive: Real-time Interactive Long Video Generation(https://arxiv.org/abs/2509.22622)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.</li>
</ul>

<h3>Title: A Theoretical Analysis of Discrete Flow Matching Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Maojiang Su, Mingcheng Lu, Jerry Yao-Chieh Hu, Shang Wu, Zhao Song, Alex Reneau, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22623">https://arxiv.org/abs/2509.22623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22623">https://arxiv.org/pdf/2509.22623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22623]] A Theoretical Analysis of Discrete Flow Matching Generative Models(https://arxiv.org/abs/2509.22623)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We provide a theoretical analysis for end-to-end training Discrete Flow Matching (DFM) generative models. DFM is a promising discrete generative modeling framework that learns the underlying generative dynamics by training a neural network to approximate the transformative velocity field. Our analysis establishes a clear chain of guarantees by decomposing the final distribution estimation error. We first prove that the total variation distance between the generated and target distributions is controlled by the risk of the learned velocity field. We then bound this risk by analyzing its two primary sources: (i) Approximation Error, where we quantify the capacity of the Transformer architecture to represent the true velocity, and (ii) Estimation Error, where we derive statistical convergence rates that bound the error from training on a finite dataset. By composing these results, we provide the first formal proof that the distribution generated by a trained DFM model provably converges to the true data distribution as the training set size increases.</li>
</ul>

<h3>Title: SPARK: Synergistic Policy And Reward Co-Evolving Framework</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22624">https://arxiv.org/abs/2509.22624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22624">https://arxiv.org/pdf/2509.22624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22624]] SPARK: Synergistic Policy And Reward Co-Evolving Framework(https://arxiv.org/abs/2509.22624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.</li>
</ul>

<h3>Title: CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Lopes, Roberto Souza, Helio Pedrini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22627">https://arxiv.org/abs/2509.22627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22627">https://arxiv.org/pdf/2509.22627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22627]] CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach(https://arxiv.org/abs/2509.22627)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Depth Estimation plays a crucial role in recent applications in robotics, autonomous vehicles, and augmented reality. These scenarios commonly operate under constraints imposed by computational power. Stereo image pairs offer an effective solution for depth estimation since it only needs to estimate the disparity of pixels in image pairs to determine the depth in a known rectified system. Due to the difficulty in acquiring reliable ground-truth depth data across diverse scenarios, self-supervised techniques emerge as a solution, particularly when large unlabeled datasets are available. We propose a novel self-supervised convolutional approach that outperforms existing state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) while balancing computational cost. The proposed CCNeXt architecture employs a modern CNN feature extractor with a novel windowed epipolar cross-attention module in the encoder, complemented by a comprehensive redesign of the depth estimation decoder. Our experiments demonstrate that CCNeXt achieves competitive metrics on the KITTI Eigen Split test data while being 10.18$\times$ faster than the current best model and achieves state-of-the-art results in all metrics in the KITTI Eigen Split Improved Ground Truth and Driving Stereo datasets when compared to recently proposed techniques. To ensure complete reproducibility, our project is accessible at \href{this https URL}{\texttt{this https URL}}.</li>
</ul>

<h3>Title: UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Chen, Guangrun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22628">https://arxiv.org/abs/2509.22628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22628">https://arxiv.org/pdf/2509.22628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22628]] UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning(https://arxiv.org/abs/2509.22628)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism.</li>
</ul>

<h3>Title: StateX: Enhancing RNN Recall via Post-training State Expansion</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22630">https://arxiv.org/abs/2509.22630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22630">https://arxiv.org/pdf/2509.22630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22630]] StateX: Enhancing RNN Recall via Post-training State Expansion(https://arxiv.org/abs/2509.22630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities.</li>
</ul>

<h3>Title: LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22631">https://arxiv.org/abs/2509.22631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22631">https://arxiv.org/pdf/2509.22631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22631]] LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision(https://arxiv.org/abs/2509.22631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Curating high-quality, domain-specific datasets is a major bottleneck for deploying robust vision systems, requiring complex trade-offs between data quality, diversity, and cost when researching vast, unlabeled data lakes. We introduce Labeling Copilot, the first data curation deep research agent for computer vision. A central orchestrator agent, powered by a large multimodal language model, uses multi-step reasoning to execute specialized tools across three core capabilities: (1) Calibrated Discovery sources relevant, in-distribution data from large repositories; (2) Controllable Synthesis generates novel data for rare scenarios with robust filtering; and (3) Consensus Annotation produces accurate labels by orchestrating multiple foundation models via a novel consensus mechanism incorporating non-maximum suppression and voting. Our large-scale validation proves the effectiveness of Labeling Copilot's components. The Consensus Annotation module excels at object discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per image-nearly double the 7.4 ground-truth objects-achieving a final annotation mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class imbalance to discover 903 new bounding box categories, expanding its capability to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a 10-million sample scale, features an active learning strategy that is up to 40x more computationally efficient than alternatives with equivalent sample efficiency. These experiments validate that an agentic workflow with optimized, scalable tools provides a robust foundation for curating industrial-scale datasets.</li>
</ul>

<h3>Title: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance</h3>
<ul>
<li><strong>Authors: </strong>Luc Boudier, Loris Manganelli, Eleftherios Tsonis, Nicolas Dufour, Vicky Kalogeiton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22635">https://arxiv.org/abs/2509.22635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22635">https://arxiv.org/pdf/2509.22635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22635]] Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance(https://arxiv.org/abs/2509.22635)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.</li>
</ul>

<h3>Title: Scale-Wise VAR is Secretly Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Amandeep Kumar, Nithin Gopalakrishnan Nair, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22636">https://arxiv.org/abs/2509.22636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22636">https://arxiv.org/pdf/2509.22636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22636]] Scale-Wise VAR is Secretly Discrete Diffusion(https://arxiv.org/abs/2509.22636)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) transformers have emerged as a powerful paradigm for visual generation, largely due to their scalability, computational efficiency and unified architecture with language and vision. Among them, next scale prediction Visual Autoregressive Generation (VAR) has recently demonstrated remarkable performance, even surpassing diffusion-based models. In this work, we revisit VAR and uncover a theoretical insight: when equipped with a Markovian attention mask, VAR is mathematically equivalent to a discrete diffusion. We term this reinterpretation as Scalable Visual Refinement with Discrete Diffusion (SRDD), establishing a principled bridge between AR transformers and diffusion models. Leveraging this new perspective, we show how one can directly import the advantages of diffusion such as iterative refinement and reduce architectural inefficiencies into VAR, yielding faster convergence, lower inference cost, and improved zero-shot reconstruction. Across multiple datasets, we show that the diffusion based perspective of VAR leads to consistent gains in efficiency and generation.</li>
</ul>

<h3>Title: WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22644">https://arxiv.org/abs/2509.22644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22644">https://arxiv.org/pdf/2509.22644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22644]] WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning(https://arxiv.org/abs/2509.22644)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.</li>
</ul>

<h3>Title: RefAM: Attention Magnets for Zero-Shot Referral Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22650">https://arxiv.org/abs/2509.22650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22650">https://arxiv.org/pdf/2509.22650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22650]] RefAM: Attention Magnets for Zero-Shot Referral Segmentation(https://arxiv.org/abs/2509.22650)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components.</li>
</ul>

<h3>Title: VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing</h3>
<ul>
<li><strong>Authors: </strong>Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.22651">https://arxiv.org/abs/2509.22651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.22651">https://arxiv.org/pdf/2509.22651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.22651]] VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing(https://arxiv.org/abs/2509.22651)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at this https URL .</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
