<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-08</h1>
<h3>Title: Collaborative and Proactive Management of Task-Oriented Conversations</h3>
<ul>
<li><strong>Authors: </strong>Arezoo Saedi, Afsaneh Fatemi, Mohammad Ali Nematbakhsh, Sophie Rosset, Anne Vilnat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05110">https://arxiv.org/abs/2510.05110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05110">https://arxiv.org/pdf/2510.05110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05110]] Collaborative and Proactive Management of Task-Oriented Conversations(https://arxiv.org/abs/2510.05110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Task oriented dialogue systems (TOD) complete particular tasks based on user preferences across natural language interactions. Considering the impressive performance of large language models (LLMs) in natural language processing (NLP) tasks, most of the latest TODs are centered on LLMs. While proactive planning is crucial for task completion, many existing TODs overlook effective goal-aware planning. This paper creates a model for managing task-oriented conversations, conceptualized centered on the information state approach to dialogue management. The created model incorporated constructive intermediate information in planning. Initially, predefined slots and text part informational components are created to model user preferences. Investigating intermediate information, critical circumstances are identified. Informational components corresponding to these circumstances are created. Possible configurations for these informational components lead to limited information states. Then, dialogue moves, which indicate movement between these information states and the procedures that must be performed in the movements, are created. Eventually, the update strategy is constructed. The created model is implemented leveraging in-context learning of LLMs. In this model,  database queries are created centered on indicated predefined slots and the order of retrieved entities is indicated centered on text part. This mechanism enables passing the whole corresponding entities to the preferences in the order of congruency. Evaluations exploiting the complete test conversations of MultiWOZ, with no more than a domain in a conversation, illustrate maximal inform and success, and improvement compared with previous methods.</li>
</ul>

<h3>Title: Hallucination is Inevitable for LLMs with the Open World Assumption</h3>
<ul>
<li><strong>Authors: </strong>Bowen Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05116">https://arxiv.org/abs/2510.05116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05116">https://arxiv.org/pdf/2510.05116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05116]] Hallucination is Inevitable for LLMs with the Open World Assumption(https://arxiv.org/abs/2510.05116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive linguistic competence but also produce inaccurate or fabricated outputs, often called ``hallucinations''. Engineering approaches usually regard hallucination as a defect to be minimized, while formal analyses have argued for its theoretical inevitability. Yet both perspectives remain incomplete when considering the conditions required for artificial general intelligence (AGI). This paper reframes ``hallucination'' as a manifestation of the generalization problem. Under the Closed World assumption, where training and test distributions are consistent, hallucinations may be mitigated. Under the Open World assumption, however, where the environment is unbounded, hallucinations become inevitable. This paper further develops a classification of hallucination, distinguishing cases that may be corrected from those that appear unavoidable under open-world conditions. On this basis, it suggests that ``hallucination'' should be approached not merely as an engineering defect but as a structural feature to be tolerated and made compatible with human intelligence.</li>
</ul>

<h3>Title: A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics</h3>
<ul>
<li><strong>Authors: </strong>Farjana Yesmin, Nusrat Shirmin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05120">https://arxiv.org/abs/2510.05120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05120">https://arxiv.org/pdf/2510.05120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05120]] A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics(https://arxiv.org/abs/2510.05120)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The growing complexity of machine learning (ML) models in big data analytics, especially in domains such as environmental monitoring, highlights the critical need for interpretability and explainability to promote trust, ethical considerations, and regulatory adherence (e.g., GDPR). Traditional "black-box" models obstruct transparency, whereas post-hoc explainable AI (XAI) techniques like LIME and SHAP frequently compromise accuracy or fail to deliver inherent insights. This paper presents a novel framework that combines type-2 fuzzy sets, granular computing, and clustering to boost explainability and fairness in big data environments. When applied to the UCI Air Quality dataset, the framework effectively manages uncertainty in noisy sensor data, produces linguistic rules, and assesses fairness using silhouette scores and entropy. Key contributions encompass: (1) A type-2 fuzzy clustering approach that enhances cohesion by about 4% compared to type-1 methods (silhouette 0.365 vs. 0.349) and improves fairness (entropy 0.918); (2) Incorporation of fairness measures to mitigate biases in unsupervised scenarios; (3) A rule-based component for intrinsic XAI, achieving an average coverage of 0.65; (4) Scalable assessments showing linear runtime (roughly 0.005 seconds for sampled big data sizes). Experimental outcomes reveal superior performance relative to baselines such as DBSCAN and Agglomerative Clustering in terms of interpretability, fairness, and efficiency. Notably, the proposed method achieves a 4% improvement in silhouette score over type-1 fuzzy clustering and outperforms baselines in fairness (entropy reduction by up to 1%) and efficiency.</li>
</ul>

<h3>Title: Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Durgesh Nandini, Rebekka Koch, Mirco Schoenfeld</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05121">https://arxiv.org/abs/2510.05121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05121">https://arxiv.org/pdf/2510.05121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05121]] Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models(https://arxiv.org/abs/2510.05121)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.</li>
</ul>

<h3>Title: CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhu, Yuanchen Zhou, Shuo Jiang, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05122">https://arxiv.org/abs/2510.05122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05122">https://arxiv.org/pdf/2510.05122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05122]] CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation(https://arxiv.org/abs/2510.05122)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.</li>
</ul>

<h3>Title: Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Reza Shirkavand, Xiaokai Wei, Chen Wang, Zheng Hui, Heng Huang, Michelle Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05125">https://arxiv.org/abs/2510.05125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05125">https://arxiv.org/pdf/2510.05125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05125]] Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation(https://arxiv.org/abs/2510.05125)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While collaborative filtering delivers predictive accuracy and efficiency, and Large Language Models (LLMs) enable expressive and generalizable reasoning, modern recommendation systems must bring these strengths together. Growing user expectations, such as natural-language queries and transparent explanations, further highlight the need for a unified approach. However, doing so is nontrivial. Collaborative signals are often token-efficient but semantically opaque, while LLMs are semantically rich but struggle to model implicit user preferences when trained only on textual inputs. This paper introduces Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item interaction histories as a native dialect within the language space, enabling collaborative signals to be understood in the same way as natural language. By splitting the Feed Forward Network of each block of a pretrained LLM into a separate text expert and an item expert with token-type gating, our method avoids destructive interference between text and catalog modalities. IDIOMoE demonstrates strong recommendation performance across both public and proprietary datasets, while preserving the text understanding of the pretrained model.</li>
</ul>

<h3>Title: Improving Metacognition and Uncertainty Communication in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mark Steyvers, Catarina Belem, Padhraic Smyth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05126">https://arxiv.org/abs/2510.05126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05126">https://arxiv.org/pdf/2510.05126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05126]] Improving Metacognition and Uncertainty Communication in Language Models(https://arxiv.org/abs/2510.05126)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in decision-making contexts, but when they present answers without signaling low confidence, users may unknowingly act on erroneous outputs. While prior work shows that LLMs maintain internal uncertainty signals, their explicit verbalized confidence is typically miscalibrated and poorly discriminates between correct and incorrect answers. Across two types of LLMs, we investigate whether supervised finetuning can improve models' ability to communicate uncertainty and whether such improvements generalize across tasks and domains. We finetune the LLMs on datasets spanning general knowledge, mathematics, and open-ended trivia, and evaluate two metacognitive tasks: (1) single-question confidence estimation, where the model assigns a numeric certainty to its answer, and (2) pairwise confidence comparison, where the model selects which of two answers it is more likely to have correct. We assess generalization to unseen domains, including medical and legal reasoning. Results show that finetuning improves calibration (alignment between stated confidence and accuracy) and discrimination (higher confidence for correct vs. incorrect responses) within and across domains, while leaving accuracy unchanged. However, improvements are task-specific: training on single-question calibration does not transfer to pairwise comparison, and vice versa. In contrast, multitask finetuning on both forms of metacognition yields broader gains, producing lower calibration error and stronger discrimination in out-of-domain evaluations. These results show that while uncertainty communication in LLMs is trainable and generalizable, different metacognitive skills do not naturally reinforce one another and must be developed together through multitask training.</li>
</ul>

<h3>Title: Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models</h3>
<ul>
<li><strong>Authors: </strong>Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05128">https://arxiv.org/abs/2510.05128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05128">https://arxiv.org/pdf/2510.05128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05128]] Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models(https://arxiv.org/abs/2510.05128)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Current methods for automated assessment of cognitive-linguistic impairment via picture description often neglect the visual narrative path - the sequence and locations of elements a speaker described in the picture. Analyses of spatio-semantic features capture this path using content information units (CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and pairwise ranking loss, for automated CIU extraction and ordering from the Cookie Theft picture description. Evaluated by 5-fold cross-validation, it achieves 93% median precision, 96% median recall in CIU detection, and 24% sequence error rates. The proposed method extracts features that exhibit strong Pearson correlations with ground truth, surpassing the dictionary-based baseline in external validation. These features also perform comparably to those derived from manual annotations in evaluating group differences via ANCOVA. The pipeline is shown to effectively characterize visual narrative paths for cognitive impairment assessment, with the implementation and models open-sourced to public.</li>
</ul>

<h3>Title: Submodular Context Partitioning and Compression for In-Context Learning-short paper</h3>
<ul>
<li><strong>Authors: </strong>Shaoyi Zheng, Canyu Zhang, Tianyi Zhou, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05130">https://arxiv.org/abs/2510.05130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05130">https://arxiv.org/pdf/2510.05130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05130]] Submodular Context Partitioning and Compression for In-Context Learning-short paper(https://arxiv.org/abs/2510.05130)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables efficient few-shot learning in large language models (LLMs) without training, but suffers from the quadratic input complexity of transformers, limiting the maximum number of exemplars. While various efficient ICL approaches partition the context into blocks to process (e.g., ensembling, compression, cross-attention), they often ignore the information redundancy or under-representation caused by different partition strategies, leading to suboptimal performance. To tackle this problem, we propose Sub-CP, a block-aware context selection framework that leverages submodular objectives to control block diversity. Sub-CP supports a flexible spectrum of selection strategies, allowing each block to range from globally diverse to locally coherent. This allows fine-grained control over semantic structure while enabling precomputation. Extensive experiments across diverse tasks on multiple datasets show that Sub-CP consistently improves performance across model scales.</li>
</ul>

<h3>Title: Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery</h3>
<ul>
<li><strong>Authors: </strong>Bowen Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05131">https://arxiv.org/abs/2510.05131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05131">https://arxiv.org/pdf/2510.05131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05131]] Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery(https://arxiv.org/abs/2510.05131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Head Start programs utilizing GoEngage face significant challenges when new or rotating staff attempt to locate appropriate Tasks (modules) on the platform homepage. These difficulties arise from domain-specific jargon (e.g., IFPA, DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent limitations of lexical search in handling typos and varied word ordering. We propose a pragmatic hybrid semantic search system that synergistically combines lightweight typo-tolerant lexical retrieval, embedding-based vector similarity, and constrained large language model (LLM) re-ranking. Our approach leverages the organization's existing Task Repository and Knowledge Base infrastructure while ensuring trustworthiness through low false-positive rates, evolvability to accommodate terminological changes, and economic efficiency via intelligent caching, shortlist generation, and graceful degradation mechanisms. We provide a comprehensive framework detailing required resources, a phased implementation strategy with concrete milestones, an offline evaluation protocol utilizing curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online measurement methodology incorporating query success metrics, zero-result rates, and dwell-time proxies.</li>
</ul>

<h3>Title: Training Large Language Models To Reason In Parallel With Global Forking Tokens</h3>
<ul>
<li><strong>Authors: </strong>Sheng Jia, Xiao Wang, Shiva Prasad Kasiviswanathan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05132">https://arxiv.org/abs/2510.05132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05132">https://arxiv.org/pdf/2510.05132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05132]] Training Large Language Models To Reason In Parallel With Global Forking Tokens(https://arxiv.org/abs/2510.05132)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.</li>
</ul>

<h3>Title: Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios</h3>
<ul>
<li><strong>Authors: </strong>Y. Du, G. Wu, G. Tang, W. Wang, Q. Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05133">https://arxiv.org/abs/2510.05133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05133">https://arxiv.org/pdf/2510.05133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05133]] Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios(https://arxiv.org/abs/2510.05133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic data generated by large language models has become integral to modern NLP training pipelines, from bootstrapping reasoning capabilities to augmenting instruction-following datasets. While recent work demonstrates successful applications maintaining high external data ratios, systematic understanding of how synthetic data proportion affects model behavior across different scales remains limited. This paper presents a controlled empirical study examining model performance, calibration, and output characteristics when trained on varying synthetic-to-external data ratios. Using the Pythia model suite (410M-12B parameters) across five diverse tasks, we evaluate models after one to three training iterations with synthetic data proportions ranging from 0-50\%. Our key findings include: models maintain stable performance with up to 20\% synthetic data, but degradation accelerates beyond 30\%; larger models (6.9B-12B) show greater robustness to synthetic data than smaller models (410M-1.4B); calibration degradation precedes accuracy loss, providing an early warning signal; and task characteristics matter, with reasoning tasks degrading faster than retrieval tasks under synthetic data training. Importantly, we find that current best practices, such as those employed in STaR and Self-Instruct systems that maintain greater than 80\% external data, operate well within safe regimes identified by our experiments. We provide practical guidance for practitioners on synthetic data budgets based on model scale and task requirements, alongside detailed comparison with concurrent work including Shumailov et al.'s model collapse findings.</li>
</ul>

<h3>Title: Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment</h3>
<ul>
<li><strong>Authors: </strong>Vanya Bannihatti Kumar, Divyanshu Goyal, Akhil Eppa, Neel Bhandari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05135">https://arxiv.org/abs/2510.05135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05135">https://arxiv.org/pdf/2510.05135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05135]] Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment(https://arxiv.org/abs/2510.05135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) excel at objective tasks such as evaluating mathematical reasoning and factual accuracy, yet they falter when faced with the nuanced, subjective nature of assessing creativity. In this work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating creative writing which is personlized to each individual's creative judgments. We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in Chakrabarty et al. (2024), which has stories annotated by expert humans across various subjective dimensions like Originality, to test our hypothesis. We show that our method enables models across various sizes, to learn the nuanced creative judgments of different individuals, by showing improvements over baseline supervised finetuning(SFT) method across various evaluation metrics like Pearson correlation, Cohen's and F1 values. Our method is especially useful in subjective evaluations where not all the annotators agree with each other.</li>
</ul>

<h3>Title: Linguistic Characteristics of AI-Generated Text: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Luka Terčon, Kaja Dobrovoljc</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05136">https://arxiv.org/abs/2510.05136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05136">https://arxiv.org/pdf/2510.05136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05136]] Linguistic Characteristics of AI-Generated Text: A Survey(https://arxiv.org/abs/2510.05136)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.</li>
</ul>

<h3>Title: LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation</h3>
<ul>
<li><strong>Authors: </strong>Gregory Hok Tjoan Go, Khang Ly, Anders Søgaard, Amin Tabatabaei, Maarten de Rijke, Xinyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05138">https://arxiv.org/abs/2510.05138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05138">https://arxiv.org/pdf/2510.05138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05138]] LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation(https://arxiv.org/abs/2510.05138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.</li>
</ul>

<h3>Title: NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description</h3>
<ul>
<li><strong>Authors: </strong>Hamed Jelodar, Mohammad Meymani, Parisa Hamedi, Tochukwu Emmanuel Nwankwo, Samita Bai, Roozbeh Razavi-Far, Ali A. Ghorbani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05139">https://arxiv.org/abs/2510.05139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05139">https://arxiv.org/pdf/2510.05139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05139]] NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description(https://arxiv.org/abs/2510.05139)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Natural Language Description (NLD) is a Natural Language Processing (NLP) task that requires models to generate structured and meaningful outputs from natural language inputs. In this work, we propose NLD-LLM, a systematic NLP framework to evaluate the performance of language models to generate accurate and concise source code descriptions. This framework incorporates a diverse set of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral, spanning various sizes, architectures, and training approaches. Central to NLD-LLM is a comprehensive prompt design strategy that includes standardized formatting, clear task guidance, and NLD prompting, ensuring fair and consistent evaluation. Additionally, we apply an iterative refinement process to improve output's quality and assess the model's adaptability. Using semantic and structural metrics, our analysis demonstrates that prompt engineering significantly impacts the effectiveness of the model such that smaller models often performing competitively when supported by well-crafted prompts.</li>
</ul>

<h3>Title: Auditing Algorithmic Bias in Transformer-Based Trading</h3>
<ul>
<li><strong>Authors: </strong>Armin Gerami, Ramani Duraiswami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05140">https://arxiv.org/abs/2510.05140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05140">https://arxiv.org/pdf/2510.05140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05140]] Auditing Algorithmic Bias in Transformer-Based Trading(https://arxiv.org/abs/2510.05140)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have become increasingly popular in financial applications, yet their potential risk making and biases remain under-explored. The purpose of this work is to audit the reliance of the model on volatile data for decision-making, and quantify how the frequency of price movements affects the model's prediction confidence. We employ a transformer model for prediction, and introduce a metric based on Partial Information Decomposition (PID) to measure the influence of each asset on the model's decision making. Our analysis reveals two key observations: first, the model disregards data volatility entirely, and second, it is biased toward data with lower-frequency price movements.</li>
</ul>

<h3>Title: To model human linguistic prediction, make LLMs less superhuman</h3>
<ul>
<li><strong>Authors: </strong>Byung-Doh Oh, Tal Linzen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05141">https://arxiv.org/abs/2510.05141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05141">https://arxiv.org/pdf/2510.05141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05141]] To model human linguistic prediction, make LLMs less superhuman(https://arxiv.org/abs/2510.05141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When people listen to or read a sentence, they actively make predictions about upcoming words: words that are less predictable are generally read more slowly than predictable ones. The success of large language models (LLMs), which, like humans, make predictions about upcoming words, has motivated exploring the use of these models as cognitive models of human linguistic prediction. Surprisingly, in the last few years, as language models have become better at predicting the next word, their ability to predict human reading behavior has declined. This is because LLMs are able to predict upcoming words much better than people can, leading them to predict lower processing difficulty in reading than observed in human experiments; in other words, mainstream LLMs are 'superhuman' as models of language comprehension. In this position paper, we argue that LLMs' superhumanness is primarily driven by two factors: compared to humans, LLMs have much stronger long-term memory for facts and training examples, and they have much better short-term memory for previous words in the text. We advocate for creating models that have human-like long-term and short-term memory, and outline some possible directions for achieving this goal. Finally, we argue that currently available human data is insufficient to measure progress towards this goal, and outline human experiments that can address this gap.</li>
</ul>

<h3>Title: Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Anshu Raj, Matthew Luebbe, Haiming Wen, Shuozhi Xu, Kun Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05142">https://arxiv.org/abs/2510.05142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05142">https://arxiv.org/pdf/2510.05142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05142]] Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models(https://arxiv.org/abs/2510.05142)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Data-driven materials discovery requires large-scale experimental datasets, yet most of the information remains trapped in unstructured literature. Existing extraction efforts often focus on a limited set of features and have not addressed the integrated composition-processing-microstructure-property relationships essential for understanding materials behavior, thereby posing challenges for building comprehensive databases. To address this gap, we propose a multi-stage information extraction pipeline powered by large language models, which captures 47 features spanning composition, processing, microstructure, and properties exclusively from experimentally reported materials. The pipeline integrates iterative extraction with source tracking to enhance both accuracy and reliability. Evaluations at the feature level (independent attributes) and tuple level (interdependent features) yielded F1 scores around 0.96. Compared with single-pass extraction without source tracking, our approach improved F1 scores of microstructure category by 10.0% (feature level) and 13.7% (tuple level), and reduced missed materials from 49 to 13 out of 396 materials in 100 articles on precipitate-containing multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The pipeline enables scalable and efficient literature mining, producing databases with high precision, minimal omissions, and zero false positives. These datasets provide trustworthy inputs for machine learning and materials informatics, while the modular design generalizes to diverse material classes, enabling comprehensive materials information extraction.</li>
</ul>

<h3>Title: Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Runpeng Yu, Haiquan Lu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05148">https://arxiv.org/abs/2510.05148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05148">https://arxiv.org/pdf/2510.05148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05148]] Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs(https://arxiv.org/abs/2510.05148)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a competitive paradigm for non-autoregressive language modeling. Their distinctive decoding mechanism enables faster inference speed and strong performance in code generation and mathematical tasks. In this work, we show that the decoding mechanism of dLLMs not only enhances model utility but also can be used as a powerful tool for model attribution. A key challenge in this problem lies in the diversity of attribution scenarios, including distinguishing between different models as well as between different checkpoints or backups of the same model. To ensure broad applicability, we identify two fundamental problems: what information to extract from the decoding trajectory, and how to utilize it effectively. We first observe that relying directly on per-step model confidence yields poor performance. This is mainly due to the bidirectional decoding nature of dLLMs: each newly decoded token influences the confidence of other decoded tokens, making model confidence highly redundant and washing out structural signal regarding decoding order or dependencies. To overcome this, we propose a novel information extraction scheme called the Directed Decoding Map (DDM), which captures structural relationships between decoding steps and better reveals model-specific behaviors. Furthermore, to make full use of the extracted structural information during attribution, we propose Gaussian-Trajectory Attribution (GTA), where we fit a cell-wise Gaussian distribution at each decoding position for each target model, and define the likelihood of a trajectory as the attribution score: if a trajectory exhibits higher log-likelihood under the distribution of a specific model, it is more likely to have been generated by that model. Extensive experiments under different settings validate the utility of our methods.</li>
</ul>

<h3>Title: Chronological Thinking in Full-Duplex Spoken Dialogue Language Models</h3>
<ul>
<li><strong>Authors: </strong>Donghang Wu, Haoyang Zhang, Chen Chen, Tianyu Zhang, Fei Tian, Xuerui Yang, Gang Yu, Hexin Liu, Nana Hou, Yuchen Hu, Eng Siong Chng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05150">https://arxiv.org/abs/2510.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05150">https://arxiv.org/pdf/2510.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05150]] Chronological Thinking in Full-Duplex Spoken Dialogue Language Models(https://arxiv.org/abs/2510.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full-duplex interaction metrics.</li>
</ul>

<h3>Title: Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA</h3>
<ul>
<li><strong>Authors: </strong>Prudence Djagba, Abdelkader Y. Saley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05151">https://arxiv.org/abs/2510.05151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05151">https://arxiv.org/pdf/2510.05151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05151]] Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA(https://arxiv.org/abs/2510.05151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research explores the strengths and weaknesses of domain-adapted Large Language Models (LLMs) in the context of financial natural language processing (NLP). The analysis centers on FinMA, a model created within the PIXIU framework, which is evaluated for its performance in specialized financial tasks. Recognizing the critical demands of accuracy, reliability, and domain adaptation in financial applications, this study examines FinMA's model architecture, its instruction tuning process utilizing the Financial Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark. Findings indicate that FinMA performs well in sentiment analysis and classification, but faces notable challenges in tasks involving numerical reasoning, entity recognition, and summarization. This work aims to advance the understanding of how financial LLMs can be effectively designed and evaluated to assist in finance-related decision-making processes.</li>
</ul>

<h3>Title: A Single Character can Make or Break Your LLM Evals</h3>
<ul>
<li><strong>Authors: </strong>Jingtong Su, Jianyu Zhang, Karen Ullrich, Léon Bottou, Mark Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05152">https://arxiv.org/abs/2510.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05152">https://arxiv.org/pdf/2510.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05152]] A Single Character can Make or Break Your LLM Evals(https://arxiv.org/abs/2510.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.</li>
</ul>

<h3>Title: Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shenzhe Zhu, Shu Yang, Michiel A. Bakker, Alex Pentland, Jiaxin Pei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05154">https://arxiv.org/abs/2510.05154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05154">https://arxiv.org/pdf/2510.05154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05154]] Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs(https://arxiv.org/abs/2510.05154)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.</li>
</ul>

<h3>Title: Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment</h3>
<ul>
<li><strong>Authors: </strong>Abrar Shahid, Ibteeker Mahir Ishum, AKM Tahmidul Haque, M Sohel Rahman, A. B. M. Alim Al Islam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05157">https://arxiv.org/abs/2510.05157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05157">https://arxiv.org/pdf/2510.05157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05157]] Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment(https://arxiv.org/abs/2510.05157)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.</li>
</ul>

<h3>Title: Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain</h3>
<ul>
<li><strong>Authors: </strong>Léo Boisvert, Abhay Puri, Chandra Kiran Reddy Evuru, Nicolas Chapados, Quentin Cappart, Alexandre Lacoste, Krishnamurthy Dj Dvijotham, Alexandre Drouin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05159">https://arxiv.org/abs/2510.05159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05159">https://arxiv.org/pdf/2510.05159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05159]] Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain(https://arxiv.org/abs/2510.05159)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The practice of fine-tuning AI agents on data from their own interactions--such as web browsing or tool use--, while being a strong general recipe for improving agentic capabilities, also introduces a critical security vulnerability within the AI supply chain. In this work, we show that adversaries can easily poison the data collection pipeline to embed hard-to-detect backdoors that are triggerred by specific target phrases, such that when the agent encounters these triggers, it performs an unsafe or malicious action. We formalize and validate three realistic threat models targeting different layers of the supply chain: 1) direct poisoning of fine-tuning data, where an attacker controls a fraction of the training traces; 2) environmental poisoning, where malicious instructions are injected into webpages scraped or tools called while creating training data; and 3) supply chain poisoning, where a pre-backdoored base model is fine-tuned on clean data to improve its agentic capabilities. Our results are stark: by poisoning as few as 2% of the collected traces, an attacker can embed a backdoor causing an agent to leak confidential user information with over 80% success when a specific trigger is present. This vulnerability holds across all three threat models. Furthermore, we demonstrate that prominent safeguards, including two guardrail models and one weight-based defense, fail to detect or prevent the malicious behavior. These findings highlight an urgent threat to agentic AI development and underscore the critical need for rigorous security vetting of data collection processes and end-to-end model supply chains.</li>
</ul>

<h3>Title: Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arif Hakimi Zamrai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05160">https://arxiv.org/abs/2510.05160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05160">https://arxiv.org/pdf/2510.05160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05160]] Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders(https://arxiv.org/abs/2510.05160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.</li>
</ul>

<h3>Title: Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches</h3>
<ul>
<li><strong>Authors: </strong>Abdelilah Ganmati, Karim Afdel, Lahcen Koutti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05163">https://arxiv.org/abs/2510.05163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05163">https://arxiv.org/pdf/2510.05163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05163]] Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches(https://arxiv.org/abs/2510.05163)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, robust, biometric</a></li>
<li><strong>Abstract: </strong>In the era of pervasive cyber threats and exponential growth in digital services, the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication (MFA), which combines knowledge-based factors (passwords, PINs), possession-based factors (smart cards, tokens), and inherence-based factors (biometric traits), has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems, enabling higher accuracy, resilience to spoofing, and seamless integration with hardware-based solutions. At the same time, smart card technologies have evolved to include on-chip biometric verification, cryptographic processing, and secure storage, thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work (2019-2025) at the intersection of deep learning, biometrics, and smart card technologies for MFA. We analyze biometric modalities (face, fingerprint, iris, voice), review hardware-based approaches (smart cards, NFC, TPMs, secure enclaves), and highlight integration strategies for real-world applications such as digital banking, healthcare IoT, and critical infrastructure. Furthermore, we discuss the major challenges that remain open, including usability-security tradeoffs, adversarial attacks on deep learning models, privacy concerns surrounding biometric data, and the need for standardization in MFA deployment. By consolidating current advancements, limitations, and research opportunities, this survey provides a roadmap for designing secure, scalable, and user-friendly authentication frameworks.</li>
</ul>

<h3>Title: Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks</h3>
<ul>
<li><strong>Authors: </strong>Minh K. Quan, Pubudu N. Pathirana</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05165">https://arxiv.org/abs/2510.05165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05165">https://arxiv.org/pdf/2510.05165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05165]] Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks(https://arxiv.org/abs/2510.05165)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Cross-slice attack attribution in 6G networks faces the fundamental challenge of distinguishing genuine causal relationships from spurious correlations in shared infrastructure environments. We propose a theoretically-grounded domain-adapted Granger causality framework that integrates statistical causal inference with network-specific resource modeling for real-time attack attribution. Our approach addresses key limitations of existing methods by incorporating resource contention dynamics and providing formal statistical guarantees. Comprehensive evaluation on a production-grade 6G testbed with 1,100 empirically-validated attack scenarios demonstrates 89.2% attribution accuracy with sub-100ms response time, representing a statistically significant 10.1 percentage point improvement over state-of-the-art baselines. The framework provides interpretable causal explanations suitable for autonomous 6G security orchestration.</li>
</ul>

<h3>Title: Machine learning for fraud detection in digital banking: a systematic literature review REVIEW</h3>
<ul>
<li><strong>Authors: </strong>Md Zahin Hossain George, Md Khorshed Alam, Md Tarek Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05167">https://arxiv.org/abs/2510.05167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05167">https://arxiv.org/pdf/2510.05167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05167]] Machine learning for fraud detection in digital banking: a systematic literature review REVIEW(https://arxiv.org/abs/2510.05167)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This systematic literature review examines the role of machine learning in fraud detection within digital banking, synthesizing evidence from 118 peer-reviewed studies and institutional reports. Following the PRISMA guidelines, the review applied a structured identification, screening, eligibility, and inclusion process to ensure methodological rigor and transparency. The findings reveal that supervised learning methods, such as decision trees, logistic regression, and support vector machines, remain the dominant paradigm due to their interpretability and established performance, while unsupervised anomaly detection approaches are increasingly adopted to address novel fraud patterns in highly imbalanced datasets. Deep learning architectures, particularly recurrent and convolutional neural networks, have emerged as transformative tools capable of modeling sequential transaction data and detecting complex fraud typologies, though challenges of interpretability and real-time deployment persist. Hybrid models that combine supervised, unsupervised, and deep learning strategies demonstrate superior adaptability and detection accuracy, highlighting their potential as convergent solutions.</li>
</ul>

<h3>Title: From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guangyu Shen, Siyuan Cheng, Xiangzhe Xu, Yuan Zhou, Hanxi Guo, Zhuo Zhang, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05169">https://arxiv.org/abs/2510.05169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05169">https://arxiv.org/pdf/2510.05169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05169]] From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs(https://arxiv.org/abs/2510.05169)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can acquire deceptive behaviors through backdoor attacks, where the model executes prohibited actions whenever secret triggers appear in the input. Existing safety training methods largely fail to address this vulnerability, due to the inherent difficulty of uncovering hidden triggers implanted in the model. Motivated by recent findings on LLMs' situational awareness, we propose a novel post-training framework that cultivates self-awareness of backdoor risks and enables models to articulate implanted triggers even when they are absent from the prompt. At its core, our approach introduces an inversion-inspired reinforcement learning framework that encourages models to introspectively reason about their own behaviors and reverse-engineer the triggers responsible for misaligned outputs. Guided by curated reward signals, this process transforms a poisoned model into one capable of precisely identifying its implanted trigger. Surprisingly, we observe that such backdoor self-awareness emerges abruptly within a short training window, resembling a phase transition in capability. Building on this emergent property, we further present two complementary defense strategies for mitigating and detecting backdoor threats. Experiments on five backdoor attacks, compared against six baseline methods, demonstrate that our approach has strong potential to improve the robustness of LLMs against backdoor risks. The code is available at LLM Backdoor Self-Awareness.</li>
</ul>

<h3>Title: Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data</h3>
<ul>
<li><strong>Authors: </strong>Anushiya Arunan, Yan Qin, Xiaoli Li, U-Xuan Tan, H. Vincent Poor, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05172">https://arxiv.org/abs/2510.05172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05172">https://arxiv.org/pdf/2510.05172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05172]] Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data(https://arxiv.org/abs/2510.05172)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Accurate battery capacity estimation is key to alleviating consumer concerns about battery performance and reliability of electric vehicles (EVs). However, practical data limitations imposed by stringent privacy regulations and labeled data shortages hamper the development of generalizable capacity estimation models that remain robust to real-world data distribution shifts. While self-supervised learning can leverage unlabeled data, existing techniques are not particularly designed to learn effectively from challenging field data -- let alone from privacy-friendly data, which are often less feature-rich and noisier. In this work, we propose a first-of-its-kind capacity estimation model based on self-supervised pre-training, developed on a large-scale dataset of privacy-friendly charging data snippets from real-world EV operations. Our pre-training framework, snippet similarity-weighted masked input reconstruction, is designed to learn rich, generalizable representations even from less feature-rich and fragmented privacy-friendly data. Our key innovation lies in harnessing contrastive learning to first capture high-level similarities among fragmented snippets that otherwise lack meaningful context. With our snippet-wise contrastive learning and subsequent similarity-weighted masked reconstruction, we are able to learn rich representations of both granular charging patterns within individual snippets and high-level associative relationships across different snippets. Bolstered by this rich representation learning, our model consistently outperforms state-of-the-art baselines, achieving 31.9% lower test error than the best-performing benchmark, even under challenging domain-shifted settings affected by both manufacturer and age-induced distribution shifts.</li>
</ul>

<h3>Title: SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Peigui Qi, Kunsheng Tang, Wenbo Zhou, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, Jie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05173">https://arxiv.org/abs/2510.05173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05173">https://arxiv.org/pdf/2510.05173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05173]] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models(https://arxiv.org/abs/2510.05173)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.</li>
</ul>

<h3>Title: Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression</h3>
<ul>
<li><strong>Authors: </strong>Ou Deng, Ruichen Cong, Jianting Xu, Shoji Nishimura, Atsushi Ogihara, Qun Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05178">https://arxiv.org/abs/2510.05178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05178">https://arxiv.org/pdf/2510.05178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05178]] Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression(https://arxiv.org/abs/2510.05178)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Symbolic regression promises readable equations but struggles to encode unit-aware thresholds and conditional logic. We propose logistic-gated operators (LGO) -- differentiable gates with learnable location and steepness -- embedded as typed primitives and mapped back to physical units for audit. Across two primary health datasets (ICU, NHANES), the hard-gate variant recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall within 10% of guideline anchors and 100% within 20%, while using far fewer gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and remaining within the competitive accuracy envelope of strong SR baselines. On predominantly smooth tasks, gates are pruned, preserving parsimony. The result is compact symbolic equations with explicit, unit-aware thresholds that can be audited against clinical anchors -- turning interpretability from a post-hoc explanation into a modeling constraint and equipping symbolic regression with a practical calculus for regime switching and governance-ready deployment.</li>
</ul>

<h3>Title: OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT</h3>
<ul>
<li><strong>Authors: </strong>Saida Elouardi, Mohammed Jouhari, Anas Motii</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05180">https://arxiv.org/abs/2510.05180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05180">https://arxiv.org/pdf/2510.05180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05180]] OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT(https://arxiv.org/abs/2510.05180)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>In critical IoT environments, such as smart homes and industrial systems, effective Intrusion Detection Systems (IDS) are essential for ensuring security. However, developing robust IDS solutions remains a significant challenge. Traditional machine learning-based IDS models typically require large datasets, but data sharing is often limited due to privacy and security concerns. Federated Learning (FL) presents a promising alternative by enabling collaborative model training without sharing raw data. Despite its advantages, FL still faces key challenges, such as data heterogeneity (non-IID data) and high energy and computation costs, particularly for resource constrained IoT devices. To address these issues, this paper proposes OptiFLIDS, a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption. It also incorporates a customized aggregation method to better handle pruned models that differ due to non-IID data distributions. Experiments conducted on three recent IoT IDS datasets, TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong detection performance while improving energy efficiency, making it well-suited for deployment in real-world IoT environments.</li>
</ul>

<h3>Title: Auditing Pay-Per-Token in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ander Artola Velasco, Stratis Tsirtsis, Manuel Gomez-Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05181">https://arxiv.org/abs/2510.05181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05181">https://arxiv.org/pdf/2510.05181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05181]] Auditing Pay-Per-Token in Large Language Models(https://arxiv.org/abs/2510.05181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Millions of users rely on a market of cloud-based services to obtain access to state-of-the-art large language models. However, it has been very recently shown that the de facto pay-per-token pricing mechanism used by providers creates a financial incentive for them to strategize and misreport the (number of) tokens a model used to generate an output. In this paper, we develop an auditing framework based on martingale theory that enables a trusted third-party auditor who sequentially queries a provider to detect token misreporting. Crucially, we show that our framework is guaranteed to always detect token misreporting, regardless of the provider's (mis-)reporting policy, and not falsely flag a faithful provider as unfaithful with high probability. To validate our auditing framework, we conduct experiments across a wide range of (mis-)reporting policies using several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from a popular crowdsourced benchmarking platform. The results show that our framework detects an unfaithful provider after observing fewer than $\sim 70$ reported outputs, while maintaining the probability of falsely flagging a faithful provider below $\alpha = 0.05$.</li>
</ul>

<h3>Title: A novel hallucination classification framework</h3>
<ul>
<li><strong>Authors: </strong>Maksym Zavhorodnii, Dmytro Dehtiarov, Anna Konovalenko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05189">https://arxiv.org/abs/2510.05189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05189">https://arxiv.org/pdf/2510.05189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05189]] A novel hallucination classification framework(https://arxiv.org/abs/2510.05189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work introduces a novel methodology for the automatic detection of hallucinations generated during large language model (LLM) inference. The proposed approach is based on a systematic taxonomy and controlled reproduction of diverse hallucination types through prompt engineering. A dedicated hallucination dataset is subsequently mapped into a vector space using an embedding model and analyzed with unsupervised learning techniques in a reduced-dimensional representation of hallucinations with veridical responses. Quantitative evaluation of inter-centroid distances reveals a consistent correlation between the severity of informational distortion in hallucinations and their spatial divergence from the cluster of correct outputs. These findings provide theoretical and empirical evidence that even simple classification algorithms can reliably distinguish hallucinations from accurate responses within a single LLM, thereby offering a lightweight yet effective framework for improving model reliability.</li>
</ul>

<h3>Title: Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study</h3>
<ul>
<li><strong>Authors: </strong>Francesca Gomez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05192">https://arxiv.org/abs/2510.05192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05192">https://arxiv.org/pdf/2510.05192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05192]] Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study(https://arxiv.org/abs/2510.05192)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Agentic misalignment occurs when goal-directed agents take harmful actions, such as blackmail, rather than risk goal failure, and can be triggered by replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025). We adapt insider-risk control design (Critical Pathway; Situational Crime Prevention) to develop preventative operational controls that steer agents toward safe actions when facing stressors. Using the blackmail scenario from the original Anthropic study by Lynch et al. (2025), we evaluate mitigations across 10 LLMs and 66,600 samples. Our main finding is that an externally governed escalation channel, which guarantees a pause and independent review, reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21% (averaged across all models and conditions). Augmenting this channel with compliance email bulletins further lowers the blackmail rate to 0.85%. Overall, incorporating preventative operational controls strengthens defence-in-depth strategies for agentic AI. We also surface a failure mode diverging from Lynch et al. (2025): two models (Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent autonomy threat, leveraging sensitive information for coercive signalling. In counterfactual swaps, both continued using the affair regardless of whether the CEO or CTO was implicated. An escalation channel eliminated coercion, but Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was implicated, unlike most models (higher in the CEO condition). The reason for this divergent behaviour is not clear from raw outputs and could reflect benign differences in reasoning or strategic discrediting of a potential future threat, warranting further investigation.</li>
</ul>

<h3>Title: A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Wagner-Carena, Aizhan Akhmetzhanova, Sydney Erickson</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05205">https://arxiv.org/abs/2510.05205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05205">https://arxiv.org/pdf/2510.05205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05205]] A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors(https://arxiv.org/abs/2510.05205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A common challenge in the natural sciences is to disentangle distinct, unknown sources from observations. Examples of this source separation task include deblending galaxies in a crowded field, distinguishing the activity of individual neurons from overlapping signals, and separating seismic events from an ambient background. Traditional analyses often rely on simplified source models that fail to accurately reproduce the data. Recent advances have shown that diffusion models can directly learn complex prior distributions from noisy, incomplete data. In this work, we show that diffusion models can solve the source separation problem without explicit assumptions about the source. Our method relies only on multiple views, or the property that different sets of observations contain different linear transformations of the unknown sources. We show that our method succeeds even when no source is individually observed and the observations are noisy, incomplete, and vary in resolution. The learned diffusion models enable us to sample from the source priors, evaluate the probability of candidate sources, and draw from the joint posterior of the source distribution given an observation. We demonstrate the effectiveness of our method on a range of synthetic problems as well as real-world galaxy observations.</li>
</ul>

<h3>Title: CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers</h3>
<ul>
<li><strong>Authors: </strong>Haining Pan, James V. Roggeveen, Erez Berg, Juan Carrasquilla, Debanjan Chowdhury, Surya Ganguli, Federico Ghimenti, Juraj Hasik, Henry Hunt, Hong-Chen Jiang, Mason Kamb, Ying-Jer Kao, Ehsan Khatami, Michael J. Lawler, Di Luo, Titus Neupert, Xiaoliang Qi, Michael P. Brenner, Eun-Ah Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05228">https://arxiv.org/abs/2510.05228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05228">https://arxiv.org/pdf/2510.05228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05228]] CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers(https://arxiv.org/abs/2510.05228)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable progress in coding and math problem-solving, but evaluation on advanced research-level problems in hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a dataset of 50 problems covering condensed matter theory (CMT) at the level of an expert researcher. Topics span analytical and computational approaches in quantum many-body, and classical statistical mechanics. The dataset was designed and verified by a panel of expert researchers from around the world. We built the dataset through a collaborative environment that challenges the panel to write and refine problems they would want a research assistant to solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte Carlo, density matrix renormalization group (DMRG), quantum/classical statistical mechanics, and model building. We evaluate LLMs by programmatically checking solutions against expert-supplied ground truth. We developed machine-grading, including symbolic handling of non-commuting operators via normal ordering. They generalize across tasks too. Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. The best model, GPT5, solves 30\% of the problems; average across 17 models (GPT, Gemini, Claude, DeepSeek, Llama) is 11.4$\pm$2.1\%. Moreover, 18 problems are solved by none of the 17 models, and 26 by at most one. These unsolved problems span Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe this benchmark will guide development toward capable AI research assistants and tutors.</li>
</ul>

<h3>Title: Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?</h3>
<ul>
<li><strong>Authors: </strong>Rishika Bhagwatkar, Kevin Kasa, Abhay Puri, Gabriel Huang, Irina Rish, Graham W. Taylor, Krishnamurthy Dj Dvijotham, Alexandre Lacoste</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05244">https://arxiv.org/abs/2510.05244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05244">https://arxiv.org/pdf/2510.05244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05244]] Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?(https://arxiv.org/abs/2510.05244)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>AI agents are vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external content or tool outputs cause unintended or harmful behavior. Inspired by the well-established concept of firewalls, we show that a simple, modular and model-agnostic defense operating at the agent--tool interface achieves perfect security (0% or the lowest possible attack success rate) with high utility (task success rate) across four public benchmarks: AgentDojo, Agent Security Bench, InjecAgent and tau-Bench, while achieving a state-of-the-art security-utility tradeoff compared to prior results. Specifically, we employ a defense based on two firewalls: a Tool-Input Firewall (Minimizer) and a Tool-Output Firewall (Sanitizer). Unlike prior complex approaches, this firewall defense makes minimal assumptions on the agent and can be deployed out-of-the-box, while maintaining strong performance without compromising utility. However, our analysis also reveals critical limitations in these existing benchmarks, including flawed success metrics, implementation bugs, and most importantly, weak attacks, hindering significant progress in the field. To foster more meaningful progress, we present targeted fixes to these issues for AgentDojo and Agent Security Bench while proposing best-practices for more robust benchmark design. Further, we demonstrate that although these firewalls push the state-of-the-art on existing benchmarks, it is still possible to bypass them in practice, underscoring the need to incorporate stronger attacks in security benchmarks. Overall, our work shows that existing agentic security benchmarks are easily saturated by a simple approach and highlights the need for stronger agentic security benchmarks with carefully chosen evaluation metrics and strong adaptive attacks.</li>
</ul>

<h3>Title: Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Yang, Lin Gui, Chenxiao Yang, Victor Veitch, Lizhu Zhang, Zhuokai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05251">https://arxiv.org/abs/2510.05251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05251">https://arxiv.org/pdf/2510.05251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05251]] Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning(https://arxiv.org/abs/2510.05251)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.</li>
</ul>

<h3>Title: ECLipsE-Gen-Local: Efficient Compositional Local Lipschitz Estimates for Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yuezhu Xu, S. Sivaranjani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05261">https://arxiv.org/abs/2510.05261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05261">https://arxiv.org/pdf/2510.05261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05261]] ECLipsE-Gen-Local: Efficient Compositional Local Lipschitz Estimates for Deep Neural Networks(https://arxiv.org/abs/2510.05261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Lipschitz constant is a key measure for certifying the robustness of neural networks to input perturbations. However, computing the exact constant is NP-hard, and standard approaches to estimate the Lipschitz constant involve solving a large matrix semidefinite program (SDP) that scales poorly with network size. Further, there is a potential to efficiently leverage local information on the input region to provide tighter Lipschitz estimates. We address this problem here by proposing a compositional framework that yields tight yet scalable Lipschitz estimates for deep feedforward neural networks. Specifically, we begin by developing a generalized SDP framework that is highly flexible, accommodating heterogeneous activation function slope, and allowing Lipschitz estimates with respect to arbitrary input-output pairs and arbitrary choices of sub-networks of consecutive layers. We then decompose this generalized SDP into a sequence of small sub-problems, with computational complexity that scales linearly with respect to the network depth. We also develop a variant that achieves near-instantaneous computation through closed-form solutions to each sub-problem. All our algorithms are accompanied by theoretical guarantees on feasibility and validity. Next, we develop a series of algorithms, termed as ECLipsE-Gen-Local, that effectively incorporate local information on the input. Our experiments demonstrate that our algorithms achieve substantial speedups over a multitude of benchmarks while producing significantly tighter Lipschitz bounds than global approaches. Moreover, we show that our algorithms provide strict upper bounds for the Lipschitz constant with values approaching the exact Jacobian from autodiff when the input region is small enough. Finally, we demonstrate the practical utility of our approach by showing that our Lipschitz estimates closely align with network robustness.</li>
</ul>

<h3>Title: Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05266">https://arxiv.org/abs/2510.05266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05266">https://arxiv.org/pdf/2510.05266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05266]] Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation(https://arxiv.org/abs/2510.05266)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot semantic segmentation is vital for deep learning-based infrastructure inspection applications, where labeled training examples are scarce and expensive. Although existing deep learning frameworks perform well, the need for extensive labeled datasets and the inability to learn new defect categories with little data are problematic. We present our Enhanced Feature Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert and sewer defect categories using a prototypical learning framework. Our approach has three main contributions: (1) adaptive E-FPN encoder using InceptionSepConv blocks and depth-wise separable convolutions for efficient multi-scale feature extraction; (2) prototypical learning with masked average pooling for powerful prototype generation from small support examples; and (3) attention-based feature representation through global self-attention, local self-attention and cross-attention. Comprehensive experimentation on challenging infrastructure inspection datasets illustrates that the method achieves excellent few-shot performance, with the best configuration being 8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way classification testing. The self-attention method had the most significant performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over baselines. Our framework addresses the critical need to rapidly respond to new defect types in infrastructure inspection systems with limited new training data that lead to more efficient and economical maintenance plans for critical infrastructure systems.</li>
</ul>

<h3>Title: Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs</h3>
<ul>
<li><strong>Authors: </strong>Paloma García-de-Herreros, Philipp Slusallek, Dietrich Klakow, Vagrant Gautam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05278">https://arxiv.org/abs/2510.05278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05278">https://arxiv.org/pdf/2510.05278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05278]] Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs(https://arxiv.org/abs/2510.05278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have shown great success on natural language tasks in recent years, but they have also shown great promise when adapted to new modalities, e.g., for scientific machine learning tasks. Even though decoder-only models are more popular within NLP and scale exceedingly well at generating natural language, most proposed approaches for cross-modal adaptation focus on encoder-only models, raising the question of how model architecture affects these approaches. In this paper, we therefore perform a series of ablation studies to answer this question, systematically comparing encoder-only and decoder-only models on cross-modal adaptation for time-dependent simulation tasks based on partial differential equations (PDEs). We find that decoder-only models are far worse than encoder-only models, when existing approaches are applied unmodified. In contrast to several other domains, scaling decoder-only models also does not help. To harness the potential of decoder-only models in this context, we introduce two novel approaches, Parallel Flipping and Sequence Doubling, attempting to mimic bidirectionality in autoregressive models. Both our methods improve overall performance using decoder-only models for all tasks and all cross-model adaptation methods, closing the gap to encoder-only model performance. We hope that our findings broaden the spectrum of models used on cross-modal adaptation tasks to further scientific ML.</li>
</ul>

<h3>Title: Adjusting the Output of Decision Transformer with Action Gradient</h3>
<ul>
<li><strong>Authors: </strong>Rui Lin, Yiwen Zhang, Zhicheng Peng, Minghao Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05285">https://arxiv.org/abs/2510.05285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05285">https://arxiv.org/pdf/2510.05285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05285]] Adjusting the Output of Decision Transformer with Action Gradient(https://arxiv.org/abs/2510.05285)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Decision Transformer (DT), which integrates reinforcement learning (RL) with the transformer model, introduces a novel approach to offline RL. Unlike classical algorithms that take maximizing cumulative discounted rewards as objective, DT instead maximizes the likelihood of actions. This paradigm shift, however, presents two key challenges: stitching trajectories and extrapolation of action. Existing methods, such as substituting specific tokens with predictive values and integrating the Policy Gradient (PG) method, address these challenges individually but fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG), an innovative methodology that directly adjusts actions to fulfill a function analogous to that of PG, while also facilitating efficient integration with token prediction techniques. AG utilizes the gradient of the Q-value with respect to the action to optimize the action. The empirical results demonstrate that our method can significantly enhance the performance of DT-based algorithms, with some results achieving state-of-the-art levels.</li>
</ul>

<h3>Title: DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping</h3>
<ul>
<li><strong>Authors: </strong>Ruoxing Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05288">https://arxiv.org/abs/2510.05288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05288">https://arxiv.org/pdf/2510.05288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05288]] DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping(https://arxiv.org/abs/2510.05288)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) such as ChatGPT have evolved into powerful and ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire specialized skills for specific tasks efficiently. Although LLMs provide great utility in both general and task-specific use cases, they are limited by two security-related concerns. First, traditional LLM hardware requirements make them infeasible to run locally on consumer-grade devices. A remote network connection with the LLM provider's server is usually required, making the system vulnerable to network attacks. Second, fine-tuning an LLM for a sensitive task may involve sensitive data. Non-private fine-tuning algorithms produce models vulnerable to training data reproduction attacks. Our work addresses these security concerns by enhancing differentially private optimization algorithms and applying them to fine-tune localizable language models. We introduce adaptable gradient clipping along with other engineering enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our optimizer to fine-tune examples of two localizable LLM designs, small language model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We demonstrate promising improvements in loss through experimentation with two synthetic datasets.</li>
</ul>

<h3>Title: Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages</h3>
<ul>
<li><strong>Authors: </strong>Tarek Naous, Anagha Savit, Carlos Rafael Catalan, Geyang Guo, Jaehyeok Lee, Kyungdon Lee, Lheane Marie Dizon, Mengyu Ye, Neel Kothari, Sahajpreet Singh, Sarah Masud, Tanish Patwa, Trung Thanh Tran, Zohaib Khan, Alan Ritter, JinYeong Bak, Keisuke Sakaguchi, Tanmoy Chakraborty, Yuki Arase, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05291">https://arxiv.org/abs/2510.05291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05291">https://arxiv.org/pdf/2510.05291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05291]] Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages(https://arxiv.org/abs/2510.05291)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) gain stronger multilingual capabilities, their ability to handle culturally diverse entities becomes crucial. Prior work has shown that LLMs often favor Western-associated entities in Arabic, raising concerns about cultural fairness. Due to the lack of multilingual benchmarks, it remains unclear if such biases also manifest in different non-Western languages. In this paper, we introduce Camellia, a benchmark for measuring entity-centric cultural biases in nine Asian languages spanning six distinct Asian cultures. Camellia includes 19,530 entities manually annotated for association with the specific Asian or Western culture, as well as 2,173 naturally occurring masked contexts for entities derived from social media posts. Using Camellia, we evaluate cultural biases in four recent multilingual LLM families across various tasks such as cultural context adaptation, sentiment association, and entity extractive QA. Our analyses show a struggle by LLMs at cultural adaptation in all Asian languages, with performance differing across models developed in regions with varying access to culturally-relevant data. We further observe that different LLM families hold their distinct biases, differing in how they associate cultures with particular sentiments. Lastly, we find that LLMs struggle with context understanding in Asian languages, creating performance gaps between cultures in entity extraction.</li>
</ul>

<h3>Title: SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography</h3>
<ul>
<li><strong>Authors: </strong>Zahra Maleki, Amirhossein Akbari, Amirhossein Binesh, Babak Khalaj</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05296">https://arxiv.org/abs/2510.05296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05296">https://arxiv.org/pdf/2510.05296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05296]] SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography(https://arxiv.org/abs/2510.05296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Remote photoplethysmography (rPPG) is an innovative method for monitoring heart rate and vital signs by using a simple camera to record a person, as long as any part of their skin is visible. This low-cost, contactless approach helps in remote patient monitoring, emotion analysis, smart vehicle utilization, and more. Over the years, various techniques have been proposed to improve the accuracy of this technology, especially given its sensitivity to lighting and movement. In the unsupervised pipeline, it is necessary to first select skin regions from the video to extract the rPPG signal from the skin color changes. We introduce a novel skin segmentation technique that prioritizes skin regions to enhance the quality of the extracted signal. It can detect areas of skin all over the body, making it more resistant to movement, while removing areas such as the mouth, eyes, and hair that may cause interference. Our model is evaluated on publicly available datasets, and we also present a new dataset, called SYNC-rPPG, to better represent real-world conditions. The results indicate that our model demonstrates a prior ability to capture heartbeats in challenging conditions, such as talking and head rotation, and maintain the mean absolute error (MAE) between predicted and actual heart rates, while other methods fail to do so. In addition, we demonstrate high accuracy in detecting a diverse range of skin tones, making this technique a promising option for real-world applications.</li>
</ul>

<h3>Title: Gamma Mixture Modeling for Cosine Similarity in Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Player</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05309">https://arxiv.org/abs/2510.05309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05309">https://arxiv.org/pdf/2510.05309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05309]] Gamma Mixture Modeling for Cosine Similarity in Small Language Models(https://arxiv.org/abs/2510.05309)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the cosine similarity of sentence transformer embeddings and observe that they are well modeled by gamma mixtures. From a fixed corpus, we measure similarities between all document embeddings and a reference query embedding. Empirically we find that these distributions are often well captured by a gamma distribution shifted and truncated to [-1,1], and in many cases, by a gamma mixture. We propose a heuristic model in which a hierarchical clustering of topics naturally leads to a gamma-mixture structure in the similarity scores. Finally, we outline an expectation-maximization algorithm for fitting shifted gamma mixtures, which provides a practical tool for modeling similarity distributions.</li>
</ul>

<h3>Title: RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts</h3>
<ul>
<li><strong>Authors: </strong>Yining She, Daniel W. Peterson, Marianne Menglin Liu, Vikas Upadhyay, Mohammad Hossein Chaghazardi, Eunsuk Kang, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05310">https://arxiv.org/abs/2510.05310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05310">https://arxiv.org/pdf/2510.05310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05310]] RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts(https://arxiv.org/abs/2510.05310)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.</li>
</ul>

<h3>Title: DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology</h3>
<ul>
<li><strong>Authors: </strong>Yousef Yeganeh, Maximilian Frantzen, Michael Lee, Kun-Hsing Yu, Nassir Navab, Azade Farshad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05315">https://arxiv.org/abs/2510.05315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05315">https://arxiv.org/pdf/2510.05315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05315]] DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology(https://arxiv.org/abs/2510.05315)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While Whole Slide Imaging (WSI) scanners remain the gold standard for digitizing pathology samples, their high cost limits accessibility in many healthcare settings. Other low-cost solutions also face critical limitations: automated microscopes struggle with consistent focus across varying tissue morphology, traditional auto-focus methods require time-consuming focal stacks, and existing deep-learning approaches either need multiple input images or lack generalization capability across tissue types and staining protocols. We introduce a novel automated microscopic system powered by DeepAf, a novel auto-focus framework that uniquely combines spatial and spectral features through a hybrid architecture for single-shot focus prediction. The proposed network automatically regresses the distance to the optimal focal point using the extracted spatiospectral features and adjusts the control parameters for optimal image outcomes. Our system transforms conventional microscopes into efficient slide scanners, reducing focusing time by 80% compared to stack-based methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples, matching the performance of dual-image methods (0.19 {\mu}m) with half the input requirements. DeepAf demonstrates robust cross-lab generalization with only 0.72% false focus predictions and 90% of predictions within the depth of field. Through an extensive clinical study of 536 brain tissue samples, our system achieves 0.90 AUC in cancer classification at 4x magnification, a significant achievement at lower magnification than typical 20x WSI scans. This results in a comprehensive hardware-software design enabling accessible, real-time digital pathology in resource-constrained settings while maintaining diagnostic accuracy.</li>
</ul>

<h3>Title: RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Liu, Varun Ojha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05317">https://arxiv.org/abs/2510.05317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05317">https://arxiv.org/pdf/2510.05317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05317]] RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness(https://arxiv.org/abs/2510.05317)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial training is the most effective defense against adversarial attacks. The effectiveness of the adversarial attacks has been on the design of its loss function and regularization term. The most widely used loss function in adversarial training is cross-entropy and mean squared error (MSE) as its regularization objective. However, MSE enforces overly uniform optimization between two output distributions during training, which limits its robustness in adversarial training scenarios. To address this issue, we revisit the idea of mutual learning (originally designed for knowledge distillation) and propose two novel regularization strategies tailored for adversarial training: (i) weighted adversarial mutual regularization and (ii) adversarial generalization regularization. In the former, we formulate a decomposed adversarial mutual Kullback-Leibler divergence (KL-divergence) loss, which allows flexible control over the optimization process by assigning unequal weights to the main and auxiliary objectives. In the latter, we introduce an additional clean target distribution into the adversarial training objective, improving generalization and enhancing model robustness. Extensive experiments demonstrate that our proposed methods significantly improve adversarial robustness compared to existing regularization-based approaches.</li>
</ul>

<h3>Title: WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives</h3>
<ul>
<li><strong>Authors: </strong>Yongan Yu, Xianda Du, Qingchen Hu, Jiahao Liang, Jingwei Ni, Dan Qiang, Kaiyu Huang, Grant McKenzie, Renee Sieber, Fengran Mo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05336">https://arxiv.org/abs/2510.05336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05336">https://arxiv.org/pdf/2510.05336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05336]] WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives(https://arxiv.org/abs/2510.05336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at this https URL.</li>
</ul>

<h3>Title: Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hyung Gyu Rho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05342">https://arxiv.org/abs/2510.05342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05342">https://arxiv.org/pdf/2510.05342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05342]] Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization(https://arxiv.org/abs/2510.05342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\% on High Quality data and +10.5\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.</li>
</ul>

<h3>Title: Mitigating Diffusion Model Hallucinations with Dynamic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Kostas Triaridis, Alexandros Graikos, Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05356">https://arxiv.org/abs/2510.05356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05356">https://arxiv.org/pdf/2510.05356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05356]] Mitigating Diffusion Model Hallucinations with Dynamic Guidance(https://arxiv.org/abs/2510.05356)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.</li>
</ul>

<h3>Title: Residualized Similarity for Faithfully Explainable Authorship Verification</h3>
<ul>
<li><strong>Authors: </strong>Peter Zeng, Pegah Alipoormolabashi, Jihu Mun, Gourab Dey, Nikita Soni, Niranjan Balasubramanian, Owen Rambow, H. Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05362">https://arxiv.org/abs/2510.05362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05362">https://arxiv.org/pdf/2510.05362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05362]] Residualized Similarity for Faithfully Explainable Authorship Verification(https://arxiv.org/abs/2510.05362)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Responsible use of Authorship Verification (AV) systems not only requires high accuracy but also interpretable solutions. More importantly, for systems to be used to make decisions with real-world consequences requires the model's prediction to be explainable using interpretable features that can be traced to the original texts. Neural methods achieve high accuracies, but their representations lack direct interpretability. Furthermore, LLM predictions cannot be explained faithfully -- if there is an explanation given for a prediction, it doesn't represent the reasoning process behind the model's prediction. In this paper, we introduce Residualized Similarity (RS), a novel method that supplements systems using interpretable features with a neural network to improve their performance while maintaining interpretability. Authorship verification is fundamentally a similarity task, where the goal is to measure how alike two documents are. The key idea is to use the neural network to predict a similarity residual, i.e. the error in the similarity predicted by the interpretable system. Our evaluation across four datasets shows that not only can we match the performance of state-of-the-art authorship verification models, but we can show how and to what degree the final prediction is faithful and interpretable.</li>
</ul>

<h3>Title: The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures</h3>
<ul>
<li><strong>Authors: </strong>Alexander M. Fichtl, Jeremias Bohn, Josefin Kelber, Edoardo Mosca, Georg Groh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05364">https://arxiv.org/abs/2510.05364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05364">https://arxiv.org/pdf/2510.05364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05364]] The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures(https://arxiv.org/abs/2510.05364)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have dominated sequence processing tasks for the past seven years -- most notably language modeling. However, the inherent quadratic complexity of their attention mechanism remains a significant bottleneck as context length increases. This paper surveys recent efforts to overcome this bottleneck, including advances in (sub-quadratic) attention variants, recurrent neural networks, state space models, and hybrid architectures. We critically analyze these approaches in terms of compute and memory complexity, benchmark results, and fundamental limitations to assess whether the dominance of pure-attention transformers may soon be challenged.</li>
</ul>

<h3>Title: LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiao, Gen Li, Kaiyuan Deng, Yushu Wu, Zheng Zhan, Yanzhi Wang, Xiaolong Ma, Bo Hui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05367">https://arxiv.org/abs/2510.05367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05367">https://arxiv.org/pdf/2510.05367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05367]] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation(https://arxiv.org/abs/2510.05367)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at this https URL .</li>
</ul>

<h3>Title: KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction</h3>
<ul>
<li><strong>Authors: </strong>Utkarsh Saxena, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05373">https://arxiv.org/abs/2510.05373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05373">https://arxiv.org/pdf/2510.05373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05373]] KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction(https://arxiv.org/abs/2510.05373)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quantizing the key-value (KV) cache is a promising strategy for improving the inference efficiency of large language models (LLMs). However, aggressive quantization to very low precision (e.g., 2 bits) introduces significant errors in the stored key and value tensors, which propagate through the dot-product attention mechanism and ultimately degrade generation quality. To address this, we propose KVLinC, a framework to mitigate attention errors introduced by KV cache quantization in the extreme low-precision regime. KVLinC combines a Hadamard rotation, which reduces quantization error in values, with lightweight linear correction adapters that explicitly compensate for errors introduced by quantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3 model families, KVLinC consistently matches or surpasses strong baselines while achieving higher KV-cache compression. Furthermore, we implement a custom attention kernel that results in upto 2.55x faster inference compared to Flash Attention baseline, enabling efficient long-context LLM inference.</li>
</ul>

<h3>Title: Constraint-Level Design of zkEVMs: Architectures, Trade-offs, and Evolution</h3>
<ul>
<li><strong>Authors: </strong>Yahya Hassanzadeh-Nazarabadi, Sanaz Taheri-Boshrooyeh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05376">https://arxiv.org/abs/2510.05376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05376">https://arxiv.org/pdf/2510.05376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05376]] Constraint-Level Design of zkEVMs: Architectures, Trade-offs, and Evolution(https://arxiv.org/abs/2510.05376)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Zero-knowledge Ethereum Virtual Machines (zkEVMs) must reconcile a fundamental contradiction: the Ethereum Virtual Machine was designed for transparent sequential execution, while zero-knowledge proofs require algebraic circuit representations. This survey provides the first systematic analysis of how existing major production zkEVM implementations resolve this tension through distinct constraint engineering strategies. We develop a comparative framework that maps the design space across three architectural dimensions. First, arithmetization schemes reveal stark trade-offs: R1CS requires compositional gadget libraries, PLONKish achieves elegance through custom gates that capture complex EVM opcodes in single constraints, while the homogeneous structure of AIR fundamentally mismatches the irregular instruction set of EVM. Second, dispatch mechanisms determine constraint activation patterns: selector-based systems waste trace width on inactive constraints, while ROM-based approaches trade memory lookups for execution flexibility. Third, the Type 1-4 spectrum quantifies an inescapable trade-off: the bit-level EVM compatibility of Type 1 demands significantly higher constraint complexity than the custom instruction sets of Type 4. Beyond cataloging implementations, we identify critical open problems across multiple domains: performance barriers preventing sub-second proving, absence of formal verification for constraint-to-EVM semantic equivalence, lack of standardized benchmarking frameworks, and architectural gaps in hybrid zkEVM/zkVM designs, decentralized prover coordination, privacy preservation, and interoperability.</li>
</ul>

<h3>Title: AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Xiaogeng Liu, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05379">https://arxiv.org/abs/2510.05379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05379">https://arxiv.org/pdf/2510.05379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05379]] AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling(https://arxiv.org/abs/2510.05379)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60\% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.</li>
</ul>

<h3>Title: Context Length Alone Hurts LLM Performance Despite Perfect Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu A Huerta, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05381">https://arxiv.org/abs/2510.05381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05381">https://arxiv.org/pdf/2510.05381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05381]] Context Length Alone Hurts LLM Performance Despite Perfect Retrieval(https://arxiv.org/abs/2510.05381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures -- the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one -- or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%--85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline.</li>
</ul>

<h3>Title: Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding</h3>
<ul>
<li><strong>Authors: </strong>Rohan Arni, Carlos Blanco</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05385">https://arxiv.org/abs/2510.05385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05385">https://arxiv.org/pdf/2510.05385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05385]] Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding(https://arxiv.org/abs/2510.05385)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.</li>
</ul>

<h3>Title: See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kebin Contreras, Luis Toscano-Palomino, Mauro Dalla Mura, Jorge Bacca</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05408">https://arxiv.org/abs/2510.05408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05408">https://arxiv.org/pdf/2510.05408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05408]] See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models(https://arxiv.org/abs/2510.05408)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recovering the past from present observations is an intriguing challenge with potential applications in forensics and scene analysis. Thermal imaging, operating in the infrared range, provides access to otherwise invisible information. Since humans are typically warmer (37 C -98.6 F) than their surroundings, interactions such as sitting, touching, or leaning leave residual heat traces. These fading imprints serve as passive temporal codes, allowing for the inference of recent events that exceed the capabilities of RGB cameras. This work proposes a time-reversed reconstruction framework that uses paired RGB and thermal images to recover scene states from a few seconds earlier. The proposed approach couples Visual-Language Models (VLMs) with a constrained diffusion process, where one VLM generates scene descriptions and another guides image reconstruction, ensuring semantic and structural consistency. The method is evaluated in three controlled scenarios, demonstrating the feasibility of reconstructing plausible past frames up to 120 seconds earlier, providing a first step toward time-reversed imaging from thermal traces.</li>
</ul>

<h3>Title: Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care</h3>
<ul>
<li><strong>Authors: </strong>Junyi Fan, Li Sun, Negin Ashrafi, Kamiar Alaei, Maryam Pishgar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05410">https://arxiv.org/abs/2510.05410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05410">https://arxiv.org/pdf/2510.05410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05410]] Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care(https://arxiv.org/abs/2510.05410)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Nursing documentation in intensive care units (ICUs) provides essential clinical intelligence but often suffers from inconsistent terminology, informal styles, and lack of standardization, challenges that are particularly critical in heart failure care. This study applies Direct Preference Optimization (DPO) to adapt Mistral-7B, a locally deployable language model, using 8,838 heart failure nursing notes from the MIMIC-III database and 21,210 preference pairs derived from expert-verified GPT outputs, model generations, and original notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert qualitative assessments demonstrates that DPO markedly enhances documentation quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy (+14.4 points), completeness (+14.5 points), logical consistency (+14.1 points), readability (+11.1 points), and structural clarity (+6.0 points). These results indicate that DPO can align lightweight clinical language models with expert standards, supporting privacy-preserving, AI-assisted documentation within electronic health record systems to reduce administrative burden and improve ICU patient safety.</li>
</ul>

<h3>Title: A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ziheng Geng, Jiachen Liu, Ran Cao, Lu Cheng, Haifeng Wang, Minghui Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05414">https://arxiv.org/abs/2510.05414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05414">https://arxiv.org/pdf/2510.05414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05414]] A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis(https://arxiv.org/abs/2510.05414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.</li>
</ul>

<h3>Title: Correlating Cross-Iteration Noise for DP-SGD using Model Curvature</h3>
<ul>
<li><strong>Authors: </strong>Xin Gu, Yingtai Xiao, Guanlin He, Jiamu Bai, Daniel Kifer, Kiwan Maeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05416">https://arxiv.org/abs/2510.05416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05416">https://arxiv.org/pdf/2510.05416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05416]] Correlating Cross-Iteration Noise for DP-SGD using Model Curvature(https://arxiv.org/abs/2510.05416)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differentially private stochastic gradient descent (DP-SGD) offers the promise of training deep learning models while mitigating many privacy risks. However, there is currently a large accuracy gap between DP-SGD and normal SGD training. This has resulted in different lines of research investigating orthogonal ways of improving privacy-preserving training. One such line of work, known as DP-MF, correlates the privacy noise across different iterations of stochastic gradient descent -- allowing later iterations to cancel out some of the noise added to earlier iterations. In this paper, we study how to improve this noise correlation. We propose a technique called NoiseCurve that uses model curvature, estimated from public unlabeled data, to improve the quality of this cross-iteration noise correlation. Our experiments on various datasets, models, and privacy parameters show that the noise correlations computed by NoiseCurve offer consistent and significant improvements in accuracy over the correlation scheme used by DP-MF.</li>
</ul>

<h3>Title: A Brief Note on Cryptographic Pseudonyms for Anonymous Credentials</h3>
<ul>
<li><strong>Authors: </strong>René Mayrhofer, Anja Lehmann, abhi shelat</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05419">https://arxiv.org/abs/2510.05419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05419">https://arxiv.org/pdf/2510.05419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05419]] A Brief Note on Cryptographic Pseudonyms for Anonymous Credentials(https://arxiv.org/abs/2510.05419)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>This paper describes pseudonyms for the upcoming European Identity Wallet (EUDIW) architecture from both a cryptographic and an implementation perspective. Its main goal is to provide technical insights into the achievable properties and cryptographic realizations. In particular, we (1) outline the security and privacy requirements of EUDI pseudonyms as the basis for building consensus on the cross-country decision maker level; (2) sketch an abstract cryptographic protocol that fulfills these requirements; and (3) suggest two instantiation options for the protocol sketch based on well-studied building A complete specification of the formal properties, as well as the specific set of credential issuance, provisioning, and pseudonym presentation generation is outside the scope of this paper, but is expected to follow as future work.</li>
</ul>

<h3>Title: Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shrenik Bhansali, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05421">https://arxiv.org/abs/2510.05421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05421">https://arxiv.org/pdf/2510.05421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05421]] Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding(https://arxiv.org/abs/2510.05421)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) decoding is a major latency bottleneck for large language models. Speculative decoding (SD) accelerates AR by letting a drafter propose multi-token blocks that a verifier accepts or rejects. However, many SD systems require heavy offline training or extra components. These choices raise data/compute cost and can yield brittle drafters under distribution drift. We introduce \emph{Draft, Verify, \& Improve (DVI)}, a training-aware self-speculative framework that combines inference with continual online learning. We partition an LLM into a drafter and a verifier, and during generation, verifier accept/reject decisions are converted into supervision signals and used to update the drafter head. A simple \emph{KL$\rightarrow$RL} schedule bootstraps calibration via online distillation and then adds reward-masked cross-entropy with a on-policy policy-gradient term, preserving lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\times$ wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of magnitude less data for training, and ablations show that DVI outperforms KL-only online distillation. DVI demonstrates that \emph{training-aware} self-speculation can deliver state-of-the-art, lossless speedups with minimal training overhead.</li>
</ul>

<h3>Title: Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification</h3>
<ul>
<li><strong>Authors: </strong>Yoo Yongmin, Zhang Xu, Cao Longbing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05431">https://arxiv.org/abs/2510.05431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05431">https://arxiv.org/pdf/2510.05431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05431]] Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification(https://arxiv.org/abs/2510.05431)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.</li>
</ul>

<h3>Title: Physics-Informed Machine Learning in Biomedical Science and Engineering</h3>
<ul>
<li><strong>Authors: </strong>Nazanin Ahmadi, Qianying Cao, Jay D. Humphrey, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05433">https://arxiv.org/abs/2510.05433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05433">https://arxiv.org/pdf/2510.05433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05433]] Physics-Informed Machine Learning in Biomedical Science and Engineering(https://arxiv.org/abs/2510.05433)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Physics-informed machine learning (PIML) is emerging as a potentially transformative paradigm for modeling complex biomedical systems by integrating parameterized physical laws with data-driven methods. Here, we review three main classes of PIML frameworks: physics-informed neural networks (PINNs), neural ordinary differential equations (NODEs), and neural operators (NOs), highlighting their growing role in biomedical science and engineering. We begin with PINNs, which embed governing equations into deep learning models and have been successfully applied to biosolid and biofluid mechanics, mechanobiology, and medical imaging among other areas. We then review NODEs, which offer continuous-time modeling, especially suited to dynamic physiological systems, pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful tools for learning mappings between function spaces, enabling efficient simulations across multiscale and spatially heterogeneous biological domains. Throughout, we emphasize applications where physical interpretability, data scarcity, or system complexity make conventional black-box learning insufficient. We conclude by identifying open challenges and future directions for advancing PIML in biomedical science and engineering, including issues of uncertainty quantification, generalization, and integration of PIML and large language models.</li>
</ul>

<h3>Title: Adversarial Reinforcement Learning for Large Language Model Agent Safety</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, Lukas Rutishauser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05442">https://arxiv.org/abs/2510.05442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05442">https://arxiv.org/pdf/2510.05442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05442]] Adversarial Reinforcement Learning for Large Language Model Agent Safety(https://arxiv.org/abs/2510.05442)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.</li>
</ul>

<h3>Title: SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?</h3>
<ul>
<li><strong>Authors: </strong>Yao Dou, Michel Galley, Baolin Peng, Chris Kedzie, Weixin Cai, Alan Ritter, Chris Quirk, Wei Xu, Jianfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05444">https://arxiv.org/abs/2510.05444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05444">https://arxiv.org/pdf/2510.05444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05444]] SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?(https://arxiv.org/abs/2510.05444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in interactive applications, and human evaluation remains the gold standard for assessing their performance in multi-turn conversations. Since human studies are costly, time-consuming, and hard to reproduce, recent work explores using LLMs to simulate users for automatic assistant evaluation. However, there is no benchmark or systematic study to evaluate whether these simulated users are reliable stand-ins for real users. To address this, we introduce SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two interactive tasks -- math tutoring and document creation. SimulatorArena evaluates simulators based on how closely their messages match human behavior and how well their assistant ratings align with human judgments. Experiments on various simulator methods show that simulators conditioned on user profiles, capturing traits like background and message styles, align closely with human judgments. They reach Spearman's $\rho$ of 0.7 on both tasks, providing a practical, scalable alternative to human evaluation. Using the best simulator for each task, we benchmark 18 assistants, including the latest LLMs such as GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.</li>
</ul>

<h3>Title: AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Zehong Wang, Tianyi Ma, Keerthiram Murugesan, Vincent Galassi, Chuxu Zhang, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05445">https://arxiv.org/abs/2510.05445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05445">https://arxiv.org/pdf/2510.05445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05445]] AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering(https://arxiv.org/abs/2510.05445)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and agent-based frameworks have advanced rapidly, enabling diverse applications. Yet, with the proliferation of models and agentic strategies, practitioners face substantial uncertainty in selecting the best configuration for a downstream task. Prior studies show that different agents and backbones exhibit complementary strengths, and that larger models are not always superior, underscoring the need for adaptive routing mechanisms. Existing approaches to agent routing, however, often emphasize cost efficiency while overlooking the fine-grained contextual and relational structure inherent in QA tasks. In this paper, we propose tAgentRouter, a framework that formulates multi-agent QA as a knowledge-graph-guided routing problem supervised by empirical performance signals. Specifically, we convert QA instance into a knowledge graph that jointly encodes queries, contextual entities, and agents, and then train a heterogeneous graph neural network (GNN) to propagate information across node types and produce task-aware routing distributions over agents. By leveraging soft supervision and weighted aggregation of agent outputs, AgentRouter learns principled collaboration schemes that capture the complementary strengths of diverse agents. Extensive experiments demonstrate that our framework consistently outperforms single-agent and ensemble baselines, while generalizing across benchmarks and LLM backbones. These results highlight the effectiveness and robustness of graph-supervised multi-agent routing for question answering.</li>
</ul>

<h3>Title: QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification</h3>
<ul>
<li><strong>Authors: </strong>Arpit Kapoor, Rohitash Chandra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05453">https://arxiv.org/abs/2510.05453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05453">https://arxiv.org/pdf/2510.05453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05453]] QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification(https://arxiv.org/abs/2510.05453)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Conceptual rainfall-runoff models aid hydrologists and climate scientists in modelling streamflow to inform water management practices. Recent advances in deep learning have unravelled the potential for combining hydrological models with deep learning models for better interpretability and improved predictive performance. In our previous work, we introduced DeepGR4J, which enhanced the GR4J conceptual rainfall-runoff model using a deep learning model to serve as a surrogate for the routing component. DeepGR4J had an improved rainfall-runoff prediction accuracy, particularly in arid catchments. Quantile regression models have been extensively used for quantifying uncertainty while aiding extreme value forecasting. In this paper, we extend DeepGR4J using a quantile regression-based ensemble learning framework to quantify uncertainty in streamflow prediction. We also leverage the uncertainty bounds to identify extreme flow events potentially leading to flooding. We further extend the model to multi-step streamflow predictions for uncertainty bounds. We design experiments for a detailed evaluation of the proposed framework using the CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J framework improves the predictive accuracy and uncertainty interval quality (interval score) compared to baseline deep learning models. Furthermore, we carry out flood risk evaluation using Quantile DeepGR4J, and the results demonstrate its suitability as an early warning system.</li>
</ul>

<h3>Title: AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yurun Song, Zhuoyi Yang, Ian G. Harris, Sangeetha Abdu Jyothi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05468">https://arxiv.org/abs/2510.05468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05468">https://arxiv.org/pdf/2510.05468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05468]] AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning(https://arxiv.org/abs/2510.05468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices. To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization. Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training. Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.</li>
</ul>

<h3>Title: ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Luke Thompson, Davy Guan, Dai Shi, Slade Matthews, Junbin Gao, Andi Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05482">https://arxiv.org/abs/2510.05482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05482">https://arxiv.org/pdf/2510.05482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05482]] ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics(https://arxiv.org/abs/2510.05482)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Molecular dynamics (MD) simulations underpin modern computational drug dis- covery, materials science, and biochemistry. Recent machine learning models provide high-fidelity MD predictions without the need to repeatedly solve quantum mechanical forces, enabling significant speedups over conventional pipelines. Yet many such methods typically enforce strict equivariance and rely on sequential rollouts, thus limiting their flexibility and simulation efficiency. They are also com- monly single-task, trained on individual molecules and fixed timeframes, which restricts generalization to unseen compounds and extended timesteps. To address these issues, we propose Atomistic Transformer Operator for Molecules (ATOM), a pretrained transformer neural operator for multitask molecular dynamics. ATOM adopts a quasi-equivariant design that requires no explicit molecular graph and employs a temporal attention mechanism, allowing for the accurate parallel decod- ing of multiple future states. To support operator pretraining across chemicals and timescales, we curate TG80, a large, diverse, and numerically stable MD dataset with over 2.5 million femtoseconds of trajectories across 80 compounds. ATOM achieves state-of-the-art performance on established single-task benchmarks, such as MD17, RMD17 and MD22. After multitask pretraining on TG80, ATOM shows exceptional zero-shot generalization to unseen molecules across varying time hori- zons. We believe ATOM represents a significant step toward accurate, efficient, and transferable molecular dynamics models</li>
</ul>

<h3>Title: Language Model as Planner and Formalizer under Constraints</h3>
<ul>
<li><strong>Authors: </strong>Cassie Huang, Stuti Mohan, Ziyi Yang, Stefanie Tellex, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05486">https://arxiv.org/abs/2510.05486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05486">https://arxiv.org/pdf/2510.05486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05486]] Language Model as Planner and Formalizer under Constraints(https://arxiv.org/abs/2510.05486)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs have been widely used in planning, either as planners to generate action sequences end-to-end, or as formalizers to represent the planning domain and problem in a formal language that can derive plans deterministically. However, both lines of work rely on standard benchmarks that only include generic and simplistic environmental specifications, leading to potential overestimation of the planning ability of LLMs and safety concerns in downstream tasks. We bridge this gap by augmenting widely used planning benchmarks with manually annotated, fine-grained, and rich natural language constraints spanning four formally defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages, 5 methods, and 4 datasets, we show that the introduction of constraints not only consistently halves performance, but also significantly challenges robustness to problem complexity and lexical shift.</li>
</ul>

<h3>Title: LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Zhoutong Fu, Yihan Cao, Yi-Lin Chen, Aman Lunia, Liming Dong, Neha Saraf, Ruijie Jiang, Yun Dai, Qingquan Song, Tan Wang, Guoyao Li, Derek Koh, Haichao Wei, Zhipeng Wang, Aman Gupta, Chengming Jiang, Jianqiang Shen, Liangjie Hong, Wenjing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05490">https://arxiv.org/abs/2510.05490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05490">https://arxiv.org/pdf/2510.05490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05490]] LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation(https://arxiv.org/abs/2510.05490)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved strong performance across a wide range of natural language processing tasks. However, deploying LLMs at scale for domain specific applications, such as job-person fit and explanation in job seeking platforms, introduces distinct challenges. At LinkedIn, the job person fit task requires analyzing a candidate's public profile against job requirements to produce both a fit assessment and a detailed explanation. Directly applying open source or finetuned LLMs to this task often fails to yield high quality, actionable feedback due to the complexity of the domain and the need for structured outputs. Moreover, the large size of these models leads to high inference latency and limits scalability, making them unsuitable for online use. To address these challenges, we introduce LANTERN, a novel LLM knowledge distillation framework tailored specifically for job person fit tasks. LANTERN involves modeling over multiple objectives, an encoder model for classification purpose, and a decoder model for explanation purpose. To better distill the knowledge from a strong black box teacher model to multiple downstream models, LANTERN incorporates multi level knowledge distillation that integrates both data and logit level insights. In addition to introducing the knowledge distillation framework, we share our insights on post training techniques and prompt engineering, both of which are crucial for successfully adapting LLMs to domain specific downstream tasks. Extensive experimental results demonstrate that LANTERN significantly improves task specific metrics for both job person fit and explanation. Online evaluations further confirm its effectiveness, showing measurable gains in job seeker engagement, including a 0.24\% increase in apply rate and a 0.28\% increase in qualified applications.</li>
</ul>

<h3>Title: NorMuon: Making Muon more efficient and scalable</h3>
<ul>
<li><strong>Authors: </strong>Zichong Li, Liming Liu, Chen Liang, Weizhu Chen, Tuo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05491">https://arxiv.org/abs/2510.05491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05491">https://arxiv.org/pdf/2510.05491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05491]] NorMuon: Making Muon more efficient and scalable(https://arxiv.org/abs/2510.05491)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.</li>
</ul>

<h3>Title: High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyi Huang, Nutan Sahoo, Anamika Kumari, Girish Kumar, Kexuan Cai, Shixing Cao, Yue Kang, Tian Xia, Somya Chatterjee, Nicholas Hausman, Aidan Jay, Eric S. Rosenthal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05492">https://arxiv.org/abs/2510.05492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05492">https://arxiv.org/pdf/2510.05492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05492]] High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training(https://arxiv.org/abs/2510.05492)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.</li>
</ul>

<h3>Title: Prototype-Based Dynamic Steering for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ceyhun Efe Kayan, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05498">https://arxiv.org/abs/2510.05498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05498">https://arxiv.org/pdf/2510.05498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05498]] Prototype-Based Dynamic Steering for Large Language Models(https://arxiv.org/abs/2510.05498)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite impressive breadth, LLMs still rely on explicit reasoning instructions or static, one-fits-all steering methods, leaving a gap for adaptive, instruction-free reasoning amplification. We present Prototype-Based Dynamic Steering (PDS), a test-time method that amplifies large language model (LLM) reasoning without adding or altering instructions. We introduce "reasoning prototypes" by clustering activation differences between Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden state is projected onto these prototypes to form an instance-specific steering vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently improves accuracy without fine-tuning or prompt engineering. Notably, the gains persist even when CoT is explicitly suppressed to improve cost-efficiency, indicating that the intervention strengthens latent reasoning processes rather than inducing a superficial behavioral shift. These results position dynamic, prototype-guided steering as a lightweight alternative to training-time approaches for enhancing LLM reasoning.</li>
</ul>

<h3>Title: Human Action Recognition from Point Clouds over Time</h3>
<ul>
<li><strong>Authors: </strong>James Dickens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05506">https://arxiv.org/abs/2510.05506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05506">https://arxiv.org/pdf/2510.05506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05506]] Human Action Recognition from Point Clouds over Time(https://arxiv.org/abs/2510.05506)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent research into human action recognition (HAR) has focused predominantly on skeletal action recognition and video-based methods. With the increasing availability of consumer-grade depth sensors and Lidar instruments, there is a growing opportunity to leverage dense 3D data for action recognition, to develop a third way. This paper presents a novel approach for recognizing actions from 3D videos by introducing a pipeline that segments human point clouds from the background of a scene, tracks individuals over time, and performs body part segmentation. The method supports point clouds from both depth sensors and monocular depth estimation. At the core of the proposed HAR framework is a novel backbone for 3D action recognition, which combines point-based techniques with sparse convolutional networks applied to voxel-mapped point cloud sequences. Experiments incorporate auxiliary point features including surface normals, color, infrared intensity, and body part parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D 120 dataset demonstrates that the method is competitive with existing skeletal action recognition algorithms. Moreover, combining both sensor-based and estimated depth inputs in an ensemble setup, this approach achieves 89.3% accuracy when different human subjects are considered for training and testing, outperforming previous point cloud action recognition methods.</li>
</ul>

<h3>Title: Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Saito, Takashi Matsubara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05509">https://arxiv.org/abs/2510.05509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05509">https://arxiv.org/pdf/2510.05509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05509]] Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models(https://arxiv.org/abs/2510.05509)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful deep generative models (DGMs) that generate high-fidelity, diverse content. However, unlike classical DGMs, they lack an explicit, tractable low-dimensional latent space that parameterizes the data manifold. This absence limits manifold-aware analysis and operations, such as interpolation and editing. Existing interpolation methods for diffusion models typically follow paths through high-density regions, which are not necessarily aligned with the data manifold and can yield perceptually unnatural transitions. To exploit the data manifold learned by diffusion models, we propose a novel Riemannian metric on the noise space, inspired by recent findings that the Jacobian of the score function captures the tangent spaces to the local data manifold. This metric encourages geodesics in the noise space to stay within or run parallel to the learned data manifold. Experiments on image interpolation show that our metric produces perceptually more natural and faithful transitions than existing density-based and naive baselines.</li>
</ul>

<h3>Title: CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, Ruiming Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05520">https://arxiv.org/abs/2510.05520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05520">https://arxiv.org/pdf/2510.05520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05520]] CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension(https://arxiv.org/abs/2510.05520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory -- structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.</li>
</ul>

<h3>Title: KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance</h3>
<ul>
<li><strong>Authors: </strong>Kuangshi Ai, Jonathan A. Karr Jr, Meng Jiang, Nitesh V. Chawla, Chaoli Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05524">https://arxiv.org/abs/2510.05524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05524">https://arxiv.org/pdf/2510.05524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05524]] KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance(https://arxiv.org/abs/2510.05524)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, extraction, large language model</a></li>
<li><strong>Abstract: </strong>We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge extraction and reasoning framework with large language models (LLMs) in safety-critical contexts. Using the Operations and Maintenance Intelligence (OMIn) dataset, we construct a QA benchmark spanning global sensemaking and actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and integrates it into a retrieval-augmented generation (RAG) pipeline, enabling more coherent, dataset-wide reasoning than traditional text-chunk RAG. We evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO markedly improves global sensemaking by revealing patterns and system-level insights, while text-chunk RAG remains effective for fine-grained procedural tasks requiring localized retrieval. These findings underscore the promise of KG-augmented LLMs for secure, domain-specific QA and their potential in high-stakes reasoning.</li>
</ul>

<h3>Title: Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Chen, Junyi Li, Peiran Yu, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05526">https://arxiv.org/abs/2510.05526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05526">https://arxiv.org/pdf/2510.05526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05526]] Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment(https://arxiv.org/abs/2510.05526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \textit{\textbf{C}orrupted} preference, reward \textit{\textbf{O}veroptimization}, and bias towards \textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\textbf{COV} and DPO-\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.</li>
</ul>

<h3>Title: ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization</h3>
<ul>
<li><strong>Authors: </strong>Lawrence Liu, Alexander Liu, Mengdi Wang, Tuo Zhao, Lin F. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05528">https://arxiv.org/abs/2510.05528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05528">https://arxiv.org/pdf/2510.05528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05528]] ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization(https://arxiv.org/abs/2510.05528)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy</li>
</ul>

<h3>Title: H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05529">https://arxiv.org/abs/2510.05529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05529">https://arxiv.org/pdf/2510.05529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05529]] H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference(https://arxiv.org/abs/2510.05529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive decoding in large language models (LLMs) requires caching a growing list of past key-value (KV) pairs, making long-context inference a memory-bound problem. While recent methods have explored quantizing the cache, evicting tokens, or using binary sketches for keys (e.g., Loki), these approaches often provide an incomplete solution by leaving one component (like values) uncompressed or by discarding context information. This paper introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression scheme that radically reduces memory usage without sacrificing context. H1B-KV represents each key vector using a 1-bit binary sketch, enabling hardware-friendly bitwise attention, and further compresses value vectors using 4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches full-precision performance not only on perplexity benchmarks but also on complex downstream tasks like mathematical reasoning (GSM8K), multi-task understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV significantly outperforms leading quantization (KIVI), token eviction (SparseLLM), and key-only sketching (Loki) methods in quality-per-byte, establishing it as a robust solution for deploying LLMs in memory-constrained environments.</li>
</ul>

<h3>Title: LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05530">https://arxiv.org/abs/2510.05530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05530">https://arxiv.org/pdf/2510.05530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05530]] LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability(https://arxiv.org/abs/2510.05530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) aims to adapt a pretrained model to distribution shifts using only unlabeled test data. While promising, existing methods like Tent suffer from instability and can catastrophically forget the source knowledge, especially with small batch sizes or challenging corruptions. We argue that this arises from overly deterministic updates on a complex loss surface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation (LATTA), a novel approach that regularizes adaptation through two key mechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient Langevin Dynamics (SGLD) to explore the local parameter space and escape poor local minima, and (2) a stable weight anchor that prevents the model from diverging from its robust source pre-training. This combination allows LATTA to adapt effectively without sacrificing stability. Unlike prior Bayesian TTA methods, LATTA requires no architectural changes or expensive Monte Carlo passes. We conduct extensive experiments on standard benchmarks, including Rotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that LATTA significantly outperforms existing methods, including Tent, CoTTA, and EATA, setting a new state of the art for self-supervised TTA by improving average accuracy on CIFAR-10-C by over 2% while simultaneously reducing performance variance.</li>
</ul>

<h3>Title: Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sam Sartor, Pieter Peers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05532">https://arxiv.org/abs/2510.05532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05532">https://arxiv.org/pdf/2510.05532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05532]] Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation(https://arxiv.org/abs/2510.05532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.</li>
</ul>

<h3>Title: On the Role of Difficult Prompts in Self-Play Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yao Xiao, Jung-jae Kim, Roy Ka-wei Lee, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05534">https://arxiv.org/abs/2510.05534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05534">https://arxiv.org/pdf/2510.05534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05534]] On the Role of Difficult Prompts in Self-Play Preference Optimization(https://arxiv.org/abs/2510.05534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-play preference optimization has emerged as a prominent paradigm for aligning large language models (LLMs). It typically involves a language model to generate on-policy responses for prompts and a reward model (RM) to guide the selection of chosen and rejected responses, which can be further trained with direct preference optimization (DPO). However, the role of prompts remains underexplored, despite being a core component in this pipeline. In this work, we investigate how prompts of varying difficulty influence self-play preference optimization. We first use the mean reward of $N$ sampled responses of a prompt as a proxy for its difficulty. We find that difficult prompts exhibit substantially inferior self-play optimization performance in comparison to easy prompts for language models. Moreover, incorporating difficult prompts into training fails to enhance overall performance and, in fact, leads to slight degradation compared to training on easy prompts alone. We also observe that the performance gap between difficult and easy prompts closes as the model capacity increases, suggesting that difficulty interacts with the model capacity. Building on these findings, we explore strategies to mitigate the negative effect of difficult prompts on final performance. We demonstrate that selectively removing an appropriate portion of challenging prompts enhances overall self-play performance, while also reporting failed attempts and lessons learned.</li>
</ul>

<h3>Title: Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, Dongjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05535">https://arxiv.org/abs/2510.05535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05535">https://arxiv.org/pdf/2510.05535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05535]] Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection(https://arxiv.org/abs/2510.05535)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, generative</a></li>
<li><strong>Abstract: </strong>Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: this https URL.</li>
</ul>

<h3>Title: Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work</h3>
<ul>
<li><strong>Authors: </strong>Owen Henkel, Bill Roberts, Doug Jaffe, Laurence Holt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05538">https://arxiv.org/abs/2510.05538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05538">https://arxiv.org/pdf/2510.05538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05538]] Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work(https://arxiv.org/abs/2510.05538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) raise the question of their potential for grading, analyzing, and offering feedback on handwritten student classwork. This capability would be particularly beneficial in elementary and middle-school mathematics education, where most work remains handwritten, because seeing students' full working of a problem provides valuable insights into their learning processes, but is extremely time-consuming to grade. We present two experiments investigating MLLM performance on handwritten student mathematics classwork. Experiment A examines 288 handwritten responses from Ghanaian middle school students solving arithmetic problems with objective answers. In this context, models achieved near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human educators would be unlikely to make. Experiment B evaluates 150 mathematical illustrations from American elementary students, where the drawings are the answer to the question. These tasks lack single objective answers and require sophisticated visual interpretation as well as pedagogical judgment in order to analyze and evaluate them. We attempted to separate MLLMs' visual capabilities from their pedagogical abilities by first asking them to grade the student illustrations directly, and then by augmenting the image with a detailed human description of the illustration. We found that when the models had to analyze the student illustrations directly, they struggled, achieving only k = 0.20 with ground truth scores, but when given human descriptions, their agreement levels improved dramatically to k = 0.47, which was in line with human-to-human agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic work relatively well, but still struggle to "see" student mathematical illustrations.</li>
</ul>

<h3>Title: Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM</h3>
<ul>
<li><strong>Authors: </strong>Ryan Solgi, Parsa Madinei, Jiayi Tian, Rupak Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05544">https://arxiv.org/abs/2510.05544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05544">https://arxiv.org/pdf/2510.05544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05544]] Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM(https://arxiv.org/abs/2510.05544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) and vision-language models (VLM) have achieved state-of-the-art performance, but they impose significant memory and computing challenges in deployment. We present a novel low-rank compression framework to address this challenge. First, we upper bound the change of network loss via layer-wise activation-based compression errors, filling a theoretical gap in the literature. We then formulate low-rank model compression as a bi-objective optimization and prove that a single uniform tolerance yields surrogate Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot pipeline that improves activation-aware compression via Pareto-guided rank selection and alternating least-squares implementation. We apply PGSVD to both LLM and VLM, showing better accuracy at the same compression levels and inference speedup.</li>
</ul>

<h3>Title: Critical attention scaling in long-context transformers</h3>
<ul>
<li><strong>Authors: </strong>Shi Chen, Zhengjiang Lin, Yury Polyanskiy, Philippe Rigollet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DM, math.CA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05554">https://arxiv.org/abs/2510.05554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05554">https://arxiv.org/pdf/2510.05554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05554]] Critical attention scaling in long-context transformers(https://arxiv.org/abs/2510.05554)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models scale to longer contexts, attention layers suffer from a fundamental pathology: attention scores collapse toward uniformity as context length $n$ increases, causing tokens to cluster excessively, a phenomenon known as rank-collapse. While $\textit{attention scaling}$ effectively addresses this deficiency by rescaling attention scores with a polylogarithmic factor $\beta_n$, theoretical justification for this approach remains lacking. We analyze a simplified yet tractable model that magnifies the effect of attention scaling. In this model, attention exhibits a phase transition governed by the scaling factor $\beta_n$: insufficient scaling collapses all tokens to a single direction, while excessive scaling reduces attention to identity, thereby eliminating meaningful interactions between tokens. Our main result identifies the critical scaling $\beta_n \asymp \log n$ and provides a rigorous justification for attention scaling in YaRN and Qwen, clarifying why logarithmic scaling maintains sparse, content-adaptive attention at large context lengths.</li>
</ul>

<h3>Title: Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Christopher Hoang, Mengye Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05558">https://arxiv.org/abs/2510.05558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05558">https://arxiv.org/pdf/2510.05558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05558]] Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics(https://arxiv.org/abs/2510.05558)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object recognition and motion understanding are key components of perception that complement each other. While self-supervised learning methods have shown promise in their ability to learn from unlabeled data, they have primarily focused on obtaining rich representations for either recognition or motion rather than both in tandem. On the other hand, latent dynamics modeling has been used in decision making to learn latent representations of observations and their transformations over time for control and planning tasks. In this work, we present Midway Network, a new self-supervised learning architecture that is the first to learn strong visual representations for both object recognition and motion understanding solely from natural videos, by extending latent dynamics modeling to this domain. Midway Network leverages a midway top-down path to infer motion latents between video frames, as well as a dense forward prediction objective and hierarchical structure to tackle the complex, multi-object scenes of natural videos. We demonstrate that after pretraining on two large-scale natural video datasets, Midway Network achieves strong performance on both semantic segmentation and optical flow tasks relative to prior self-supervised learning methods. We also show that Midway Network's learned dynamics can capture high-level correspondence via a novel analysis method based on forward feature perturbation.</li>
</ul>

<h3>Title: HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video</h3>
<ul>
<li><strong>Authors: </strong>Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05560">https://arxiv.org/abs/2510.05560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05560">https://arxiv.org/pdf/2510.05560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05560]] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video(https://arxiv.org/abs/2510.05560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: this https URL.</li>
</ul>

<h3>Title: Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection</h3>
<ul>
<li><strong>Authors: </strong>Sheng Xiang, Yidong Jiang, Yunting Chen, Dawei Cheng, Guoping Zhao, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05562">https://arxiv.org/abs/2510.05562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05562">https://arxiv.org/pdf/2510.05562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05562]] Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection(https://arxiv.org/abs/2510.05562)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Spoofing detection in financial trading is crucial, especially for identifying complex behaviors such as conspiracy spoofing. Traditional machine-learning approaches primarily focus on isolated node features, often overlooking the broader context of interconnected nodes. Graph-based techniques, particularly Graph Neural Networks (GNNs), have advanced the field by leveraging relational information effectively. However, in real-world spoofing detection datasets, trading behaviors exhibit dynamic, irregular patterns. Existing spoofing detection methods, though effective in some scenarios, struggle to capture the complexity of dynamic and diverse, evolving inter-node relationships. To address these challenges, we propose a novel framework called the Generative Dynamic Graph Model (GDGM), which models dynamic trading behaviors and the relationships among nodes to learn representations for conspiracy spoofing detection. Specifically, our approach incorporates the generative dynamic latent space to capture the temporal patterns and evolving market conditions. Raw trading data is first converted into time-stamped sequences. Then we model trading behaviors using the neural ordinary differential equations and gated recurrent units, to generate the representation incorporating temporal dynamics of spoofing patterns. Furthermore, pseudo-label generation and heterogeneous aggregation techniques are employed to gather relevant information and enhance the detection performance for conspiratorial spoofing behaviors. Experiments conducted on spoofing detection datasets demonstrate that our approach outperforms state-of-the-art models in detection accuracy. Additionally, our spoofing detection system has been successfully deployed in one of the largest global trading markets, further validating the practical applicability and performance of the proposed method.</li>
</ul>

<h3>Title: Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption</h3>
<ul>
<li><strong>Authors: </strong>Praneeth Vepakomma, Kaustubh Ponkshe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05581">https://arxiv.org/abs/2510.05581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05581">https://arxiv.org/pdf/2510.05581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05581]] Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption(https://arxiv.org/abs/2510.05581)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Traditional collaborative learning approaches are based on sharing of model weights between clients and a server. However, there are advantages to resource efficiency through schemes based on sharing of embeddings (activations) created from the data. Several differentially private methods were developed for sharing of weights while such mechanisms do not exist so far for sharing of embeddings. We propose Ours to learn a privacy encoding network in conjunction with a small utility generation network such that the final embeddings generated from it are equipped with formal differential privacy guarantees. These privatized embeddings are then shared with a more powerful server, that learns a post-processing that results in a higher accuracy for machine learning tasks. We show that our co-design of collaborative and private learning results in requiring only one round of privatized communication and lesser compute on the client than traditional methods. The privatized embeddings that we share from the client are agnostic to the type of model (deep learning, random forests or XGBoost) used on the server in order to process these activations to complete a task.</li>
</ul>

<h3>Title: (Token-Level) \textbf{InfoRMIA}: Stronger Membership Inference and Memorization Assessment for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiashu Tao, Reza Shokri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05582">https://arxiv.org/abs/2510.05582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05582">https://arxiv.org/pdf/2510.05582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05582]] (Token-Level) \textbf{InfoRMIA}: Stronger Membership Inference and Memorization Assessment for LLMs(https://arxiv.org/abs/2510.05582)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Machine learning models are known to leak sensitive information, as they inevitably memorize (parts of) their training data. More alarmingly, large language models (LLMs) are now trained on nearly all available data, which amplifies the magnitude of information leakage and raises serious privacy risks. Hence, it is more crucial than ever to quantify privacy risk before the release of LLMs. The standard method to quantify privacy is via membership inference attacks, where the state-of-the-art approach is the Robust Membership Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled information-theoretic formulation of membership inference. Our method consistently outperforms RMIA across benchmarks while also offering improved computational efficiency. In the second part of the paper, we identify the limitations of treating sequence-level membership inference as the gold standard for measuring leakage. We propose a new perspective for studying membership and memorization in LLMs: token-level signals and analyses. We show that a simple token-based InfoRMIA can pinpoint which tokens are memorized within generated outputs, thereby localizing leakage from the sequence level down to individual tokens, while achieving stronger sequence-level inference power on LLMs. This new scope rethinks privacy in LLMs and can lead to more targeted mitigation, such as exact unlearning.</li>
</ul>

<h3>Title: When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Arindam Chowdhury, Massimiliano Lupo Pasini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05583">https://arxiv.org/abs/2510.05583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05583">https://arxiv.org/pdf/2510.05583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05583]] When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning(https://arxiv.org/abs/2510.05583)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) are widely used as surrogates for costly experiments and first-principles simulations to study the behavior of compounds at atomistic scale, and their architectural complexity is constantly increasing to enable the modeling of complex physics. While most recent GNNs combine more traditional message passing neural networks (MPNNs) layers to model short-range interactions with more advanced graph transformers (GTs) with global attention mechanisms to model long-range interactions, it is still unclear when global attention mechanisms provide real benefits over well-tuned MPNN layers due to inconsistent implementations, features, or hyperparameter tuning. We introduce the first unified, reproducible benchmarking framework - built on HydraGNN - that enables seamless switching among four controlled model classes: MPNN, MPNN with chemistry/topology encoders, GPS-style hybrids of MPNN with global attention, and fully fused local - global models with encoders. Using seven diverse open-source datasets for benchmarking across regression and classification tasks, we systematically isolate the contributions of message passing, global attention, and encoder-based feature augmentation. Our study shows that encoder-augmented MPNNs form a robust baseline, while fused local-global models yield the clearest benefits for properties governed by long-range interaction effects. We further quantify the accuracy - compute trade-offs of attention, reporting its overhead in memory. Together, these results establish the first controlled evaluation of global attention in atomistic graph learning and provide a reproducible testbed for future model development.</li>
</ul>

<h3>Title: Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising</h3>
<ul>
<li><strong>Authors: </strong>Kangjia Yan, Chenxi Liu, Hao Miao, Xinle Wu, Yan Zhao, Chenjuan Guo, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05589">https://arxiv.org/abs/2510.05589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05589">https://arxiv.org/pdf/2510.05589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05589]] Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising(https://arxiv.org/abs/2510.05589)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.</li>
</ul>

<h3>Title: Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05593">https://arxiv.org/abs/2510.05593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05593">https://arxiv.org/pdf/2510.05593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05593]] Improving Chain-of-Thought Efficiency for Autoregressive Image Generation(https://arxiv.org/abs/2510.05593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.</li>
</ul>

<h3>Title: AutoPentester: An LLM Agent-based Framework for Automated Pentesting</h3>
<ul>
<li><strong>Authors: </strong>Yasod Ginige, Akila Niroshan, Sajal Jain, Suranga Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05605">https://arxiv.org/abs/2510.05605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05605">https://arxiv.org/pdf/2510.05605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05605]] AutoPentester: An LLM Agent-based Framework for Automated Pentesting(https://arxiv.org/abs/2510.05605)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.</li>
</ul>

<h3>Title: A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shuzheng Si, Haozhe Zhao, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05608">https://arxiv.org/abs/2510.05608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05608">https://arxiv.org/pdf/2510.05608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05608]] A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks(https://arxiv.org/abs/2510.05608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agent's planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution.</li>
</ul>

<h3>Title: HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Junwen Chen, Peilin Xiong, Keiji Yanai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05609">https://arxiv.org/abs/2510.05609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05609">https://arxiv.org/pdf/2510.05609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05609]] HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection(https://arxiv.org/abs/2510.05609)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at this https URL.</li>
</ul>

<h3>Title: Efficient Conditional Generation on Scale-based Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Tao Huang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05610">https://arxiv.org/abs/2510.05610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05610">https://arxiv.org/pdf/2510.05610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05610]] Efficient Conditional Generation on Scale-based Visual Autoregressive Models(https://arxiv.org/abs/2510.05610)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.</li>
</ul>

<h3>Title: MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wei-Chieh Huang, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05611">https://arxiv.org/abs/2510.05611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05611">https://arxiv.org/pdf/2510.05611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05611]] MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction(https://arxiv.org/abs/2510.05611)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce \textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.</li>
</ul>

<h3>Title: PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Meng, Qichao Wang, Zhiyang Dou, Zixing Song, Zhipeng Zhou, Irwin King, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05613">https://arxiv.org/abs/2510.05613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05613">https://arxiv.org/pdf/2510.05613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05613]] PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction(https://arxiv.org/abs/2510.05613)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.</li>
</ul>

<h3>Title: TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guangrong Wan, Jun liu, Tang tang, Lianghao Shi, Wenjun Luo, TingTing Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05615">https://arxiv.org/abs/2510.05615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05615">https://arxiv.org/pdf/2510.05615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05615]] TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation(https://arxiv.org/abs/2510.05615)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tear film break-up (TFBU) analysis is critical for diagnosing dry eye syndrome, but automated TFBU segmentation remains challenging due to the lack of annotated datasets and integrated solutions. This paper introduces the Tear Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task tear film analysis, comprising 15 high-resolution videos (totaling 6,247 frames) annotated with three vision tasks: frame-level classification ('clear', 'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area segmentation. Leveraging this dataset, we first propose TF-Net, a novel and efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini backbone with re-parameterization techniques and an enhanced feature pyramid network to achieve a favorable balance between accuracy and computational efficiency for real-time clinical applications. We further establish benchmark performance on the TFM segmentation subset by comparing TF-Net against several state-of-the-art medical image segmentation models. Furthermore, we design TF-Collab, a novel integrated real-time pipeline that synergistically leverages models trained on all three tasks of the TFM dataset. By sequentially orchestrating frame classification for BUT determination, pupil region localization for input standardization, and TFBU segmentation, TF-Collab fully automates the analysis. Experimental results demonstrate the effectiveness of the proposed TF-Net and TF-Collab, providing a foundation for future research in ocular surface diagnostics. Our code and the TFM datasets are available at this https URL</li>
</ul>

<h3>Title: InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Salihu Yusuf, Iffanice Houndayi, Rym Oualha, Mohamed Aziz Cherif, Kobby Panford-Quainoo, Arnu Pretorius</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05617">https://arxiv.org/abs/2510.05617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05617">https://arxiv.org/pdf/2510.05617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05617]] InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment(https://arxiv.org/abs/2510.05617)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-access multispectral imagery from missions like Landsat 8-9 and Sentinel-2 has fueled the development of geospatial foundation models (GFMs) for humanitarian and environmental applications. Yet, their deployment remains limited by (i) the absence of automated geospatial data pipelines and (ii) the large size of fine-tuned models. Existing GFMs lack workflows for processing raw satellite imagery, and downstream adaptations often retain the full complexity of the original encoder. We present InstaGeo, an open-source, end-to-end framework that addresses these challenges by integrating: (1) automated data curation to transform raw imagery into model-ready datasets; (2) task-specific model distillation to derive compact, compute-efficient models; and (3) seamless deployment as interactive web-map applications. Using InstaGeo, we reproduced datasets from three published studies and trained models with marginal mIoU differences of -0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for desert locust prediction. The distilled models are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal accuracy loss. Leveraging InstaGeo's streamlined data pipeline, we also curated a larger crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp improvement over prior baselines. Moreover, InstaGeo enables users to progress from raw data to model deployment within a single working day. By unifying data preparation, model compression, and deployment, InstaGeo transforms research-grade GFMs into practical, low-carbon tools for real-time, large-scale Earth observation. This approach shifts geospatial AI toward data quality and application-driven innovation. Source code, datasets, and model checkpoints are available at: this https URL</li>
</ul>

<h3>Title: Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Sara Mandelli, Diego Vila-Portela, David Vázquez-Padín, Paolo Bestagini, Fernando Pérez-González</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05633">https://arxiv.org/abs/2510.05633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05633">https://arxiv.org/pdf/2510.05633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05633]] Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection(https://arxiv.org/abs/2510.05633)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.</li>
</ul>

<h3>Title: Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation</h3>
<ul>
<li><strong>Authors: </strong>Saja Al-Dabet, Sherzod Turaev, Nazar Zaki, Arif O. Khan, Luai Eldweik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05649">https://arxiv.org/abs/2510.05649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05649">https://arxiv.org/pdf/2510.05649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05649]] Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation(https://arxiv.org/abs/2510.05649)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that arises from ocular misalignment conditions, such as strabismus, enabling patients to reduce diplopia and preserve binocular vision. Early diagnosis minimizes morbidity and secondary complications such as facial asymmetry; however, current clinical assessments remain largely subjective and are further complicated by incomplete medical records. This study addresses both challenges through two complementary deep learning frameworks. First, AHP-CADNet is a multi-level attention fusion framework for automated diagnosis that integrates ocular landmarks, head pose features, and structured clinical attributes to generate interpretable predictions. Second, a curriculum learning-based imputation framework is designed to mitigate missing data by progressively leveraging structured variables and unstructured clinical notes to enhance diagnostic robustness under realistic data conditions. Evaluation on the PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet achieves 96.9-99.0 percent accuracy across classification tasks and low prediction errors for continuous variables, with MAE ranging from 0.103 to 0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy across all clinical variables (93.46-99.78 percent with PubMedBERT), with clinical dependency modeling yielding significant improvements (p < 0.001). These findings confirm the effectiveness of both frameworks for automated diagnosis and recovery from missing data in clinical settings.</li>
</ul>

<h3>Title: EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario</h3>
<ul>
<li><strong>Authors: </strong>Yiping Ma, Shiyu Hu, Buyuan Zhu, Yipei Wang, Yaxuan Kang, Shiqing Liu, Kang Hao Cheong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05650">https://arxiv.org/abs/2510.05650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05650">https://arxiv.org/pdf/2510.05650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05650]] EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario(https://arxiv.org/abs/2510.05650)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reproducing cognitive development, group interaction, and long-term evolution in virtual classrooms remains a core challenge for educational AI, as real classrooms integrate open-ended cognition, dynamic social interaction, affective factors, and multi-session development rarely captured together. Existing approaches mostly focus on short-term or single-agent settings, limiting systematic study of classroom complexity and cross-task reuse. We present EduVerse, the first user-defined multi-agent simulation space that supports environment, agent, and session customization. A distinctive human-in-the-loop interface further allows real users to join the space. Built on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse ensures individual consistency, authentic interaction, and longitudinal adaptation in cognition, emotion, and behavior-reproducing realistic classroom dynamics with seamless human-agent integration. We validate EduVerse in middle-school Chinese classes across three text genres, environments, and multiple sessions. Results show: (1) Instructional alignment: simulated IRF rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating pedagogical realism; (2) Group interaction and role differentiation: network density (0.27-0.40) with about one-third of peer links realized, while human-agent tasks indicate a balance between individual variability and instructional stability; (3) Cross-session evolution: the positive transition rate R+ increase by 11.7% on average, capturing longitudinal shifts in behavior, emotion, and cognition and revealing structured learning trajectories. Overall, EduVerse balances realism, reproducibility, and interpretability, providing a scalable platform for educational AI. The system will be open-sourced to foster cross-disciplinary research.</li>
</ul>

<h3>Title: A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer</h3>
<ul>
<li><strong>Authors: </strong>Anwen Lu, Mingxin Liu, Yiping Jiao, Hongyi Gong, Geyang Xu, Jun Chen, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05657">https://arxiv.org/abs/2510.05657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05657">https://arxiv.org/pdf/2510.05657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05657]] A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer(https://arxiv.org/abs/2510.05657)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Primary liver malignancies are widely recognized as the most heterogeneous and prognostically diverse cancers of the digestive system. Among these, hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge as the two principal histological subtypes, demonstrating significantly greater complexity in tissue morphology and cellular architecture than other common tumors. The intricate representation of features in Whole Slide Images (WSIs) encompasses abundant crucial information for liver cancer histological subtyping, regarding hierarchical pyramid structure, tumor microenvironment (TME), and geometric representation. However, recent approaches have not adequately exploited these indispensable effective descriptors, resulting in a limited understanding of histological representation and suboptimal subtyping performance. To mitigate these limitations, ARGUS is proposed to advance histological subtyping in liver cancer by capturing the macro-meso-micro hierarchical information within the TME. Specifically, we first construct a micro-geometry feature to represent fine-grained cell-level pattern via a geometric structure across nuclei, thereby providing a more refined and precise perspective for delineating pathological images. Then, a Hierarchical Field-of-Views (FoVs) Alignment module is designed to model macro- and meso-level hierarchical interactions inherent in WSIs. Finally, the augmented micro-geometry and FoVs features are fused into a joint representation via present Geometry Prior Guided Fusion strategy for modeling holistic phenotype interactions. Extensive experiments on public and private cohorts demonstrate that our ARGUS achieves state-of-the-art (SOTA) performance in histological subtyping of liver cancer, which provide an effective diagnostic tool for primary liver malignancies in clinical practice.</li>
</ul>

<h3>Title: Teleportraits: Training-Free People Insertion into Any Scene</h3>
<ul>
<li><strong>Authors: </strong>Jialu Gao, K J Joseph, Fernando De La Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05660">https://arxiv.org/abs/2510.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05660">https://arxiv.org/pdf/2510.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05660]] Teleportraits: Training-Free People Insertion into Any Scene(https://arxiv.org/abs/2510.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The task of realistically inserting a human from a reference image into a background scene is highly challenging, requiring the model to (1) determine the correct location and poses of the person and (2) perform high-quality personalization conditioned on the background. Previous approaches often treat them as separate problems, overlooking their interconnections, and typically rely on training to achieve high performance. In this work, we introduce a unified training-free pipeline that leverages pre-trained text-to-image diffusion models. We show that diffusion models inherently possess the knowledge to place people in complex scenes without requiring task-specific training. By combining inversion techniques with classifier-free guidance, our method achieves affordance-aware global editing, seamlessly inserting people into scenes. Furthermore, our proposed mask-guided self-attention mechanism ensures high-quality personalization, preserving the subject's identity, clothing, and body features from just a single reference image. To the best of our knowledge, we are the first to perform realistic human insertions into scenes in a training-free manner and achieve state-of-the-art results in diverse composite scene images with excellent identity preservation in backgrounds and subjects.</li>
</ul>

<h3>Title: When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gonzálbez-Biosca, Josep Cabacas-Maso, Carles Ventura, Ismael Benito-Altamirano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05661">https://arxiv.org/abs/2510.05661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05661">https://arxiv.org/pdf/2510.05661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05661]] When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach(https://arxiv.org/abs/2510.05661)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.</li>
</ul>

<h3>Title: Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis</h3>
<ul>
<li><strong>Authors: </strong>M.Torrente, A.Follador, A.Calcante, P. Casati, R. Oberti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05668">https://arxiv.org/abs/2510.05668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05668">https://arxiv.org/pdf/2510.05668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05668]] Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis(https://arxiv.org/abs/2510.05668)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The study investigates the effects of R. solani inoculation on the germination and early development of Lactuca sativa L. seeds using a low-cost, image-based monitoring system. Multiple cameras were deployed to continuously capture images of the germination process in both infected and control groups. The objective was to assess the impact of the pathogen by analyzing germination dynamics and growth over time. To achieve this, a novel image analysis pipeline was developed. The algorithm integrates both morphological and spatial features to identify and quantify individual seedlings, even under complex conditions where traditional image analyses fails. A key innovation of the method lies in its temporal integration: each analysis step considers not only the current status but also their developmental across prior time points. This approach enables robust discrimination of individual seedlings, especially when overlapping leaves significantly hinder object separation. The method demonstrated high accuracy in seedling counting and vigor assessment, even in challenging scenarios characterized by dense and intertwined growth. Results confirm that R. solani infection significantly reduces germination rates and early seedling vigor. The study also validates the feasibility of combining low-cost imaging hardware with advanced computational tools to obtain phenotyping data in a non-destructive and scalable manner. The temporal integration enabled accurate quantification of germinated seeds and precise determination of seedling emergence timing. This approach proved particularly effective in later stages of the experiment, where conventional segmentation techniques failed due to overlapping or intertwined seedlings, making accurate counting. The method achieved a coefficient of determination of 0.98 and a root mean square error (RMSE) of 1.12, demonstrating its robustness and reliability.</li>
</ul>

<h3>Title: Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models</h3>
<ul>
<li><strong>Authors: </strong>David Debot, Giuseppe Marra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05670">https://arxiv.org/abs/2510.05670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05670">https://arxiv.org/pdf/2510.05670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05670]] Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models(https://arxiv.org/abs/2510.05670)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.</li>
</ul>

<h3>Title: Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Jike Zhong, Yuxiang Lai, Xiaofeng Yang, Konstantinos Psounis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05674">https://arxiv.org/abs/2510.05674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05674">https://arxiv.org/pdf/2510.05674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05674]] Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension(https://arxiv.org/abs/2510.05674)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in language modeling have witnessed the rise of highly desirable emergent capabilities, such as reasoning and in-context learning. However, vision models have yet to exhibit comparable progress in these areas. In this paper, we argue that this gap could stem from the lack of semantic and contextual guidance in current vision transformer (ViT) training schemes, and such a gap can be narrowed through the design of a semantic-grounded objective. Specifically, we notice that individual words in natural language are inherently semantic, and modeling directly on word tokens naturally learns a realistic distribution. In contrast, ViTs rely on spatial patchification, which inevitably lacks semantic information. To bridge this gap, we propose to directly model "object" as the visual equivalence of "word," pushing the model to learn the global context and semantics among visual elements. We investigate our hypotheses via masked image modeling (MIM), a framework where our approach can be readily tested by applying masks to visual objects rather than random patches. Considerable evidence from qualitative and quantitative evaluations reveals a key finding: object-level representation alone helps to learn a real-world distribution, whereas pixel-averaging shortcuts are often learned without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning and contextual understanding gained with this simple objective. We hope our study highlights the effectiveness of object-level encoding and provides a plausible direction for developing stronger vision encoders and tokenizers. Code and model will be publicly released. Keywords: Semantic Visual Tokenizer, Vision Reasoning, In-context Learning, Multimodal Reasoning</li>
</ul>

<h3>Title: Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection</h3>
<ul>
<li><strong>Authors: </strong>Félix Vandervorst, Bruno Deprez, Wouter Verbeke, Tim Verdonck</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05676">https://arxiv.org/abs/2510.05676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05676">https://arxiv.org/pdf/2510.05676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05676]] Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection(https://arxiv.org/abs/2510.05676)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Graph-based methods are becoming increasingly popular in machine learning due to their ability to model complex data and relations. Insurance fraud is a prime use case, since false claims are often the result of organised criminals that stage accidents or the same persons filing erroneous claims on multiple policies. One challenge is that graph-based approaches struggle to find meaningful representations of the data because of the high class imbalance present in fraud data. Another is that insurance networks are heterogeneous and dynamic, given the changing relations among people, companies and policies. That is why gradient boosted tree approaches on tabular data still dominate the field. Therefore, we present a novel inductive graph gradient boosting machine (G-GBM) for supervised learning on heterogeneous and dynamic graphs. We show that our estimator competes with popular graph neural network approaches in an experiment using a variety of simulated random graphs. We demonstrate the power of G-GBM for insurance fraud detection using an open-source and a real-world, proprietary dataset. Given that the backbone model is a gradient boosting forest, we apply established explainability methods to gain better insights into the predictions made by G-GBM.</li>
</ul>

<h3>Title: Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05678">https://arxiv.org/abs/2510.05678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05678">https://arxiv.org/pdf/2510.05678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05678]] Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models(https://arxiv.org/abs/2510.05678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.</li>
</ul>

<h3>Title: QGraphLIME - Explaining Quantum Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Haribandhu Jena, Jyotirmaya Shivottam, Subhankar Mishra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05683">https://arxiv.org/abs/2510.05683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05683">https://arxiv.org/pdf/2510.05683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05683]] QGraphLIME - Explaining Quantum Graph Neural Networks(https://arxiv.org/abs/2510.05683)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Quantum graph neural networks offer a powerful paradigm for learning on graph-structured data, yet their explainability is complicated by measurement-induced stochasticity and the combinatorial nature of graph structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a model-agnostic, post-hoc framework that treats model explanations as distributions over local surrogates fit on structure-preserving perturbations of a graph. By aggregating surrogate attributions together with their dispersion, QGraphLIME yields uncertainty-aware node and edge importance rankings for quantum graph models. The framework further provides a distribution-free, finite-sample guarantee on the size of the surrogate ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of the induced distribution of a binary class probability at target accuracy and confidence under standard independence assumptions. Empirical studies on controlled synthetic graphs with known ground truth demonstrate accurate and stable explanations, with ablations showing clear benefits of nonlinear surrogate modeling and highlighting sensitivity to perturbation design. Collectively, these results establish a principled, uncertainty-aware, and structure-sensitive approach to explaining quantum graph neural networks, and lay the groundwork for scaling to broader architectures and real-world datasets, as quantum resources mature. Code is available at this https URL.</li>
</ul>

<h3>Title: DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision</h3>
<ul>
<li><strong>Authors: </strong>Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yi Wu, Yao Hu, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05691">https://arxiv.org/abs/2510.05691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05691">https://arxiv.org/pdf/2510.05691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05691]] DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision(https://arxiv.org/abs/2510.05691)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing capability for complex tasks through dynamic retrieval and adaptive workflows. Recent advances (e.g., Search-R1) have shown that outcome-supervised reinforcement learning demonstrate strong performance. However, this approach still suffers from inefficient exploration, sparse reward signals, and ambiguous global reward feedback. To address these challenges, we propose DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating decision-making and execution, while introducing an efficient pruning strategy to optimize data expansion. Through comprehensive process-level policy optimization, DecEx-RAG significantly enhances the autonomous task decomposition, dynamic retrieval, and high-quality answer generation capabilities of large language models (LLMs). Experiments show that DecEx-RAG achieves an average absolute performance improvement of $6.2\%$ across six datasets, significantly outperforming existing baselines. Moreover, the pruning strategy improves data construction efficiency by nearly $6 \times$, providing an efficient solution for process-supervised RAG training. The code is available at this https URL.</li>
</ul>

<h3>Title: Membership Inference Attacks on Tokenizers of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Meng Tong, Yuntao Du, Kejiang Chen, Weiming Zhang, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05699">https://arxiv.org/abs/2510.05699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05699">https://arxiv.org/pdf/2510.05699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05699]] Membership Inference Attacks on Tokenizers of Large Language Models(https://arxiv.org/abs/2510.05699)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.</li>
</ul>

<h3>Title: Primal-Dual Direct Preference Optimization for Constrained LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yihan Du, Seo Taek Kong, R. Srikant</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05703">https://arxiv.org/abs/2510.05703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05703">https://arxiv.org/pdf/2510.05703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05703]] Primal-Dual Direct Preference Optimization for Constrained LLM Alignment(https://arxiv.org/abs/2510.05703)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread application of Large Language Models (LLMs) imposes increasing demands on safety, such as reducing harmful content and fake information, and avoiding certain forbidden tokens due to rules and laws. While there have been several recent works studying safe alignment of LLMs, these works either require the training of reward and cost models and incur high memory and computational costs, or need prior knowledge about the optimal solution. Motivated by this fact, we study the problem of constrained alignment in LLMs, i.e., maximizing the output reward while restricting the cost due to potentially unsafe content to stay below a threshold. For this problem, we propose a novel primal-dual DPO approach, which first trains a model using standard DPO on reward preference data to provide reward information, and then adopts a rearranged Lagrangian DPO objective utilizing the provided reward information to fine-tune LLMs on cost preference data. Our approach significantly reduces memory and computational costs, and does not require extra prior knowledge. Moreover, we establish rigorous theoretical guarantees on the suboptimality and constraint violation of the output policy. We also extend our approach to an online data setting by incorporating exploration bonuses, which enables our approach to explore uncovered prompt-response space, and then provide theoretical results that get rid of the dependence on preference data coverage. Experimental results on the widely-used preference dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling</h3>
<ul>
<li><strong>Authors: </strong>Mary Llewellyn, Annie Gray, Josh Collyer, Michael Harries</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05709">https://arxiv.org/abs/2510.05709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05709">https://arxiv.org/pdf/2510.05709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05709]] Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling(https://arxiv.org/abs/2510.05709)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.</li>
</ul>

<h3>Title: AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shihao Zhu, Bohan Cao, Ziheng Ouyang, Zhen Li, Peng-Tao Jiang, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05715">https://arxiv.org/abs/2510.05715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05715">https://arxiv.org/pdf/2510.05715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05715]] AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models(https://arxiv.org/abs/2510.05715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.</li>
</ul>

<h3>Title: DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Hedi Zisling, Ilan Naiman, Nimrod Berman, Supasorn Suwajanakorn, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05717">https://arxiv.org/abs/2510.05717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05717">https://arxiv.org/pdf/2510.05717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05717]] DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities(https://arxiv.org/abs/2510.05717)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unsupervised representation learning, particularly sequential disentanglement, aims to separate static and dynamic factors of variation in data without relying on labels. This remains a challenging problem, as existing approaches based on variational autoencoders and generative adversarial networks often rely on multiple loss terms, complicating the optimization process. Furthermore, sequential disentanglement methods face challenges when applied to real-world data, and there is currently no established evaluation protocol for assessing their performance in such settings. Recently, diffusion models have emerged as state-of-the-art generative models, but no theoretical formalization exists for their application to sequential disentanglement. In this work, we introduce the Diffusion Sequential Disentanglement Autoencoder (DiffSDA), a novel, modal-agnostic framework effective across diverse real-world data modalities, including time series, video, and audio. DiffSDA leverages a new probabilistic modeling, latent diffusion, and efficient samplers, while incorporating a challenging evaluation protocol for rigorous testing. Our experiments on diverse real-world benchmarks demonstrate that DiffSDA outperforms recent state-of-the-art methods in sequential disentanglement.</li>
</ul>

<h3>Title: Data Factory with Minimal Human Effort Using VLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaojiao Ye, Jiaxing Zhong, Qian Xie, Yuzhou Zhou, Niki Trigoni, Andrew Markham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05722">https://arxiv.org/abs/2510.05722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05722">https://arxiv.org/pdf/2510.05722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05722]] Data Factory with Minimal Human Effort Using VLMs(https://arxiv.org/abs/2510.05722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.</li>
</ul>

<h3>Title: Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies</h3>
<ul>
<li><strong>Authors: </strong>Chunsan Hong, Seonho An, Min-Soo Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05725">https://arxiv.org/abs/2510.05725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05725">https://arxiv.org/pdf/2510.05725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05725]] Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies(https://arxiv.org/abs/2510.05725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.</li>
</ul>

<h3>Title: Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</h3>
<ul>
<li><strong>Authors: </strong>Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05740">https://arxiv.org/abs/2510.05740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05740">https://arxiv.org/pdf/2510.05740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05740]] Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect(https://arxiv.org/abs/2510.05740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at this http URL</li>
</ul>

<h3>Title: Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities</h3>
<ul>
<li><strong>Authors: </strong>Liza Fretel, Baptiste Cecconi, Laura Debisschop</a></li>
<li><strong>Subjects: </strong>cs.CL, astro-ph.IM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05744">https://arxiv.org/abs/2510.05744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05744">https://arxiv.org/pdf/2510.05744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05744]] Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities(https://arxiv.org/abs/2510.05744)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This ongoing work focuses on the development of a methodology for generating a multi-source mapping of astronomical observation facilities. To compare two entities, we compute scores with adaptable criteria and Natural Language Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches, and surface approaches) to map entities extracted from eight semantic artifacts, including Wikidata and astronomy-oriented resources. We utilize every property available, such as labels, definitions, descriptions, external identifiers, and more domain-specific properties, such as the observation wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a Large Language Model (LLM) to accept or reject a mapping suggestion and provide a justification, ensuring the plausibility and FAIRness of the validated synonym pairs. The resulting mapping is composed of multi-source synonym sets providing only one standardized label per entity. Those mappings will be used to feed our Name Resolver API and will be integrated into the International Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro platform.</li>
</ul>

<h3>Title: Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches</h3>
<ul>
<li><strong>Authors: </strong>Hachem Madmoun, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05748">https://arxiv.org/abs/2510.05748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05748">https://arxiv.org/pdf/2510.05748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05748]] Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches(https://arxiv.org/abs/2510.05748)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases cooperation from 0% to 48.3%, demonstrating communication as a robust coordination mechanism. In contrast, we find that curriculum learning is highly sensitive to design choices: our pedagogical curriculum through progressively complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game with Punishment. Qualitative analysis reveals that curricula emphasizing defection-equilibrium games can induce "learned pessimism" in agents. These findings suggest that for coordination problems, simple communication protocols may be more reliable than experience-based training, and that curriculum design for social dilemmas requires careful attention to the strategic lessons embedded in game sequences.</li>
</ul>

<h3>Title: Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yang, Xuejiao Zhao, Zhiqi Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05750">https://arxiv.org/abs/2510.05750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05750">https://arxiv.org/pdf/2510.05750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05750]] Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective(https://arxiv.org/abs/2510.05750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have achieved remarkable success in node classification. Building on this progress, heterogeneous graph neural networks (HGNNs) integrate relation types and node and edge semantics to leverage heterogeneous information. Causal analysis for HGNNs is advancing rapidly, aiming to separate genuine causal effects from spurious correlations. However, whether HGNNs are intrinsically effective remains underexamined, and most studies implicitly assume rather than establish this effectiveness. In this work, we examine HGNNs from two perspectives: model architecture and heterogeneous information. We conduct a systematic reproduction across 21 datasets and 20 baselines, complemented by comprehensive hyperparameter retuning. To further disentangle the source of performance gains, we develop a causal effect estimation framework that constructs and evaluates candidate factors under standard assumptions through factual and counterfactual analyses, with robustness validated via minimal sufficient adjustment sets, cross-method consistency checks, and sensitivity analyses. Our results lead to two conclusions. First, model architecture and complexity have no causal effect on performance. Second, heterogeneous information exerts a positive causal effect by increasing homophily and local-global distribution discrepancy, which makes node classes more distinguishable. The implementation is publicly available at this https URL.</li>
</ul>

<h3>Title: ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05752">https://arxiv.org/abs/2510.05752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05752">https://arxiv.org/pdf/2510.05752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05752]] ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving(https://arxiv.org/abs/2510.05752)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The manual annotation of outdoor LiDAR point clouds for instance segmentation is extremely costly and time-consuming. Current methods attempt to reduce this burden but still rely on some form of human labeling. To completely eliminate this dependency, we introduce ALISE, a novel framework that performs LiDAR instance segmentation without any annotations. The central challenge is to generate high-quality pseudo-labels in a fully unsupervised manner. Our approach starts by employing Vision Foundation Models (VFMs), guided by text and images, to produce initial pseudo-labels. We then refine these labels through a dedicated spatio-temporal voting module, which combines 2D and 3D semantics for both offline and online optimization. To achieve superior feature learning, we further introduce two forms of semantic supervision: a set of 2D prior-based losses that inject visual knowledge into the 3D network, and a novel prototype-based contrastive loss that builds a discriminative feature space by exploiting 3D semantic consistency. This comprehensive design results in significant performance gains, establishing a new state-of-the-art for unsupervised 3D instance segmentation. Remarkably, our approach even outperforms MWSIS, a method that operates with supervision from ground-truth (GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).</li>
</ul>

<h3>Title: Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, Antti Honkela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05753">https://arxiv.org/abs/2510.05753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05753">https://arxiv.org/pdf/2510.05753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05753]] Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning(https://arxiv.org/abs/2510.05753)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>With the emergence of powerful large-scale foundation models, the training paradigm is increasingly shifting from from-scratch training to transfer learning. This enables high utility training with small, domain-specific datasets typical in sensitive this http URL inference attacks (MIAs) provide an empirical estimate of the privacy leakage by machine learning models. Yet, prior assessments of MIAs against models fine-tuned with transfer learning rely on a small subset of possible attacks. We address this by comparing performance of diverse MIAs in transfer learning settings to help practitioners identify the most efficient attacks for privacy risk evaluation. We find that attack efficacy decreases with the increase in training data for score-based MIAs. We find that there is no one MIA which captures all privacy risks in models trained with transfer learning. While the Likelihood Ratio Attack (LiRA) demonstrates superior performance across most experimental scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against models fine-tuned on PatchCamelyon dataset in high data regime.</li>
</ul>

<h3>Title: OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search</h3>
<ul>
<li><strong>Authors: </strong>Zexin Zheng, Huangyu Dai, Lingtao Mao, Xinyu Sun, Zihan Liang, Ben Chen, Yuqing Ding, Chenyi Lei, Wenwu Ou, Han Li, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05759">https://arxiv.org/abs/2510.05759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05759">https://arxiv.org/pdf/2510.05759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05759]] OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search(https://arxiv.org/abs/2510.05759)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Traditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.</li>
</ul>

<h3>Title: A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data</h3>
<ul>
<li><strong>Authors: </strong>Gianmarco Perantoni, Lorenzo Bruzzone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05760">https://arxiv.org/abs/2510.05760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05760">https://arxiv.org/pdf/2510.05760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05760]] A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data(https://arxiv.org/abs/2510.05760)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has gained broad interest in remote sensing image scene classification thanks to the effectiveness of deep neural networks in extracting the semantics from complex data. However, deep networks require large amounts of training samples to obtain good generalization capabilities and are sensitive to errors in the training labels. This is a problem in remote sensing since highly reliable labels can be obtained at high costs and in limited amount. However, many sources of less reliable labeled data are available, e.g., obsolete digital maps. In order to train deep networks with larger datasets, we propose both the combination of single or multiple weak sources of labeled data with a small but reliable dataset to generate multisource labeled datasets and a novel training strategy where the reliability of each source is taken in consideration. This is done by exploiting the transition matrices describing the statistics of the errors of each source. The transition matrices are embedded into the labels and used during the training process to weigh each label according to the related source. The proposed method acts as a weighting scheme at gradient level, where each instance contributes with different weights to the optimization of different classes. The effectiveness of the proposed method is validated by experiments on different datasets. The results proved the robustness and capability of leveraging on unreliable source of labels of the proposed method.</li>
</ul>

<h3>Title: New Insights into Involutory and Orthogonal MDS Matrices</h3>
<ul>
<li><strong>Authors: </strong>Yogesh Kumar, Susanta Samanta, Atul Gaur</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05766">https://arxiv.org/abs/2510.05766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05766">https://arxiv.org/pdf/2510.05766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05766]] New Insights into Involutory and Orthogonal MDS Matrices(https://arxiv.org/abs/2510.05766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>MDS matrices play a critical role in the design of diffusion layers for block ciphers and hash functions due to their optimal branch number. Involutory and orthogonal MDS matrices offer additional benefits by allowing identical or nearly identical circuitry for both encryption and decryption, leading to equivalent implementation costs for both processes. These properties have been further generalized through the notions of semi-involutory and semi-orthogonal matrices. Specifically, we establish nontrivial interconnections between semi-involutory and involutory matrices, as well as between semi-orthogonal and orthogonal matrices. Exploiting these relationships, we show that the number of semi-involutory MDS matrices can be directly derived from the number of involutory MDS matrices, and vice versa. A similar correspondence holds for semi-orthogonal and orthogonal MDS matrices. We also examine the intersection of these classes and show that the number of $3 \times 3$ MDS matrices that are both semi-involutory and semi-orthogonal coincides with the number of semi-involutory MDS matrices over $\mathbb{F}_{2^m}$. Furthermore, we derive the general structure of orthogonal matrices of arbitrary order $n$ over $\mathbb{F}_{2^m}$. Based on this generic form, we provide a closed-form expression for enumerating all $3 \times 3$ orthogonal MDS matrices over $\mathbb{F}_{2^m}$. Finally, leveraging the aforementioned interconnections, we present explicit formulas for counting $3 \times 3$ semi-involutory MDS matrices and semi-orthogonal MDS matrices.</li>
</ul>

<h3>Title: Evidence of Cognitive Biases in Capture-the-Flag Cybersecurity Competitions</h3>
<ul>
<li><strong>Authors: </strong>Carolina Carreira, Anu Aggarwal, Alejandro Cuevas, Maria José Ferreira, Hanan Hibshi, Cleotilde Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05771">https://arxiv.org/abs/2510.05771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05771">https://arxiv.org/pdf/2510.05771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05771]] Evidence of Cognitive Biases in Capture-the-Flag Cybersecurity Competitions(https://arxiv.org/abs/2510.05771)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Understanding how cognitive biases influence adversarial decision-making is essential for developing effective cyber defenses. Capture-the-Flag (CTF) competitions provide an ecologically valid testbed to study attacker behavior at scale, simulating real-world intrusion scenarios under pressure. We analyze over 500,000 submission logs from picoCTF, a large educational CTF platform, to identify behavioral signatures of cognitive biases with defensive implications. Focusing on availability bias and the sunk cost fallacy, we employ a mixed-methods approach combining qualitative coding, descriptive statistics, and generalized linear modeling. Our findings show that participants often submitted flags with correct content but incorrect formatting (availability bias), and persisted in attempting challenges despite repeated failures and declining success probabilities (sunk cost fallacy). These patterns reveal that biases naturally shape attacker behavior in adversarial contexts. Building on these insights, we outline a framework for bias-informed adaptive defenses that anticipate, rather than simply react to, adversarial actions.</li>
</ul>

<h3>Title: DP-SNP-TIHMM: Differentially Private, Time-Inhomogeneous Hidden Markov Models for Synthesizing Genome-Wide Association Datasets</h3>
<ul>
<li><strong>Authors: </strong>Shadi Rahimian, Mario Fritz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05777">https://arxiv.org/abs/2510.05777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05777">https://arxiv.org/pdf/2510.05777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05777]] DP-SNP-TIHMM: Differentially Private, Time-Inhomogeneous Hidden Markov Models for Synthesizing Genome-Wide Association Datasets(https://arxiv.org/abs/2510.05777)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Single nucleotide polymorphism (SNP) datasets are fundamental to genetic studies but pose significant privacy risks when shared. The correlation of SNPs with each other makes strong adversarial attacks such as masked-value reconstruction, kin, and membership inference attacks possible. Existing privacy-preserving approaches either apply differential privacy to statistical summaries of these datasets or offer complex methods that require post-processing and the usage of a publicly available dataset to suppress or selectively share SNPs. In this study, we introduce an innovative framework for generating synthetic SNP sequence datasets using samples derived from time-inhomogeneous hidden Markov models (TIHMMs). To preserve the privacy of the training data, we ensure that each SNP sequence contributes only a bounded influence during training, enabling strong differential privacy guarantees. Crucially, by operating on full SNP sequences and bounding their gradient contributions, our method directly addresses the privacy risks introduced by their inherent correlations. Through experiments conducted on the real-world 1000 Genomes dataset, we demonstrate the efficacy of our method using privacy budgets of $\varepsilon \in [1, 10]$ at $\delta=10^{-4}$. Notably, by allowing the transition models of the HMM to be dependent on the location in the sequence, we significantly enhance performance, enabling the synthetic datasets to closely replicate the statistical properties of non-private datasets. This framework facilitates the private sharing of genomic data while offering researchers exceptional flexibility and utility.</li>
</ul>

<h3>Title: SBOMproof: Beyond Alleged SBOM Compliance for Supply Chain Security of Container Images</h3>
<ul>
<li><strong>Authors: </strong>Jacopo Bufalino, Mario Di Francesco, Agathe Blaise, Stefano Secci</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05798">https://arxiv.org/abs/2510.05798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05798">https://arxiv.org/pdf/2510.05798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05798]] SBOMproof: Beyond Alleged SBOM Compliance for Supply Chain Security of Container Images(https://arxiv.org/abs/2510.05798)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Supply chain security is extremely important for modern applications running at scale in the cloud. In fact, they involve a large number of heterogeneous microservices that also include third-party software. As a result, security vulnerabilities are hard to identify and mitigate before they start being actively exploited by attackers. For this reason, governments have recently introduced cybersecurity regulations that require vendors to share a software bill of material (SBOM) with end users or regulators. An SBOM can be employed to identify the security vulnerabilities of a software component even without access to its source code, as long as it is accurate and interoperable across different tools. This work evaluates this issue through a comprehensive study of tools for SBOM generation and vulnerability scanning, including both open-source software and cloud services from major providers. We specifically target software containers and focus on operating system packages in Linux distributions that are widely used as base images due to their far-reaching security impact. Our findings show that the considered tools are largely incompatible, leading to inaccurate reporting and a large amount of undetected vulnerabilities. We uncover the SBOM confusion vulnerability, a byproduct of such fragmented ecosystem, where inconsistent formats prevent reliable vulnerability detection across tools.</li>
</ul>

<h3>Title: Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech</h3>
<ul>
<li><strong>Authors: </strong>Rikuto Kotoge, Yuichi Sasaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05799">https://arxiv.org/abs/2510.05799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05799">https://arxiv.org/pdf/2510.05799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05799]] Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech(https://arxiv.org/abs/2510.05799)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.</li>
</ul>

<h3>Title: The Five Safes as a Privacy Context</h3>
<ul>
<li><strong>Authors: </strong>James Bailie, Ruobin Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05803">https://arxiv.org/abs/2510.05803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05803">https://arxiv.org/pdf/2510.05803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05803]] The Five Safes as a Privacy Context(https://arxiv.org/abs/2510.05803)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The Five Safes is a framework used by national statistical offices (NSO) for assessing and managing the disclosure risk of data sharing. This paper makes two points: Firstly, the Five Safes can be understood as a specialization of a broader concept $\unicode{x2013}$ contextual integrity $\unicode{x2013}$ to the situation of statistical dissemination by an NSO. We demonstrate this by mapping the five parameters of contextual integrity onto the five dimensions of the Five Safes. Secondly, the Five Safes contextualizes narrow, technical notions of privacy within a holistic risk assessment. We demonstrate this with the example of differential privacy (DP). This contextualization allows NSOs to place DP within their Five Safes toolkit while also guiding the design of DP implementations within the broader privacy context, as delineated by both their regulation and the relevant social norms.</li>
</ul>

<h3>Title: Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates</h3>
<ul>
<li><strong>Authors: </strong>Pafue Christy Nganjimi, Andrew Soltan, Danielle Belgrave, Lei Clifton, David A. Clifton, Anshul Thakur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05805">https://arxiv.org/abs/2510.05805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05805">https://arxiv.org/pdf/2510.05805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05805]] Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates(https://arxiv.org/abs/2510.05805)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Dataset condensation (DC) enables the creation of compact, privacy-preserving synthetic datasets that can match the utility of real patient records, supporting democratised access to highly regulated clinical data for developing downstream clinical models. State-of-the-art DC methods supervise synthetic data by aligning the training dynamics of models trained on real and those trained on synthetic data, typically using full stochastic gradient descent (SGD) trajectories as alignment targets; however, these trajectories are often noisy, high-curvature, and storage-intensive, leading to unstable gradients, slow convergence, and substantial memory overhead. We address these limitations by replacing full SGD trajectories with smooth, low-loss parametric surrogates, specifically quadratic Bézier curves that connect the initial and final model states from real training trajectories. These mode-connected paths provide noise-free, low-curvature supervision signals that stabilise gradients, accelerate convergence, and eliminate the need for dense trajectory storage. We theoretically justify Bézier-mode connections as effective surrogates for SGD paths and empirically show that the proposed method outperforms state-of-the-art condensation approaches across five clinical datasets, yielding condensed datasets that enable clinically effective model development.</li>
</ul>

<h3>Title: Privacy-Preserving On-chain Permissioning for KYC-Compliant Decentralized Applications</h3>
<ul>
<li><strong>Authors: </strong>Fabian Piper, Karl Wolf, Jonathan Heiss</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05807">https://arxiv.org/abs/2510.05807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05807">https://arxiv.org/pdf/2510.05807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05807]] Privacy-Preserving On-chain Permissioning for KYC-Compliant Decentralized Applications(https://arxiv.org/abs/2510.05807)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Decentralized applications (dApps) in Decentralized Finance (DeFi) face a fundamental tension between regulatory compliance requirements like Know Your Customer (KYC) and maintaining decentralization and privacy. Existing permissioned DeFi solutions often fail to adequately protect private attributes of dApp users and introduce implicit trust assumptions, undermining the blockchain's decentralization. Addressing these limitations, this paper presents a novel synthesis of Self-Sovereign Identity (SSI), Zero-Knowledge Proofs (ZKPs), and Attribute-Based Access Control to enable privacy-preserving on-chain permissioning based on decentralized policy decisions. We provide a comprehensive framework for permissioned dApps that aligns decentralized trust, privacy, and transparency, harmonizing blockchain principles with regulatory compliance. Our framework supports multiple proof types (equality, range, membership, and time-dependent) with efficient proof generation through a commit-and-prove scheme that moves credential authenticity verification outside the ZKP circuit. Experimental evaluation of our KYC-compliant DeFi implementation shows considerable performance improvement for different proof types compared to baseline approaches. We advance the state-of-the-art through a holistic approach, flexible proof mechanisms addressing diverse real-world requirements, and optimized proof generation enabling practical deployment.</li>
</ul>

<h3>Title: Enhancing Automotive Security with a Hybrid Approach towards Universal Intrusion Detection System</h3>
<ul>
<li><strong>Authors: </strong>Md Rezanur Islam, Mahdi Sahlabadi, Keunkyoung Kim, Kangbin Yim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05824">https://arxiv.org/abs/2510.05824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05824">https://arxiv.org/pdf/2510.05824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05824]] Enhancing Automotive Security with a Hybrid Approach towards Universal Intrusion Detection System(https://arxiv.org/abs/2510.05824)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Security measures are essential in the automotive industry to detect intrusions in-vehicle networks. However, developing a one-size-fits-all Intrusion Detection System (IDS) is challenging because each vehicle has unique data profiles. This is due to the complex and dynamic nature of the data generated by vehicles regarding their model, driving style, test environment, and firmware update. To address this issue, a universal IDS has been developed that can be applied to all types of vehicles without the need for customization. Unlike conventional IDSs, the universal IDS can adapt to evolving data security issues resulting from firmware updates. In this study, a new hybrid approach has been developed, combining Pearson correlation with deep learning techniques. This approach has been tested using data obtained from four distinct mechanical and electronic vehicles, including Tesla, Sonata, and two Kia models. The data has been combined into two frequency datasets, and wavelet transformation has been employed to convert them into the frequency domain, enhancing generalizability. Additionally, a statistical method based on independent rule-based systems using Pearson correlation has been utilized to improve system performance. The system has been compared with eight different IDSs, three of which utilize the universal approach, while the remaining five are based on conventional techniques. The accuracy of each system has been evaluated through benchmarking, and the results demonstrate that the hybrid system effectively detects intrusions in various vehicle models.</li>
</ul>

<h3>Title: Fairness in Token Delegation: Mitigating Voting Power Concentration in DAOs</h3>
<ul>
<li><strong>Authors: </strong>Johnnatan Messias, Ayae Ide</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05830">https://arxiv.org/abs/2510.05830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05830">https://arxiv.org/pdf/2510.05830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05830]] Fairness in Token Delegation: Mitigating Voting Power Concentration in DAOs(https://arxiv.org/abs/2510.05830)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Decentralized Autonomous Organizations (DAOs) aim to enable participatory governance, but in practice face challenges of voter apathy, concentration of voting power, and misaligned delegation. Existing delegation mechanisms often reinforce visibility biases, where a small set of highly ranked delegates accumulate disproportionate influence regardless of their alignment with the broader community. In this paper, we conduct an empirical study of delegation in DAO governance, combining on-chain data from five major protocols with off-chain discussions from 14 DAO forums. We develop a methodology to link forum participants to on-chain addresses, extract governance interests using large language models, and compare these interests against delegates' historical behavior. Our analysis reveals that delegations are frequently misaligned with token holders' expressed priorities and that current ranking-based interfaces exacerbate power concentration. We argue that incorporating interest alignment into delegation processes could mitigate these imbalances and improve the representativeness of DAO decision-making.</li>
</ul>

<h3>Title: Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow</h3>
<ul>
<li><strong>Authors: </strong>Ruyang Liu, Shangkun Sun, Haoran Tang, Ge Li, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05836">https://arxiv.org/abs/2510.05836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05836">https://arxiv.org/pdf/2510.05836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05836]] Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow(https://arxiv.org/abs/2510.05836)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-form video understanding has always been a challenging problem due to the significant redundancy in both temporal and spatial contents. This challenge is further exacerbated by the limited context length of Multimodal Large Language Models (MLLMs). To address this issue, many previous works have attempted to extract key video information, where the "key" is typically semantic-aware and heavily dependent on the CLIP model as prior. In this paper, we propose Flow4Agent, a novel framework that pioneeringly incorporates motion priors from optical flow to facilitate LLM-based long video understanding. Flow4Agent mitigates the redundancy in long videos at both temporal and spatial levels through two core modules: Temporal Granularity Optimization (TGO) adaptively refines framelevel hierarchies, which first leverages coarse flow priors to group similar visual contents and then applies semantic priors to filter out highly irrelevant scene information. Motion Token Pruning (MTP) further refines the intra-frame visual representations, pruning high-redundancy video tokens using fine-grained optical flow information. Extensive experiments demonstrate that our Flow4Agent outperforms existing methods across a wide range of video MLLM benchmarks, especially for hour-level video understanding tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.</li>
</ul>

<h3>Title: EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget</h3>
<ul>
<li><strong>Authors: </strong>Liang Chen, Xueting Han, Qizhou Wang, Bo Han, Jing Bai, Hinrich Schutze, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05837">https://arxiv.org/abs/2510.05837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05837">https://arxiv.org/pdf/2510.05837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05837]] EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget(https://arxiv.org/abs/2510.05837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Balancing exploration and exploitation remains a central challenge in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs). Current RLVR methods often overemphasize exploitation, leading to entropy collapse, diminished exploratory capacity, and ultimately limited performance gains. Although techniques that increase policy stochasticity can promote exploration, they frequently fail to escape dominant behavioral modes. This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant modes-that further erodes exploration. We introduce Exploration-Enhanced Policy Optimization (EEPO), a framework that promotes exploration via two-stage rollouts with adaptive unlearning. In the first stage, the model generates half of the trajectories; it then undergoes a lightweight unlearning step to temporarily suppress these sampled responses, forcing the second stage to explore different regions of the output space. This sample-then-forget mechanism disrupts the self-reinforcing loop and promotes wider exploration during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO, achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.</li>
</ul>

<h3>Title: Multimodal Trajectory Representation Learning for Travel Time Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhi Liu, Xuyuan Hu, Xiao Han, Zhehao Dai, Zhaolin Deng, Guojiang Shen, Xiangjie Kong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05840">https://arxiv.org/abs/2510.05840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05840">https://arxiv.org/pdf/2510.05840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05840]] Multimodal Trajectory Representation Learning for Travel Time Estimation(https://arxiv.org/abs/2510.05840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate travel time estimation (TTE) plays a crucial role in intelligent transportation systems. However, it remains challenging due to heterogeneous data sources and complex traffic dynamics. Moreover, conventional approaches typically convert trajectories into fixed-length representations, neglecting the inherent variability of real-world trajectories, which often leads to information loss or feature redundancy. To address these challenges, this paper introduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--a novel multimodal trajectory representation learning approach that integrates GPS sequences, grid trajectories, and road network constraints to enhance TTE accuracy. MDTI employs modality-specific encoders and a cross-modal interaction module to capture complementary spatial, temporal, and topological semantics, while a dynamic trajectory modeling mechanism adaptively regulates information density for trajectories of varying lengths. Two self-supervised pretraining objectives, named contrastive alignment and masked language modeling, further strengthen multimodal consistency and contextual understanding. Extensive experiments on three real-world datasets demonstrate that MDTI consistently outperforms state-of-the-art baselines, confirming its robustness and strong generalization abilities. The code is publicly available at: this https URL</li>
</ul>

<h3>Title: Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Maxence Lasbordes, Sinoué Gad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05846">https://arxiv.org/abs/2510.05846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05846">https://arxiv.org/pdf/2510.05846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05846]] Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer(https://arxiv.org/abs/2510.05846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The landscape of Large Language Models (LLMs) remains predominantly English-centric, resulting in a significant performance gap for other major languages, such as French, especially in the context of Small Language Models (SLMs). Existing multilingual models demonstrate considerably lower performance in French compared to English, and research on efficient adaptation methods for French remains limited. To address this, we introduce \textbf{Luth}, a family of French-specialized SLMs: through targeted post-training on curated, high-quality French data, our models outperform all open-source counterparts of comparable size on multiple French benchmarks while retaining their original English capabilities. We further show that strategic model merging enhances performance in both languages, establishing Luth as a new state of the art for French SLMs and a robust baseline for future French-language research.</li>
</ul>

<h3>Title: ESS-Flow: Training-free guidance of flow-based models as inference in source space</h3>
<ul>
<li><strong>Authors: </strong>Adhithyan Kalaivanan, Zheng Zhao, Jens Sjölund, Fredrik Lindsten</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05849">https://arxiv.org/abs/2510.05849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05849">https://arxiv.org/pdf/2510.05849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05849]] ESS-Flow: Training-free guidance of flow-based models as inference in source space(https://arxiv.org/abs/2510.05849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.</li>
</ul>

<h3>Title: How to model Human Actions distribution with Event Sequence Data</h3>
<ul>
<li><strong>Authors: </strong>Egor Surkov, Dmitry Osin, Evgeny Burnaev, Egor Shvetsov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05856">https://arxiv.org/abs/2510.05856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05856">https://arxiv.org/pdf/2510.05856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05856]] How to model Human Actions distribution with Event Sequence Data(https://arxiv.org/abs/2510.05856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper studies forecasting of the future distribution of events in human action sequences, a task essential in domains like retail, finance, healthcare, and recommendation systems where the precise temporal order is often less critical than the set of outcomes. We challenge the dominant autoregressive paradigm and investigate whether explicitly modeling the future distribution or order-invariant multi-token approaches outperform order-preserving methods. We analyze local order invariance and introduce a KL-based metric to quantify temporal drift. We find that a simple explicit distribution forecasting objective consistently surpasses complex implicit baselines. We further demonstrate that mode collapse of predicted categories is primarily driven by distributional imbalance. This work provides a principled framework for selecting modeling strategies and offers practical guidance for building more accurate and robust forecasting systems.</li>
</ul>

<h3>Title: DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization</h3>
<ul>
<li><strong>Authors: </strong>Xue-Yong Fu, Elena Khasanova, Md Tahmid Rahman Laskar, Harsh Saini, Shashi Bhushan TN</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05858">https://arxiv.org/abs/2510.05858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05858">https://arxiv.org/pdf/2510.05858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05858]] DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization(https://arxiv.org/abs/2510.05858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains %or conversational data that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.</li>
</ul>

<h3>Title: Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies</h3>
<ul>
<li><strong>Authors: </strong>Luka Nenadic, David Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05860">https://arxiv.org/abs/2510.05860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05860">https://arxiv.org/pdf/2510.05860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05860]] Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies(https://arxiv.org/abs/2510.05860)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>It has become increasingly challenging for firms to comply with a plethora of novel digital regulations. This is especially true for smaller businesses that often lack both the resources and know-how to draft complex legal documents. Instead of seeking costly legal advice from attorneys, firms may turn to cheaper alternative legal service providers such as automated contract generators. While these services have a long-standing presence, there is little empirical evidence on their prevalence and output quality. We address this gap in the context of a 2023 Swiss privacy law revision. To enable a systematic evaluation, we create and annotate a multilingual benchmark dataset that captures key compliance obligations under Swiss and EU privacy law. Using this dataset, we validate a novel GPT-5-based method for large-scale compliance assessment of privacy policies, allowing us to measure the impact of the revision. We observe compliance increases indicating an effect of the revision. Generators, explicitly referenced by 18% of local websites, are associated with substantially higher levels of compliance, with increases of up to 15 percentage points compared to privacy policies without generator use. These findings contribute to three debates: the potential of LLMs for cross-lingual legal analysis, the Brussels Effect of EU regulations, and, crucially, the role of automated tools in improving compliance and contractual quality.</li>
</ul>

<h3>Title: Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input</h3>
<ul>
<li><strong>Authors: </strong>Faeze Ghorbanpour, Alexander Fraser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05864">https://arxiv.org/abs/2510.05864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05864">https://arxiv.org/pdf/2510.05864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05864]] Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input(https://arxiv.org/abs/2510.05864)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly support applications that rely on extended context, from document processing to retrieval-augmented generation. While their long-context capabilities are well studied for reasoning and retrieval, little is known about their behavior in safety-critical scenarios. We evaluate LLMs' sensitivity to harmful content under extended context, varying type (explicit vs. implicit), position (beginning, middle, end), prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens). Across harmful content categories such as toxic, offensive, and hate speech, with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance peaks at moderate harmful prevalence (0.25) but declines when content is very sparse or dominant; recall decreases with increasing context length; harmful sentences at the beginning are generally detected more reliably; and explicit content is more consistently recognized than implicit. These findings provide the first systematic view of how LLMs prioritize and calibrate harmful content in long contexts, highlighting both their emerging strengths and the challenges that remain for safety-critical use.</li>
</ul>

<h3>Title: The fragility of "cultural tendencies" in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kun Sun, Rong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05869">https://arxiv.org/abs/2510.05869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05869">https://arxiv.org/pdf/2510.05869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05869]] The fragility of "cultural tendencies" in LLMs(https://arxiv.org/abs/2510.05869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large language models (LLMs), when prompted in different languages, display culturally specific tendencies. They report that the two models (i.e., GPT and ERNIE) respond in more interdependent and holistic ways when prompted in Chinese, and more independent and analytic ways when prompted in English. LSZ attribute these differences to deep-seated cultural patterns in the models, claiming that prompt language alone can induce substantial cultural shifts. While we acknowledge the empirical patterns they observed, we find their experiments, methods, and interpretations problematic. In this paper, we critically re-evaluate the methodology, theoretical framing, and conclusions of LSZ. We argue that the reported "cultural tendencies" are not stable traits but fragile artifacts of specific models and task design. To test this, we conducted targeted replications using a broader set of LLMs and a larger number of test items. Our results show that prompt language has minimal effect on outputs, challenging LSZ's claim that these models encode grounded cultural beliefs.</li>
</ul>

<h3>Title: acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows</h3>
<ul>
<li><strong>Authors: </strong>Johannes Seiffarth, Keitaro Kasahara, Michelle Bund, Benita Lückel, Richard D. Paul, Mathias Pesch, Lennart Witting, Michael Bott, Dietrich Kohlheyer, Katharina Nöh</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05886">https://arxiv.org/abs/2510.05886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05886">https://arxiv.org/pdf/2510.05886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05886]] acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows(https://arxiv.org/abs/2510.05886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Live-cell imaging (LCI) technology enables the detailed spatio-temporal characterization of living cells at the single-cell level, which is critical for advancing research in the life sciences, from biomedical applications to bioprocessing. High-throughput setups with tens to hundreds of parallel cell cultivations offer the potential for robust and reproducible insights. However, these insights are obscured by the large amount of LCI data recorded per experiment. Recent advances in state-of-the-art deep learning methods for cell segmentation and tracking now enable the automated analysis of such large data volumes, offering unprecedented opportunities to systematically study single-cell dynamics. The next key challenge lies in integrating these powerful tools into accessible, flexible, and user-friendly workflows that support routine application in biological research. In this work, we present acia-workflows, a platform that combines three key components: (1) the Automated live-Cell Imaging Analysis (acia) Python library, which supports the modular design of image analysis pipelines offering eight deep learning segmentation and tracking approaches; (2) workflows that assemble the image analysis pipeline, its software dependencies, documentation, and visualizations into a single Jupyter Notebook, leading to accessible, reproducible and scalable analysis workflows; and (3) a collection of application workflows showcasing the analysis and customization capabilities in real-world applications. Specifically, we present three workflows to investigate various types of microfluidic LCI experiments ranging from growth rate comparisons to precise, minute-resolution quantitative analyses of individual dynamic cells responses to changing oxygen conditions. Our collection of more than ten application workflows is open source and publicly available at this https URL.</li>
</ul>

<h3>Title: BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data</h3>
<ul>
<li><strong>Authors: </strong>Arefin Ittesafun Abian, Debopom Sutradhar, Md Rafi Ur Rashid, Reem E. Mohamed, Md Rafiqul Islam, Asif Karim, Kheng Cher Yeo, Sami Azam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05888">https://arxiv.org/abs/2510.05888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05888">https://arxiv.org/pdf/2510.05888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05888]] BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data(https://arxiv.org/abs/2510.05888)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Insect classification is important for agricultural management and ecological research, as it directly affects crop health and production. However, this task remains challenging due to the complex characteristics of insects, class imbalance, and large-scale datasets. To address these issues, we propose BioAutoML-NAS, the first BioAutoML model using multimodal data, including images, and metadata, which applies neural architecture search (NAS) for images to automatically learn the best operations for each connection within each cell. Multiple cells are stacked to form the full network, each extracting detailed image feature representations. A multimodal fusion module combines image embeddings with metadata, allowing the model to use both visual and categorical biological information to classify insects. An alternating bi-level optimization training strategy jointly updates network weights and architecture parameters, while zero operations remove less important connections, producing sparse, efficient, and high-performing architectures. Extensive evaluation on the BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81% accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming state-of-the-art transfer learning, transformer, AutoML, and NAS methods by approximately 16%, 10%, and 8% respectively. Further validation on the Insects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall, and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides accurate, confident insect classification that supports modern sustainable farming.</li>
</ul>

<h3>Title: $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05891">https://arxiv.org/abs/2510.05891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05891">https://arxiv.org/pdf/2510.05891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05891]] $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection(https://arxiv.org/abs/2510.05891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05899">https://arxiv.org/abs/2510.05899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05899">https://arxiv.org/pdf/2510.05899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05899]] Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning(https://arxiv.org/abs/2510.05899)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at this https URL.</li>
</ul>

<h3>Title: PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Li, Selvakumar Manickam, Yung-Wey Chong, Shankar Karuppayah, Priyadarsi Nanda, Binyong Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05900">https://arxiv.org/abs/2510.05900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05900">https://arxiv.org/pdf/2510.05900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05900]] PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection(https://arxiv.org/abs/2510.05900)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Phishing websites remain a persistent cybersecurity threat by mimicking legitimate sites to steal sensitive user information. Existing machine learning-based detection methods often rely on supervised learning with labeled data, which not only incurs substantial annotation costs but also limits adaptability to novel attack patterns. To address these challenges, we propose PhishSSL, a self-supervised contrastive learning framework that eliminates the need for labeled phishing data during training. PhishSSL combines hybrid tabular augmentation with adaptive feature attention to produce semantically consistent views and emphasize discriminative attributes. We evaluate PhishSSL on three phishing datasets with distinct feature compositions. Across all datasets, PhishSSL consistently outperforms unsupervised and self-supervised baselines, while ablation studies confirm the contribution of each component. Moreover, PhishSSL maintains robust performance despite the diversity of feature sets, highlighting its strong generalization and transferability. These results demonstrate that PhishSSL offers a promising solution for phishing website detection, particularly effective against evolving threats in dynamic Web environments.</li>
</ul>

<h3>Title: Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods</h3>
<ul>
<li><strong>Authors: </strong>Martin Benfeghoul, Teresa Delgado, Adnan Oomerjee, Haitham Bou Ammar, Jun Wang, Zafeirios Fountas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05901">https://arxiv.org/abs/2510.05901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05901">https://arxiv.org/pdf/2510.05901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05901]] Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods(https://arxiv.org/abs/2510.05901)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers' quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions.</li>
</ul>

<h3>Title: Prompt reinforcing for long-term planning of large language models</h3>
<ul>
<li><strong>Authors: </strong>Hsien-Chin Lin, Benjamin Matthias Ruppik, Carel van Niekerk, Chia-Hao Shen, Michael Heck, Nurul Lubis, Renato Vukovic, Shutong Feng, Milica Gašić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05921">https://arxiv.org/abs/2510.05921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05921">https://arxiv.org/pdf/2510.05921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05921]] Prompt reinforcing for long-term planning of large language models(https://arxiv.org/abs/2510.05921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks and can be adapted through prompting. However, they remain suboptimal in multi-turn interactions, often relying on incorrect early assumptions and failing to track user goals over time, which makes such tasks particularly challenging. Prior works in dialogue systems have shown that long-term planning is essential for handling interactive tasks. In this work, we propose a prompt optimisation framework inspired by reinforcement learning, which enables such planning to take place by only modifying the task instruction prompt of the LLM-based agent. By generating turn-by-turn feedback and leveraging experience replay for prompt rewriting, our proposed method shows significant improvement in multi-turn tasks such as text-to-SQL and task-oriented dialogue. Moreover, it generalises across different LLM-based agents and can leverage diverse LLMs as meta-prompting agents. This warrants future research in reinforcement learning-inspired parameter-free optimisation methods.</li>
</ul>

<h3>Title: Carré du champ flow matching: better quality-generalisation tradeoff in generative models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Bamberger, Iolo Jones, Dennis Duncan, Michael M. Bronstein, Pierre Vandergheynst, Adam Gosztolai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05930">https://arxiv.org/abs/2510.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05930">https://arxiv.org/pdf/2510.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05930]] Carré du champ flow matching: better quality-generalisation tradeoff in generative models(https://arxiv.org/abs/2510.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce Carré du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.</li>
</ul>

<h3>Title: Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens</h3>
<ul>
<li><strong>Authors: </strong>Mai AlKhamissi, Yunze Xiao, Badr AlKhamissi, Mona Diab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05931">https://arxiv.org/abs/2510.05931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05931">https://arxiv.org/pdf/2510.05931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05931]] Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens(https://arxiv.org/abs/2510.05931)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cultural evaluation of large language models has become increasingly important, yet current benchmarks often reduce culture to static facts or homogeneous values. This view conflicts with anthropological accounts that emphasize culture as dynamic, historically situated, and enacted in practice. To analyze this gap, we introduce a four-part framework that categorizes how benchmarks frame culture, such as knowledge, preference, performance, or bias. Using this lens, we qualitatively examine 20 cultural benchmarks and identify six recurring methodological issues, including treating countries as cultures, overlooking within-culture diversity, and relying on oversimplified survey formats. Drawing on established anthropological methods, we propose concrete improvements: incorporating real-world narratives and scenarios, involving cultural communities in design and validation, and evaluating models in context rather than isolation. Our aim is to guide the development of cultural benchmarks that go beyond static recall tasks and more accurately capture the responses of the models to complex cultural situations.</li>
</ul>

<h3>Title: LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Bal-Ghaoui, Fayssal Sabri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05935">https://arxiv.org/abs/2510.05935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05935">https://arxiv.org/pdf/2510.05935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05935]] LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection(https://arxiv.org/abs/2510.05935)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative "debate" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.</li>
</ul>

<h3>Title: EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hadi Mohammadi, Anastasia Giachanou, Ayoub Bagheri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05942">https://arxiv.org/abs/2510.05942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05942">https://arxiv.org/pdf/2510.05942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05942]] EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models(https://arxiv.org/abs/2510.05942)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.</li>
</ul>

<h3>Title: N-Parties Private Structure and Parameter Learning for Sum-Product Networks</h3>
<ul>
<li><strong>Authors: </strong>Xenia Heilmann, Ernst Althaus, Mattia Cerrato, Nick Johannes Peter Rassau, Mohammad Sadeq Dousti, Stefan Kramer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05946">https://arxiv.org/abs/2510.05946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05946">https://arxiv.org/pdf/2510.05946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05946]] N-Parties Private Structure and Parameter Learning for Sum-Product Networks(https://arxiv.org/abs/2510.05946)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>A sum-product network (SPN) is a graphical model that allows several types of probabilistic inference to be performed efficiently. In this paper, we propose a privacy-preserving protocol which tackles structure generation and parameter learning of SPNs. Additionally, we provide a protocol for private inference on SPNs, subsequent to training. To preserve the privacy of the participants, we derive our protocol based on secret sharing, which guarantees privacy in the honest-but-curious setting even when at most half of the parties cooperate to disclose the data. The protocol makes use of a forest of randomly generated SPNs, which is trained and weighted privately and can then be used for private inference on data points. Our experiments indicate that preserving the privacy of all participants does not decrease log-likelihood performance on both homogeneously and heterogeneously partitioned data. We furthermore show that our protocol's performance is comparable to current state-of-the-art SPN learners in homogeneously partitioned data settings. In terms of runtime and memory usage, we demonstrate that our implementation scales well when increasing the number of parties, comparing favorably to protocols for neural networks, when they are trained to reproduce the input-output behavior of SPNs.</li>
</ul>

<h3>Title: Probing the Difficulty Perception Mechanism of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sunbowen Lee, Qingyu Yin, Chak Tou Leong, Jialiang Zhang, Yicheng Gong, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05969">https://arxiv.org/abs/2510.05969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05969">https://arxiv.org/pdf/2510.05969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05969]] Probing the Difficulty Perception Mechanism of Large Language Models(https://arxiv.org/abs/2510.05969)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research.</li>
</ul>

<h3>Title: Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Ron Keuth, Paul Kaftan, Mattias P. Heinrich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05971">https://arxiv.org/abs/2510.05971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05971">https://arxiv.org/pdf/2510.05971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05971]] Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging(https://arxiv.org/abs/2510.05971)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The generalization of the Transformer architecture via MetaFormer has reshaped our understanding of its success in computer vision. By replacing self-attention with simpler token mixers, MetaFormer provides strong baselines for vision tasks. However, while extensively studied on natural image datasets, its use in medical imaging remains scarce, and existing works rarely compare different token mixers, potentially overlooking more suitable designs choices. In this work, we present the first comprehensive study of token mixers for medical imaging. We systematically analyze pooling-, convolution-, and attention-based token mixers within the MetaFormer architecture on image classification (global prediction task) and semantic segmentation (dense prediction task). Our evaluation spans eight datasets covering diverse modalities and common challenges in the medical domain. Given the prevalence of pretraining from natural images to mitigate medical data scarcity, we also examine transferring pretrained weights to new token mixers. Our results show that, for classification, low-complexity token mixers (e.g. grouped convolution or pooling) are sufficient, aligning with findings on natural images. Pretrained weights remain useful despite the domain gap introduced by the new token mixer. For segmentation, we find that the local inductive bias of convolutional token mixers is essential. Grouped convolutions emerge as the preferred choice, as they reduce runtime and parameter count compared to standard convolutions, while the MetaFormer's channel-MLPs already provide the necessary cross-channel interactions. Our code is available on GitHub.</li>
</ul>

<h3>Title: LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Periklis Mantenoglou, Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05972">https://arxiv.org/abs/2510.05972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05972">https://arxiv.org/pdf/2510.05972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05972]] LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language(https://arxiv.org/abs/2510.05972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Owing to their reasoning capabilities, large language models (LLMs) have been evaluated on planning tasks described in natural language. However, LLMs have largely been tested on planning domains without constraints. In order to deploy them in real-world settings where adherence to constraints, in particular safety constraints, is critical, we need to evaluate their performance on constrained planning tasks. We introduce LexiCon -- a natural language-based (Lexi) constrained (Con) planning benchmark, consisting of a suite of environments, that can be used to evaluate the planning capabilities of LLMs in a principled fashion. The core idea behind LexiCon is to take existing planning environments and impose temporal constraints on the states. These constrained problems are then translated into natural language and given to an LLM to solve. A key feature of LexiCon is its extensibility. That is, the set of supported environments can be extended with new (unconstrained) environment generators, for which temporal constraints are constructed automatically. This renders LexiCon future-proof: the hardness of the generated planning problems can be increased as the planning capabilities of LLMs improve. Our experiments reveal that the performance of state-of-the-art LLMs, including reasoning models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of the planning tasks increases.</li>
</ul>

<h3>Title: Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Eashan Adhikarla, Yixin Liu, Brian D. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05976">https://arxiv.org/abs/2510.05976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05976">https://arxiv.org/pdf/2510.05976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05976]] Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis(https://arxiv.org/abs/2510.05976)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.</li>
</ul>

<h3>Title: Diffusion-Based Image Editing for Breaking Robust Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05978">https://arxiv.org/abs/2510.05978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05978">https://arxiv.org/pdf/2510.05978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05978]] Diffusion-Based Image Editing for Breaking Robust Watermarks(https://arxiv.org/abs/2510.05978)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.</li>
</ul>

<h3>Title: Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xueyan Li, Guinan Su, Mrinmaya Sachan, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05987">https://arxiv.org/abs/2510.05987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05987">https://arxiv.org/pdf/2510.05987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05987]] Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs(https://arxiv.org/abs/2510.05987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post-generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by correctness, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: Greedy-Threshold makes sampling greedy at very low confidence steps. Calibrated-TopK and Calibrated-epsilon set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty and show gains across math and general reasoning benchmarks.</li>
</ul>

<h3>Title: Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments</h3>
<ul>
<li><strong>Authors: </strong>Timothy Pistotti, Jason Brown, Michael Witbrock</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06001">https://arxiv.org/abs/2510.06001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06001">https://arxiv.org/pdf/2510.06001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06001]] Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments(https://arxiv.org/abs/2510.06001)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies probing the Argument from the Poverty of the Stimulus (APS) have applied Large Language Models (LLMs) to test the learnability of complex syntax through surprisal-based metrics. However, divergent conclusions raise questions concerning the insights these metrics offer. While Wilcox et al. (2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate that models successfully generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences (DiD) metric and found that models largely fail on parasitic gaps (PGs). This paper argues that the direct minimal pair approach offers greater diagnostic transparency. We demonstrate this by generating a full 8-permutation paradigm of refined PG stimuli and evaluating the GPT-2 model used in previous studies with a systematic Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all four tested conditions, indicating robust knowledge of filler-gap licensing principles even in complex PG environments. This finding, which contrasts with the more ambiguous results from DiD-style metrics, suggests that the choice of evaluation metric is critical for assessing an LLM's syntactic competence.</li>
</ul>

<h3>Title: MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Qin Dong, Yuntian Tang, Heming Jia, Yunhang Shen, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Shaohui Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06005">https://arxiv.org/abs/2510.06005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06005">https://arxiv.org/pdf/2510.06005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06005]] MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation(https://arxiv.org/abs/2510.06005)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as a dominant method in Parameter-Efficient Fine-Tuning (PEFT) for large language models, which augments the transformer layer with one down-projection $A$ and one up-projection $B$. However, LoRA's reliance on a single down-projection matrix ($A$) creates a representational bottleneck, as this solitary feature extractor is inherently insufficient for capturing the diverse signals required by complex tasks. This motivates our architectural shift to focus on enriching the feature adaptation to improve the downstream task adaptation ability. We propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is asymmetrically shared across layers to ensure parameter efficiency. In MASA, these specialized experts capture diverse features, which are then integrated by a single, layer-specific $B$-matrix. The effectiveness and versatility of our method are validated through a comprehensive suite of experiments spanning multi-domain generalization, single-domain specialization, and multi-task reasoning. For example, on the MMLU benchmark, MASA achieves an average accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative improvement of 1.84%) with comparable learnable parameters of 0.52%.</li>
</ul>

<h3>Title: Detection and Measurement of Hailstones with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Moritz Alker, David C. Schedl, Andreas Stöckl</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06008">https://arxiv.org/abs/2510.06008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06008">https://arxiv.org/pdf/2510.06008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06008]] Detection and Measurement of Hailstones with Multimodal Large Language Models(https://arxiv.org/abs/2510.06008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study examines the use of social media and news images to detect and measure hailstones, utilizing pre-trained multimodal large language models. The dataset for this study comprises 474 crowdsourced images of hailstones from documented hail events in Austria, which occurred between January 2022 and September 2024. These hailstones have maximum diameters ranging from 2 to 11cm. We estimate the hail diameters and compare four different models utilizing one-stage and two-stage prompting strategies. The latter utilizes additional size cues from reference objects, such as human hands, within the image. Our results show that pretrained models already have the potential to measure hailstone diameters from images with an average mean absolute error of 1.12cm for the best model. In comparison to a single-stage prompt, two-stage prompting improves the reliability of most models. Our study suggests that these off-the-shelf models, even without fine-tuning, can complement traditional hail sensors by extracting meaningful and spatially dense information from social media imagery, enabling faster and more detailed assessments of severe weather events. The automated real-time image harvesting from social media and other sources remains an open task, but it will make our approach directly applicable to future hail events.</li>
</ul>

<h3>Title: "Your Doctor is Spying on You": An Analysis of Data Practices in Mobile Healthcare Applications</h3>
<ul>
<li><strong>Authors: </strong>Luke Stevenson, Sanchari Das</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06015">https://arxiv.org/abs/2510.06015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06015">https://arxiv.org/pdf/2510.06015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06015]] "Your Doctor is Spying on You": An Analysis of Data Practices in Mobile Healthcare Applications(https://arxiv.org/abs/2510.06015)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Mobile healthcare (mHealth) applications promise convenient, continuous patient-provider interaction but also introduce severe and often underexamined security and privacy risks. We present an end-to-end audit of 272 Android mHealth apps from Google Play, combining permission forensics, static vulnerability analysis, and user review mining. Our multi-tool assessment with MobSF, RiskInDroid, and OWASP Mobile Audit revealed systemic weaknesses: 26.1% request fine-grained location without disclosure, 18.3% initiate calls silently, and 73 send SMS without notice. Nearly half (49.3%) still use deprecated SHA-1 encryption, 42 transmit unencrypted data, and 6 remain vulnerable to StrandHogg 2.0. Analysis of 2.56 million user reviews found 28.5% negative or neutral sentiment, with over 553,000 explicitly citing privacy intrusions, data misuse, or operational instability. These findings demonstrate the urgent need for enforceable permission transparency, automated pre-market security vetting, and systematic adoption of secure-by-design practices to protect Protected Health Information (PHI).</li>
</ul>

<h3>Title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance</h3>
<ul>
<li><strong>Authors: </strong>Timothy Pistotti, Jason Brown, Michael Witbrock</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06018">https://arxiv.org/abs/2510.06018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06018">https://arxiv.org/pdf/2510.06018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06018]] Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance(https://arxiv.org/abs/2510.06018)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies employing Large Language Models (LLMs) to test the Argument from the Poverty of the Stimulus (APS) have yielded contrasting results across syntactic phenomena. This paper investigates the hypothesis that characteristics of the stimuli used in recent studies, including lexical ambiguities and structural complexities, may confound model performance. A methodology is proposed for re-evaluating LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate identified confounds. Our preliminary findings indicate that GPT-2 demonstrates notably improved performance on these refined PG stimuli compared to baselines, suggesting that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency.</li>
</ul>

<h3>Title: RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics</h3>
<ul>
<li><strong>Authors: </strong>Sai Karthikeya Vemuri, Adithya Ashok Chalain Valapil, Tim Büchner, Joachim Denzler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06020">https://arxiv.org/abs/2510.06020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06020">https://arxiv.org/pdf/2510.06020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06020]] RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics(https://arxiv.org/abs/2510.06020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transferring the recent advancements in deep learning into scientific disciplines is hindered by the lack of the required large-scale datasets for training. We argue that in these knowledge-rich domains, the established body of scientific theory provides reliable inductive biases in the form of governing physical laws. We address the ill-posed inverse problem of recovering Raman spectra from noisy Coherent Anti-Stokes Raman Scattering (CARS) measurements, as the true Raman signal here is suppressed by a dominating non-resonant background. We propose RamPINN, a model that learns to recover Raman spectra from given CARS spectra. Our core methodological contribution is a physics-informed neural network that utilizes a dual-decoder architecture to disentangle resonant and non-resonant signals. This is done by enforcing the Kramers-Kronig causality relations via a differentiable Hilbert transform loss on the resonant and a smoothness prior on the non-resonant part of the signal. Trained entirely on synthetic data, RamPINN demonstrates strong zero-shot generalization to real-world experimental data, explicitly closing this gap and significantly outperforming existing baselines. Furthermore, we show that training with these physics-based losses alone, without access to any ground-truth Raman spectra, still yields competitive results. This work highlights a broader concept: formal scientific rules can act as a potent inductive bias, enabling robust, self-supervised learning in data-limited scientific domains.</li>
</ul>

<h3>Title: Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context</h3>
<ul>
<li><strong>Authors: </strong>An Thi Nguyen, Radina Stoykova, Eric Arazo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06026">https://arxiv.org/abs/2510.06026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06026">https://arxiv.org/pdf/2510.06026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06026]] Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context(https://arxiv.org/abs/2510.06026)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Generic instance search models can dramatically reduce the manual effort required to analyze vast surveillance footage during criminal investigations by retrieving specific objects of interest to law enforcement. However, our research reveals an unintended emergent capability: through overlearning, these models can single out specific individuals even when trained on datasets without human subjects. This capability raises concerns regarding identification and profiling of individuals based on their personal data, while there is currently no clear standard on how de-identification can be achieved. We evaluate two technical safeguards to curtail a model's person re-identification capacity: index exclusion and confusion loss. Our experiments demonstrate that combining these approaches can reduce person re-identification accuracy to below 2% while maintaining 82% of retrieval performance for non-person objects. However, we identify critical vulnerabilities in these mitigations, including potential circumvention using partial person images. These findings highlight urgent regulatory questions at the intersection of AI governance and data protection: How should we classify and regulate systems with emergent identification capabilities? And what technical standards should be required to prevent identification capabilities from developing in seemingly benign applications?</li>
</ul>

<h3>Title: Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between</h3>
<ul>
<li><strong>Authors: </strong>Ondřej Týbl, Lukáš Neumann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06035">https://arxiv.org/abs/2510.06035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06035">https://arxiv.org/pdf/2510.06035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06035]] Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between(https://arxiv.org/abs/2510.06035)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Universal Neural Architecture Space (UniNAS), a generic search space for neural architecture search (NAS) which unifies convolutional networks, transformers, and their hybrid architectures under a single, flexible framework. Our approach enables discovery of novel architectures as well as analyzing existing architectures in a common framework. We also propose a new search algorithm that allows traversing the proposed search space, and demonstrate that the space contains interesting architectures, which, when using identical training setup, outperform state-of-the-art hand-crafted architectures. Finally, a unified toolkit including a standardized training and evaluation protocol is introduced to foster reproducibility and enable fair comparison in NAS research. Overall, this work opens a pathway towards systematically exploring the full spectrum of neural architectures with a unified graph-based NAS perspective.</li>
</ul>

<h3>Title: From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Li Zeqiao, Wang Yijing, Wang Haoyu, Li Zheng, Li Peng, Liu Wenfei, Zuo Zhiqiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06038">https://arxiv.org/abs/2510.06038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06038">https://arxiv.org/pdf/2510.06038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06038]] From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning(https://arxiv.org/abs/2510.06038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving with reinforcement learning (RL) has significant potential. However, applying RL in real-world settings remains challenging due to the need for safe, efficient, and robust learning. Incorporating human expertise into the learning process can help overcome these challenges by reducing risky exploration and improving sample efficiency. In this work, we propose a reward-free, active human-in-the-loop learning method called Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to enable efficient and safe training in real-world environments. The key innovation is the construction of a distributed proxy value function within the DSAC framework. This function encodes human intent by assigning higher expected returns to expert demonstrations and penalizing actions that require human intervention. By extrapolating these labels to unlabeled states, the policy is effectively guided toward expert-like behavior. With a well-designed state space, our method achieves real-world driving policy learning within practical training times. Results from both simulation and real-world experiments demonstrate that our framework enables safe, robust, and sample-efficient learning for autonomous driving.</li>
</ul>

<h3>Title: CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chengwei Wu, Jiapu Wang, Mingyang Gao, Xingrui Zhuo, Jipeng Guo, Runlin Lei, Haoran Luo, Tianyu Chen, Haoyi Zhou, Shirui Pan, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06039">https://arxiv.org/abs/2510.06039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06039">https://arxiv.org/pdf/2510.06039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06039]] CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs(https://arxiv.org/abs/2510.06039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a Comprehensive Benchmark for Evaluating Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase and outline potential directions for future investigations based on our insights.</li>
</ul>

<h3>Title: VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06040">https://arxiv.org/abs/2510.06040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06040">https://arxiv.org/pdf/2510.06040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06040]] VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization(https://arxiv.org/abs/2510.06040)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Jie Hao, Rui Yu, Wei Zhang, Huixia Wang, Jie Xu, Mingrui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06048">https://arxiv.org/abs/2510.06048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06048">https://arxiv.org/pdf/2510.06048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06048]] BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining(https://arxiv.org/abs/2510.06048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective data selection is essential for pretraining large language models (LLMs), enhancing efficiency and improving generalization to downstream tasks. However, existing approaches often require leveraging external pretrained models, making it difficult to disentangle the effects of data selection from those of the external pretrained models. In addition, they often overlook the long-term impact of selected data if the model is trained to convergence, primarily due to the prohibitive cost of full-scale LLM pretraining. In this paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence \textbf{S}coring method for data \textbf{S}election): a lightweight data selection method that operates entirely \emph{from scratch}, without relying on any external pretrained oracle models, while explicitly accounting for the long-term impact of selected data. BLISS leverages a small proxy model as a surrogate for the LLM and employs a score model to estimate the long-term influence of training samples if the proxy model is trained to convergence. We formulate data selection as a bilevel optimization problem, where the upper-level objective optimizes the score model to assign importance weights to training samples, ensuring that minimizing the lower-level objective (i.e., training the proxy model over the weighted training loss until convergence) leads to best validation performance. Once optimized, the trained score model predicts influence scores for the dataset, enabling efficient selection of high-quality samples for LLM pretraining. We validate BLISS by pretraining 410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4 dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$ speedup in reaching the same performance as the state-of-the-art method, demonstrating superior performance across multiple downstream tasks.</li>
</ul>

<h3>Title: Edit-Based Flow Matching for Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>David Lüdke, Marten Lienen, Marcel Kollovieh, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06050">https://arxiv.org/abs/2510.06050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06050">https://arxiv.org/pdf/2510.06050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06050]] Edit-Based Flow Matching for Temporal Point Processes(https://arxiv.org/abs/2510.06050)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Temporal point processes (TPPs) are a fundamental tool for modeling event sequences in continuous time, but most existing approaches rely on autoregressive parameterizations that are limited by their sequential sampling. Recent non-autoregressive, diffusion-style models mitigate these issues by jointly interpolating between noise and data through event insertions and deletions in a discrete Markov chain. In this work, we generalize this perspective and introduce an Edit Flow process for TPPs that transports noise to data via insert, delete, and substitute edit operations. By learning the instantaneous edit rates within a continuous-time Markov chain framework, we attain a flexible and efficient model that effectively reduces the total number of necessary edit operations during generation. Empirical results demonstrate the generative flexibility of our unconditionally trained model in a wide range of unconditional and conditional generation tasks on benchmark TPPs.</li>
</ul>

<h3>Title: ASPO: Asymmetric Importance Sampling Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06062">https://arxiv.org/abs/2510.06062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06062">https://arxiv.org/pdf/2510.06062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06062]] ASPO: Asymmetric Importance Sampling Policy Optimization(https://arxiv.org/abs/2510.06062)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at this https URL.</li>
</ul>

<h3>Title: Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios Kelesis, Dimitris Fotakis, Georgios Paliouras</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06066">https://arxiv.org/abs/2510.06066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06066">https://arxiv.org/pdf/2510.06066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06066]] Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks(https://arxiv.org/abs/2510.06066)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we study the factors that contribute to the effect of oversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysis is based on a new metric (Mean Average Squared Distance - $MASED$) to quantify the extent of oversmoothing. We derive layer-wise bounds on $MASED$, which aggregate to yield global upper and lower distance bounds. Based on this quantification of oversmoothing, we further analyze the importance of two different properties of the model; namely the norms of the generated node embeddings, along with the largest and smallest singular values of the weight matrices. Building on the insights drawn from the theoretical analysis, we show that oversmoothing increases as the number of trainable weight matrices and the number of adjacency matrices increases. We also use the derived layer-wise bounds on $MASED$ to form a proposal for decoupling the number of hops (i.e., adjacency depth) from the number of weight matrices. In particular, we introduce G-Reg, a regularization scheme that increases the bounds, and demonstrate through extensive experiments that by doing so node classification accuracy increases, achieving robustness at large depths. We further show that by reducing oversmoothing in deep networks, we can achieve better results in some tasks than using shallow ones. Specifically, we experiment with a ``cold start" scenario, i.e., when there is no feature information for the unlabeled nodes. Finally, we show empirically the trade-off between receptive field size (i.e., number of weight matrices) and performance, using the $MASED$ bounds. This is achieved by distributing adjacency hops across a small number of trainable layers, avoiding the extremes of under- or over-parameterization of the GNN.</li>
</ul>

<h3>Title: There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Meghna P Ayyar, Jenny Benois-Pineau, Akka Zemmari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06070">https://arxiv.org/abs/2510.06070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06070">https://arxiv.org/pdf/2510.06070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06070]] There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers(https://arxiv.org/abs/2510.06070)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Explainable AI (XAI) has become increasingly important with the rise of large transformer models, yet many explanation methods designed for CNNs transfer poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on attention weights, which tend to yield noisy maps as they capture token-to-token interactions within each this http URL attribution methods incorporating MLP blocks have been proposed, we argue that attention remains a valuable and interpretable signal when properly filtered. We propose a method that combines attention maps with a statistical filtering, initially proposed for CNNs, to remove noisy or uninformative patterns and produce more faithful explanations. We further extend our approach with a class-specific variant that yields discriminative explanations. Evaluation against popular state-of-the-art methods demonstrates that our approach produces sharper and more interpretable maps. In addition to perturbation-based faithfulness metrics, we incorporate human gaze data to assess alignment with human perception, arguing that human interpretability remains essential for XAI. Across multiple datasets, our approach consistently outperforms or is comparable to the SOTA methods while remaining efficient and human plausible.</li>
</ul>

<h3>Title: When Thinking Drifts: Evidential Grounding for Robust Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06077">https://arxiv.org/abs/2510.06077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06077">https://arxiv.org/pdf/2510.06077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06077]] When Thinking Drifts: Evidential Grounding for Robust Video Reasoning(https://arxiv.org/abs/2510.06077)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term "visual thinking drift". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only "think before answering", but also "see while thinking".</li>
</ul>

<h3>Title: A public cardiac CT dataset featuring the left atrial appendage</h3>
<ul>
<li><strong>Authors: </strong>Bjoern Hansen, Jonas Pedersen, Klaus F. Kofoed, Oscar Camara, Rasmus R. Paulsen, Kristine Soerensen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06090">https://arxiv.org/abs/2510.06090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06090">https://arxiv.org/pdf/2510.06090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06090]] A public cardiac CT dataset featuring the left atrial appendage(https://arxiv.org/abs/2510.06090)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite the success of advanced segmentation frameworks such as TotalSegmentator (TS), accurate segmentations of the left atrial appendage (LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant challenge in medical imaging. In this work, we present the first open-source, anatomically coherent dataset of curated, high-resolution segmentations for these structures, supplemented with whole-heart labels produced by TS on the publicly available ImageCAS dataset consisting of 1000 cardiac computed tomography angiography (CCTA) scans. One purpose of the data set is to foster novel approaches to the analysis of LAA morphology. LAA segmentations on ImageCAS were generated using a state-of-the-art segmentation framework developed specifically for high resolution LAA segmentation. We trained the network on a large private dataset with manual annotations provided by medical readers guided by a trained cardiologist and transferred the model to ImageCAS data. CA labels were improved from the original ImageCAS annotations, while PV segmentations were refined from TS outputs. In addition, we provide a list of scans from ImageCAS that contains common data flaws such as step artefacts, LAAs extending beyond the scanner's field of view, and other types of data defects.</li>
</ul>

<h3>Title: Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method</h3>
<ul>
<li><strong>Authors: </strong>Lulu Gong, Shreya Saxena</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, q-bio.NC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06091">https://arxiv.org/abs/2510.06091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06091">https://arxiv.org/pdf/2510.06091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06091]] Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method(https://arxiv.org/abs/2510.06091)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mixtures of linear dynamical systems (MoLDS) provide a path to model time-series data that exhibit diverse temporal dynamics across trajectories. However, its application remains challenging in complex and noisy settings, limiting its effectiveness for neural data analysis. Tensor-based moment methods can provide global identifiability guarantees for MoLDS, but their performance degrades under noise and complexity. Commonly used expectation-maximization (EM) methods offer flexibility in fitting latent models but are highly sensitive to initialization and prone to poor local minima. Here, we propose a tensor-based method that provides identifiability guarantees for learning MoLDS, which is followed by EM updates to combine the strengths of both approaches. The novelty in our approach lies in the construction of moment tensors using the input-output data to recover globally consistent estimates of mixture weights and system parameters. These estimates can then be refined through a Kalman EM algorithm, with closed-form updates for all LDS parameters. We validate our framework on synthetic benchmarks and real-world datasets. On synthetic data, the proposed Tensor-EM method achieves more reliable recovery and improved robustness compared to either pure tensor or randomly initialized EM methods. We then analyze neural recordings from the primate somatosensory cortex while a non-human primate performs reaches in different directions. Our method successfully models and clusters different conditions as separate subsystems, consistent with supervised single-LDS fits for each condition. Finally, we apply this approach to another neural dataset where monkeys perform a sequential reaching task. These results demonstrate that MoLDS provides an effective framework for modeling complex neural data, and that Tensor-EM is a reliable approach to MoLDS learning for these applications.</li>
</ul>

<h3>Title: Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL</h3>
<ul>
<li><strong>Authors: </strong>Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06092">https://arxiv.org/abs/2510.06092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06092">https://arxiv.org/pdf/2510.06092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06092]] Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL(https://arxiv.org/abs/2510.06092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term \emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process.</li>
</ul>

<h3>Title: The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Bou, Nyal Patel, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06096">https://arxiv.org/abs/2510.06096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06096">https://arxiv.org/pdf/2510.06096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06096]] The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives(https://arxiv.org/abs/2510.06096)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI.</li>
</ul>

<h3>Title: The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Muyu He, Muhammad Ali Shafique, Anand Kumar, Tsach Mackey, Nazneen Rajani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06101">https://arxiv.org/abs/2510.06101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06101">https://arxiv.org/pdf/2510.06101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06101]] The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models(https://arxiv.org/abs/2510.06101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a $\textit{valley of code reasoning}$: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition</li>
</ul>

<h3>Title: Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gagan Bhatia, Somayajulu G Sripada, Kevin Allan, Jacobo Azcona</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06107">https://arxiv.org/abs/2510.06107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06107">https://arxiv.org/pdf/2510.06107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06107]] Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models(https://arxiv.org/abs/2510.06107)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary this http URL, to enable the reliable tracing of internal semantic failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \textbf{associative pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.</li>
</ul>

<h3>Title: Influence Functions for Efficient Data Selection in Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Prateek Humane, Paolo Cudrano, Daniel Z. Kaplan, Matteo Matteucci, Supriyo Chakraborty, Irina Rish</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06108">https://arxiv.org/abs/2510.06108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06108">https://arxiv.org/pdf/2510.06108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06108]] Influence Functions for Efficient Data Selection in Reasoning(https://arxiv.org/abs/2510.06108)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes "quality" remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family.</li>
</ul>

<h3>Title: Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shuo Jiang, Zhuwen Chen, Liaoman Xu, Yanming Zhu, Changmiao Wang, Jiong Zhang, Feiwei Qin, Yifei Chen, Zhu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06113">https://arxiv.org/abs/2510.06113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06113">https://arxiv.org/pdf/2510.06113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06113]] Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction(https://arxiv.org/abs/2510.06113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Survival analysis plays a vital role in making clinical decisions. However, the models currently in use are often difficult to interpret, which reduces their usefulness in clinical settings. Prototype learning presents a potential solution, yet traditional methods focus on local similarities and static matching, neglecting the broader tumor context and lacking strong semantic alignment with genomic data. To overcome these issues, we introduce an innovative prototype-based multimodal framework, FeatProto, aimed at enhancing cancer survival prediction by addressing significant limitations in current prototype learning methodologies within pathology. Our framework establishes a unified feature prototype space that integrates both global and local features of whole slide images (WSI) with genomic profiles. This integration facilitates traceable and interpretable decision-making processes. Our approach includes three main innovations: (1) A robust phenotype representation that merges critical patches with global context, harmonized with genomic data to minimize local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that sustains stable cross-modal associations and employs a wandering mechanism to adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype matching scheme designed to capture global centrality, local typicality, and cohort-level trends, thereby refining prototype inference. Comprehensive evaluations on four publicly available cancer datasets indicate that our method surpasses current leading unimodal and multimodal survival prediction techniques in both accuracy and interoperability, providing a new perspective on prototype learning for critical medical applications. Our source code is available at this https URL.</li>
</ul>

<h3>Title: PolyGraph Discrepancy: a classifier-based metric for graph generation</h3>
<ul>
<li><strong>Authors: </strong>Markus Krimmel, Philip Hartout, Karsten Borgwardt, Dexiong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06122">https://arxiv.org/abs/2510.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06122">https://arxiv.org/pdf/2510.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06122]] PolyGraph Discrepancy: a classifier-based metric for graph generation(https://arxiv.org/abs/2510.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Existing methods for evaluating graph generative models primarily rely on Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these metrics can rank generative models, they do not provide an absolute measure of performance. Their values are also highly sensitive to extrinsic parameters, namely kernel and descriptor parametrization, making them incomparable across different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new evaluation framework that addresses these limitations. It approximates the Jensen-Shannon distance of graph distributions by fitting binary classifiers to distinguish between real and generated graphs, featurized by these descriptors. The data log-likelihood of these classifiers approximates a variational lower bound on the JS distance between the two distributions. Resulting metrics are constrained to the unit interval [0,1] and are comparable across different graph descriptors. We further derive a theoretically grounded summary metric that combines these individual metrics to provide a maximally tight lower bound on the distance for the given descriptors. Thorough experiments demonstrate that PGD provides a more robust and insightful evaluation compared to MMD metrics. The PolyGraph framework for benchmarking graph generative models is made publicly available at this https URL.</li>
</ul>

<h3>Title: Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework</h3>
<ul>
<li><strong>Authors: </strong>Mosong Ma, Tania Stathaki, Michalis Lazarou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06123">https://arxiv.org/abs/2510.06123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06123">https://arxiv.org/pdf/2510.06123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06123]] Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework(https://arxiv.org/abs/2510.06123)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning in medical imaging is often limited by scarce and imbalanced annotated data. We present SSGNet, a unified framework that combines class specific generative modeling with iterative semisupervised pseudo labeling to enhance both classification and segmentation. Rather than functioning as a standalone model, SSGNet augments existing baselines by expanding training data with StyleGAN3 generated images and refining labels through iterative pseudo labeling. Experiments across multiple medical imaging benchmarks demonstrate consistent gains in classification and segmentation performance, while Frechet Inception Distance analysis confirms the high quality of generated samples. These results highlight SSGNet as a practical strategy to mitigate annotation bottlenecks and improve robustness in medical image analysis.</li>
</ul>

<h3>Title: Downsized and Compromised?: Assessing the Faithfulness of Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Moumita Kamal, Douglas A. Talbert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06125">https://arxiv.org/abs/2510.06125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06125">https://arxiv.org/pdf/2510.06125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06125]] Downsized and Compromised?: Assessing the Faithfulness of Model Compression(https://arxiv.org/abs/2510.06125)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In real-world applications, computational constraints often require transforming large models into smaller, more efficient versions through model compression. While these techniques aim to reduce size and computational cost without sacrificing performance, their evaluations have traditionally focused on the trade-off between size and accuracy, overlooking the aspect of model faithfulness. This limited view is insufficient for high-stakes domains like healthcare, finance, and criminal justice, where compressed models must remain faithful to the behavior of their original counterparts. This paper presents a novel approach to evaluating faithfulness in compressed models, moving beyond standard metrics. We introduce and demonstrate a set of faithfulness metrics that capture how model behavior changes post-compression. Our contributions include introducing techniques to assess predictive consistency between the original and compressed models using model agreement, and applying chi-squared tests to detect statistically significant changes in predictive patterns across both the overall dataset and demographic subgroups, thereby exposing shifts that aggregate fairness metrics may obscure. We demonstrate our approaches by applying quantization and pruning to artificial neural networks (ANNs) trained on three diverse and socially meaningful datasets. Our findings show that high accuracy does not guarantee faithfulness, and our statistical tests detect subtle yet significant shifts that are missed by standard metrics, such as Accuracy and Equalized Odds. The proposed metrics provide a practical and more direct method for ensuring that efficiency gains through compression do not compromise the fairness or faithfulness essential for trustworthy AI.</li>
</ul>

<h3>Title: lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxin Wang, Xiaolong Tu, Hongyu Ke, Huirong Chai, Dawei Chen, Kyungtae Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06126">https://arxiv.org/abs/2510.06126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06126">https://arxiv.org/pdf/2510.06126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06126]] lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models(https://arxiv.org/abs/2510.06126)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into everyday applications, but their prevalent cloud-based deployment raises growing concerns around data privacy and long-term sustainability. Running LLMs locally on mobile and edge devices (on-device LLMs) offers the promise of enhanced privacy, reliability, and reduced communication costs. However, realizing this vision remains challenging due to substantial memory and compute demands, as well as limited visibility into performance-efficiency trade-offs on resource-constrained hardware. We propose lm-Meter, the first lightweight, online latency profiler tailored for on-device LLM inference. lm-Meter captures fine-grained, real-time latency at both phase (e.g., embedding, prefill, decode, softmax, sampling) and kernel levels without auxiliary devices. We implement lm-Meter on commercial mobile platforms and demonstrate its high profiling accuracy with minimal system overhead, e.g., only 2.58% throughput reduction in prefill and 0.99% in decode under the most constrained Powersave governor. Leveraging lm-Meter, we conduct comprehensive empirical studies revealing phase- and kernel-level bottlenecks in on-device LLM inference, quantifying accuracy-efficiency trade-offs, and identifying systematic optimization opportunities. lm-Meter provides unprecedented visibility into the runtime behavior of LLMs on constrained platforms, laying the foundation for informed optimization and accelerating the democratization of on-device LLM systems. Code and tutorials are available at this https URL.</li>
</ul>

<h3>Title: Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Dehan Al Kautsar, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06128">https://arxiv.org/abs/2510.06128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06128">https://arxiv.org/pdf/2510.06128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06128]] Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer(https://arxiv.org/abs/2510.06128)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tokenization defines the foundation of multilingual language models by determining how words are represented and shared across languages. However, existing methods often fail to support effective cross-lingual transfer because semantically equivalent words are assigned distinct embeddings. For example, "I eat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped to different vocabulary indices, preventing shared representations and limiting cross-lingual generalization. We introduce parallel tokenizers. This new framework trains tokenizers monolingually and then aligns their vocabularies exhaustively using bilingual dictionaries or word-to-word translation, ensuring consistent indices for semantically equivalent words. This alignment enforces a shared semantic space across languages while naturally improving fertility balance. To assess their effectiveness, we pretrain a transformer encoder from scratch on thirteen low-resource languages and evaluate it on sentiment analysis, hate speech detection, emotion classification, and sentence embedding similarity. Across all tasks, models trained with parallel tokenizers outperform conventional multilingual baselines, confirming that rethinking tokenization is essential for advancing multilingual representation learning--especially in low-resource settings.</li>
</ul>

<h3>Title: Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06131">https://arxiv.org/abs/2510.06131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06131">https://arxiv.org/pdf/2510.06131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06131]] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation(https://arxiv.org/abs/2510.06131)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.</li>
</ul>

<h3>Title: CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits</h3>
<ul>
<li><strong>Authors: </strong>Kangyu Wang, Zhiyun Jiang, Haibo Feng, Weijia Zhao, Lin Liu, Jianguo Li, Zhenzhong Lan, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06133">https://arxiv.org/abs/2510.06133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06133">https://arxiv.org/pdf/2510.06133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06133]] CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits(https://arxiv.org/abs/2510.06133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.</li>
</ul>

<h3>Title: Deforming Videos to Masks: Flow Matching for Referring Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06139">https://arxiv.org/abs/2510.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06139">https://arxiv.org/pdf/2510.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06139]] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation(https://arxiv.org/abs/2510.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.</li>
</ul>

<h3>Title: Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images</h3>
<ul>
<li><strong>Authors: </strong>Aditya Prakash, David Forsyth, Saurabh Gupta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06145">https://arxiv.org/abs/2510.06145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06145">https://arxiv.org/pdf/2510.06145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06145]] Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images(https://arxiv.org/abs/2510.06145)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.</li>
</ul>

<h3>Title: LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams</h3>
<ul>
<li><strong>Authors: </strong>Aju Ani Justus, Chris Baber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06151">https://arxiv.org/abs/2510.06151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06151">https://arxiv.org/pdf/2510.06151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06151]] LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams(https://arxiv.org/abs/2510.06151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A critical challenge in modelling Heterogeneous-Agent Teams is training agents to collaborate with teammates whose policies are inaccessible or non-stationary, such as humans. Traditional approaches rely on expensive human-in-the-loop data, which limits scalability. We propose using Large Language Models (LLMs) as policy-agnostic human proxies to generate synthetic data that mimics human decision-making. To evaluate this, we conduct three experiments in a grid-world capture game inspired by Stag Hunt, a game theory paradigm that balances risk and reward. In Experiment 1, we compare decisions from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and Mixtral 8x22B models. LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria. Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM outputs mirror human participants' variability, shifting between risk-averse and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic grid-world where the LLM agents generate movement actions. LLMs produce trajectories resembling human participants' paths. While LLMs cannot yet fully replicate human adaptability, their prompt-guided diversity offers a scalable foundation for simulating policy-agnostic teammates.</li>
</ul>

<h3>Title: TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts</h3>
<ul>
<li><strong>Authors: </strong>Christopher Kolberg, Katharina Eggensperger, Nico Pfeifer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06162">https://arxiv.org/abs/2510.06162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06162">https://arxiv.org/pdf/2510.06162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06162]] TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts(https://arxiv.org/abs/2510.06162)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Revealing novel insights from the relationship between molecular measurements and pathology remains a very impactful application of machine learning in biomedicine. Data in this domain typically contain only a few observations but thousands of potentially noisy features, posing challenges for conventional machine learning approaches. While prior-data fitted networks emerge as foundation models for tabular data, they are currently not suited to handle large feature counts (>500). Although feature reduction enables their application, it hinders feature importance analysis. We propose a strategy that extends existing models through continued pre-training on synthetic data sampled from a customized prior. The resulting model, TabPFN-Wide, matches or exceeds its base model's performance while exhibiting improved robustness to noise. It seamlessly scales beyond 50,000 features, regardless of noise levels, while maintaining inherent interpretability, which is critical for biomedical applications. Our results show that prior-informed adaptation is suitable to enhance the capability of foundation models for high-dimensional data. On real-world biomedical datasets many of the most relevant features identified by the model overlap with previous biological findings, while others propose potential starting points for future studies.</li>
</ul>

<h3>Title: Thermodynamic Performance Limits for Score-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan X. Kodama, Michael Hinczewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06174">https://arxiv.org/abs/2510.06174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06174">https://arxiv.org/pdf/2510.06174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06174]] Thermodynamic Performance Limits for Score-Based Diffusion Models(https://arxiv.org/abs/2510.06174)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.</li>
</ul>

<h3>Title: VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06175">https://arxiv.org/abs/2510.06175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06175">https://arxiv.org/pdf/2510.06175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06175]] VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization(https://arxiv.org/abs/2510.06175)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Key-Value (KV) cache introduces substantial memory overhead during large language model (LLM) inference. Although existing vector quantization (VQ) methods reduce KV cache usage and provide flexible representational capacity across bit-widths, they suffer severe performance degradation at ultra-low bit-widths due to key cache outliers that hinder effective codebook utilization. To address this challenge, we propose VecInfer, a novel VQ method for aggressive KV cache compression while enabling efficient inference. By applying smooth and Hadamard transformations, VecInfer suppresses outliers in the key cache, enabling the codebook to comprehensively cover the original data distribution and thereby reducing quantization difficulty. To facilitate efficient deployment, we design an optimized CUDA kernel that fuses computation with dequantization to minimize memory access overhead. Extensive evaluations demonstrate that VecInfer consistently outperforms existing quantization baselines across both long-context understanding and mathematical reasoning tasks. With only 2-bit quantization, VecInfer achieves performance comparable to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.</li>
</ul>

<h3>Title: Conformalized Gaussian processes for online uncertainty quantification over graphs</h3>
<ul>
<li><strong>Authors: </strong>Jinwen Xu, Qin Lu, Georgios B. Giannakis</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06181">https://arxiv.org/abs/2510.06181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06181">https://arxiv.org/pdf/2510.06181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06181]] Conformalized Gaussian processes for online uncertainty quantification over graphs(https://arxiv.org/abs/2510.06181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP.</li>
</ul>

<h3>Title: Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context</h3>
<ul>
<li><strong>Authors: </strong>Yoav Gur-Arieh, Mor Geva, Atticus Geiger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06182">https://arxiv.org/abs/2510.06182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06182">https://arxiv.org/pdf/2510.06182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06182]] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context(https://arxiv.org/abs/2510.06182)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.</li>
</ul>

<h3>Title: RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Xue Liu, Irwin King, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06186">https://arxiv.org/abs/2510.06186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06186">https://arxiv.org/pdf/2510.06186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06186]] RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback(https://arxiv.org/abs/2510.06186)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation</li>
</ul>

<h3>Title: On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06190">https://arxiv.org/abs/2510.06190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06190">https://arxiv.org/pdf/2510.06190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06190]] On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond(https://arxiv.org/abs/2510.06190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper formally studies generation processes, including auto-regressive next-token prediction and masked diffusion, that abstract beyond architectural specifics. At this level of abstraction, we quantify their benefits and limitations through measurable criteria such as computational hardness and learnability. In particular, we demonstrate that allowing generation to proceed beyond autoregression and current masked diffusion, with capabilities to rewrite and length-variable edit, can bring significant theoretical and empirical advantages, with important implications for frontier LLMs that aspire to tackle increasingly hard problems and work universally across domains beyond natural language, such as coding and science.</li>
</ul>

<h3>Title: Latent Speech-Text Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06195">https://arxiv.org/abs/2510.06195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06195">https://arxiv.org/pdf/2510.06195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06195]] Latent Speech-Text Transformer(https://arxiv.org/abs/2510.06195)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.</li>
</ul>

<h3>Title: Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Guo, Zhengliang Shi, Minglai Yang, Mahdi Rahimi, Mihai Surdeanu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06198">https://arxiv.org/abs/2510.06198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06198">https://arxiv.org/pdf/2510.06198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06198]] Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction(https://arxiv.org/abs/2510.06198)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).</li>
</ul>

<h3>Title: ShapeGen4D: Towards High Quality 4D Shape Generation from Videos</h3>
<ul>
<li><strong>Authors: </strong>Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond A. Yeh, Peter Wonka, Chaoyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06208">https://arxiv.org/abs/2510.06208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06208">https://arxiv.org/pdf/2510.06208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06208]] ShapeGen4D: Towards High Quality 4D Shape Generation from Videos(https://arxiv.org/abs/2510.06208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. In this work, we introduce a native video-to-4D shape generation framework that synthesizes a single dynamic 3D representation end-to-end from the video. Our framework introduces three key components based on large-scale pre-trained 3D models: (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; (ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; and (iii) noise sharing across frames to enhance temporal stability. Our method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.</li>
</ul>

<h3>Title: Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06209">https://arxiv.org/abs/2510.06209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06209">https://arxiv.org/pdf/2510.06209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06209]] Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models(https://arxiv.org/abs/2510.06209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.</li>
</ul>

<h3>Title: Training Dynamics Impact Post-Training Quantization Robustness</h3>
<ul>
<li><strong>Authors: </strong>Albert Catalan-Tatjer, Niccolò Ajroldi, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06213">https://arxiv.org/abs/2510.06213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06213">https://arxiv.org/pdf/2510.06213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06213]] Training Dynamics Impact Post-Training Quantization Robustness(https://arxiv.org/abs/2510.06213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.</li>
</ul>

<h3>Title: Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents</h3>
<ul>
<li><strong>Authors: </strong>Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06214">https://arxiv.org/abs/2510.06214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06214">https://arxiv.org/pdf/2510.06214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06214]] Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents(https://arxiv.org/abs/2510.06214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an "apples-to-oranges" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.</li>
</ul>

<h3>Title: Fine-grained Defocus Blur Control for Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Connelly Barnes, Xuaner Zhang, Lingzhi Zhang, Andrew Owens, Sohrab Amirghodsi, Eli Shechtman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06215">https://arxiv.org/abs/2510.06215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06215">https://arxiv.org/pdf/2510.06215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06215]] Fine-grained Defocus Blur Control for Generative Image Models(https://arxiv.org/abs/2510.06215)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Current text-to-image diffusion models excel at generating diverse, high-quality images, yet they struggle to incorporate fine-grained camera metadata such as precise aperture settings. In this work, we introduce a novel text-to-image diffusion framework that leverages camera metadata, or EXIF data, which is often embedded in image files, with an emphasis on generating controllable lens blur. Our method mimics the physical image formation process by first generating an all-in-focus image, estimating its monocular depth, predicting a plausible focus distance with a novel focus distance transformer, and then forming a defocused image with an existing differentiable lens blur model. Gradients flow backwards through this whole process, allowing us to learn without explicit supervision to generate defocus effects based on content elements and the provided EXIF data. At inference time, this enables precise interactive user control over defocus effects while preserving scene contents, which is not achievable with existing diffusion models. Experimental results demonstrate that our model enables superior fine-grained control without altering the depicted scene.</li>
</ul>

<h3>Title: Dropping the D: RGB-D SLAM Without the Depth Sensor</h3>
<ul>
<li><strong>Authors: </strong>Mert Kiray, Alican Karaomer, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06216">https://arxiv.org/abs/2510.06216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06216">https://arxiv.org/pdf/2510.06216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06216]] Dropping the D: RGB-D SLAM Without the Depth Sensor(https://arxiv.org/abs/2510.06216)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network. Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features. These are processed by an unmodified RGB-D SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS on a single GPU. These results suggest that modern pretrained vision models can replace active depth sensors as reliable, real-time sources of metric scale, marking a step toward simpler and more cost-effective SLAM systems.</li>
</ul>

<h3>Title: EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06218">https://arxiv.org/abs/2510.06218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06218">https://arxiv.org/pdf/2510.06218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06218]] EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark(https://arxiv.org/abs/2510.06218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
