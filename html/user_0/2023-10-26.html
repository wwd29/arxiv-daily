<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: RIPencapsulation: Defeating IP Encapsulation on TI MSP Devices. (arXiv:2310.16433v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16433">http://arxiv.org/abs/2310.16433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16433]] RIPencapsulation: Defeating IP Encapsulation on TI MSP Devices(http://arxiv.org/abs/2310.16433)</code></li>
<li>Summary: <p>Internet of Things (IoT) devices sit at the intersection of unwieldy software
complexity and unprecedented attacker access. This unique position comes with a
daunting security challenge: how can I protect both proprietary code and
confidential data on a device that the attacker has unfettered access to?
Trusted Execution Environments (TEEs) promise to solve this challenge through
hardware-based separation of trusted and untrusted computation and data. While
TEEs do an adequate job of protecting secrets on desktop-class devices, we
reveal that trade-offs made in one of the most widely-used commercial IoT
devices undermine their TEE's security.
</p>
<p>This paper uncovers two fundamental weaknesses in IP Encapsulation (IPE), the
TEE deployed by Texas Instruments for MSP430 and MSP432 devices. We observe
that lack of call site enforcement and residual state after unexpected TEE
exits enable an attacker to reveal all proprietary code and secret data within
the IPE. We design and implement an attack called RIPencapsulation, which
systematically executes portions of code within the IPE and uses the partial
state revealed through the register file to exfiltrate secret data and to
identify gadget instructions. The attack then uses gadget instructions to
reveal all proprietary code within the IPE. Our evaluation with commodity
devices and a production compiler and settings shows that -- even after
following all manufacturer-recommended secure coding practices --
RIPencapsultaion reveals, within minutes, both the code and keys from
third-party cryptographic implementations protected by the IPE.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection. (arXiv:2310.16569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16569">http://arxiv.org/abs/2310.16569</a></li>
<li>Code URL: https://github.com/josephcao0327/fasten</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16569]] Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection(http://arxiv.org/abs/2310.16569)</code></li>
<li>Summary: <p>Anti-spoofing detection has become a necessity for face recognition systems
due to the security threat posed by spoofing attacks. Despite great success in
traditional attacks, most deep-learning-based methods perform poorly in 3D
masks, which can highly simulate real faces in appearance and structure,
suffering generalizability insufficiency while focusing only on the spatial
domain with single frame input. This has been mitigated by the recent
introduction of a biomedical technology called rPPG (remote
photoplethysmography). However, rPPG-based methods are sensitive to noisy
interference and require at least one second (&gt; 25 frames) of observation time,
which induces high computational overhead. To address these challenges, we
propose a novel 3D mask detection framework, called FASTEN
(Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the
network for focusing more on fine-grained details in large movements, which can
eliminate redundant spatio-temporal feature interference and quickly capture
splicing traces of 3D masks in fewer frames. Our proposed network contains
three key modules: 1) a facial optical flow network to obtain non-RGB
inter-frame flow information; 2) flow attention to assign different
significance to each frame; 3) spatio-temporal aggregation to aggregate
high-level spatial features and temporal transition features. Through extensive
experiments, FASTEN only requires five frames of input and outperforms eight
competitors for both intra-dataset and cross-dataset evaluations in terms of
multiple detection metrics. Moreover, FASTEN has been deployed in real-world
mobile devices for practical 3D mask detection.
</p></li>
</ul>

<h3>Title: Grid Frequency Forecasting in University Campuses using Convolutional LSTM. (arXiv:2310.16071v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16071">http://arxiv.org/abs/2310.16071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16071]] Grid Frequency Forecasting in University Campuses using Convolutional LSTM(http://arxiv.org/abs/2310.16071)</code></li>
<li>Summary: <p>The modern power grid is facing increasing complexities, primarily stemming
from the integration of renewable energy sources and evolving consumption
patterns. This paper introduces an innovative methodology that harnesses
Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks
to establish robust time series forecasting models for grid frequency. These
models effectively capture the spatiotemporal intricacies inherent in grid
frequency data, significantly enhancing prediction accuracy and bolstering
power grid reliability. The research explores the potential and development of
individualized Convolutional LSTM (ConvLSTM) models for buildings within a
university campus, enabling them to be independently trained and evaluated for
each building. Individual ConvLSTM models are trained on power consumption data
for each campus building and forecast the grid frequency based on historical
trends. The results convincingly demonstrate the superiority of the proposed
models over traditional forecasting techniques, as evidenced by performance
metrics such as Mean Square Error (MSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE). Additionally, an Ensemble Model is formulated
to aggregate insights from the building-specific models, delivering
comprehensive forecasts for the entire campus. This approach ensures the
privacy and security of power consumption data specific to each building.
</p></li>
</ul>

<h3>Title: GADY: Unsupervised Anomaly Detection on Dynamic Graphs. (arXiv:2310.16376v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16376">http://arxiv.org/abs/2310.16376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16376]] GADY: Unsupervised Anomaly Detection on Dynamic Graphs(http://arxiv.org/abs/2310.16376)</code></li>
<li>Summary: <p>Anomaly detection on dynamic graphs refers to detecting entities whose
behaviors obviously deviate from the norms observed within graphs and their
temporal information. This field has drawn increasing attention due to its
application in finance, network security, social networks, and more. However,
existing methods face two challenges: dynamic structure constructing challenge
- difficulties in capturing graph structure with complex time information and
negative sampling challenge - unable to construct excellent negative samples
for unsupervised learning. To address these challenges, we propose Unsupervised
Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first
challenge, we propose a continuous dynamic graph model to capture the
fine-grained information, which breaks the limit of existing discrete methods.
Specifically, we employ a message-passing framework combined with positional
features to get edge embeddings, which are decoded to identify anomalies. For
the second challenge, we pioneer the use of Generative Adversarial Networks to
generate negative interactions. Moreover, we design a loss function to alter
the training goal of the generator while ensuring the diversity and quality of
generated samples. Extensive experiments demonstrate that our proposed GADY
significantly outperforms the previous state-of-the-art method on three
real-world datasets. Supplementary experiments further validate the
effectiveness of our model design and the necessity of each module.
</p></li>
</ul>

<h3>Title: A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16485">http://arxiv.org/abs/2310.16485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16485]] A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP(http://arxiv.org/abs/2310.16485)</code></li>
<li>Summary: <p>Event detection in time series data is crucial in various domains, including
finance, healthcare, cybersecurity, and science. Accurately identifying events
in time series data is vital for making informed decisions, detecting
anomalies, and predicting future trends. Despite extensive research exploring
diverse methods for event detection in time series, with deep learning
approaches being among the most advanced, there is still room for improvement
and innovation in this field. In this paper, we present a new deep learning
supervised method for detecting events in multivariate time series data. Our
method combines four distinct novelties compared to existing deep-learning
supervised methods. Firstly, it is based on regression instead of binary
classification. Secondly, it does not require labeled datasets where each point
is labeled; instead, it only requires reference events defined as time points
or intervals of time. Thirdly, it is designed to be robust by using a stacked
ensemble learning meta-model that combines deep learning models, ranging from
classic feed-forward neural networks (FFNs) to state-of-the-art architectures
like transformers. This ensemble approach can mitigate individual model
weaknesses and biases, resulting in more robust predictions. Finally, to
facilitate practical implementation, we have developed a Python package to
accompany our proposed method. The package, called eventdetector-ts, can be
installed through the Python Package Index (PyPI). In this paper, we present
our method and provide a comprehensive guide on the usage of the package. We
showcase its versatility and effectiveness through different real-world use
cases from natural language processing (NLP) to financial security domains.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World. (arXiv:2310.16061v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16061">http://arxiv.org/abs/2310.16061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16061]] Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World(http://arxiv.org/abs/2310.16061)</code></li>
<li>Summary: <p>The widespread use of face recognition technology has given rise to privacy
concerns, as many individuals are worried about the collection and utilization
of their facial data. To address these concerns, researchers are actively
exploring the concept of ``unlearnable examples", by adding imperceptible
perturbation to data in the model training stage, which aims to prevent the
model from learning discriminate features of the target face. However, current
methods are inefficient and cannot guarantee transferability and robustness at
the same time, causing impracticality in the real world. To remedy it, we
propose a novel method called Segue: Side-information guided generative
unlearnable examples. Specifically, we leverage a once-trained multiple-used
model to generate the desired perturbation rather than the time-consuming
gradient-based method. To improve transferability, we introduce side
information such as true labels and pseudo labels, which are inherently
consistent across different scenarios. For robustness enhancement, a distortion
layer is integrated into the training pipeline. Extensive experiments
demonstrate that the proposed Segue is much faster than previous methods
(1000$\times$) and achieves transferable effectiveness across different
datasets and model architectures. Furthermore, it can resist JPEG compression,
adversarial training, and some standard data augmentations.
</p></li>
</ul>

<h3>Title: Locally Differentially Private Document Generation Using Zero Shot Prompting. (arXiv:2310.16111v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16111">http://arxiv.org/abs/2310.16111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16111]] Locally Differentially Private Document Generation Using Zero Shot Prompting(http://arxiv.org/abs/2310.16111)</code></li>
<li>Summary: <p>Numerous studies have highlighted the privacy risks associated with
pretrained large language models. In contrast, our research offers a unique
perspective by demonstrating that pretrained large language models can
effectively contribute to privacy preservation. We propose a locally
differentially private mechanism called DP-Prompt, which leverages the power of
pretrained large language models and zero-shot prompting to counter author
de-anonymization attacks while minimizing the impact on downstream utility.
When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5),
we observe a notable reduction in the success rate of de-anonymization attacks,
showing that it surpasses existing approaches by a considerable margin despite
its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt
(with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving
a 46\% reduction in author identification F1 score against static attackers and
a 26\% reduction against adaptive attackers. We conduct extensive experiments
across six open-source large language models, ranging up to 7 billion
parameters, to analyze various effects of the privacy-utility tradeoff.
</p></li>
</ul>

<h3>Title: FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16152">http://arxiv.org/abs/2310.16152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16152]] FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering(http://arxiv.org/abs/2310.16152)</code></li>
<li>Summary: <p>Federated learning (FL) is becoming a key component in many technology-based
applications including language modeling -- where individual FL participants
often have privacy-sensitive text data in their local datasets. However,
realizing the extent of privacy leakage in federated language models is not
straightforward and the existing attacks only intend to extract data regardless
of how sensitive or naive it is. To fill this gap, in this paper, we introduce
two novel findings with regard to leaking privacy-sensitive user data from
federated language models. Firstly, we make a key observation that model
snapshots from the intermediate rounds in FL can cause greater privacy leakage
than the final trained model. Secondly, we identify that privacy leakage can be
aggravated by tampering with a model's selective weights that are specifically
responsible for memorizing the sensitive training data. We show how a malicious
client can leak the privacy-sensitive data of some other user in FL even
without any cooperation from the server. Our best-performing method improves
the membership inference recall by 29% and achieves up to 70% private data
reconstruction, evidently outperforming existing attacks with stronger
assumptions of adversary capabilities.
</p></li>
</ul>

<h3>Title: Toward Practical Privacy-Preserving Convolutional Neural Networks Exploiting Fully Homomorphic Encryption. (arXiv:2310.16530v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16530">http://arxiv.org/abs/2310.16530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16530]] Toward Practical Privacy-Preserving Convolutional Neural Networks Exploiting Fully Homomorphic Encryption(http://arxiv.org/abs/2310.16530)</code></li>
<li>Summary: <p>Incorporating fully homomorphic encryption (FHE) into the inference process
of a convolutional neural network (CNN) draws enormous attention as a viable
approach for achieving private inference (PI). FHE allows delegating the entire
computation process to the server while ensuring the confidentiality of
sensitive client-side data. However, practical FHE implementation of a CNN
faces significant hurdles, primarily due to FHE's substantial computational and
memory overhead. To address these challenges, we propose a set of
optimizations, which includes GPU/ASIC acceleration, an efficient activation
function, and an optimized packing scheme. We evaluate our method using the
ResNet models on the CIFAR-10 and ImageNet datasets, achieving several orders
of magnitude improvement compared to prior work and reducing the latency of the
encrypted CNN inference to 1.4 seconds on an NVIDIA A100 GPU. We also show that
the latency drops to a mere 0.03 seconds with a custom hardware design.
</p></li>
</ul>

<h3>Title: Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs. (arXiv:2310.16105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16105">http://arxiv.org/abs/2310.16105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16105]] Locally Differentially Private Gradient Tracking for Distributed Online Learning over Directed Graphs(http://arxiv.org/abs/2310.16105)</code></li>
<li>Summary: <p>Distributed online learning has been proven extremely effective in solving
large-scale machine learning problems involving streaming data. However,
information sharing between learners in distributed learning also raises
concerns about the potential leakage of individual learners' sensitive data. To
mitigate this risk, differential privacy, which is widely regarded as the "gold
standard" for privacy protection, has been widely employed in many existing
results on distributed online learning. However, these results often face a
fundamental tradeoff between learning accuracy and privacy. In this paper, we
propose a locally differentially private gradient tracking based distributed
online learning algorithm that successfully circumvents this tradeoff. Our
analysis shows that the proposed algorithm converges in mean square to the
exact optimal solution while ensuring rigorous local differential privacy, with
the cumulative privacy budget guaranteed to be finite even when the number of
iterations tends to infinity. The algorithm is applicable even when the
communication graph among learners is directed. To the best of our knowledge,
this is the first result that simultaneously ensures learning accuracy and
rigorous local differential privacy in distributed online learning over
directed graphs. We evaluate our algorithm's performance by using multiple
benchmark machine-learning applications, including logistic regression on the
"Mushrooms" dataset and CNN-based image classification on the "MNIST" and
"CIFAR-10" datasets, respectively. The experimental results confirm that the
proposed algorithm outperforms existing counterparts in both training and
testing accuracies.
</p></li>
</ul>

<h3>Title: Brainchop: Next Generation Web-Based Neuroimaging Application. (arXiv:2310.16162v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16162">http://arxiv.org/abs/2310.16162</a></li>
<li>Code URL: https://github.com/neuroneural/brainchop</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16162]] Brainchop: Next Generation Web-Based Neuroimaging Application(http://arxiv.org/abs/2310.16162)</code></li>
<li>Summary: <p>Performing volumetric image processing directly within the browser,
particularly with medical data, presents unprecedented challenges compared to
conventional backend tools. These challenges arise from limitations inherent in
browser environments, such as constrained computational resources and the
availability of frontend machine learning libraries. Consequently, there is a
shortage of neuroimaging frontend tools capable of providing comprehensive
end-to-end solutions for whole brain preprocessing and segmentation while
preserving end-user data privacy and residency. In light of this context, we
introduce Brainchop (<a href="http://www.brainchop.org">this http URL</a>) as a groundbreaking in-browser
neuroimaging tool that enables volumetric analysis of structural MRI using
pre-trained full-brain deep learning models, all without requiring technical
expertise or intricate setup procedures. Beyond its commitment to data privacy,
this frontend tool offers multiple features, including scalability, low
latency, user-friendly operation, cross-platform compatibility, and enhanced
accessibility. This paper outlines the processing pipeline of Brainchop and
evaluates the performance of models across various software and hardware
configurations. The results demonstrate the practicality of client-side
processing for volumetric data, owing to the robust MeshNet architecture, even
within the resource-constrained environment of web browsers.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Can Virtual Reality Protect Users from Keystroke Inference Attacks?. (arXiv:2310.16191v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16191">http://arxiv.org/abs/2310.16191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16191]] Can Virtual Reality Protect Users from Keystroke Inference Attacks?(http://arxiv.org/abs/2310.16191)</code></li>
<li>Summary: <p>Virtual Reality (VR) has gained popularity by providing immersive and
interactive experiences without geographical limitations. It also provides a
sense of personal privacy through physical separation. In this paper, we show
that despite assumptions of enhanced privacy, VR is unable to shield its users
from side-channel attacks that steal private information. Ironically, this
vulnerability arises from VR's greatest strength, its immersive and interactive
nature. We demonstrate this by designing and implementing a new set of
keystroke inference attacks in shared virtual environments, where an attacker
(VR user) can recover the content typed by another VR user by observing their
avatar. While the avatar displays noisy telemetry of the user's hand motion, an
intelligent attacker can use that data to recognize typed keys and reconstruct
typed content, without knowing the keyboard layout or gathering labeled data.
We evaluate the proposed attacks using IRB-approved user studies across
multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately
recognize 86%-98% of typed keys, and the recovered content retains up to 98% of
the meaning of the original typed content. We also discuss potential defenses.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping. (arXiv:2310.16540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16540">http://arxiv.org/abs/2310.16540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16540]] Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping(http://arxiv.org/abs/2310.16540)</code></li>
<li>Summary: <p>The malicious applications of deep forgery, represented by face swapping,
have introduced security threats such as misinformation dissemination and
identity fraud. While some research has proposed the use of robust watermarking
methods to trace the copyright of facial images for post-event traceability,
these methods cannot effectively prevent the generation of forgeries at the
source and curb their dissemination. To address this problem, we propose a
novel comprehensive active defense mechanism that combines traceability and
adversariality, called Dual Defense. Dual Defense invisibly embeds a single
robust watermark within the target face to actively respond to sudden cases of
malicious face swapping. It disrupts the output of the face swapping model
while maintaining the integrity of watermark information throughout the entire
dissemination process. This allows for watermark extraction at any stage of
image tracking for traceability. Specifically, we introduce a watermark
embedding network based on original-domain feature impersonation attack. This
network learns robust adversarial features of target facial images and embeds
watermarks, seeking a well-balanced trade-off between watermark invisibility,
adversariality, and traceability through perceptual adversarial encoding
strategies. Extensive experiments demonstrate that Dual Defense achieves
optimal overall defense success rates and exhibits promising universality in
anti-face swapping tasks and dataset generalization ability. It maintains
impressive adversariality and traceability in both original and robust
settings, surpassing current forgery defense methods that possess only one of
these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD
methods.
</p></li>
</ul>

<h3>Title: Defense Against Model Extraction Attacks on Recommender Systems. (arXiv:2310.16335v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16335">http://arxiv.org/abs/2310.16335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16335]] Defense Against Model Extraction Attacks on Recommender Systems(http://arxiv.org/abs/2310.16335)</code></li>
<li>Summary: <p>The robustness of recommender systems has become a prominent topic within the
research community. Numerous adversarial attacks have been proposed, but most
of them rely on extensive prior knowledge, such as all the white-box attacks or
most of the black-box attacks which assume that certain external knowledge is
available. Among these attacks, the model extraction attack stands out as a
promising and practical method, involving training a surrogate model by
repeatedly querying the target model. However, there is a significant gap in
the existing literature when it comes to defending against model extraction
attacks on recommender systems. In this paper, we introduce Gradient-based
Ranking Optimization (GRO), which is the first defense strategy designed to
counter such attacks. We formalize the defense as an optimization problem,
aiming to minimize the loss of the protected target model while maximizing the
loss of the attacker's surrogate model. Since top-k ranking lists are
non-differentiable, we transform them into swap matrices which are instead
differentiable. These swap matrices serve as input to a student model that
emulates the surrogate model's behavior. By back-propagating the loss of the
student model, we obtain gradients for the swap matrices. These gradients are
used to compute a swap loss, which maximizes the loss of the student model. We
conducted experiments on three benchmark datasets to evaluate the performance
of GRO, and the results demonstrate its superior effectiveness in defending
against model extraction attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Length is a Curse and a Blessing for Document-level Semantics. (arXiv:2310.16193v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16193">http://arxiv.org/abs/2310.16193</a></li>
<li>Code URL: https://github.com/gowitheflow-1998/la-ser-cubed</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16193]] Length is a Curse and a Blessing for Document-level Semantics(http://arxiv.org/abs/2310.16193)</code></li>
<li>Summary: <p>In recent years, contrastive learning (CL) has been extensively utilized to
recover sentence and document-level encoding capability from pre-trained
language models. In this work, we question the length generalizability of
CL-based models, i.e., their vulnerability towards length-induced semantic
shift. We verify not only that length vulnerability is a significant yet
overlooked research gap, but we can devise unsupervised CL methods solely
depending on the semantic signal provided by document length. We first derive
the theoretical foundations underlying length attacks, showing that elongating
a document would intensify the high intra-document similarity that is already
brought by CL. Moreover, we found that isotropy promised by CL is highly
dependent on the length range of text exposed in training. Inspired by these
findings, we introduce a simple yet universal document representation learning
framework, LA(SER)$^{3}$: length-agnostic self-reference for semantically
robust sentence representation learning, achieving state-of-the-art
unsupervised performance on the standard information retrieval benchmark.
</p></li>
</ul>

<h3>Title: Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks. (arXiv:2310.16224v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16224">http://arxiv.org/abs/2310.16224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16224]] Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks(http://arxiv.org/abs/2310.16224)</code></li>
<li>Summary: <p>The performance of machine learning models depends on the quality of the
underlying data. Malicious actors can attack the model by poisoning the
training data. Current detectors are tied to either specific data types,
models, or attacks, and therefore have limited applicability in real-world
scenarios. This paper presents a novel fully-agnostic framework, DIVA
(Detecting InVisible Attacks), that detects attacks solely relying on analyzing
the potentially poisoned data set. DIVA is based on the idea that poisoning
attacks can be detected by comparing the classifier's accuracy on poisoned and
clean data and pre-trains a meta-learner using Complexity Measures to estimate
the otherwise unknown accuracy on a hypothetical clean dataset. The framework
applies to generic poisoning attacks. For evaluation purposes, in this paper,
we test DIVA on label-flipping attacks.
</p></li>
</ul>

<h3>Title: On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts. (arXiv:2310.16613v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16613">http://arxiv.org/abs/2310.16613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16613]] On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts(http://arxiv.org/abs/2310.16613)</code></li>
<li>Summary: <p>Text-to-image models like Stable Diffusion have had a profound impact on
daily life by enabling the generation of photorealistic images from textual
prompts, fostering creativity, and enhancing visual experiences across various
applications. However, these models also pose risks. Previous studies have
successfully demonstrated that manipulated prompts can elicit text-to-image
models to generate unsafe images, e.g., hateful meme variants. Yet, these
studies only unleash the harmful power of text-to-image models in a passive
manner. In this work, we focus on the proactive generation of unsafe images
using targeted benign prompts via poisoning attacks. We propose two poisoning
attacks: a basic attack and a utility-preserving attack. We qualitatively and
quantitatively evaluate the proposed attacks using four representative hateful
memes and multiple query prompts. Experimental results indicate that
text-to-image models are vulnerable to the basic attack even with five
poisoning samples. However, the poisoning effect can inadvertently spread to
non-targeted prompts, leading to undesirable side effects. Root cause analysis
identifies conceptual similarity as an important contributing factor to the
side effects. To address this, we introduce the utility-preserving attack as a
viable mitigation strategy to maintain the attack stealthiness, while ensuring
decent attack performance. Our findings underscore the potential risks of
adopting text-to-image models in real-world scenarios, calling for future
research and safety measures in this space.
</p></li>
</ul>

<h3>Title: Context-aware feature attribution through argumentation. (arXiv:2310.16157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16157">http://arxiv.org/abs/2310.16157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16157]] Context-aware feature attribution through argumentation(http://arxiv.org/abs/2310.16157)</code></li>
<li>Summary: <p>Feature attribution is a fundamental task in both machine learning and data
analysis, which involves determining the contribution of individual features or
variables to a model's output. This process helps identify the most important
features for predicting an outcome. The history of feature attribution methods
can be traced back to General Additive Models (GAMs), which extend linear
regression models by incorporating non-linear relationships between dependent
and independent variables. In recent years, gradient-based methods and
surrogate models have been applied to unravel complex Artificial Intelligence
(AI) systems, but these methods have limitations. GAMs tend to achieve lower
accuracy, gradient-based methods can be difficult to interpret, and surrogate
models often suffer from stability and fidelity issues. Furthermore, most
existing methods do not consider users' contexts, which can significantly
influence their preferences. To address these limitations and advance the
current state-of-the-art, we define a novel feature attribution framework
called Context-Aware Feature Attribution Through Argumentation (CA-FATA). Our
framework harnesses the power of argumentation by treating each feature as an
argument that can either support, attack or neutralize a prediction.
Additionally, CA-FATA formulates feature attribution as an argumentation
procedure, and each computation has explicit semantics, which makes it
inherently interpretable. CA-FATA also easily integrates side information, such
as users' contexts, resulting in more accurate predictions.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery. (arXiv:2310.16212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16212">http://arxiv.org/abs/2310.16212</a></li>
<li>Code URL: https://github.com/rudrakshkapil/shadowsense</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16212]] ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery(http://arxiv.org/abs/2310.16212)</code></li>
<li>Summary: <p>Accurate detection of individual tree crowns from remote sensing data poses a
significant challenge due to the dense nature of forest canopy and the presence
of diverse environmental variations, e.g., overlapping canopies, occlusions,
and varying lighting conditions. Additionally, the lack of data for training
robust models adds another limitation in effectively studying complex forest
conditions. This paper presents a novel method for detecting shadowed tree
crowns and provides a challenging dataset comprising roughly 50k paired
RGB-thermal images to facilitate future research for illumination-invariant
detection. The proposed method (ShadowSense) is entirely self-supervised,
leveraging domain adversarial training without source domain annotations for
feature extraction and foreground feature alignment for feature pyramid
networks to adapt domain-invariant representations by focusing on visible
foreground regions, respectively. It then fuses complementary information of
both modalities to effectively improve upon the predictions of an RGB-trained
detector and boost the overall accuracy. Extensive experiments demonstrate the
superiority of the proposed method over both the baseline RGB-trained detector
and state-of-the-art techniques that rely on unsupervised domain adaptation or
early image fusion. Our code and data are available:
https://github.com/rudrakshkapil/ShadowSense
</p></li>
</ul>

<h3>Title: Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16221">http://arxiv.org/abs/2310.16221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16221]] Hierarchical Randomized Smoothing(http://arxiv.org/abs/2310.16221)</code></li>
<li>Summary: <p>Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p></li>
</ul>

<h3>Title: TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16226">http://arxiv.org/abs/2310.16226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16226]] TiC-CLIP: Continual Training of CLIP Models(http://arxiv.org/abs/2310.16226)</code></li>
<li>Summary: <p>Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to continually train these models. This problem is exacerbated by
the lack of any large scale continual learning benchmarks or baselines. We
introduce the first set of web-scale Time-Continual (TiC) benchmarks for
training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with
over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first
use our benchmarks to curate various dynamic evaluations to measure temporal
robustness of existing models. We show OpenAI's CLIP (trained on data up to
2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from
2021--2022 compared with more recently trained models in OpenCLIP repository.
We then study how to efficiently train models on time-continuous data. We
demonstrate that a simple rehearsal-based approach that continues training from
the last checkpoint and replays old data reduces compute by $2.5\times$ when
compared to the standard practice of retraining from scratch.
</p></li>
</ul>

<h3>Title: SCB-ST-Dataset4: Extending the Spatio-Temporal Behavior Dataset in Student Classroom Scenarios Through Image Dataset Method. (arXiv:2310.16267v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16267">http://arxiv.org/abs/2310.16267</a></li>
<li>Code URL: https://github.com/whiffe/scb-dataset</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16267]] SCB-ST-Dataset4: Extending the Spatio-Temporal Behavior Dataset in Student Classroom Scenarios Through Image Dataset Method(http://arxiv.org/abs/2310.16267)</code></li>
<li>Summary: <p>Using deep learning methods to detect students' classroom behavior
automatically is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
spatio-temporal datasets on student behavior, as well as the high cost of
manually labeling such datasets, pose significant challenges for researchers in
this field. To address this issue, we proposed a method for extending the
spatio-temporal behavior dataset in Student Classroom Scenarios
(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 754094
images with 25670 labels, focusing on 3 behaviors: hand-raising, reading,
writing. Our proposed method can rapidly generate spatio-temporal behavioral
datasets without requiring annotation. Furthermore, we proposed a Behavior
Similarity Index (BSI) to explore the similarity of behaviors. We evaluated the
dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast algorithms, achieving a
mean average precision (map) of up to 82.3%. The experiment further
demonstrates the effectiveness of our method. This dataset provides a robust
foundation for future research in student behavior detection, potentially
contributing to advancements in this field. The SCB-ST-Dataset4 is available
for download at: https://github.com/Whiffe/SCB-dataset.
</p></li>
</ul>

<h3>Title: Towards Large-scale Masked Face Recognition. (arXiv:2310.16364v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16364">http://arxiv.org/abs/2310.16364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16364]] Towards Large-scale Masked Face Recognition(http://arxiv.org/abs/2310.16364)</code></li>
<li>Summary: <p>During the COVID-19 coronavirus epidemic, almost everyone is wearing masks,
which poses a huge challenge for deep learning-based face recognition
algorithms. In this paper, we will present our \textbf{championship} solutions
in ICCV MFR WebFace260M and InsightFace unconstrained tracks. We will focus on
four challenges in large-scale masked face recognition, i.e., super-large scale
training, data noise handling, masked and non-masked face recognition accuracy
balancing, and how to design inference-friendly model architecture. We hope
that the discussion on these four aspects can guide future research towards
more robust masked face recognition systems.
</p></li>
</ul>

<h3>Title: Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization. (arXiv:2310.16391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16391">http://arxiv.org/abs/2310.16391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16391]] Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization(http://arxiv.org/abs/2310.16391)</code></li>
<li>Summary: <p>Out-of-Distribution (OOD) Generalization aims to learn robust models that
generalize well to various environments without fitting to
distribution-specific features. Recent studies based on Lottery Ticket
Hypothesis (LTH) address this problem by minimizing the learning target to find
some of the parameters that are critical to the task. However, in OOD problems,
such solutions are suboptimal as the learning task contains severe distribution
noises, which can mislead the optimization process. Therefore, apart from
finding the task-related parameters (i.e., invariant parameters), we propose
Exploring Variant parameters for Invariant Learning (EVIL) which also leverages
the distribution knowledge to find the parameters that are sensitive to
distribution shift (i.e., variant parameters). Once the variant parameters are
left out of invariant learning, a robust subnetwork that is resistant to
distribution shift can be found. Additionally, the parameters that are
relatively stable across distributions can be considered invariant ones to
improve invariant learning. By fully exploring both variant and invariant
parameters, our EVIL can effectively identify a robust subnetwork to improve
OOD generalization. In extensive experiments on integrated testbed: DomainBed,
EVIL can effectively and efficiently enhance many popular methods, such as ERM,
IRM, SAM, etc.
</p></li>
</ul>

<h3>Title: An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing XGBoost and xDeepFM Algorithms. (arXiv:2310.16430v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16430">http://arxiv.org/abs/2310.16430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16430]] An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing XGBoost and xDeepFM Algorithms(http://arxiv.org/abs/2310.16430)</code></li>
<li>Summary: <p>Stroke prediction plays a crucial role in preventing and managing this
debilitating condition. In this study, we address the challenge of stroke
prediction using a comprehensive dataset, and propose an ensemble model that
combines the power of XGBoost and xDeepFM algorithms. Our work aims to improve
upon existing stroke prediction models by achieving higher accuracy and
robustness. Through rigorous experimentation, we validate the effectiveness of
our ensemble model using the AUC metric. Through comparing our findings with
those of other models in the field, we gain valuable insights into the merits
and drawbacks of various approaches. This, in turn, contributes significantly
to the progress of machine learning and deep learning techniques specifically
in the domain of stroke prediction.
</p></li>
</ul>

<h3>Title: DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction. (arXiv:2310.16459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16459">http://arxiv.org/abs/2310.16459</a></li>
<li>Code URL: https://github.com/cwangai/dualmatch</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16459]] DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction(http://arxiv.org/abs/2310.16459)</code></li>
<li>Summary: <p>Semi-supervised learning provides an expressive framework for exploiting
unlabeled data when labels are insufficient. Previous semi-supervised learning
methods typically match model predictions of different data-augmented views in
a single-level interaction manner, which highly relies on the quality of
pseudo-labels and results in semi-supervised learning not robust. In this
paper, we propose a novel SSL method called DualMatch, in which the class
prediction jointly invokes feature embedding in a dual-level interaction
manner. DualMatch requires consistent regularizations for data augmentation,
specifically, 1) ensuring that different augmented views are regulated with
consistent class predictions, and 2) ensuring that different data of one class
are regulated with similar feature embeddings. Extensive experiments
demonstrate the effectiveness of DualMatch. In the standard SSL setting, the
proposal achieves 9% error reduction compared with SOTA methods, even in a more
challenging class-imbalanced setting, the proposal can still achieve 6% error
reduction. Code is available at https://github.com/CWangAI/DualMatch
</p></li>
</ul>

<h3>Title: Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents. (arXiv:2310.16527v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16527">http://arxiv.org/abs/2310.16527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16527]] Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents(http://arxiv.org/abs/2310.16527)</code></li>
<li>Summary: <p>This paper introduces a deep learning model tailored for document information
analysis, emphasizing document classification, entity relation extraction, and
document visual question answering. The proposed model leverages
transformer-based models to encode all the information present in a document
image, including textual, visual, and layout information. The model is
pre-trained and subsequently fine-tuned for various document image analysis
tasks. The proposed model incorporates three additional tasks during the
pre-training phase, including reading order identification of different layout
segments in a document image, layout segments categorization as per PubLayNet,
and generation of the text sequence within a given layout segment (text block).
The model also incorporates a collective pre-training scheme where losses of
all the tasks under consideration, including pre-training and fine-tuning tasks
with all datasets, are considered. Additional encoder and decoder blocks are
added to the RoBERTa network to generate results for all tasks. The proposed
model achieved impressive results across all tasks, with an accuracy of 95.87%
on the RVL-CDIP dataset for document classification, F1 scores of 0.9306,
0.9804, 0.9794, and 0.8742 on the FUNSD, CORD, SROIE, and Kleister-NDA datasets
respectively for entity relation extraction, and an ANLS score of 0.8468 on the
DocVQA dataset for visual question answering. The results highlight the
effectiveness of the proposed model in understanding and interpreting complex
document layouts and content, making it a promising tool for document analysis
tasks.
</p></li>
</ul>

<h3>Title: Learning Robust Deep Visual Representations from EEG Brain Recordings. (arXiv:2310.16532v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16532">http://arxiv.org/abs/2310.16532</a></li>
<li>Code URL: https://github.com/prajwalsingh/eegstylegan-ada</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16532]] Learning Robust Deep Visual Representations from EEG Brain Recordings(http://arxiv.org/abs/2310.16532)</code></li>
<li>Summary: <p>Decoding the human brain has been a hallmark of neuroscientists and
Artificial Intelligence researchers alike. Reconstruction of visual images from
brain Electroencephalography (EEG) signals has garnered a lot of interest due
to its applications in brain-computer interfacing. This study proposes a
two-stage method where the first step is to obtain EEG-derived features for
robust learning of deep representations and subsequently utilize the learned
representation for image generation and classification. We demonstrate the
generalizability of our feature extraction pipeline across three different
datasets using deep-learning architectures with supervised and contrastive
learning methods. We have performed the zero-shot EEG classification task to
support the generalizability claim further. We observed that a subject
invariant linearly separable visual representation was learned using EEG data
alone in an unimodal setting that gives better k-means accuracy as compared to
a joint representation learning between EEG and images. Finally, we propose a
novel framework to transform unseen images into the EEG space and reconstruct
them with approximation, showcasing the potential for image reconstruction from
EEG signals. Our proposed image synthesis method from EEG shows 62.9% and
36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz
datasets, which is better than state-of-the-art performance in GAN.
</p></li>
</ul>

<h3>Title: Real-time 6-DoF Pose Estimation by an Event-based Camera using Active LED Markers. (arXiv:2310.16618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16618">http://arxiv.org/abs/2310.16618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16618]] Real-time 6-DoF Pose Estimation by an Event-based Camera using Active LED Markers(http://arxiv.org/abs/2310.16618)</code></li>
<li>Summary: <p>Real-time applications for autonomous operations depend largely on fast and
robust vision-based localization systems. Since image processing tasks require
processing large amounts of data, the computational resources often limit the
performance of other processes. To overcome this limitation, traditional
marker-based localization systems are widely used since they are easy to
integrate and achieve reliable accuracy. However, classical marker-based
localization systems significantly depend on standard cameras with low frame
rates, which often lack accuracy due to motion blur. In contrast, event-based
cameras provide high temporal resolution and a high dynamic range, which can be
utilized for fast localization tasks, even under challenging visual conditions.
This paper proposes a simple but effective event-based pose estimation system
using active LED markers (ALM) for fast and accurate pose estimation. The
proposed algorithm is able to operate in real time with a latency below
\SI{0.5}{\milli\second} while maintaining output rates of \SI{3}{\kilo \hertz}.
Experimental results in static and dynamic scenarios are presented to
demonstrate the performance of the proposed approach in terms of computational
speed and absolute accuracy, using the OptiTrack system as the basis for
measurement.
</p></li>
</ul>

<h3>Title: EdgeCalib: Multi-Frame Weighted Edge Features for Automatic Targetless LiDAR-Camera Calibration. (arXiv:2310.16629v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16629">http://arxiv.org/abs/2310.16629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16629]] EdgeCalib: Multi-Frame Weighted Edge Features for Automatic Targetless LiDAR-Camera Calibration(http://arxiv.org/abs/2310.16629)</code></li>
<li>Summary: <p>In multimodal perception systems, achieving precise extrinsic calibration
between LiDAR and camera is of critical importance. Previous calibration
methods often required specific targets or manual adjustments, making them both
labor-intensive and costly. Online calibration methods based on features have
been proposed, but these methods encounter challenges such as imprecise feature
extraction, unreliable cross-modality associations, and high scene-specific
requirements. To address this, we introduce an edge-based approach for
automatic online calibration of LiDAR and cameras in real-world scenarios. The
edge features, which are prevalent in various environments, are aligned in both
images and point clouds to determine the extrinsic parameters. Specifically,
stable and robust image edge features are extracted using a SAM-based method
and the edge features extracted from the point cloud are weighted through a
multi-frame weighting strategy for feature filtering. Finally, accurate
extrinsic parameters are optimized based on edge correspondence constraints. We
conducted evaluations on both the KITTI dataset and our dataset. The results
show a state-of-the-art rotation accuracy of 0.086{\deg} and a translation
accuracy of 0.977 cm, outperforming existing edge-based calibration methods in
both precision and robustness.
</p></li>
</ul>

<h3>Title: Robust Source-Free Domain Adaptation for Fundus Image Segmentation. (arXiv:2310.16665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16665">http://arxiv.org/abs/2310.16665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16665]] Robust Source-Free Domain Adaptation for Fundus Image Segmentation(http://arxiv.org/abs/2310.16665)</code></li>
<li>Summary: <p>Unsupervised Domain Adaptation (UDA) is a learning technique that transfers
knowledge learned in the source domain from labelled training data to the
target domain with only unlabelled data. It is of significant importance to
medical image segmentation because of the usual lack of labelled training data.
Although extensive efforts have been made to optimize UDA techniques to improve
the accuracy of segmentation models in the target domain, few studies have
addressed the robustness of these models under UDA. In this study, we propose a
two-stage training strategy for robust domain adaptation. In the source
training stage, we utilize adversarial sample augmentation to enhance the
robustness and generalization capability of the source model. And in the target
training stage, we propose a novel robust pseudo-label and pseudo-boundary
(PLPB) method, which effectively utilizes unlabeled target data to generate
pseudo labels and pseudo boundaries that enable model self-adaptation without
requiring source data. Extensive experimental results on cross-domain fundus
image segmentation confirm the effectiveness and versatility of our method.
Source code of this study is openly accessible at
https://github.com/LinGrayy/PLPB.
</p></li>
</ul>

<h3>Title: GenKIE: Robust Generative Multimodal Document Key Information Extraction. (arXiv:2310.16131v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16131">http://arxiv.org/abs/2310.16131</a></li>
<li>Code URL: https://github.com/glasgow-ai4biomed/genkie</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16131]] GenKIE: Robust Generative Multimodal Document Key Information Extraction(http://arxiv.org/abs/2310.16131)</code></li>
<li>Summary: <p>Key information extraction (KIE) from scanned documents has gained increasing
attention because of its applications in various domains. Although promising
results have been achieved by some recent KIE approaches, they are usually
built based on discriminative models, which lack the ability to handle optical
character recognition (OCR) errors and require laborious token-level labelling.
In this paper, we propose a novel generative end-to-end model, named GenKIE, to
address the KIE task. GenKIE is a sequence-to-sequence multimodal generative
model that utilizes multimodal encoders to embed visual, layout and textual
features and a decoder to generate the desired output. Well-designed prompts
are leveraged to incorporate the label semantics as the weakly supervised
signals and entice the generation of the key information. One notable advantage
of the generative model is that it enables automatic correction of OCR errors.
Besides, token-level granular annotation is not required. Extensive experiments
on multiple public real-world datasets show that GenKIE effectively generalizes
over different types of documents and achieves state-of-the-art results. Our
experiments also validate the model's robustness against OCR errors, making
GenKIE highly applicable in real-world scenarios.
</p></li>
</ul>

<h3>Title: Can You Follow Me? Testing Situational Understanding in ChatGPT. (arXiv:2310.16135v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16135">http://arxiv.org/abs/2310.16135</a></li>
<li>Code URL: https://github.com/yangalan123/situationaltesting</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16135]] Can You Follow Me? Testing Situational Understanding in ChatGPT(http://arxiv.org/abs/2310.16135)</code></li>
<li>Summary: <p>Understanding sentence meanings and updating information states appropriately
across time -- what we call "situational understanding" (SU) -- is a critical
ability for human-like AI agents. SU is essential in particular for chat
models, such as ChatGPT, to enable consistent, coherent, and effective dialogue
between humans and AI. Previous works have identified certain SU limitations in
non-chatbot Large Language models (LLMs), but the extent and causes of these
limitations are not well understood, and capabilities of current chat-based
models in this domain have not been explored. In this work we tackle these
questions, proposing a novel synthetic environment for SU testing which allows
us to do controlled and systematic testing of SU in chat-oriented models,
through assessment of models' ability to track and enumerate environment
states. Our environment also allows for close analysis of dynamics of model
performance, to better understand underlying causes for performance patterns.
We apply our test to ChatGPT, the state-of-the-art chatbot, and find that
despite the fundamental simplicity of the task, the model's performance
reflects an inability to retain correct environment states across time. Our
follow-up analyses suggest that performance degradation is largely because
ChatGPT has non-persistent in-context memory (although it can access the full
dialogue history) and it is susceptible to hallucinated updates -- including
updates that artificially inflate accuracies. Our findings suggest overall that
ChatGPT is not currently equipped for robust tracking of situation states, and
that trust in the impressive dialogue performance of ChatGPT comes with risks.
We release the codebase for reproducing our test environment, as well as all
prompts and API responses from ChatGPT, at
https://github.com/yangalan123/SituationalTesting.
</p></li>
</ul>

<h3>Title: Is ChatGPT a Good Multi-Party Conversation Solver?. (arXiv:2310.16301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16301">http://arxiv.org/abs/2310.16301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16301]] Is ChatGPT a Good Multi-Party Conversation Solver?(http://arxiv.org/abs/2310.16301)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have emerged as influential instruments within
the realm of natural language processing; nevertheless, their capacity to
handle multi-party conversations (MPCs) -- a scenario marked by the presence of
multiple interlocutors involved in intricate information exchanges -- remains
uncharted. In this paper, we delve into the potential of generative LLMs such
as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is
conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by
subjecting them to evaluation across three MPC datasets that encompass five
representative tasks. The findings reveal that ChatGPT's performance on a
number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results
portend a promising future. Additionally, we endeavor to bolster performance
through the incorporation of MPC structures, encompassing both speaker and
addressee architecture. This study provides an exhaustive evaluation and
analysis of applying generative LLMs to MPCs, casting a light upon the
conception and creation of increasingly effective and robust MPC agents.
Concurrently, this work underscores the challenges implicit in the utilization
of LLMs for MPCs, such as deciphering graphical information flows and
generating stylistically consistent responses.
</p></li>
</ul>

<h3>Title: Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors. (arXiv:2310.16609v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16609">http://arxiv.org/abs/2310.16609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16609]] Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors(http://arxiv.org/abs/2310.16609)</code></li>
<li>Summary: <p>In a spoken dialogue system, an NLU model is preceded by a speech recognition
system that can deteriorate the performance of natural language understanding.
This paper proposes a method for investigating the impact of speech recognition
errors on the performance of natural language understanding models. The
proposed method combines the back transcription procedure with a fine-grained
technique for categorizing the errors that affect the performance of NLU
models. The method relies on the usage of synthesized speech for NLU
evaluation. We show that the use of synthesized speech in place of audio
recording does not change the outcomes of the presented technique in a
significant way.
</p></li>
</ul>

<h3>Title: SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations. (arXiv:2310.16676v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16676">http://arxiv.org/abs/2310.16676</a></li>
<li>Code URL: https://github.com/taoshi1998/sslcl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16676]] SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations(http://arxiv.org/abs/2310.16676)</code></li>
<li>Summary: <p>Emotion recognition in conversations (ERC) is a rapidly evolving task within
the natural language processing community, which aims to detect the emotions
expressed by speakers during a conversation. Recently, a growing number of ERC
methods have focused on leveraging supervised contrastive learning (SCL) to
enhance the robustness and generalizability of learned features. However,
current SCL-based approaches in ERC are impeded by the constraint of large
batch sizes and the lack of compatibility with most existing ERC models. To
address these challenges, we propose an efficient and model-agnostic SCL
framework named Supervised Sample-Label Contrastive Learning with Soft-HGR
Maximal Correlation (SSLCL), which eliminates the need for a large batch size
and can be seamlessly integrated with existing ERC models without introducing
any model-specific assumptions. Specifically, we introduce a novel perspective
on utilizing label representations by projecting discrete labels into dense
embeddings through a shallow multilayer perceptron, and formulate the training
objective to maximize the similarity between sample features and their
corresponding ground-truth label embeddings, while minimizing the similarity
between sample features and label embeddings of disparate classes. Moreover, we
innovatively adopt the Soft-HGR maximal correlation as a measure of similarity
between sample features and label embeddings, leading to significant
performance improvements over conventional similarity measures. Additionally,
multimodal cues of utterances are effectively leveraged by SSLCL as data
augmentations to boost model performances. Extensive experiments on two ERC
benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and
superiority of our proposed SSLCL framework compared to existing
state-of-the-art SCL methods. Our code is available at
\url{https://github.com/TaoShi1998/SSLCL}.
</p></li>
</ul>

<h3>Title: The Hyperdimensional Transform: a Holographic Representation of Functions. (arXiv:2310.16065v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16065">http://arxiv.org/abs/2310.16065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16065]] The Hyperdimensional Transform: a Holographic Representation of Functions(http://arxiv.org/abs/2310.16065)</code></li>
<li>Summary: <p>Integral transforms are invaluable mathematical tools to map functions into
spaces where they are easier to characterize. We introduce the hyperdimensional
transform as a new kind of integral transform. It converts square-integrable
functions into noise-robust, holographic, high-dimensional representations
called hyperdimensional vectors. The central idea is to approximate a function
by a linear combination of random functions. We formally introduce a set of
stochastic, orthogonal basis functions and define the hyperdimensional
transform and its inverse. We discuss general transform-related properties such
as its uniqueness, approximation properties of the inverse transform, and the
representation of integrals and derivatives. The hyperdimensional transform
offers a powerful, flexible framework that connects closely with other integral
transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it
provides theoretical foundations and new insights for the field of
hyperdimensional computing, a computing paradigm that is rapidly gaining
attention for efficient and explainable machine learning algorithms, with
potential applications in statistical modelling and machine learning. In
addition, we provide straightforward and easily understandable code, which can
function as a tutorial and allows for the reproduction of the demonstrated
examples, from computing the transform to solving differential equations.
</p></li>
</ul>

<h3>Title: Efficient deep data assimilation with sparse observations and time-varying sensors. (arXiv:2310.16187v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16187">http://arxiv.org/abs/2310.16187</a></li>
<li>Code URL: https://github.com/dl-wg/vivid</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16187]] Efficient deep data assimilation with sparse observations and time-varying sensors(http://arxiv.org/abs/2310.16187)</code></li>
<li>Summary: <p>Variational Data Assimilation (DA) has been broadly used in engineering
problems for field reconstruction and prediction by performing a weighted
combination of multiple sources of noisy data. In recent years, the integration
of deep learning (DL) techniques in DA has shown promise in improving the
efficiency and accuracy in high-dimensional dynamical systems. Nevertheless,
existing deep DA approaches face difficulties in dealing with unstructured
observation data, especially when the placement and number of sensors are
dynamic over time. We introduce a novel variational DA scheme, named
Voronoi-tessellation Inverse operator for VariatIonal Data assimilation
(VIVID), that incorporates a DL inverse operator into the assimilation
objective function. By leveraging the capabilities of the Voronoi-tessellation
and convolutional neural networks, VIVID is adept at handling sparse,
unstructured, and time-varying sensor data. Furthermore, the incorporation of
the DL inverse operator establishes a direct link between observation and state
space, leading to a reduction in the number of minimization steps required for
DA. Additionally, VIVID can be seamlessly integrated with Proper Orthogonal
Decomposition (POD) to develop an end-to-end reduced-order DA scheme, which can
further expedite field reconstruction. Numerical experiments in a fluid
dynamics system demonstrate that VIVID can significantly outperform existing DA
and DL algorithms. The robustness of VIVID is also accessed through the
application of various levels of prior error, the utilization of varying
numbers of sensors, and the misspecification of error covariance in DA.
</p></li>
</ul>

<h3>Title: ELM Ridge Regression Boosting. (arXiv:2310.16209v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16209">http://arxiv.org/abs/2310.16209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16209]] ELM Ridge Regression Boosting(http://arxiv.org/abs/2310.16209)</code></li>
<li>Summary: <p>We discuss a boosting approach for the Ridge Regression (RR) method, with
applications to the Extreme Learning Machine (ELM), and we show that the
proposed method significantly improves the classification performance and
robustness of ELMs.
</p></li>
</ul>

<h3>Title: Corrupting Neuron Explanations of Deep Visual Features. (arXiv:2310.16332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16332">http://arxiv.org/abs/2310.16332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16332]] Corrupting Neuron Explanations of Deep Visual Features(http://arxiv.org/abs/2310.16332)</code></li>
<li>Summary: <p>The inability of DNNs to explain their black-box behavior has led to a recent
surge of explainability methods. However, there are growing concerns that these
explainability methods are not robust and trustworthy. In this work, we perform
the first robustness analysis of Neuron Explanation Methods under a unified
pipeline and show that these explanations can be significantly corrupted by
random noises and well-designed perturbations added to their probing data. We
find that even adding small random noise with a standard deviation of 0.02 can
already change the assigned concepts of up to 28% neurons in the deeper layers.
Furthermore, we devise a novel corruption algorithm and show that our algorithm
can manipulate the explanation of more than 80% neurons by poisoning less than
10% of probing data. This raises the concern of trusting Neuron Explanation
Methods in real-life safety and fairness critical applications.
</p></li>
</ul>

<h3>Title: ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training. (arXiv:2310.16453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16453">http://arxiv.org/abs/2310.16453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16453]] ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training(http://arxiv.org/abs/2310.16453)</code></li>
<li>Summary: <p>Due to costly efforts during data acquisition and model training, Deep Neural
Networks (DNNs) belong to the intellectual property of the model creator.
Hence, unauthorized use, theft, or modification may lead to legal
repercussions. Existing DNN watermarking methods for ownership proof are often
non-intuitive, embed human-invisible marks, require trust in algorithmic
assessment that lacks human-understandable attributes, and rely on rigid
thresholds, making it susceptible to failure in cases of partial watermark
erasure.
</p>
<p>This paper introduces ClearMark, the first DNN watermarking method designed
for intuitive human assessment. ClearMark embeds visible watermarks, enabling
human decision-making without rigid value thresholds while allowing
technology-assisted evaluations. ClearMark defines a transposed model
architecture allowing to use of the model in a backward fashion to interwove
the watermark with the main task within all model parameters. Compared to
existing watermarking methods, ClearMark produces visual watermarks that are
easy for humans to understand without requiring complex verification algorithms
or strict thresholds. The watermark is embedded within all model parameters and
entangled with the main task, exhibiting superior robustness. It shows an
8,544-bit watermark capacity comparable to the strongest existing work.
Crucially, ClearMark's effectiveness is model and dataset-agnostic, and
resilient against adversarial model manipulations, as demonstrated in a
comprehensive study performed with four datasets and seven architectures.
</p></li>
</ul>

<h3>Title: TSONN: Time-stepping-oriented neural network for solving partial differential equations. (arXiv:2310.16491v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16491">http://arxiv.org/abs/2310.16491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16491]] TSONN: Time-stepping-oriented neural network for solving partial differential equations(http://arxiv.org/abs/2310.16491)</code></li>
<li>Summary: <p>Deep neural networks (DNNs), especially physics-informed neural networks
(PINNs), have recently become a new popular method for solving forward and
inverse problems governed by partial differential equations (PDEs). However,
these methods still face challenges in achieving stable training and obtaining
correct results in many problems, since minimizing PDE residuals with PDE-based
soft constraint make the problem ill-conditioned. Different from all existing
methods that directly minimize PDE residuals, this work integrates
time-stepping method with deep learning, and transforms the original
ill-conditioned optimization problem into a series of well-conditioned
sub-problems over given pseudo time intervals. The convergence of model
training is significantly improved by following the trajectory of the pseudo
time-stepping process, yielding a robust optimization-based PDE solver. Our
results show that the proposed method achieves stable training and correct
results in many problems that standard PINNs fail to solve, requiring only a
simple modification on the loss function. In addition, we demonstrate several
novel properties and advantages of time-stepping methods within the framework
of neural network-based optimization approach, in comparison to traditional
grid-based numerical method. Specifically, explicit scheme allows significantly
larger time step, while implicit scheme can be implemented as straightforwardly
as explicit scheme.
</p></li>
</ul>

<h3>Title: Data Optimization in Deep Learning: A Survey. (arXiv:2310.16499v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16499">http://arxiv.org/abs/2310.16499</a></li>
<li>Code URL: https://github.com/yaorujing/data-optimization</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16499]] Data Optimization in Deep Learning: A Survey(http://arxiv.org/abs/2310.16499)</code></li>
<li>Summary: <p>Large-scale, high-quality data are considered an essential factor for the
successful application of many deep learning techniques. Meanwhile, numerous
real-world deep learning tasks still have to contend with the lack of
sufficient amounts of high-quality data. Additionally, issues such as model
robustness, fairness, and trustworthiness are also closely related to training
data. Consequently, a huge number of studies in the existing literature have
focused on the data aspect in deep learning tasks. Some typical data
optimization techniques include data augmentation, logit perturbation, sample
weighting, and data condensation. These techniques usually come from different
deep learning divisions and their theoretical inspirations or heuristic
motivations may seem unrelated to each other. This study aims to organize a
wide range of existing data optimization methodologies for deep learning from
the previous literature, and makes the effort to construct a comprehensive
taxonomy for them. The constructed taxonomy considers the diversity of split
dimensions, and deep sub-taxonomies are constructed for each dimension. On the
basis of the taxonomy, connections among the extensive data optimization
methods for deep learning are built in terms of four aspects. We probe into
rendering several promising and interesting future directions. The constructed
taxonomy and the revealed connections will enlighten the better understanding
of existing methods and the design of novel data optimization techniques.
Furthermore, our aspiration for this survey is to promote data optimization as
an independent subdivision of deep learning. A curated, up-to-date list of
resources related to data optimization in deep learning is available at
\url{https://github.com/YaoRujing/Data-Optimization}.
</p></li>
</ul>

<h3>Title: How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels. (arXiv:2310.16652v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16652">http://arxiv.org/abs/2310.16652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16652]] How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels(http://arxiv.org/abs/2310.16652)</code></li>
<li>Summary: <p>Because of its privacy-preserving capability, federated learning (FL) has
attracted significant attention from both academia and industry. However, when
being implemented over wireless networks, it is not clear how much
communication error can be tolerated by FL. This paper investigates the
robustness of FL to the uplink and downlink communication error. Our
theoretical analysis reveals that the robustness depends on two critical
parameters, namely the number of clients and the numerical range of model
parameters. It is also shown that the uplink communication in FL can tolerate a
higher bit error rate (BER) than downlink communication, and this difference is
quantified by a proposed formula. The findings and theoretical analyses are
further validated by extensive experiments.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites. (arXiv:2310.16148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16148">http://arxiv.org/abs/2310.16148</a></li>
<li>Code URL: https://github.com/nosaveddata/yinyang_cnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16148]] Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites(http://arxiv.org/abs/2310.16148)</code></li>
<li>Summary: <p>Computer vision in general presented several advances such as training
optimizations, new architectures (pure attention, efficient block, vision
language models, generative models, among others). This have improved
performance in several tasks such as classification, and others. However, the
majority of these models focus on modifications that are taking distance from
realistic neuroscientific approaches related to the brain. In this work, we
adopt a more bio-inspired approach and present the Yin Yang Convolutional
Network, an architecture that extracts visual manifold, its blocks are intended
to separate analysis of colors and forms at its initial layers, simulating
occipital lobe's operations. Our results shows that our architecture provides
State-of-the-Art efficiency among low parameter architectures in the dataset
CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the
older SOTA in this category, while having 150k less parameters (726k in total).
Our second model uses 52k parameters, losing only 3.86\% test accuracy. We also
performed an analysis on ImageNet, where we reached 66.49\% validation accuracy
with 1.6M parameters. We make the code publicly available at:
https://github.com/NoSavedDATA/YinYang_CNN.
</p></li>
</ul>

<h3>Title: Deepfake Detection: Leveraging the Power of 2D and 3D CNN Ensembles. (arXiv:2310.16388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16388">http://arxiv.org/abs/2310.16388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16388]] Deepfake Detection: Leveraging the Power of 2D and 3D CNN Ensembles(http://arxiv.org/abs/2310.16388)</code></li>
<li>Summary: <p>In the dynamic realm of deepfake detection, this work presents an innovative
approach to validate video content. The methodology blends advanced
2-dimensional and 3-dimensional Convolutional Neural Networks. The 3D model is
uniquely tailored to capture spatiotemporal features via sliding filters,
extending through both spatial and temporal dimensions. This configuration
enables nuanced pattern recognition in pixel arrangement and temporal evolution
across frames. Simultaneously, the 2D model leverages EfficientNet
architecture, harnessing auto-scaling in Convolutional Neural Networks.
Notably, this ensemble integrates Voting Ensembles and Adaptive Weighted
Ensembling. Strategic prioritization of the 3-dimensional model's output
capitalizes on its exceptional spatio-temporal feature extraction. Experimental
validation underscores the effectiveness of this strategy, showcasing its
potential in countering deepfake generation's deceptive practices.
</p></li>
</ul>

<h3>Title: XFEVER: Exploring Fact Verification across Languages. (arXiv:2310.16278v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16278">http://arxiv.org/abs/2310.16278</a></li>
<li>Code URL: https://github.com/nii-yamagishilab/xfever</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16278]] XFEVER: Exploring Fact Verification across Languages(http://arxiv.org/abs/2310.16278)</code></li>
<li>Summary: <p>This paper introduces the Cross-lingual Fact Extraction and VERification
(XFEVER) dataset designed for benchmarking the fact verification models across
different languages. We constructed it by translating the claim and evidence
texts of the Fact Extraction and VERification (FEVER) dataset into six
languages. The training and development sets were translated using machine
translation, whereas the test set includes texts translated by professional
translators and machine-translated texts. Using the XFEVER dataset, two
cross-lingual fact verification scenarios, zero-shot learning and
translate-train learning, are defined, and baseline models for each scenario
are also proposed in this paper. Experimental results show that the
multilingual language model can be used to build fact verification models in
different languages efficiently. However, the performance varies by language
and is somewhat inferior to the English case. We also found that we can
effectively mitigate model miscalibration by considering the prediction
similarity between the English and target languages. The XFEVER dataset, code,
and model checkpoints are available at
https://github.com/nii-yamagishilab/xfever.
</p></li>
</ul>

<h3>Title: Unraveling Feature Extraction Mechanisms in Neural Networks. (arXiv:2310.16350v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16350">http://arxiv.org/abs/2310.16350</a></li>
<li>Code URL: https://github.com/richardsun-voyager/ufemnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16350]] Unraveling Feature Extraction Mechanisms in Neural Networks(http://arxiv.org/abs/2310.16350)</code></li>
<li>Summary: <p>The underlying mechanism of neural networks in capturing precise knowledge
has been the subject of consistent research efforts. In this work, we propose a
theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such
mechanisms. Specifically, considering the infinite network width, we
hypothesize the learning dynamics of target models may intuitively unravel the
features they acquire from training data, deepening our insights into their
internal mechanisms. We apply our approach to several fundamental models and
reveal how these models leverage statistical features during gradient descent
and how they are integrated into final decisions. We also discovered that the
choice of activation function can affect feature extraction. For instance, the
use of the \textit{ReLU} activation function could potentially introduce a bias
in features, providing a plausible explanation for its replacement with
alternative functions in recent pre-trained language models. Additionally, we
find that while self-attention and CNN models may exhibit limitations in
learning n-grams, multiplication-based models seem to excel in this area. We
verify these theoretical findings through experiments and find that they can be
applied to analyze language modeling tasks, which can be regarded as a special
variant of classification. Our contributions offer insights into the roles and
capacities of fundamental components within large language models, thereby
aiding the broader understanding of these complex systems.
</p></li>
</ul>

<h3>Title: From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction. (arXiv:2310.16358v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16358">http://arxiv.org/abs/2310.16358</a></li>
<li>Code URL: https://github.com/zhangyx0417/simple_to_complex</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16358]] From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction(http://arxiv.org/abs/2310.16358)</code></li>
<li>Summary: <p>Document-level Event Argument Extraction (EAE) requires the model to extract
arguments of multiple events from a single document. Considering the underlying
dependencies between these events, recent efforts leverage the idea of
"memory", where the results of already predicted events are cached and can be
retrieved to help the prediction of upcoming events. These methods extract
events according to their appearance order in the document, however, the event
that appears in the first sentence does not mean that it is the easiest to
extract. Existing methods might introduce noise to the extraction of upcoming
events if they rely on an incorrect prediction of previous events. In order to
provide more reliable memory, we propose a simple-to-complex progressive
framework for document-level EAE. Specifically, we first calculate the
difficulty of each event and then, we conduct the extraction following a
simple-to-complex order. In this way, the memory will store the most certain
results, and the model could use these reliable sources to help the prediction
of more difficult events. Experiments on WikiEvents show that our model
outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex
framework is useful in the EAE task.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning. (arXiv:2310.16538v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16538">http://arxiv.org/abs/2310.16538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16538]] FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning(http://arxiv.org/abs/2310.16538)</code></li>
<li>Summary: <p>Psychiatrists diagnose mental disorders via the linguistic use of patients.
Still, due to data privacy, existing passive mental health monitoring systems
use alternative features such as activity, app usage, and location via mobile
devices. We propose FedTherapist, a mobile mental health monitoring system that
utilizes continuous speech and keyboard input in a privacy-preserving way via
federated learning. We explore multiple model designs by comparing their
performance and overhead for FedTherapist to overcome the complex nature of
on-device language model training on smartphones. We further propose a
Context-Aware Language Learning (CALL) methodology to effectively utilize
smartphones' large and noisy text for mental health signal sensing. Our
IRB-approved evaluation of the prediction of self-reported depression, stress,
anxiety, and mood from 46 participants shows higher accuracy of FedTherapist
compared with the performance with non-language features, achieving 0.15 AUROC
improvement and 8.21% MAE reduction.
</p></li>
</ul>

<h3>Title: Over-the-air Federated Policy Gradient. (arXiv:2310.16592v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16592">http://arxiv.org/abs/2310.16592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16592]] Over-the-air Federated Policy Gradient(http://arxiv.org/abs/2310.16592)</code></li>
<li>Summary: <p>In recent years, over-the-air aggregation has been widely considered in
large-scale distributed learning, optimization, and sensing. In this paper, we
propose the over-the-air federated policy gradient algorithm, where all agents
simultaneously broadcast an analog signal carrying local information to a
common wireless channel, and a central controller uses the received aggregated
waveform to update the policy parameters. We investigate the effect of noise
and channel distortion on the convergence of the proposed algorithm, and
establish the complexities of communication and sampling for finding an
$\epsilon$-approximate stationary point. Finally, we present some simulation
results to show the effectiveness of the algorithm.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception. (arXiv:2310.16542v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16542">http://arxiv.org/abs/2310.16542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16542]] ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception(http://arxiv.org/abs/2310.16542)</code></li>
<li>Summary: <p>LiDAR is a sensor system that supports autonomous driving by gathering
precise geometric information about the scene. Exploiting this information for
perception is interesting as the amount of available data increases.
</p>
<p>As the quantitative performance of various perception tasks has improved, the
focus has shifted from source-to-source perception to domain adaptation and
domain generalization for perception. These new goals require access to a large
variety of domains for evaluation. Unfortunately, the various annotation
strategies of data providers complicate the computation of cross-domain
performance based on the available data
</p>
<p>This paper provides a novel dataset, specifically designed for cross-domain
evaluation to make it easier to evaluate the performance of various source
datasets. Alongside the dataset, a flexible online benchmark is provided to
ensure a fair comparison across methods.
</p></li>
</ul>

<h3>Title: Diversity Enhanced Narrative Question Generation for Storybooks. (arXiv:2310.16446v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16446">http://arxiv.org/abs/2310.16446</a></li>
<li>Code URL: https://github.com/hkyoon95/mqg</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16446]] Diversity Enhanced Narrative Question Generation for Storybooks(http://arxiv.org/abs/2310.16446)</code></li>
<li>Summary: <p>Question generation (QG) from a given context can enhance comprehension,
engagement, assessment, and overall efficacy in learning or conversational
environments. Despite recent advancements in QG, the challenge of enhancing or
measuring the diversity of generated questions often remains unaddressed. In
this paper, we introduce a multi-question generation model (mQG), which is
capable of generating multiple, diverse, and answerable questions by focusing
on context and questions. To validate the answerability of the generated
questions, we employ a SQuAD2.0 fine-tuned question answering model,
classifying the questions as answerable or not. We train and evaluate mQG on
the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with
narrative questions. We further apply a zero-shot adaptation on the TellMeWhy
and SQuAD1.1 datasets. mQG shows promising results across various evaluation
metrics, among strong baselines.
</p></li>
</ul>

<h3>Title: On the Interplay between Fairness and Explainability. (arXiv:2310.16607v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16607">http://arxiv.org/abs/2310.16607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16607]] On the Interplay between Fairness and Explainability(http://arxiv.org/abs/2310.16607)</code></li>
<li>Summary: <p>In order to build reliable and trustworthy NLP applications, models need to
be both fair across different demographics and explainable. Usually these two
objectives, fairness and explainability, are optimized and/or examined
independently of each other. Instead, we argue that forthcoming, trustworthy
NLP systems should consider both. In this work, we perform a first study to
understand how they influence each other: do fair(er) models rely on more
plausible rationales? and vice versa. To this end, we conduct experiments on
two English multi-class text classification datasets, BIOS and ECtHR, that
provide information on gender and nationality, respectively, as well as
human-annotated rationales. We fine-tune pre-trained language models with
several methods for (i) bias mitigation, which aims to improve fairness; (ii)
rationale extraction, which aims to produce plausible explanations. We find
that bias mitigation algorithms do not always lead to fairer models. Moreover,
we discover that empirical fairness and explainability are orthogonal.
</p></li>
</ul>

<h3>Title: Identifying Reasons for Bias: An Argumentation-Based Approach. (arXiv:2310.16506v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16506">http://arxiv.org/abs/2310.16506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16506]] Identifying Reasons for Bias: An Argumentation-Based Approach(http://arxiv.org/abs/2310.16506)</code></li>
<li>Summary: <p>As algorithmic decision-making systems become more prevalent in society,
ensuring the fairness of these systems is becoming increasingly important.
Whilst there has been substantial research in building fair algorithmic
decision-making systems, the majority of these methods require access to the
training data, including personal characteristics, and are not transparent
regarding which individuals are classified unfairly. In this paper, we propose
a novel model-agnostic argumentation-based method to determine why an
individual is classified differently in comparison to similar individuals. Our
method uses a quantitative argumentation framework to represent attribute-value
pairs of an individual and of those similar to them, and uses a well-known
semantics to identify the attribute-value pairs in the individual contributing
most to their different classification. We evaluate our method on two datasets
commonly used in the fairness literature and illustrate its effectiveness in
the identification of bias.
</p></li>
</ul>

<h3>Title: Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16524">http://arxiv.org/abs/2310.16524</a></li>
<li>Code URL: https://github.com/seedatnabeel/3s-testing</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16524]] Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data(http://arxiv.org/abs/2310.16524)</code></li>
<li>Summary: <p>Evaluating the performance of machine learning models on diverse and
underrepresented subgroups is essential for ensuring fairness and reliability
in real-world applications. However, accurately assessing model performance
becomes challenging due to two main issues: (1) a scarcity of test data,
especially for small subgroups, and (2) possible distributional shifts in the
model's deployment setting, which may not align with the available test data.
In this work, we introduce 3S Testing, a deep generative modeling framework to
facilitate model evaluation by generating synthetic test sets for small
subgroups and simulating distributional shifts. Our experiments demonstrate
that 3S Testing outperforms traditional baselines -- including real test data
alone -- in estimating model performance on minority subgroups and under
plausible distributional shifts. In addition, 3S offers intervals around its
performance estimates, exhibiting superior coverage of the ground truth
compared to existing approaches. Overall, these results raise the question of
whether we need a paradigm shift away from limited real test data towards
synthetic test data.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Instance-wise Linearization of Neural Network for Model Interpretation. (arXiv:2310.16295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16295">http://arxiv.org/abs/2310.16295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16295]] Instance-wise Linearization of Neural Network for Model Interpretation(http://arxiv.org/abs/2310.16295)</code></li>
<li>Summary: <p>Neural network have achieved remarkable successes in many scientific fields.
However, the interpretability of the neural network model is still a major
bottlenecks to deploy such technique into our daily life. The challenge can
dive into the non-linear behavior of the neural network, which rises a critical
question that how a model use input feature to make a decision. The classical
approach to address this challenge is feature attribution, which assigns an
important score to each input feature and reveal its importance of current
prediction. However, current feature attribution approaches often indicate the
importance of each input feature without detail of how they are actually
processed by a model internally. These attribution approaches often raise a
concern that whether they highlight correct features for a model prediction.
</p>
<p>For a neural network model, the non-linear behavior is often caused by
non-linear activation units of a model. However, the computation behavior of a
prediction from a neural network model is locally linear, because one
prediction has only one activation pattern. Base on the observation, we propose
an instance-wise linearization approach to reformulates the forward computation
process of a neural network prediction. This approach reformulates different
layers of convolution neural networks into linear matrix multiplication.
Aggregating all layers' computation, a prediction complex convolution neural
network operations can be described as a linear matrix multiplication $F(x) = W
\cdot x + b$. This equation can not only provides a feature attribution map
that highlights the important of the input features but also tells how each
input feature contributes to a prediction exactly. Furthermore, we discuss the
application of this technique in both supervise classification and unsupervised
neural network learning parametric t-SNE dimension reduction.
</p></li>
</ul>

<h3>Title: Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training. (arXiv:2310.16484v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16484">http://arxiv.org/abs/2310.16484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16484]] Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training(http://arxiv.org/abs/2310.16484)</code></li>
<li>Summary: <p>Representational spaces learned via language modeling are fundamental to
Natural Language Processing (NLP), however there has been limited understanding
regarding how and when during training various types of linguistic information
emerge and interact. Leveraging a novel information theoretic probing suite,
which enables direct comparisons of not just task performance, but their
representational subspaces, we analyze nine tasks covering syntax, semantics
and reasoning, across 2M pre-training steps and five seeds. We identify
critical learning phases across tasks and time, during which subspaces emerge,
share information, and later disentangle to specialize. Across these phases,
syntactic knowledge is acquired rapidly after 0.5% of full training. Continued
performance improvements primarily stem from the acquisition of open-domain
knowledge, while semantics and reasoning tasks benefit from later boosts to
long-range contextualization and higher specialization. Measuring cross-task
similarity further reveals that linguistically related tasks share information
throughout training, and do so more during the critical phase of learning than
before or after. Our findings have implications for model interpretability,
multi-task learning, and learning from limited data.
</p></li>
</ul>

<h3>Title: Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16316">http://arxiv.org/abs/2310.16316</a></li>
<li>Code URL: https://github.com/debugml/sop</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16316]] Sum-of-Parts Models: Faithful Attributions for Groups of Features(http://arxiv.org/abs/2310.16316)</code></li>
<li>Summary: <p>An explanation of a machine learning model is considered "faithful" if it
accurately reflects the model's decision-making process. However, explanations
such as feature attributions for deep learning are not guaranteed to be
faithful, and can produce potentially misleading interpretations. In this work,
we develop Sum-of-Parts (SOP), a class of models whose predictions come with
grouped feature attributions that are faithful-by-construction. This model
decomposes a prediction into an interpretable sum of scores, each of which is
directly attributable to a sparse group of features. We evaluate SOP on
benchmarks with standard interpretability metrics, and in a case study, we use
the faithful explanations from SOP to help astrophysicists discover new
knowledge about galaxy formation.
</p></li>
</ul>

<h3>Title: Towards Self-Interpretable Graph-Level Anomaly Detection. (arXiv:2310.16520v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16520">http://arxiv.org/abs/2310.16520</a></li>
<li>Code URL: https://github.com/yixinliu233/signet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16520]] Towards Self-Interpretable Graph-Level Anomaly Detection(http://arxiv.org/abs/2310.16520)</code></li>
<li>Summary: <p>Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit
notable dissimilarity compared to the majority in a collection. However,
current works primarily focus on evaluating graph-level abnormality while
failing to provide meaningful explanations for the predictions, which largely
limits their reliability and application scope. In this paper, we investigate a
new challenging problem, explainable GLAD, where the learning objective is to
predict the abnormality of each graph sample with corresponding explanations,
i.e., the vital subgraph that leads to the predictions. To address this
challenging problem, we propose a Self-Interpretable Graph aNomaly dETection
model (SIGNET for short) that detects anomalous graphs as well as generates
informative explanations simultaneously. Specifically, we first introduce the
multi-view subgraph information bottleneck (MSIB) framework, serving as the
design basis of our self-interpretable GLAD approach. This way SIGNET is able
to not only measure the abnormality of each graph based on cross-view mutual
information but also provide informative graph rationales by extracting
bottleneck subgraphs from the input graph and its dual hypergraph in a
self-supervised way. Extensive experiments on 16 datasets demonstrate the
anomaly detection capability and self-interpretability of SIGNET.
</p></li>
</ul>

<h3>Title: Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach. (arXiv:2310.16647v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16647">http://arxiv.org/abs/2310.16647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16647]] Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach(http://arxiv.org/abs/2310.16647)</code></li>
<li>Summary: <p>Regularizing Deep Neural Networks (DNNs) is essential for improving
generalizability and preventing overfitting. Fixed penalty methods, though
common, lack adaptability and suffer from hyperparameter sensitivity. In this
paper, we propose a novel approach to DNN regularization by framing the
training process as a constrained optimization problem. Where the data fidelity
term is the minimization objective and the regularization terms serve as
constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method
to achieve a more flexible and efficient regularization mechanism. Our approach
extends beyond black-box regularization, demonstrating significant improvements
in white-box models, where weights are often subject to hard constraints to
ensure interpretability. Experimental results on image-based classification on
MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our
approach. SAL consistently achieves higher Accuracy while also achieving better
constraint satisfaction, thus showcasing its potential for optimizing DNNs
under constrained settings.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16436">http://arxiv.org/abs/2310.16436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16436]] DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models(http://arxiv.org/abs/2310.16436)</code></li>
<li>Summary: <p>A long-standing goal of AI systems is to perform complex multimodal reasoning
like humans. Recently, large language models (LLMs) have made remarkable
strides in such multi-step reasoning on the language modality solely by
leveraging the chain of thought (CoT) to mimic human thinking. However, the
transfer of these advancements to multimodal contexts introduces heightened
challenges, including but not limited to the impractical need for
labor-intensive annotation and the limitations in terms of flexibility,
generalizability, and explainability. To evoke CoT reasoning in multimodality,
this work first conducts an in-depth analysis of these challenges posed by
multimodality and presents two key insights: "keeping critical thinking" and
"letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this
study proposes a novel DDCoT prompting that maintains a critical attitude
through negative-space prompting and incorporates multimodality into reasoning
by first dividing the reasoning responsibility of LLMs into reasoning and
recognition and then integrating the visual recognition capability of visual
models into the joint reasoning process. The rationales generated by DDCoT not
only improve the reasoning abilities of both large and small language models in
zero-shot prompting and fine-tuning learning, significantly outperforming
state-of-the-art methods but also exhibit impressive generalizability and
explainability.
</p></li>
</ul>

<h3>Title: Towards Explainability in Monocular Depth Estimation. (arXiv:2310.16457v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16457">http://arxiv.org/abs/2310.16457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16457]] Towards Explainability in Monocular Depth Estimation(http://arxiv.org/abs/2310.16457)</code></li>
<li>Summary: <p>The estimation of depth in two-dimensional images has long been a challenging
and extensively studied subject in computer vision. Recently, significant
progress has been made with the emergence of Deep Learning-based approaches,
which have proven highly successful. This paper focuses on the explainability
in monocular depth estimation methods, in terms of how humans perceive depth.
This preliminary study emphasizes on one of the most significant visual cues,
the relative size, which is prominent in almost all viewed images. We designed
a specific experiment to mimic the experiments in humans and have tested
state-of-the-art methods to indirectly assess the explainability in the context
defined. In addition, we observed that measuring the accuracy required further
attention and a particular approach is proposed to this end. The results show
that a mean accuracy of around 77% across methods is achieved, with some of the
methods performing markedly better, thus, indirectly revealing their
corresponding potential to uncover monocular depth cues, like relative size.
</p></li>
</ul>

<h3>Title: Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models. (arXiv:2310.16584v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16584">http://arxiv.org/abs/2310.16584</a></li>
<li>Code URL: https://github.com/ltx-code/ltx</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16584]] Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models(http://arxiv.org/abs/2310.16584)</code></li>
<li>Summary: <p>We present Learning to Explain (LTX), a model-agnostic framework designed for
providing post-hoc explanations for vision models. The LTX framework introduces
an "explainer" model that generates explanation maps, highlighting the crucial
regions that justify the predictions made by the model being explained. To
train the explainer, we employ a two-stage process consisting of initial
pretraining followed by per-instance finetuning. During both stages of
training, we utilize a unique configuration where we compare the explained
model's prediction for a masked input with its original prediction for the
unmasked input. This approach enables the use of a novel counterfactual
objective, which aims to anticipate the model's output using masked versions of
the input image. Importantly, the LTX framework is not restricted to a specific
model architecture and can provide explanations for both Transformer-based and
convolutional models. Through our evaluations, we demonstrate that LTX
significantly outperforms the current state-of-the-art in explainability across
various metrics.
</p></li>
</ul>

<h3>Title: Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks. (arXiv:2310.16639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16639">http://arxiv.org/abs/2310.16639</a></li>
<li>Code URL: https://github.com/jessicamecht/concept_gridlock</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16639]] Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks(http://arxiv.org/abs/2310.16639)</code></li>
<li>Summary: <p>Concept bottleneck models have been successfully used for explainable machine
learning by encoding information within the model with a set of human-defined
concepts. In the context of human-assisted or autonomous driving,
explainability models can help user acceptance and understanding of decisions
made by the autonomous vehicle, which can be used to rationalize and explain
driver or vehicle behavior. We propose a new approach using concept bottlenecks
as visual features for control command predictions and explanations of user and
vehicle behavior. We learn a human-understandable concept layer that we use to
explain sequential driving scenes while learning vehicle control commands. This
approach can then be used to determine whether a change in a preferred gap or
steering commands from a human (or autonomous vehicle) is led by an external
stimulus or change in preferences. We achieve competitive performance to latent
visual features while gaining interpretability within our model setup.
</p></li>
</ul>

<h3>Title: CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset. (arXiv:2310.16225v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16225">http://arxiv.org/abs/2310.16225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16225]] CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset(http://arxiv.org/abs/2310.16225)</code></li>
<li>Summary: <p>The CoNLL-03 corpus is arguably the most well-known and utilized benchmark
dataset for named entity recognition (NER). However, prior works found
significant numbers of annotation errors, incompleteness, and inconsistencies
in the data. This poses challenges to objectively comparing NER approaches and
analyzing their errors, as current state-of-the-art models achieve F1-scores
that are comparable to or even exceed the estimated noise level in CoNLL-03. To
address this issue, we present a comprehensive relabeling effort assisted by
automatic consistency checking that corrects 7.0% of all labels in the English
CoNLL-03. Our effort adds a layer of entity linking annotation both for better
explainability of NER labels and as additional safeguard of annotation quality.
Our experimental evaluation finds not only that state-of-the-art approaches
reach significantly higher F1-scores (97.1%) on our data, but crucially that
the share of correct predictions falsely counted as errors due to annotation
noise drops from 47% to 6%. This indicates that our resource is well suited to
analyze the remaining errors made by state-of-the-art models, and that the
theoretical upper bound even on high resource, coarse-grained NER is not yet
reached. To facilitate such analysis, we make CleanCoNLL publicly available to
the research community.
</p></li>
</ul>

<h3>Title: DyExplainer: Explainable Dynamic Graph Neural Networks. (arXiv:2310.16375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16375">http://arxiv.org/abs/2310.16375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16375]] DyExplainer: Explainable Dynamic Graph Neural Networks(http://arxiv.org/abs/2310.16375)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) resurge as a trending research subject owing to
their impressive ability to capture representations from graph-structured data.
However, the black-box nature of GNNs presents a significant challenge in terms
of comprehending and trusting these models, thereby limiting their practical
applications in mission-critical scenarios. Although there has been substantial
progress in the field of explaining GNNs in recent years, the majority of these
studies are centered on static graphs, leaving the explanation of dynamic GNNs
largely unexplored. Dynamic GNNs, with their ever-evolving graph structures,
pose a unique challenge and require additional efforts to effectively capture
temporal dependencies and structural relationships. To address this challenge,
we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly.
DyExplainer trains a dynamic GNN backbone to extract representations of the
graph at each snapshot, while simultaneously exploring structural relationships
and temporal dependencies through a sparse attention technique. To preserve the
desired properties of the explanation, such as structural consistency and
temporal continuity, we augment our approach with contrastive learning
techniques to provide priori-guided regularization. To model longer-term
temporal dependencies, we develop a buffer-based live-updating scheme for
training. The results of our extensive experiments on various datasets
demonstrate the superiority of DyExplainer, not only providing faithful
explainability of the model predictions but also significantly improving the
model prediction accuracy, as evidenced in the link prediction task.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis. (arXiv:2310.16074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16074">http://arxiv.org/abs/2310.16074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16074]] RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis(http://arxiv.org/abs/2310.16074)</code></li>
<li>Summary: <p>Pose-guided person image synthesis task requires re-rendering a reference
image, which should have a photorealistic appearance and flawless pose
transfer. Since person images are highly structured, existing approaches
require dense connections for complex deformations and occlusions because these
are generally handled through multi-level warping and masking in latent space.
But the feature maps generated by convolutional neural networks do not have
equivariance, and hence even the multi-level warping does not have a perfect
pose alignment. Inspired by the ability of the diffusion model to generate
photorealistic images from the given conditional guidance, we propose recurrent
pose alignment to provide pose-aligned texture features as conditional
guidance. Moreover, we propose gradient guidance from pose interaction fields,
which output the distance from the valid pose manifold given a target pose as
input. This helps in learning plausible pose transfer trajectories that result
in photorealism and undistorted texture details. Extensive results on two
large-scale benchmarks and a user study demonstrate the ability of our proposed
approach to generate photorealistic pose transfer under challenging scenarios.
Additionally, we prove the efficiency of gradient guidance in pose-guided image
generation on the HumanArt dataset with fine-tuned stable diffusion.
</p></li>
</ul>

<h3>Title: iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis. (arXiv:2310.16167v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16167">http://arxiv.org/abs/2310.16167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16167]] iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis(http://arxiv.org/abs/2310.16167)</code></li>
<li>Summary: <p>We present a method for generating consistent novel views from a single
source image. Our approach focuses on maximizing the reuse of visible pixels
from the source image. To achieve this, we use a monocular depth estimator that
transfers visible pixels from the source view to the target view. Starting from
a pre-trained 2D inpainting diffusion model, we train our method on the
large-scale Objaverse dataset to learn 3D object priors. While training we use
a novel masking mechanism based on epipolar lines to further improve the
quality of our approach. This allows our framework to perform zero-shot novel
view synthesis on a variety of objects. We evaluate the zero-shot abilities of
our framework on three challenging datasets: Google Scanned Objects, Ray Traced
Multiview, and Common Objects in 3D. See our webpage for more details:
https://yashkant.github.io/invs/
</p></li>
</ul>

<h3>Title: Dolfin: Diffusion Layout Transformers without Autoencoder. (arXiv:2310.16305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16305">http://arxiv.org/abs/2310.16305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16305]] Dolfin: Diffusion Layout Transformers without Autoencoder(http://arxiv.org/abs/2310.16305)</code></li>
<li>Summary: <p>In this paper, we introduce a novel generative model, Diffusion Layout
Transformers without Autoencoder (Dolfin), which significantly improves the
modeling capability with reduced complexity compared to existing methods.
Dolfin employs a Transformer-based diffusion process to model layout
generation. In addition to an efficient bi-directional (non-causal joint)
sequence representation, we further propose an autoregressive diffusion model
(Dolfin-AR) that is especially adept at capturing rich semantic correlations
for the neighboring objects, such as alignment, size, and overlap. When
evaluated against standard generative layout benchmarks, Dolfin notably
improves performance across various metrics (fid, alignment, overlap, MaxIoU
and DocSim scores), enhancing transparency and interoperability in the process.
Moreover, Dolfin's applications extend beyond layout generation, making it
suitable for modeling geometric structures, such as line segments. Our
experiments present both qualitative and quantitative results to demonstrate
the advantages of Dolfin.
</p></li>
</ul>

<h3>Title: DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection. (arXiv:2310.16349v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16349">http://arxiv.org/abs/2310.16349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16349]] DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection(http://arxiv.org/abs/2310.16349)</code></li>
<li>Summary: <p>Denoising diffusion models show remarkable performances in generative tasks,
and their potential applications in perception tasks are gaining interest. In
this paper, we introduce a novel framework named DiffRef3D which adopts the
diffusion process on 3D object detection with point clouds for the first time.
Specifically, we formulate the proposal refinement stage of two-stage 3D object
detectors as a conditional diffusion process. During training, DiffRef3D
gradually adds noise to the residuals between proposals and target objects,
then applies the noisy residuals to proposals to generate hypotheses. The
refinement module utilizes these hypotheses to denoise the noisy residuals and
generate accurate box predictions. In the inference phase, DiffRef3D generates
initial hypotheses by sampling noise from a Gaussian distribution as residuals
and refines the hypotheses through iterative steps. DiffRef3D is a versatile
proposal refinement framework that consistently improves the performance of
existing 3D object detection models. We demonstrate the significance of
DiffRef3D through extensive experiments on the KITTI benchmark. Code will be
available.
</p></li>
</ul>

<h3>Title: Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16400">http://arxiv.org/abs/2310.16400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16400]] Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models(http://arxiv.org/abs/2310.16400)</code></li>
<li>Summary: <p>Latent Diffusion Models (LDMs) are renowned for their powerful capabilities
in image and video synthesis. Yet, video editing methods suffer from
insufficient pre-training data or video-by-video re-training cost. In
addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a
training-free framework to achieve text-guided video editing by applying
off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses
latents from an image LDM and an video LDM during the denoising process. In
this way, temporal consistency can be kept with video LDM while high-fidelity
from the image LDM can also be exploited. Meanwhile, FLDM possesses high
flexibility since both image LDM and video LDM can be replaced so advanced
image editing methods such as InstructPix2Pix and ControlNet can be exploited.
To the best of our knowledge, FLDM is the first method to adapt off-the-shelf
image editing methods into video LDMs for video editing. Extensive quantitative
and qualitative experiments demonstrate that FLDM can improve the textual
alignment and temporal consistency of edited videos.
</p></li>
</ul>

<h3>Title: Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models. (arXiv:2310.16573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16573">http://arxiv.org/abs/2310.16573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16573]] Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models(http://arxiv.org/abs/2310.16573)</code></li>
<li>Summary: <p>We do not pursue a novel method in this paper, but aim to study if a modern
text-to-image diffusion model can tailor any task-adaptive image classifier
across domains and categories. Existing domain adaptive image classification
works exploit both source and target data for domain alignment so as to
transfer the knowledge learned from the labeled source data to the unlabeled
target data. However, as the development of the text-to-image diffusion model,
we wonder if the high-fidelity synthetic data from the text-to-image generator
can serve as a surrogate of the source data in real world. In this way, we do
not need to collect and annotate the source data for each domain adaptation
task in a one-for-one manner. Instead, we utilize only one off-the-shelf
text-to-image model to synthesize images with category labels derived from the
corresponding text prompts, and then leverage the surrogate data as a bridge to
transfer the knowledge embedded in the task-agnostic text-to-image generator to
the task-oriented image classifier via domain adaptation. Such a one-for-all
adaptation paradigm allows us to adapt anything in the world using only one
text-to-image generator as well as the corresponding unlabeled target data.
Extensive experiments validate the feasibility of the proposed idea, which even
surpasses the state-of-the-art domain adaptation works using the source data
collected and annotated in real world.
</p></li>
</ul>

<h3>Title: A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16656">http://arxiv.org/abs/2310.16656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16656]] A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation(http://arxiv.org/abs/2310.16656)</code></li>
<li>Summary: <p>Text-to-image diffusion models achieved a remarkable leap in capabilities
over the last few years, enabling high-quality and diverse synthesis of images
from a textual prompt. However, even the most advanced models often struggle to
precisely follow all of the directions in their prompts. The vast majority of
these models are trained on datasets consisting of (image, caption) pairs where
the images often come from the web, and the captions are their HTML alternate
text. A notable example is the LAION dataset, used by Stable Diffusion and
other models. In this work we observe that these captions are often of low
quality, and argue that this significantly affects the model's capability to
understand nuanced semantics in the textual prompts. We show that by relabeling
the corpus with a specialized automatic captioning model and training a
text-to-image model on the recaptioned dataset, the model benefits
substantially across the board. First, in overall image quality: e.g. FID 14.84
vs. the baseline of 17.87, and 64.3% improvement in faithful image generation
according to human evaluation. Second, in semantic alignment, e.g. semantic
object accuracy 84.34 vs. 78.90, counting alignment errors 1.32 vs. 1.44 and
positional alignment 62.42 vs. 57.60. We analyze various ways to relabel the
corpus and provide evidence that this technique, which we call RECAP, both
reduces the train-inference discrepancy and provides the model with more
information per example, increasing sample efficiency and allowing the model to
better understand the relations between captions and images.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer. (arXiv:2310.16279v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16279">http://arxiv.org/abs/2310.16279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16279]] TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer(http://arxiv.org/abs/2310.16279)</code></li>
<li>Summary: <p>Estimating the 6D object pose is an essential task in many applications. Due
to the lack of depth information, existing RGB-based methods are sensitive to
occlusion and illumination changes. How to extract and utilize the geometry
features in depth information is crucial to achieve accurate predictions. To
this end, we propose TransPose, a novel 6D pose framework that exploits
Transformer Encoder with geometry-aware module to develop better learning of
point cloud feature representations. Specifically, we first uniformly sample
point cloud and extract local geometry features with the designed local feature
extractor base on graph convolution network. To improve robustness to
occlusion, we adopt Transformer to perform the exchange of global information,
making each local feature contains global information. Finally, we introduce
geometry-aware module in Transformer Encoder, which to form an effective
constrain for point cloud feature learning and makes the global information
exchange more tightly coupled with point cloud tasks. Extensive experiments
indicate the effectiveness of TransPose, our pose estimation pipeline achieves
competitive results on three benchmark datasets.
</p></li>
</ul>

<h3>Title: MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network. (arXiv:2310.16288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16288">http://arxiv.org/abs/2310.16288</a></li>
<li>Code URL: https://github.com/taatiteam/motionagformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16288]] MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network(http://arxiv.org/abs/2310.16288)</code></li>
<li>Summary: <p>Recent transformer-based approaches have demonstrated excellent performance
in 3D human pose estimation. However, they have a holistic view and by encoding
global relationships between all the joints, they do not capture the local
dependencies precisely. In this paper, we present a novel Attention-GCNFormer
(AGFormer) block that divides the number of channels by using two parallel
transformer and GCNFormer streams. Our proposed GCNFormer module exploits the
local relationship between adjacent joints, outputting a new representation
that is complementary to the transformer output. By fusing these two
representation in an adaptive way, AGFormer exhibits the ability to better
learn the underlying 3D structure. By stacking multiple AGFormer blocks, we
propose MotionAGFormer in four different variants, which can be chosen based on
the speed-accuracy trade-off. We evaluate our model on two popular benchmark
datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves
state-of-the-art results, with P1 errors of 38.4mm and 16.2mm, respectively.
Remarkably, it uses a quarter of the parameters and is three times more
computationally efficient than the previous leading model on Human3.6M dataset.
Code and models are available at https://github.com/TaatiTeam/MotionAGFormer.
</p></li>
</ul>

<h3>Title: Video Referring Expression Comprehension via Transformer with Content-conditioned Query. (arXiv:2310.16402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16402">http://arxiv.org/abs/2310.16402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16402]] Video Referring Expression Comprehension via Transformer with Content-conditioned Query(http://arxiv.org/abs/2310.16402)</code></li>
<li>Summary: <p>Video Referring Expression Comprehension (REC) aims to localize a target
object in videos based on the queried natural language. Recent improvements in
video REC have been made using Transformer-based methods with learnable
queries. However, we contend that this naive query design is not ideal given
the open-world nature of video REC brought by text supervision. With numerous
potential semantic categories, relying on only a few slow-updated queries is
insufficient to characterize them. Our solution to this problem is to create
dynamic queries that are conditioned on both the input video and language to
model the diverse objects referred to. Specifically, we place a fixed number of
learnable bounding boxes throughout the frame and use corresponding region
features to provide prior information. Also, we noticed that current query
features overlook the importance of cross-modal alignment. To address this, we
align specific phrases in the sentence with semantically relevant visual areas,
annotating them in existing video datasets (VID-Sentence and VidSTG). By
incorporating these two designs, our proposed model (called ConFormer)
outperforms other models on widely benchmarked datasets. For example, in the
testing split of VID-Sentence dataset, ConFormer achieves 8.75% absolute
improvement on Accu.@0.6 compared to the previous state-of-the-art model.
</p></li>
</ul>

<h3>Title: Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation. (arXiv:2310.16127v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16127">http://arxiv.org/abs/2310.16127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16127]] Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation(http://arxiv.org/abs/2310.16127)</code></li>
<li>Summary: <p>Understanding Arabic text and generating human-like responses is a
challenging endeavor. While many researchers have proposed models and solutions
for individual problems, there is an acute shortage of a comprehensive Arabic
natural language generation toolkit that is capable of handling a wide range of
tasks. In this work, we present a novel Arabic text-to-text Transformer model,
namely AraT5v2. Our new model is methodically trained on extensive and diverse
data, utilizing an extended sequence length of 2,048 tokens. We explore various
pretraining strategies including unsupervised, supervised, and joint
pertaining, under both single and multitask settings. Our models outperform
competitive baselines with large margins. We take our work one step further by
developing and publicly releasing Octopus, a Python-based package and
command-line toolkit tailored for eight Arabic generation tasks all exploiting
a single model. We release the models and the toolkit on our public repository.
</p></li>
</ul>

<h3>Title: A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. (arXiv:2310.16142v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16142">http://arxiv.org/abs/2310.16142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16142]] A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing(http://arxiv.org/abs/2310.16142)</code></li>
<li>Summary: <p>Two of the central factors believed to underpin human sentence processing
difficulty are expectations and retrieval from working memory. A recent attempt
to create a unified cognitive model integrating these two factors relied on the
parallels between the self-attention mechanism of transformer language models
and cue-based retrieval theories of working memory in human sentence processing
(Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in
specialized attention heads of GPT-2 are consistent with similarity-based
interference, a key prediction of cue-based retrieval models, their method
requires identifying syntactically specialized attention heads, and makes the
cognitively implausible assumption that hundreds of memory retrieval operations
take place in parallel. In the present work, we develop a recurrent neural
language model with a single self-attention head, which more closely parallels
the memory system assumed by cognitive theories. We show that our model's
single attention head captures semantic and syntactic interference effects
observed in human experiments.
</p></li>
</ul>

<h3>Title: Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism. (arXiv:2310.16270v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16270">http://arxiv.org/abs/2310.16270</a></li>
<li>Code URL: https://github.com/msakarvadia/attentionlens</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16270]] Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism(http://arxiv.org/abs/2310.16270)</code></li>
<li>Summary: <p>Transformer-based Large Language Models (LLMs) are the state-of-the-art for
natural language tasks. Recent work has attempted to decode, by reverse
engineering the role of linear layers, the internal mechanisms by which LLMs
arrive at their final predictions for text completion tasks. Yet little is
known about the specific role of attention heads in producing the final token
prediction. We propose Attention Lens, a tool that enables researchers to
translate the outputs of attention heads into vocabulary tokens via learned
attention-head-specific transformations called lenses. Preliminary findings
from our trained lenses indicate that attention heads play highly specialized
roles in language models. The code for Attention Lens is available at
github.com/msakarvadia/AttentionLens.
</p></li>
</ul>

<h3>Title: Samsung R&D Institute Philippines at WMT 2023. (arXiv:2310.16322v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16322">http://arxiv.org/abs/2310.16322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16322]] Samsung R&D Institute Philippines at WMT 2023(http://arxiv.org/abs/2310.16322)</code></li>
<li>Summary: <p>In this paper, we describe the constrained MT systems submitted by Samsung
R&amp;D Institute Philippines to the WMT 2023 General Translation Task for two
directions: en$\rightarrow$he and he$\rightarrow$en. Our systems comprise of
Transformer-based sequence-to-sequence models that are trained with a mix of
best practices: comprehensive data preprocessing pipelines, synthetic
backtranslated data, and the use of noisy channel reranking during online
decoding. Our models perform comparably to, and sometimes outperform, strong
baseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite
having significantly fewer parameters on two public benchmarks: FLORES-200 and
NTREX-128.
</p></li>
</ul>

<h3>Title: Transformer-based Live Update Generation for Soccer Matches from Microblog Posts. (arXiv:2310.16368v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16368">http://arxiv.org/abs/2310.16368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16368]] Transformer-based Live Update Generation for Soccer Matches from Microblog Posts(http://arxiv.org/abs/2310.16368)</code></li>
<li>Summary: <p>It has been known to be difficult to generate adequate sports updates from a
sequence of vast amounts of diverse live tweets, although the live sports
viewing experience with tweets is gaining the popularity. In this paper, we
focus on soccer matches and work on building a system to generate live updates
for soccer matches from tweets so that users can instantly grasp a match's
progress and enjoy the excitement of the match from raw tweets. Our proposed
system is based on a large pre-trained language model and incorporates a
mechanism to control the number of updates and a mechanism to reduce the
redundancy of duplicate and similar updates.
</p></li>
</ul>

<h3>Title: 1-PAGER: One Pass Answer Generation and Evidence Retrieval. (arXiv:2310.16568v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16568">http://arxiv.org/abs/2310.16568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16568]] 1-PAGER: One Pass Answer Generation and Evidence Retrieval(http://arxiv.org/abs/2310.16568)</code></li>
<li>Summary: <p>We present 1-Pager the first system that answers a question and retrieves
evidence using a single Transformer-based model and decoding process. 1-Pager
incrementally partitions the retrieval corpus using constrained decoding to
select a document and answer string, and we show that this is competitive with
comparable retrieve-and-read alternatives according to both retrieval and
answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book
question answering model, by grounding predictions in an evidence corpus. While
1-Pager is not yet on-par with more expensive systems that read many more
documents before generating an answer, we argue that it provides an important
step toward attributed generation by folding retrieval into the
sequence-to-sequence paradigm that is currently dominant in NLP. We also show
that the search paths used to partition the corpus are easy to read and
understand, paving a way forward for interpretable neural retrieval.
</p></li>
</ul>

<h3>Title: ArTST: Arabic Text and Speech Transformer. (arXiv:2310.16621v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16621">http://arxiv.org/abs/2310.16621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16621]] ArTST: Arabic Text and Speech Transformer(http://arxiv.org/abs/2310.16621)</code></li>
<li>Summary: <p>We present ArTST, a pre-trained Arabic text and speech transformer for
supporting open-source speech technologies for the Arabic language. The model
architecture follows the unified-modal framework, SpeechT5, that was recently
released for English, and is focused on Modern Standard Arabic (MSA), with
plans to extend the model for dialectal and code-switched Arabic in future
editions. We pre-trained the model from scratch on MSA speech and text data,
and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR),
Text-To-Speech synthesis (TTS), and spoken dialect identification. In our
experiments comparing ArTST with SpeechT5, as well as with previously reported
results in these tasks, ArTST performs on a par with or exceeding the current
state-of-the-art in all three tasks. Moreover, we find that our pre-training is
conducive for generalization, which is particularly evident in the low-resource
TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models
are released for research use.
</p></li>
</ul>

<h3>Title: Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions. (arXiv:2310.16076v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16076">http://arxiv.org/abs/2310.16076</a></li>
<li>Code URL: https://github.com/idsia/fwp-formal-lang</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16076]] Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions(http://arxiv.org/abs/2310.16076)</code></li>
<li>Summary: <p>Recent studies of the computational power of recurrent neural networks (RNNs)
reveal a hierarchy of RNN architectures, given real-time and finite-precision
assumptions. Here we study auto-regressive Transformers with linearised
attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs).
LTs are special in the sense that they are equivalent to RNN-like sequence
processors with a fixed-size state, while they can also be expressed as the
now-popular self-attention networks. We show that many well-known results for
the standard Transformer directly transfer to LTs/FWPs. Our formal language
recognition experiments demonstrate how recently proposed FWP extensions such
as recurrent FWPs and self-referential weight matrices successfully overcome
certain limitations of the LT, e.g., allowing for generalisation on the parity
problem. Our code is public.
</p></li>
</ul>

<h3>Title: Understanding Code Semantics: An Evaluation of Transformer Models in Summarization. (arXiv:2310.16314v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16314">http://arxiv.org/abs/2310.16314</a></li>
<li>Code URL: https://github.com/Demon702/robust_code_summary</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16314]] Understanding Code Semantics: An Evaluation of Transformer Models in Summarization(http://arxiv.org/abs/2310.16314)</code></li>
<li>Summary: <p>This paper delves into the intricacies of code summarization using advanced
transformer-based language models. Through empirical studies, we evaluate the
efficacy of code summarization by altering function and variable names to
explore whether models truly understand code semantics or merely rely on
textual cues. We have also introduced adversaries like dead code and commented
code across three programming languages (Python, Javascript, and Java) to
further scrutinize the model's understanding. Ultimately, our research aims to
offer valuable insights into the inner workings of transformer-based LMs,
enhancing their ability to understand code and contributing to more efficient
software development practices and maintenance workflows.
</p></li>
</ul>

<h3>Title: Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder. (arXiv:2310.16318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16318">http://arxiv.org/abs/2310.16318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16318]] Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder(http://arxiv.org/abs/2310.16318)</code></li>
<li>Summary: <p>Despite its practical importance across a wide range of modalities, recent
advances in self-supervised learning (SSL) have been primarily focused on a few
well-curated domains, e.g., vision and language, often relying on their
domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become
one of the popular architectures in these domains, but less has explored its
potential in other modalities. In this paper, we develop MAE as a unified,
modality-agnostic SSL framework. In turn, we argue meta-learning as a key to
interpreting MAE as a modality-agnostic learner, and propose enhancements to
MAE from the motivation to jointly improve its SSL across diverse modalities,
coined MetaMAE as a result. Our key idea is to view the mask reconstruction of
MAE as a meta-learning task: masked tokens are predicted by adapting the
Transformer meta-learner through the amortization of unmasked tokens. Based on
this novel interpretation, we propose to integrate two advanced meta-learning
techniques. First, we adapt the amortized latent of the Transformer encoder
using gradient-based meta-learning to enhance the reconstruction. Then, we
maximize the alignment between amortized and adapted latents through task
contrastive learning which guides the Transformer encoder to better encode the
task-specific knowledge. Our experiment demonstrates the superiority of MetaMAE
in the modality-agnostic SSL benchmark (called DABS), significantly
outperforming prior baselines. Code is available at
https://github.com/alinlab/MetaMAE.
</p></li>
</ul>

<h3>Title: SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process. (arXiv:2310.16336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16336">http://arxiv.org/abs/2310.16336</a></li>
<li>Code URL: https://github.com/zichongli5/smurf-thp</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16336]] SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process(http://arxiv.org/abs/2310.16336)</code></li>
<li>Summary: <p>Transformer Hawkes process models have shown to be successful in modeling
event sequence data. However, most of the existing training methods rely on
maximizing the likelihood of event sequences, which involves calculating some
intractable integral. Moreover, the existing methods fail to provide
uncertainty quantification for model predictions, e.g., confidence intervals
for the predicted event's arrival time. To address these issues, we propose
SMURF-THP, a score-based method for learning Transformer Hawkes process and
quantifying prediction uncertainty. Specifically, SMURF-THP learns the score
function of events' arrival time based on a score-matching objective that
avoids the intractable computation. With such a learned score function, we can
sample arrival time of events from the predictive distribution. This naturally
allows for the quantification of uncertainty by computing confidence intervals
over the generated samples. We conduct extensive experiments in both event type
prediction and uncertainty quantification of arrival time. In all the
experiments, SMURF-THP outperforms existing likelihood-based methods in
confidence calibration while exhibiting comparable prediction accuracy.
</p></li>
</ul>

<h3>Title: Towards Control-Centric Representations in Reinforcement Learning from Images. (arXiv:2310.16655v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16655">http://arxiv.org/abs/2310.16655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16655]] Towards Control-Centric Representations in Reinforcement Learning from Images(http://arxiv.org/abs/2310.16655)</code></li>
<li>Summary: <p>Image-based Reinforcement Learning is a practical yet challenging task. A
major hurdle lies in extracting control-centric representations while
disregarding irrelevant information. While approaches that follow the
bisimulation principle exhibit the potential in learning state representations
to address this issue, they still grapple with the limited expressive capacity
of latent dynamics and the inadaptability to sparse reward environments. To
address these limitations, we introduce ReBis, which aims to capture
control-centric information by integrating reward-free control information
alongside reward-specific knowledge. ReBis utilizes a transformer architecture
to implicitly model the dynamics and incorporates block-wise masking to
eliminate spatiotemporal redundancy. Moreover, ReBis combines
bisimulation-based loss with asymmetric reconstruction loss to prevent feature
collapse in environments with sparse rewards. Empirical studies on two large
benchmarks, including Atari games and DeepMind Control Suit, demonstrate that
ReBis has superior performance compared to existing methods, proving its
effectiveness.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16228">http://arxiv.org/abs/2310.16228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16228]] On the Foundations of Shortcut Learning(http://arxiv.org/abs/2310.16228)</code></li>
<li>Summary: <p>Deep-learning models can extract a rich assortment of features from data.
Which features a model uses depends not only on predictivity-how reliably a
feature indicates train-set labels-but also on availability-how easily the
feature can be extracted, or leveraged, from inputs. The literature on shortcut
learning has noted examples in which models privilege one feature over another,
for example texture over shape and image backgrounds over foreground objects.
Here, we test hypotheses about which input properties are more available to a
model, and systematically study how predictivity and availability interact to
shape models' feature use. We construct a minimal, explicit generative
framework for synthesizing classification datasets with two latent features
that vary in predictivity and in factors we hypothesize to relate to
availability, and quantify a model's shortcut bias-its over-reliance on the
shortcut (more available, less predictive) feature at the expense of the core
(less available, more predictive) feature. We find that linear models are
relatively unbiased, but introducing a single hidden layer with ReLU or Tanh
units yields a bias. Our empirical findings are consistent with a theoretical
account based on Neural Tangent Kernels. Finally, we study how models used in
practice trade off predictivity and availability in naturalistic datasets,
discovering availability manipulations which increase models' degree of
shortcut bias. Taken together, these findings suggest that the propensity to
learn shortcut features is a fundamental characteristic of deep nonlinear
architectures warranting systematic study given its role in shaping how models
solve tasks.
</p></li>
</ul>

<h3>Title: CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts. (arXiv:2310.16329v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16329">http://arxiv.org/abs/2310.16329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16329]] CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts(http://arxiv.org/abs/2310.16329)</code></li>
<li>Summary: <p>Coherence is a linguistic term that refers to the relations between small
textual units (sentences, propositions), which make the text logically
consistent and meaningful to the reader. With the advances of generative
foundational models in NLP, there is a pressing need to automatically assess
the human-perceived coherence of automatically generated texts. Up until now,
little work has been done on explicitly assessing the coherence of generated
texts and analyzing the factors contributing to (in)coherence. Previous work on
the topic used other tasks, e.g., sentence reordering, as proxies of coherence,
rather than approaching coherence detection heads on. In this paper, we
introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of
automatically generated texts. Our annotation protocol reflects two
perspectives; one is global, assigning a single coherence score, and the other
is incremental, scoring sentence by sentence. The incremental method produces
an (in)coherence score for each text fragment and also pinpoints reasons for
incoherence at that point. Our benchmark contains 500 automatically-generated
and human-annotated paragraphs, each annotated in both methods, by multiple
raters. Our analysis shows that the inter-annotator agreement in the
incremental mode is higher than in the holistic alternative, and our
experiments show that standard LMs fine-tuned for coherence detection show
varied performance on the different factors contributing to (in)coherence. All
in all, these models yield unsatisfactory performance, emphasizing the need for
developing more reliable methods for coherence assessment.
</p></li>
</ul>

<h3>Title: Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16119">http://arxiv.org/abs/2310.16119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16119]] Alquist 5(http://arxiv.org/abs/2310.16119)</code></li>
<li>Summary: <p>We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize
SocialBot Grand Challenge~5. Building upon previous versions of our system, we
introduce the NRG Barista and outline several innovative approaches for
integrating Barista into our SocialBot, improving the overall conversational
experience. Additionally, we extend our SocialBot to support multimodal
devices. This paper offers insights into the development of Alquist~5.0, which
meets evolving user expectations while maintaining empathetic and knowledgeable
conversational abilities across diverse topics.
</p></li>
</ul>

<h3>Title: Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16624">http://arxiv.org/abs/2310.16624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16624]] Free-form Flows: Make Any Architecture a Normalizing Flow(http://arxiv.org/abs/2310.16624)</code></li>
<li>Summary: <p>Normalizing Flows are generative models that directly maximize the
likelihood. Previously, the design of normalizing flows was largely constrained
by the need for analytical invertibility. We overcome this constraint by a
training procedure that uses an efficient estimator for the gradient of the
change of variables formula. This enables any dimension-preserving neural
network to serve as a generative model through maximum likelihood training. Our
approach allows placing the emphasis on tailoring inductive biases precisely to
the task at hand. Specifically, we achieve excellent results in molecule
generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our
method is competitive in an inverse problem benchmark, while employing
off-the-shelf ResNet architectures.
</p></li>
</ul>

<h3>Title: Posterior Consistency for Missing Data in Variational Autoencoders. (arXiv:2310.16648v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16648">http://arxiv.org/abs/2310.16648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16648]] Posterior Consistency for Missing Data in Variational Autoencoders(http://arxiv.org/abs/2310.16648)</code></li>
<li>Summary: <p>We consider the problem of learning Variational Autoencoders (VAEs), i.e., a
type of deep generative model, from data with missing values. Such data is
omnipresent in real-world applications of machine learning because complete
data is often impossible or too costly to obtain. We particularly focus on
improving a VAE's amortized posterior inference, i.e., the encoder, which in
the case of missing data can be susceptible to learning inconsistent posterior
distributions regarding the missingness. To this end, we provide a formal
definition of posterior consistency and propose an approach for regularizing an
encoder's posterior distribution which promotes this consistency. We observe
that the proposed regularization suggests a different training objective than
that typically considered in the literature when facing missing values.
Furthermore, we empirically demonstrate that our regularization leads to
improved performance in missing value settings in terms of reconstruction
quality and downstream tasks utilizing uncertainty in the latent space. This
improved performance can be observed for many classes of VAEs including VAEs
equipped with normalizing flows.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: BLP 2023 Task 2: Sentiment Analysis. (arXiv:2310.16183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16183">http://arxiv.org/abs/2310.16183</a></li>
<li>Code URL: https://github.com/blp-workshop/blp_task2</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16183]] BLP 2023 Task 2: Sentiment Analysis(http://arxiv.org/abs/2310.16183)</code></li>
<li>Summary: <p>We present an overview of the BLP Sentiment Shared Task, organized as part of
the inaugural BLP 2023 workshop, co-located with EMNLP 2023. The task is
defined as the detection of sentiment in a given piece of social media text.
This task attracted interest from 71 participants, among whom 29 and 30 teams
submitted systems during the development and evaluation phases, respectively.
In total, participants submitted 597 runs. However, a total of 15 teams
submitted system description papers. The range of approaches in the submitted
systems spans from classical machine learning models, fine-tuning pre-trained
models, to leveraging Large Language Model (LLMs) in zero- and few-shot
settings. In this paper, we provide a detailed account of the task setup,
including dataset development and evaluation setup. Additionally, we provide a
brief overview of the systems submitted by the participants. All datasets and
evaluation scripts from the shared task have been made publicly available for
the research community, to foster further research in this domain
</p></li>
</ul>

<h3>Title: Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16218">http://arxiv.org/abs/2310.16218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16218]] Knowledge Editing for Large Language Models: A Survey(http://arxiv.org/abs/2310.16218)</code></li>
<li>Summary: <p>Large language models (LLMs) have recently transformed both the academic and
industrial landscapes due to their remarkable capacity to understand, analyze,
and generate texts based on their vast knowledge and reasoning ability.
Nevertheless, one major drawback of LLMs is their substantial computational
cost for pre-training due to their unprecedented amounts of parameters. The
disadvantage is exacerbated when new knowledge frequently needs to be
introduced into the pre-trained model. Therefore, it is imperative to develop
effective and efficient techniques to update pre-trained LLMs. Traditional
methods encode new knowledge in pre-trained LLMs through direct fine-tuning.
However, naively re-training LLMs can be computationally intensive and risks
degenerating valuable pre-trained knowledge irrelevant to the update in the
model. Recently, Knowledge-based Model Editing (KME) has attracted increasing
attention, which aims to precisely modify the LLMs to incorporate specific
knowledge, without negatively influencing other irrelevant knowledge. In this
survey, we aim to provide a comprehensive and in-depth overview of recent
advances in the field of KME. We first introduce a general formulation of KME
to encompass different KME strategies. Afterward, we provide an innovative
taxonomy of KME techniques based on how the new knowledge is introduced into
pre-trained LLMs, and investigate existing KME strategies while analyzing key
insights, advantages, and limitations of methods from each category. Moreover,
representative metrics, datasets, and applications of KME are introduced
accordingly. Finally, we provide an in-depth analysis regarding the
practicality and remaining challenges of KME and suggest promising research
directions for further advancement in this field.
</p></li>
</ul>

<h3>Title: ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality. (arXiv:2310.16242v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16242">http://arxiv.org/abs/2310.16242</a></li>
<li>Code URL: https://github.com/marwahalaofi/ubicomp23-student-challenge</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16242]] ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality(http://arxiv.org/abs/2310.16242)</code></li>
<li>Summary: <p>In today's world, sleep quality is pivotal for overall well-being. While
wearable sensors offer real-time monitoring, they often lack actionable
insights, leading to user abandonment. This paper delves into the role of
technology in understanding sleep patterns. We introduce a two-stage framework,
utilizing Large Language Models (LLMs), aiming to provide accurate sleep
predictions with actionable feedback. Leveraging the GLOBEM dataset and
synthetic data from LLMs, we highlight enhanced results with models like
XGBoost. Our approach merges advanced machine learning with user-centric
design, blending scientific accuracy with practicality.
</p></li>
</ul>

<h3>Title: CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16271">http://arxiv.org/abs/2310.16271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16271]] CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment(http://arxiv.org/abs/2310.16271)</code></li>
<li>Summary: <p>Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.
</p></li>
</ul>

<h3>Title: A Comprehensive Evaluation of Constrained Text Generation for Large Language Models. (arXiv:2310.16343v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16343">http://arxiv.org/abs/2310.16343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16343]] A Comprehensive Evaluation of Constrained Text Generation for Large Language Models(http://arxiv.org/abs/2310.16343)</code></li>
<li>Summary: <p>Advancements in natural language generation (NLG) and large language models
(LLMs) have led to proficient text generation in various tasks. However,
integrating intricate constraints into neural text generation, due to LLMs'
opacity, remains challenging. This study investigates constrained text
generation for LLMs, where predefined constraints are applied during LLM's
generation process. Our research examines multiple LLMs, including ChatGPT and
GPT-4, categorizing constraints into lexical, structural, and relation-based
types. We also present various benchmarks to facilitate fair evaluation. The
study addresses some key research questions, including the extent of LLMs'
compliance with constraints. Results illuminate LLMs' capacity and deficiency
to incorporate constraints and provide insights for future developments in
constrained text generation. Codes and datasets will be released upon
acceptance.
</p></li>
</ul>

<h3>Title: Decoding Stumpers: Large Language Models vs. Human Problem-Solvers. (arXiv:2310.16411v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16411">http://arxiv.org/abs/2310.16411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16411]] Decoding Stumpers: Large Language Models vs(http://arxiv.org/abs/2310.16411)</code></li>
<li>Summary: <p>This paper investigates the problem-solving capabilities of Large Language
Models (LLMs) by evaluating their performance on stumpers, unique single-step
intuition problems that pose challenges for human solvers but are easily
verifiable. We compare the performance of four state-of-the-art LLMs
(Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our
findings reveal that the new-generation LLMs excel in solving stumpers and
surpass human performance. However, humans exhibit superior skills in verifying
solutions to the same problems. This research enhances our understanding of
LLMs' cognitive abilities and provides insights for enhancing their
problem-solving potential across various domains.
</p></li>
</ul>

<h3>Title: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. (arXiv:2310.16427v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16427">http://arxiv.org/abs/2310.16427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16427]] PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization(http://arxiv.org/abs/2310.16427)</code></li>
<li>Summary: <p>Highly effective, task-specific prompts are often heavily engineered by
experts to integrate detailed instructions and domain insights based on a deep
understanding of both instincts of large language models (LLMs) and the
intricacies of the target task. However, automating the generation of such
expert-level prompts remains elusive. Existing prompt optimization methods tend
to overlook the depth of domain knowledge and struggle to efficiently explore
the vast space of expert-level prompts. Addressing this, we present
PromptAgent, an optimization method that autonomously crafts prompts equivalent
in quality to those handcrafted by experts. At its core, PromptAgent views
prompt optimization as a strategic planning problem and employs a principled
planning algorithm, rooted in Monte Carlo tree search, to strategically
navigate the expert-level prompt space. Inspired by human-like trial-and-error
exploration, PromptAgent induces precise expert-level insights and in-depth
instructions by reflecting on model errors and generating constructive error
feedback. Such a novel framework allows the agent to iteratively examine
intermediate prompts (states), refine them based on error feedbacks (actions),
simulate future rewards, and search for high-reward paths leading to expert
prompts. We apply PromptAgent to 12 tasks spanning three practical domains:
BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing
it significantly outperforms strong Chain-of-Thought and recent prompt
optimization baselines. Extensive analyses emphasize its capability to craft
expert-level, detailed, and domain-insightful prompts with great efficiency and
generalizability.
</p></li>
</ul>

<h3>Title: CLEX: Continuous Length Extrapolation for Large Language Models. (arXiv:2310.16450v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16450">http://arxiv.org/abs/2310.16450</a></li>
<li>Code URL: https://github.com/damo-nlp-sg/clex</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16450]] CLEX: Continuous Length Extrapolation for Large Language Models(http://arxiv.org/abs/2310.16450)</code></li>
<li>Summary: <p>Transformer-based Large Language Models (LLMs) are pioneering advances in
many natural language processing tasks, however, their exceptional capabilities
are restricted within the preset context window of Transformer. Position
Embedding (PE) scaling methods, while effective in extending the context window
to a specific length, demonstrate either notable limitations in their
extrapolation abilities or sacrificing partial performance within the context
window. Length extrapolation methods, although theoretically capable of
extending the context window beyond the training sequence length, often
underperform in practical long-context applications. To address these
challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We
generalise the PE scaling approaches to model the continuous dynamics by
ordinary differential equations over the length scaling factor, thereby
overcoming the constraints of current PE scaling methods designed for specific
lengths. Moreover, by extending the dynamics to desired context lengths beyond
the training sequence length, CLEX facilitates the length extrapolation with
impressive performance in practical tasks. We demonstrate that CLEX can be
seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such
as LLaMA and GPT-NeoX, with negligible impact on training and inference
latency. Experimental results reveal that CLEX can effectively extend the
context window to over 4x or almost 8x training length, with no deterioration
in performance. Furthermore, when evaluated on the practical LongBench
benchmark, our model trained on a 4k length exhibits competitive performance
against state-of-the-art open-source models trained on context lengths up to
32k.
</p></li>
</ul>

<h3>Title: OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models. (arXiv:2310.16517v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16517">http://arxiv.org/abs/2310.16517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16517]] OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models(http://arxiv.org/abs/2310.16517)</code></li>
<li>Summary: <p>The emergence of large language models (LLMs) has revolutionized natural
language processing tasks. However, existing instruction-tuning datasets suffer
from occupational bias: the majority of data relates to only a few occupations,
which hampers the instruction-tuned LLMs to generate helpful responses to
professional queries from practitioners in specific fields. To mitigate this
issue and promote occupation-inclusive LLMs, we create an instruction-tuning
dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs
and 30,000+ dialogues covering over 1,000 occupations in 26 occupational
categories. We systematically request ChatGPT, organizing queries
hierarchically based on Occupation, Responsibility, Topic, and Question, to
ensure a comprehensive coverage of occupational specialty inquiries. By
comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we
observe that OccuQuest exhibits a more balanced distribution across
occupations. Furthermore, we assemble three test sets for comprehensive
evaluation, an occu-test set covering 25 occupational categories, an estate set
focusing on real estate, and an occu-quora set containing real-world questions
from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which
significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and
WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on
the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against
WizardLM.
</p></li>
</ul>

<h3>Title: Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting. (arXiv:2310.16523v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16523">http://arxiv.org/abs/2310.16523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16523]] Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting(http://arxiv.org/abs/2310.16523)</code></li>
<li>Summary: <p>A crucial challenge for generative large language models (LLMs) is diversity:
when a user's prompt is under-specified, models may follow implicit assumptions
while generating a response, which may result in homogenization of the
responses, as well as certain demographic groups being under-represented or
even erased from the generated responses. In this paper, we formalize diversity
of representation in generative LLMs. We present evaluation datasets and
propose metrics to measure diversity in generated responses along people and
culture axes. We find that LLMs understand the notion of diversity, and that
they can reason and critique their own responses for that goal. This finding
motivated a new prompting technique called collective-critique and self-voting
(CCSV) to self-improve people diversity of LLMs by tapping into its diversity
reasoning capabilities, without relying on handcrafted examples or prompt
tuning. Extensive empirical experiments with both human and automated
evaluations show that our proposed approach is effective at improving people
and culture diversity, and outperforms all baseline methods by a large margin.
</p></li>
</ul>

<h3>Title: R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context. (arXiv:2310.16535v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16535">http://arxiv.org/abs/2310.16535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16535]] R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context(http://arxiv.org/abs/2310.16535)</code></li>
<li>Summary: <p>With the help of Chain-of-Thought (CoT) prompting, Large Language Models
(LLMs) have achieved remarkable performance on various reasoning tasks.
However, most of them have been evaluated under noise-free context and the
dilemma for LLMs to produce inaccurate results under the noisy context has not
been fully investigated. Existing studies utilize trigger sentences to
encourage LLMs to concentrate on the relevant information but the trigger has
limited effect on final answer prediction. Inspired by interactive CoT method,
where intermediate reasoning steps are promoted by multiple rounds of
interaction between users and LLMs, we propose a novel prompting method, namely
R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$
prompting interacts with LLMs to perform key sentence extraction, variable
declaration and answer prediction, which corresponds to a thought process of
reviewing, rephrasing and resolving. The responses generated at the last
interaction will perform as hints to guide toward the responses of the next
interaction. Our experiments show that R$^3$ prompting significantly
outperforms existing CoT prompting methods on five reasoning tasks under noisy
context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on
the reasoning tasks under noisy context compared to the most competitive
prompting baseline. More analyses and ablation studies show the robustness and
generalization of R$^3$ prompting method in solving reasoning tasks in LLMs
under noisy context.
</p></li>
</ul>

<h3>Title: Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons. (arXiv:2310.16582v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16582">http://arxiv.org/abs/2310.16582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16582]] Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons(http://arxiv.org/abs/2310.16582)</code></li>
<li>Summary: <p>Personality plays a pivotal role in shaping human expression patterns, and
empowering and manipulating large language models (LLMs) with personality
traits holds significant promise in enhancing the user experience of LLMs.
However, prior approaches either rely on fine-tuning LLMs on a corpus enriched
with personalized expressions or necessitate the manual crafting of prompts to
induce LLMs to produce personalized responses. The former approaches demand
substantial time and resources for collecting sufficient training examples
while the latter might fail in enabling the precise manipulation of the
personality traits at a fine-grained level (e.g., achieving high agreeableness
while reducing openness). In this study, we introduce a novel approach for
tailoring personality traits within LLMs, allowing for the incorporation of any
combination of the Big Five factors (i.e., openness, conscientiousness,
extraversion, agreeableness, and neuroticism) in a pluggable manner. This is
achieved by employing a set of Unsupervisedly-Built Personalized Lexicons
(UBPL) that are utilized to adjust the probability of the next token predicted
by the original LLMs during the decoding phase. This adjustment encourages the
models to generate words present in the personalized lexicons while preserving
the naturalness of the generated texts. Extensive experimentation demonstrates
the effectiveness of our approach in finely manipulating LLMs' personality
traits. Furthermore, our method can be seamlessly integrated into other LLMs
without necessitating updates to their parameters.
</p></li>
</ul>

<h3>Title: ChatGPT is a Potential Zero-Shot Dependency Parser. (arXiv:2310.16654v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16654">http://arxiv.org/abs/2310.16654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16654]] ChatGPT is a Potential Zero-Shot Dependency Parser(http://arxiv.org/abs/2310.16654)</code></li>
<li>Summary: <p>Pre-trained language models have been widely used in dependency parsing task
and have achieved significant improvements in parser performance. However, it
remains an understudied question whether pre-trained language models can
spontaneously exhibit the ability of dependency parsing without introducing
additional parser structure in the zero-shot scenario. In this paper, we
propose to explore the dependency parsing ability of large language models such
as ChatGPT and conduct linguistic analysis. The experimental results
demonstrate that ChatGPT is a potential zero-shot dependency parser, and the
linguistic analysis also shows some unique preferences in parsing outputs.
</p></li>
</ul>

<h3>Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs. (arXiv:2310.16355v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16355">http://arxiv.org/abs/2310.16355</a></li>
<li>Code URL: https://github.com/tanyuqian/redco</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16355]] Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs(http://arxiv.org/abs/2310.16355)</code></li>
<li>Summary: <p>The recent progress of AI can be largely attributed to large language models
(LLMs). However, their escalating memory requirements introduce challenges for
machine learning (ML) researchers and engineers. Addressing this requires
developers to partition a large model to distribute it across multiple GPUs or
TPUs. This necessitates considerable coding and intricate configuration efforts
with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa.
These tools require users' expertise in machine learning systems (MLSys),
creating a bottleneck in LLM development, particularly for developers without
MLSys background. In this work, we present Redco, a lightweight and
user-friendly tool crafted to automate distributed training and inference for
LLMs, as well as to simplify ML pipeline development. The design of Redco
emphasizes two key aspects. Firstly, to automate model parallism, our study
identifies two straightforward rules to generate tensor parallel strategies for
any given LLM. Integrating these rules into Redco facilitates effortless
distributed LLM training and inference, eliminating the need of additional
coding or complex configurations. We demonstrate the effectiveness by applying
Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to
the size of 66B. Secondly, we propose a mechanism that allows for the
customization of diverse ML pipelines through the definition of merely three
functions, eliminating redundant and formulaic code like multi-host related
processing. This mechanism proves adaptable across a spectrum of ML algorithms,
from foundational language modeling to complex algorithms like meta-learning
and reinforcement learning. Consequently, Redco implementations exhibit much
fewer code lines compared to their official counterparts.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting. (arXiv:2310.16069v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16069">http://arxiv.org/abs/2310.16069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16069]] CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting(http://arxiv.org/abs/2310.16069)</code></li>
<li>Summary: <p>Natural scene analysis and remote sensing imagery offer immense potential for
advancements in large-scale language-guided context-aware data utilization.
This potential is particularly significant for enhancing performance in
downstream tasks such as object detection and segmentation with designed
language prompting. In light of this, we introduce the CPSeg, Chain-of-Thought
Language Prompting for Finer-grained Semantic Segmentation), an innovative
framework designed to augment image segmentation performance by integrating a
novel "Chain-of-Thought" process that harnesses textual information associated
with images. This groundbreaking approach has been applied to a flood disaster
scenario. CPSeg encodes prompt texts derived from various sentences to
formulate a coherent chain-of-thought. We propose a new vision-language
dataset, FloodPrompt, which includes images, semantic masks, and corresponding
text information. This not only strengthens the semantic understanding of the
scenario but also aids in the key task of semantic segmentation through an
interplay of pixel and text matching maps. Our qualitative and quantitative
analyses validate the effectiveness of CPSeg.
</p></li>
</ul>

<h3>Title: Anatomically-aware Uncertainty for Semi-supervised Image Segmentation. (arXiv:2310.16099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16099">http://arxiv.org/abs/2310.16099</a></li>
<li>Code URL: https://github.com/adigasu/anatomically-aware_uncertainty_for_semi-supervised_segmentation</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16099]] Anatomically-aware Uncertainty for Semi-supervised Image Segmentation(http://arxiv.org/abs/2310.16099)</code></li>
<li>Summary: <p>Semi-supervised learning relaxes the need of large pixel-wise labeled
datasets for image segmentation by leveraging unlabeled data. A prominent way
to exploit unlabeled data is to regularize model predictions. Since the
predictions of unlabeled data can be unreliable, uncertainty-aware schemes are
typically employed to gradually learn from meaningful and reliable predictions.
Uncertainty estimation methods, however, rely on multiple inferences from the
model predictions that must be computed for each training step, which is
computationally expensive. Moreover, these uncertainty maps capture pixel-wise
disparities and do not consider global information. This work proposes a novel
method to estimate segmentation uncertainty by leveraging global information
from the segmentation masks. More precisely, an anatomically-aware
representation is first learnt to model the available segmentation masks. The
learnt representation thereupon maps the prediction of a new segmentation into
an anatomically-plausible segmentation. The deviation from the plausible
segmentation aids in estimating the underlying pixel-level uncertainty in order
to further guide the segmentation network. The proposed method consequently
estimates the uncertainty using a single inference from our representation,
thereby reducing the total computation. We evaluate our method on two publicly
available segmentation datasets of left atria in cardiac MRIs and of multiple
organs in abdominal CTs. Our anatomically-aware method improves the
segmentation accuracy over the state-of-the-art semi-supervised methods in
terms of two commonly used evaluation metrics.
</p></li>
</ul>

<h3>Title: Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as a Neurodevelopmental Cue. (arXiv:2310.16138v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16138">http://arxiv.org/abs/2310.16138</a></li>
<li>Code URL: https://github.com/ostadabbas/nns-detection-and-segmentation</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16138]] Subtle Signals: Video-based Detection of Infant Non-nutritive Sucking as a Neurodevelopmental Cue(http://arxiv.org/abs/2310.16138)</code></li>
<li>Summary: <p>Non-nutritive sucking (NNS), which refers to the act of sucking on a
pacifier, finger, or similar object without nutrient intake, plays a crucial
role in assessing healthy early development. In the case of preterm infants,
NNS behavior is a key component in determining their readiness for feeding. In
older infants, the characteristics of NNS behavior offer valuable insights into
neural and motor development. Additionally, NNS activity has been proposed as a
potential safeguard against sudden infant death syndrome (SIDS). However, the
clinical application of NNS assessment is currently hindered by labor-intensive
and subjective finger-in-mouth evaluations. Consequently, researchers often
resort to expensive pressure transducers for objective NNS signal measurement.
To enhance the accessibility and reliability of NNS signal monitoring for both
clinicians and researchers, we introduce a vision-based algorithm designed for
non-contact detection of NNS activity using baby monitor footage in natural
settings. Our approach involves a comprehensive exploration of optical flow and
temporal convolutional networks, enabling the detection and amplification of
subtle infant-sucking signals. We successfully classify short video clips of
uniform length into NNS and non-NNS periods. Furthermore, we investigate manual
and learning-based techniques to piece together local classification results,
facilitating the segmentation of longer mixed-activity videos into NNS and
non-NNS segments of varying duration. Our research introduces two novel
datasets of annotated infant videos, including one sourced from our clinical
study featuring 19 infant subjects and 183 hours of overnight baby monitor
footage.
</p></li>
</ul>

<h3>Title: Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep Learning. (arXiv:2310.16210v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16210">http://arxiv.org/abs/2310.16210</a></li>
<li>Code URL: https://github.com/jonalvjusto/s_l_c_segm_hyp_img</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16210]] Sea-Land-Cloud Segmentation in Satellite Hyperspectral Imagery by Deep Learning(http://arxiv.org/abs/2310.16210)</code></li>
<li>Summary: <p>Satellites are increasingly adopting on-board Artificial Intelligence (AI)
techniques to enhance platforms' autonomy through edge inference. In this
context, the utilization of deep learning (DL) techniques for segmentation in
HS satellite imagery offers advantages for remote sensing applications, and
therefore, we train 16 different models, whose codes are made available through
our study, which we consider to be relevant for on-board multi-class
segmentation of HS imagery, focusing on classifying oceanic (sea), terrestrial
(land), and cloud formations. We employ the HYPSO-1 mission as an illustrative
case for sea-land-cloud segmentation, and to demonstrate the utility of the
segments, we introduce a novel sea-land-cloud ranking application scenario. Our
system prioritizes HS image downlink based on sea, land, and cloud coverage
levels from the segmented images. We comparatively evaluate the models for
in-orbit deployment, considering performance, parameter count, and inference
time. The models include both shallow and deep models, and after we propose
four new DL models, we demonstrate that segmenting single spectral signatures
(1D) outperforms 3D data processing comprising both spectral (1D) and spatial
(2D) contexts. We conclude that our lightweight DL model, called
1D-Justo-LiuNet, consistently surpasses state-of-the-art models for
sea-land-cloud segmentation, such as U-Net and its variations, in terms of
performance (0.93 accuracy) and parameter count (4,563). However, the 1D models
present longer inference time (15s) in the tested processing architecture,
which is clearly suboptimal. Finally, after demonstrating that in-orbit image
segmentation should occur post L1b radiance calibration rather than on raw
data, we additionally show that reducing spectral channels down to 3 lowers
models' parameters and inference time, at the cost of weaker segmentation
performance.
</p></li>
</ul>

<h3>Title: Pixel-Level Clustering Network for Unsupervised Image Segmentation. (arXiv:2310.16234v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16234">http://arxiv.org/abs/2310.16234</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16234]] Pixel-Level Clustering Network for Unsupervised Image Segmentation(http://arxiv.org/abs/2310.16234)</code></li>
<li>Summary: <p>While image segmentation is crucial in various computer vision applications,
such as autonomous driving, grasping, and robot navigation, annotating all
objects at the pixel-level for training is nearly impossible. Therefore, the
study of unsupervised image segmentation methods is essential. In this paper,
we present a pixel-level clustering framework for segmenting images into
regions without using ground truth annotations. The proposed framework includes
feature embedding modules with an attention mechanism, a feature statistics
computing module, image reconstruction, and superpixel segmentation to achieve
accurate unsupervised segmentation. Additionally, we propose a training
strategy that utilizes intra-consistency within each superpixel,
inter-similarity/dissimilarity between neighboring superpixels, and structural
similarity between images. To avoid potential over-segmentation caused by
superpixel-based losses, we also propose a post-processing method. Furthermore,
we present an extension of the proposed method for unsupervised semantic
segmentation. We conducted experiments on three publicly available datasets
(Berkeley segmentation dataset, PASCAL VOC 2012 dataset, and COCO-Stuff
dataset) to demonstrate the effectiveness of the proposed framework. The
experimental results show that the proposed framework outperforms previous
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Open-NeRF: Towards Open Vocabulary NeRF Decomposition. (arXiv:2310.16383v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16383">http://arxiv.org/abs/2310.16383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16383]] Open-NeRF: Towards Open Vocabulary NeRF Decomposition(http://arxiv.org/abs/2310.16383)</code></li>
<li>Summary: <p>In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.
</p></li>
</ul>

<h3>Title: Gramian Attention Heads are Strong yet Efficient Vision Learners. (arXiv:2310.16483v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16483">http://arxiv.org/abs/2310.16483</a></li>
<li>Code URL: https://github.com/lab-lvm/imagenet-models</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16483]] Gramian Attention Heads are Strong yet Efficient Vision Learners(http://arxiv.org/abs/2310.16483)</code></li>
<li>Summary: <p>We introduce a novel architecture design that enhances expressiveness by
incorporating multiple head classifiers (\ie, classification heads) instead of
relying on channel expansion or additional building blocks. Our approach
employs attention-based aggregation, utilizing pairwise feature similarity to
enhance multiple lightweight heads with minimal resource overhead. We compute
the Gramian matrices to reinforce class tokens in an attention layer for each
head. This enables the heads to learn more discriminative representations,
enhancing their aggregation capabilities. Furthermore, we propose a learning
algorithm that encourages heads to complement each other by reducing
correlation for aggregation. Our models eventually surpass state-of-the-art
CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and
deliver remarkable performance across various downstream tasks, such as COCO
object instance segmentation, ADE20k semantic segmentation, and fine-grained
visual classification datasets. The effectiveness of our framework is
substantiated by practical experimental results and further underpinned by
generalization error bound. We release the code publicly at:
https://github.com/Lab-LVM/imagenet-models.
</p></li>
</ul>

<h3>Title: Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network. (arXiv:2310.16616v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16616">http://arxiv.org/abs/2310.16616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16616]] Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network(http://arxiv.org/abs/2310.16616)</code></li>
<li>Summary: <p>Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that
aims to segment visual objects in images based on dense narrative captions. The
current state-of-the-art methods first refine the representation of phrase by
aggregating the most similar $k$ image pixels, and then match the refined text
representations with the pixels of the image feature map to generate
segmentation results. However, simply aggregating sampled image features
ignores the contextual information, which can lead to phrase-to-pixel
mis-match. In this paper, we propose a novel learning framework called
Deformable Attention Refined Matching Network (DRMN), whose main idea is to
bring deformable attention in the iterative process of feature learning to
incorporate essential context information of different scales of pixels. DRMN
iteratively re-encodes pixels with the deformable attention network after
updating the feature representation of the top-$k$ most similar pixels. As
such, DRMN can lead to accurate yet discriminative pixel representations,
purify the top-$k$ most similar pixels, and consequently alleviate the
phrase-to-pixel mis-match substantially.Experimental results show that our
novel design significantly improves the matching results between text phrases
and image pixels. Concretely, DRMN achieves new state-of-the-art performance on
the PNG benchmark with an average recall improvement 3.5%. The codes are
available in: https://github.com/JaMesLiMers/DRMN.
</p></li>
</ul>

<h3>Title: Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images. (arXiv:2310.16186v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16186">http://arxiv.org/abs/2310.16186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16186]] Image Segmentation using U-Net Architecture for Powder X-ray Diffraction Images(http://arxiv.org/abs/2310.16186)</code></li>
<li>Summary: <p>Scientific researchers frequently use the in situ synchrotron high-energy
powder X-ray diffraction (XRD) technique to examine the crystallographic
structures of materials in functional devices such as rechargeable battery
materials. We propose a method for identifying artifacts in experimental XRD
images. The proposed method uses deep learning convolutional neural network
architectures, such as tunable U-Nets to identify the artifacts. In particular,
the predicted artifacts are evaluated against the corresponding ground truth
(manually implemented) using the overall true positive rate or recall. The
result demonstrates that the U-Nets can consistently produce great recall
performance at 92.4% on the test dataset, which is not included in the
training, with a 34% reduction in average false positives in comparison to the
conventional method. The U-Nets also reduce the time required to identify and
separate artifacts by more than 50%. Furthermore, the exclusion of the
artifacts shows major changes in the integrated 1D XRD pattern, enhancing
further analysis of the post-processing XRD data.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
