<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-18</h1>
<h3>Title: Decoding Emotions: Unveiling Facial Expressions through Acoustic Sensing with Contrastive Attention</h3>
<ul>
<li><strong>Authors: </strong>Guangjing Wang, Juexing Wang, Ce Zhou, Weikang Ding, Huacheng Zeng, Tianxing Li, Qiben Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12811">https://arxiv.org/abs/2410.12811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12811">https://arxiv.org/pdf/2410.12811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12811]] Decoding Emotions: Unveiling Facial Expressions through Acoustic Sensing with Contrastive Attention(https://arxiv.org/abs/2410.12811)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Expression recognition holds great promise for applications such as content recommendation and mental healthcare by accurately detecting users' emotional states. Traditional methods often rely on cameras or wearable sensors, which raise privacy concerns and add extra device burdens. In addition, existing acoustic-based methods struggle to maintain satisfactory performance when there is a distribution shift between the training dataset and the inference dataset. In this paper, we introduce FacER+, an active acoustic facial expression recognition system, which eliminates the requirement for external microphone arrays. FacER+ extracts facial expression features by analyzing the echoes of near-ultrasound signals emitted between the 3D facial contour and the earpiece speaker on a smartphone. This approach not only reduces background noise but also enables the identification of different expressions from various users with minimal training data. We develop a contrastive external attention-based model to consistently learn expression features across different users, reducing the distribution differences. Extensive experiments involving 20 volunteers, both with and without masks, demonstrate that FacER+ can accurately recognize six common facial expressions with over 90% accuracy in diverse, user-independent real-life scenarios, surpassing the performance of the leading acoustic sensing methods by 10%. FacER+ offers a robust and practical solution for facial expression recognition.</li>
</ul>

<h3>Title: Leveraging generative models to characterize the failure conditions of image classifiers</h3>
<ul>
<li><strong>Authors: </strong>Adrien Le Coz, Stéphane Herbin, Faouzi Adjed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12814">https://arxiv.org/abs/2410.12814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12814">https://arxiv.org/pdf/2410.12814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12814]] Leveraging generative models to characterize the failure conditions of image classifiers(https://arxiv.org/abs/2410.12814)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications.</li>
</ul>

<h3>Title: AVID: Adapting Video Diffusion Models to World Models</h3>
<ul>
<li><strong>Authors: </strong>Marc Rigter, Tarun Gupta, Agrin Hilmkil, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12822">https://arxiv.org/abs/2410.12822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12822">https://arxiv.org/pdf/2410.12822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12822]] AVID: Adapting Video Diffusion Models to World Models(https://arxiv.org/abs/2410.12822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed-source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learned mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation.1 Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.</li>
</ul>

<h3>Title: Generative Reward Models</h3>
<ul>
<li><strong>Authors: </strong>Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, Alon Albalak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12832">https://arxiv.org/abs/2410.12832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12832">https://arxiv.org/pdf/2410.12832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12832]] Generative Reward Models(https://arxiv.org/abs/2410.12832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.</li>
</ul>

<h3>Title: A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12837">https://arxiv.org/abs/2410.12837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12837">https://arxiv.org/pdf/2410.12837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12837]] A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions(https://arxiv.org/abs/2410.12837)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.</li>
</ul>

<h3>Title: Capturing Bias Diversity in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Purva Prasad Gosavi, Vaishnavi Murlidhar Kulkarni, Alan F. Smeaton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12839">https://arxiv.org/abs/2410.12839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12839">https://arxiv.org/pdf/2410.12839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12839]] Capturing Bias Diversity in LLMs(https://arxiv.org/abs/2410.12839)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents research on enhancements to Large Language Models (LLMs) through the addition of diversity in its generated outputs. Our study introduces a configuration of multiple LLMs which demonstrates the diversities capable with a single LLM. By developing multiple customised instances of a GPT model, each reflecting biases in specific demographic characteristics including gender, age, and race, we propose, develop and evaluate a framework for a more nuanced and representative AI dialogue which we call BiasGPT. The customised GPT models will ultimately collaborate, merging their diverse perspectives on a topic into an integrated response that captures a broad spectrum of human experiences and viewpoints. In this paper, through experiments, we demonstrate the capabilities of a GPT model to embed different biases which, when combined, can open the possibilities of more inclusive AI technologies.</li>
</ul>

<h3>Title: Answering Questions in Stages: Prompt Chaining for Contract QA</h3>
<ul>
<li><strong>Authors: </strong>Adam Roegiest, Radha Chitta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12840">https://arxiv.org/abs/2410.12840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12840">https://arxiv.org/pdf/2410.12840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12840]] Answering Questions in Stages: Prompt Chaining for Contract QA(https://arxiv.org/abs/2410.12840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Finding answers to legal questions about clauses in contracts is an important form of analysis in many legal workflows (e.g., understanding market trends, due diligence, risk mitigation) but more important is being able to do this at scale. Prior work showed that it is possible to use large language models with simple zero-shot prompts to generate structured answers to questions, which can later be incorporated into legal workflows. Such prompts, while effective on simple and straightforward clauses, fail to perform when the clauses are long and contain information not relevant to the question. In this paper, we propose two-stage prompt chaining to produce structured answers to multiple-choice and multiple-select questions and show that they are more effective than simple prompts on more nuanced legal text. We analyze situations where this technique works well and areas where further refinement is needed, especially when the underlying linguistic variations are more than can be captured by simply specifying possible answers. Finally, we discuss future research that seeks to refine this work by improving stage one results by making them more question-specific.</li>
</ul>

<h3>Title: UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Guo, Liyun Zhang, Yiqin Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12841">https://arxiv.org/abs/2410.12841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12841">https://arxiv.org/pdf/2410.12841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12841]] UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models(https://arxiv.org/abs/2410.12841)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Automated Machine Learning (AutoML) has simplified complex ML processes such as data pre-processing, model selection, and hyper-parameter searching. However, traditional AutoML frameworks focus solely on discriminative tasks, often falling short in tackling AutoML for generative models. Additionally, these frameworks lack interpretability and user engagement during the training process, primarily due to the absence of human-centered design. It leads to a lack of transparency in final decision-making and limited user control, potentially reducing trust and adoption of AutoML methods. To address these limitations, we introduce UniAutoML, a human-centered AutoML framework that leverages Large Language Models (LLMs) to unify AutoML for both discriminative (e.g., Transformers and CNNs for classification or regression tasks) and generative tasks (e.g., fine-tuning diffusion models or LLMs). The human-centered design of UniAutoML innovatively features a conversational user interface (CUI) that facilitates natural language interactions, providing users with real-time guidance, feedback, and progress updates for better interpretability. This design enhances transparency and user control throughout the AutoML training process, allowing users to seamlessly break down or modify the model being trained. To mitigate potential risks associated with LLM generated content, UniAutoML incorporates a safety guardline that filters inputs and censors outputs. We evaluated UniAutoML's performance and usability through experiments on eight diverse datasets and user studies involving 25 participants, demonstrating that UniAutoML not only enhances performance but also improves user control and trust. Our human-centered design bridges the gap between AutoML capabilities and user understanding, making ML more accessible to a broader audience.</li>
</ul>

<h3>Title: Exploring Prompt Engineering: A Systematic Review with SWOT Analysis</h3>
<ul>
<li><strong>Authors: </strong>Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, Tala Talaei Khoei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12843">https://arxiv.org/abs/2410.12843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12843">https://arxiv.org/pdf/2410.12843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12843]] Exploring Prompt Engineering: A Systematic Review with SWOT Analysis(https://arxiv.org/abs/2410.12843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we conduct a comprehensive SWOT analysis of prompt engineering techniques within the realm of Large Language Models (LLMs). Emphasizing linguistic principles, we examine various techniques to identify their strengths, weaknesses, opportunities, and threats. Our findings provide insights into enhancing AI interactions and improving language model comprehension of human prompts. The analysis covers techniques including template-based approaches and fine-tuning, addressing the problems and challenges associated with each. The conclusion offers future research directions aimed at advancing the effectiveness of prompt engineering in optimizing human-machine communication.</li>
</ul>

<h3>Title: TextLap: Customizing Language Models for Text-to-Layout Planning</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Ruiyi Zhang, Yufan Zhou, Jennifer Healey, Jiuxiang Gu, Zhiqiang Xu, Changyou Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12844">https://arxiv.org/abs/2410.12844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12844">https://arxiv.org/pdf/2410.12844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12844]] TextLap: Customizing Language Models for Text-to-Layout Planning(https://arxiv.org/abs/2410.12844)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic generation of graphical layouts is crucial for many real-world applications, including designing posters, flyers, advertisements, and graphical user interfaces. Given the incredible ability of Large language models (LLMs) in both natural language understanding and generation, we believe that we could customize an LLM to help people create compelling graphical layouts starting with only text instructions from the user. We call our method TextLap (text-based layout planning). It uses a curated instruction-based layout planning dataset (InsLap) to customize LLMs as a graphic designer. We demonstrate the effectiveness of TextLap and show that it outperforms strong baselines, including GPT-4 based methods, for image generation and graphical design benchmarks.</li>
</ul>

<h3>Title: Toward Relieving Clinician Burden by Automatically Generating Progress Notes using Interim Hospital Data</h3>
<ul>
<li><strong>Authors: </strong>Sarvesh Soni, Dina Demner-Fushman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12845">https://arxiv.org/abs/2410.12845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12845">https://arxiv.org/pdf/2410.12845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12845]] Toward Relieving Clinician Burden by Automatically Generating Progress Notes using Interim Hospital Data(https://arxiv.org/abs/2410.12845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Regular documentation of progress notes is one of the main contributors to clinician burden. The abundance of structured chart information in medical records further exacerbates the burden, however, it also presents an opportunity to automate the generation of progress notes. In this paper, we propose a task to automate progress note generation using structured or tabular information present in electronic health records. To this end, we present a novel framework and a large dataset, ChartPNG, for the task which contains $7089$ annotation instances (each having a pair of progress notes and interim structured chart data) across $1616$ patients. We establish baselines on the dataset using large language models from general and biomedical domains. We perform both automated (where the best performing Biomistral model achieved a BERTScore F1 of $80.53$ and MEDCON score of $19.61$) and manual (where we found that the model was able to leverage relevant structured data with $76.9\%$ accuracy) analyses to identify the challenges with the proposed task and opportunities for future research.</li>
</ul>

<h3>Title: Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wang, Jianzhong Qi, Junhao Gan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12846">https://arxiv.org/abs/2410.12846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12846">https://arxiv.org/pdf/2410.12846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12846]] Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering(https://arxiv.org/abs/2410.12846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Question answering on free-form tables (a.k.a. TableQA) is a challenging task because of the flexible structure and the complex schema of tables. Recent studies use Large Language Models (LLMs) for this task, exploiting their capability in understanding the questions and tabular data which are typically given in natural language and contains many textual fields, respectively. While this approach has shown promising results, it overlooks the challenges brought by numerical values which are common in tabular data, while LLMs are known to struggle with such values. We aim to address this issue and answer numerical questions. We propose a model named TabLaP that uses LLMs as a planner rather than an answer generator, exploiting LLMs capability in multi-step reasoning while leaving the actual numerical calculations to a Python interpreter for accurate calculation. Recognizing the inaccurate nature of LLMs, we further make a first attempt to quantify the trustworthiness of the answers produced by TabLaP, such that users can use TabLaP in a regret-aware manner. Experimental results on two benchmark datasets show that TabLaP is substantially more accurate than the state-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the two datasets, respectively.</li>
</ul>

<h3>Title: Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions</h3>
<ul>
<li><strong>Authors: </strong>Per Niklas Waaler, Musarrat Hussain, Igor Molchanov, Lars Ailo Bongo, Brita Elvevåg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12848">https://arxiv.org/abs/2410.12848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12848">https://arxiv.org/pdf/2410.12848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12848]] Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions(https://arxiv.org/abs/2410.12848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Patients with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. These individuals could benefit greatly from education platforms that leverage the adaptability of Large Language Models (LLMs) such as GPT-4. While LLMs have the potential to make topical mental health information more accessible and engaging, their black-box nature raises concerns about ethics and safety. Prompting offers a way to produce semi-scripted chatbots with responses anchored in instructions and validated information, but prompt-engineered chatbots may drift from their intended identity as the conversation progresses. We propose a Critical Analysis Filter for achieving better control over chatbot behavior. In this system, a team of prompted LLM agents are prompt-engineered to critically analyze and refine the chatbot's response and deliver real-time feedback to the chatbot. To test this approach, we develop an informational schizophrenia chatbot and converse with it (with the filter deactivated) until it oversteps its scope. Once drift has been observed, AI-agents are used to automatically generate sample conversations in which the chatbot is being enticed to talk about out-of-bounds topics. We manually assign to each response a compliance score that quantifies the chatbot's compliance to its instructions; specifically the rules about accurately conveying sources and being transparent about limitations. Activating the Critical Analysis Filter resulted in an acceptable compliance score (>=2) in 67.0% of responses, compared to only 8.7% when the filter was deactivated. These results suggest that a self-reflection layer could enable LLMs to be used effectively and safely in mental health platforms, maintaining adaptability while reliably limiting their scope to appropriate use cases.</li>
</ul>

<h3>Title: RecurFormer: Not All Transformer Heads Need Self-Attention</h3>
<ul>
<li><strong>Authors: </strong>Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12850">https://arxiv.org/abs/2410.12850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12850">https://arxiv.org/pdf/2410.12850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12850]] RecurFormer: Not All Transformer Heads Need Self-Attention(https://arxiv.org/abs/2410.12850)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs.</li>
</ul>

<h3>Title: VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lisa Dunlap, Krishna Mandal, Trevor Darrell, Jacob Steinhardt, Joseph E Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12851">https://arxiv.org/abs/2410.12851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12851">https://arxiv.org/pdf/2410.12851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12851]] VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models(https://arxiv.org/abs/2410.12851)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often exhibit subtle yet distinctive characteristics in their outputs that users intuitively recognize, but struggle to quantify. These "vibes" - such as tone, formatting, or writing style - influence user preferences, yet traditional evaluations focus primarily on the single axis of correctness. We introduce VibeCheck, a system for automatically comparing a pair of LLMs by discovering identifying traits of a model ("vibes") that are well-defined, differentiating, and user-aligned. VibeCheck iteratively discover vibes from model outputs, then utilizes a panel of LLM judges to quantitatively measure the utility of each vibe. We validate that the vibes generated by VibeCheck align with those found in human discovery and run VibeCheck on pairwise preference data from real-world user conversations with llama-3-70b VS GPT-4. VibeCheck reveals that Llama has a friendly, funny, and somewhat controversial vibe. These vibes predict model identity with 80% accuracy and human preference with 61% accuracy. Lastly, we run VibeCheck on a variety of models and tasks including summarization, math, and captioning to provide insight into differences in model behavior. Some of the vibes we find are that Command X prefers to add concrete intros and conclusions when summarizing in comparison to TNGL, Llama-405b often over-explains its thought process on math problems compared to GPT-4o, and GPT-4 prefers to focus on the mood and emotions of the scene when captioning compared to Gemini-1.5-Flash.</li>
</ul>

<h3>Title: The Large Language Model GreekLegalRoBERTa</h3>
<ul>
<li><strong>Authors: </strong>Vasileios Saketos, Despina-Athanasia Pantazi, Manolis Koubarakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12852">https://arxiv.org/abs/2410.12852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12852">https://arxiv.org/pdf/2410.12852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12852]] The Large Language Model GreekLegalRoBERTa(https://arxiv.org/abs/2410.12852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We develop four versions of GreekLegalRoBERTa, which are four large language models trained on Greek legal and nonlegal text. We show that our models surpass the performance of GreekLegalBERT, Greek- LegalBERT-v2, and GreekBERT in two tasks involving Greek legal documents: named entity recognition and multi-class legal topic classification. We view our work as a contribution to the study of domain-specific NLP tasks in low-resource languages, like Greek, using modern NLP techniques and methodologies.</li>
</ul>

<h3>Title: Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Mahmood Hegazy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12853">https://arxiv.org/abs/2410.12853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12853">https://arxiv.org/pdf/2410.12853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12853]] Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks(https://arxiv.org/abs/2410.12853)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning. Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs. Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used. Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%. Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%). These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.</li>
</ul>

<h3>Title: TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees</h3>
<ul>
<li><strong>Authors: </strong>Weibin Liao, Xu Chu, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12854">https://arxiv.org/abs/2410.12854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12854">https://arxiv.org/pdf/2410.12854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12854]] TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees(https://arxiv.org/abs/2410.12854)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the domain of complex reasoning tasks, such as mathematical reasoning, recent advancements have proposed the use of Direct Preference Optimization (DPO) to suppress output of dispreferred responses, thereby enhancing the long-chain reasoning capabilities of large language models (LLMs). To this end, these studies employed LLMs to generate preference trees via Tree-of-thoughts (ToT) and sample the paired preference responses required by the DPO algorithm. However, the DPO algorithm based on binary preference optimization is unable to learn multiple responses with varying degrees of preference/dispreference that provided by the preference trees, resulting in incomplete preference learning. In this work, we introduce Tree Preference Optimization (TPO), that does not sample paired preference responses from the preference tree; instead, it directly learns from the entire preference tree during the fine-tuning. Specifically, TPO formulates the language model alignment as a Preference List Ranking problem, where the policy can potentially learn more effectively from a ranked preference list of responses given the prompt. In addition, to further assist LLMs in identifying discriminative steps within long-chain reasoning and increase the relative reward margin in the preference list, TPO utilizes Adaptive Step Reward to adjust the reward values of each step in trajectory for performing fine-grained preference optimization. We carry out extensive experiments on mathematical reasoning tasks to evaluate TPO. The experimental results indicate that TPO consistently outperforms DPO across three public large language models on four datasets.</li>
</ul>

<h3>Title: JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Fan Liu, Yue Feng, Zhao Xu, Lixin Su, Xinyu Ma, Dawei Yin, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12855">https://arxiv.org/abs/2410.12855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12855">https://arxiv.org/pdf/2410.12855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12855]] JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework(https://arxiv.org/abs/2410.12855)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, explainability</a></li>
<li><strong>Abstract: </strong>Despite advancements in enhancing LLM safety against jailbreak attacks, evaluating LLM defenses remains a challenge, with current methods often lacking explainability and generalization to complex scenarios, leading to incomplete assessments (e.g., direct judgment without reasoning, low F1 score of GPT-4 in complex cases, bias in multilingual scenarios). To address this, we present JAILJUDGE, a comprehensive benchmark featuring diverse risk scenarios, including synthetic, adversarial, in-the-wild, and multilingual prompts, along with high-quality human-annotated datasets. The JAILJUDGE dataset includes over 35k+ instruction-tune data with reasoning explainability and JAILJUDGETEST, a 4.5k+ labeled set for risk scenarios, and a 6k+ multilingual set across ten languages. To enhance evaluation with explicit reasoning, we propose the JailJudge MultiAgent framework, which enables explainable, fine-grained scoring (1 to 10). This framework supports the construction of instruction-tuning ground truth and facilitates the development of JAILJUDGE Guard, an end-to-end judge model that provides reasoning and eliminates API costs. Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a moderation defense, both leveraging JAILJUDGE Guard. Our experiments demonstrate the state-of-the-art performance of JailJudge methods (JailJudge MultiAgent, JAILJUDGE Guard) across diverse models (e.g., GPT-4, Llama-Guard) and zero-shot scenarios. JailBoost and GuardShield significantly improve jailbreak attack and defense tasks under zero-shot settings, with JailBoost enhancing performance by 29.24% and GuardShield reducing defense ASR from 40.46% to 0.15%.</li>
</ul>

<h3>Title: Optimized Biomedical Question-Answering Services with LLM and Multi-BERT Integration</h3>
<ul>
<li><strong>Authors: </strong>Cheng Qian, Xianglong Shi, Shanshan Yao, Yichen Liu, Fengming Zhou, Zishu Zhang, Junaid Akram, Ali Braytee, Ali Anaissi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12856">https://arxiv.org/abs/2410.12856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12856">https://arxiv.org/pdf/2410.12856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12856]] Optimized Biomedical Question-Answering Services with LLM and Multi-BERT Integration(https://arxiv.org/abs/2410.12856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a refined approach to biomedical question-answering (QA) services by integrating large language models (LLMs) with Multi-BERT configurations. By enhancing the ability to process and prioritize vast amounts of complex biomedical data, this system aims to support healthcare professionals in delivering better patient outcomes and informed decision-making. Through innovative use of BERT and BioBERT models, combined with a multi-layer perceptron (MLP) layer, we enable more specialized and efficient responses to the growing demands of the healthcare sector. Our approach not only addresses the challenge of overfitting by freezing one BERT model while training another but also improves the overall adaptability of QA services. The use of extensive datasets, such as BioASQ and BioMRC, demonstrates the system's ability to synthesize critical information. This work highlights how advanced language models can make a tangible difference in healthcare, providing reliable and responsive tools for professionals to manage complex information, ultimately serving the broader goal of improved care and data-driven insights.</li>
</ul>

<h3>Title: Enterprise Benchmarks for Large Language Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Bing Zhang, Mikio Takeuchi, Ryo Kawahara, Shubhi Asthana, Md. Maruf Hossain, Guang-Jie Ren, Kate Soule, Yada Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12857">https://arxiv.org/abs/2410.12857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12857">https://arxiv.org/pdf/2410.12857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12857]] Enterprise Benchmarks for Large Language Model Evaluation(https://arxiv.org/abs/2410.12857)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of large language models (LLMs) has led to a greater challenge of having a rigorous and systematic evaluation of complex tasks performed, especially in enterprise applications. Therefore, LLMs need to be able to benchmark enterprise datasets for various tasks. This work presents a systematic exploration of benchmarking strategies tailored to LLM evaluation, focusing on the utilization of domain-specific datasets and consisting of a variety of NLP tasks. The proposed evaluation framework encompasses 25 publicly available datasets from diverse enterprise domains like financial services, legal, cyber security, and climate and sustainability. The diverse performance of 13 models across different enterprise tasks highlights the importance of selecting the right model based on the specific requirements of each task. Code and prompts are available on GitHub.</li>
</ul>

<h3>Title: Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ameer Hamza Shakur, Michael J. Holcomb, David Hein, Shinyoung Kang, Thomas O. Dalton, Krystle K. Campbell, Daniel J. Scott, Andrew R. Jamieson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12858">https://arxiv.org/abs/2410.12858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12858">https://arxiv.org/pdf/2410.12858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12858]] Large Language Models for Medical OSCE Assessment: A Novel Approach to Transcript Analysis(https://arxiv.org/abs/2410.12858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Grading Objective Structured Clinical Examinations (OSCEs) is a time-consuming and expensive process, traditionally requiring extensive manual effort from human experts. In this study, we explore the potential of Large Language Models (LLMs) to assess skills related to medical student communication. We analyzed 2,027 video-recorded OSCE examinations from the University of Texas Southwestern Medical Center (UTSW), spanning four years (2019-2022), and several different medical cases or "stations." Specifically, our focus was on evaluating students' ability to summarize patients' medical history: we targeted the rubric item 'did the student summarize the patients' medical history?' from the communication skills rubric. After transcribing speech audio captured by OSCE videos using Whisper-v3, we studied the performance of various LLM-based approaches for grading students on this summarization task based on their examination transcripts. Using various frontier-level open-source and proprietary LLMs, we evaluated different techniques such as zero-shot chain-of-thought prompting, retrieval augmented generation, and multi-model ensemble methods. Our results show that frontier LLM models like GPT-4 achieved remarkable alignment with human graders, demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential for LLM-based OSCE grading to augment the current grading process. Open-source models also showed promising results, suggesting potential for widespread, cost-effective deployment. Further, we present a failure analysis identifying conditions where LLM grading may be less reliable in this context and recommend best practices for deploying LLMs in medical education settings.</li>
</ul>

<h3>Title: Enhancing Long Context Performance in LLMs Through Inner Loop Query Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Yimin Tang, Yurong Xu, Ning Yan, Masood Mortazavi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12859">https://arxiv.org/abs/2410.12859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12859">https://arxiv.org/pdf/2410.12859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12859]] Enhancing Long Context Performance in LLMs Through Inner Loop Query Mechanism(https://arxiv.org/abs/2410.12859)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers have a quadratic scaling of computational complexity with input size, which limits the input context window size of large language models (LLMs) in both training and inference. Meanwhile, retrieval-augmented generation (RAG) besed models can better handle longer contexts by using a retrieval system to filter out unnecessary information. However, most RAG methods only perform retrieval based on the initial query, which may not work well with complex questions that require deeper reasoning. We introduce a novel approach, Inner Loop Memory Augmented Tree Retrieval (ILM-TR), involving inner-loop queries, based not only on the query question itself but also on intermediate findings. At inference time, our model retrieves information from the RAG system, integrating data from lengthy documents at various levels of abstraction. Based on the information retrieved, the LLM generates texts stored in an area named Short-Term Memory (STM) which is then used to formulate the next query. This retrieval process is repeated until the text in STM converged. Our experiments demonstrate that retrieval with STM offers improvements over traditional retrieval-augmented LLMs, particularly in long context tests such as Multi-Needle In A Haystack (M-NIAH) and BABILong.</li>
</ul>

<h3>Title: LLMD: A Large Language Model for Interpreting Longitudinal Medical Records</h3>
<ul>
<li><strong>Authors: </strong>Robert Porter, Adam Diehl, Benjamin Pastel, J. Henry Hinnefeld, Lawson Nerenberg, Pye Maung, Sebastien Kerbrat, Gillian Hanson, Troy Astorino, Stephen J. Tarsa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12860">https://arxiv.org/abs/2410.12860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12860">https://arxiv.org/pdf/2410.12860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12860]] LLMD: A Large Language Model for Interpreting Longitudinal Medical Records(https://arxiv.org/abs/2410.12860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce LLMD, a large language model designed to analyze a patient's medical history based on their medical records. Along with domain knowledge, LLMD is trained on a large corpus of records collected over time and across facilities, as well as tasks and labels that make nuanced connections among them. This approach is critical to an accurate picture of patient health, and has distinctive advantages over models trained on knowledge alone, unlabeled records, structured EHR data, or records from a single health system. The recipe for LLMD continues pretraining a foundational model on both domain knowledge and the contents of millions of records. These span an average of 10 years of care and as many as 140 care sites per patient. LLMD is then instruction fine-tuned on structuring and abstraction tasks. The former jointly identify and normalize document metadata, provenance information, clinical named-entities, and ontology mappings, while the latter roll these into higher-level representations, such a continuous era of time a patient was on a medication. LLMD is deployed within a layered validation system that includes continual random audits and review by experts, e.g. based on uncertainty, disease-specific rules, or use-case. LLMD exhibits large gains over both more-powerful generalized models and domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state of the art accuracy on PubMedQA text responses, besting orders-of-magnitude larger models. On production tasks, we show that LLMD significantly outperforms all other models evaluated, and among alternatives, large general purpose LLMs like GPT-4o are more accurate than models emphasizing medical knowledge. We find strong evidence that accuracy on today's medical benchmarks is not the most significant factor when analyzing real-world patient data, an insight with implications for future medical LLMs.'</li>
</ul>

<h3>Title: Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM</h3>
<ul>
<li><strong>Authors: </strong>Minhajur Rahman, Yasir Arafat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12861">https://arxiv.org/abs/2410.12861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12861">https://arxiv.org/pdf/2410.12861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12861]] Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM(https://arxiv.org/abs/2410.12861)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in transformer models have yielded impressive results in Non-Intrusive Load Monitoring (NILM). However, effectively training a transformer on small-scale datasets remains a challenge. This paper addresses this issue by enhancing the attention mechanism of the original transformer to improve performance. We propose two novel mechanisms: the inter-token relation enhancement mechanism and the dynamic temperature tuning mechanism. The first mechanism reduces the prioritization of intra-token relationships in the token similarity matrix during training, thereby increasing inter-token focus. The second mechanism introduces a learnable temperature tuning for the token similarity matrix, mitigating the over-smoothing problem associated with fixed temperature values. Both mechanisms are supported by rigorous mathematical foundations. We evaluate our approach using the REDD residential NILM dataset, a relatively small-scale dataset and demonstrate that our methodology significantly enhances the performance of the original transformer model across multiple appliance types.</li>
</ul>

<h3>Title: Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs</h3>
<ul>
<li><strong>Authors: </strong>Divyanshu Kumar, Umang Jain, Sahil Agarwal, Prashanth Harshangi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12864">https://arxiv.org/abs/2410.12864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12864">https://arxiv.org/pdf/2410.12864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12864]] Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs(https://arxiv.org/abs/2410.12864)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are being adopted across a wide range of tasks, including decision-making processes in industries where bias in AI systems is a significant concern. Recent research indicates that LLMs can harbor implicit biases even when they pass explicit bias evaluations. Building upon the frameworks of the LLM Implicit Association Test (IAT) Bias and LLM Decision Bias, this study highlights that newer or larger language models do not automatically exhibit reduced bias; in some cases, they displayed higher bias scores than their predecessors, such as in Meta's Llama series and OpenAI's GPT models. This suggests that increasing model complexity without deliberate bias mitigation strategies can unintentionally amplify existing biases. The variability in bias scores within and across providers underscores the need for standardized evaluation metrics and benchmarks for bias assessment. The lack of consistency indicates that bias mitigation is not yet a universally prioritized goal in model development, which can lead to unfair or discriminatory outcomes. By broadening the detection of implicit bias, this research provides a more comprehensive understanding of the biases present in advanced models and underscores the critical importance of addressing these issues to ensure the development of fair and responsible AI systems.</li>
</ul>

<h3>Title: ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Zhang, Ning Li, Quan Gan, Weinan Zhang, David Wipf, Minjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12865">https://arxiv.org/abs/2410.12865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12865">https://arxiv.org/pdf/2410.12865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12865]] ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction(https://arxiv.org/abs/2410.12865)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Crafting effective features is a crucial yet labor-intensive and domain-specific task within machine learning pipelines. Fortunately, recent advancements in Large Language Models (LLMs) have shown promise in automating various data science tasks, including feature engineering. But despite this potential, evaluations thus far are primarily based on the end performance of a complete ML pipeline, providing limited insight into precisely how LLMs behave relative to human experts in feature engineering. To address this gap, we propose ELF-Gym, a framework for Evaluating LLM-generated Features. We curated a new dataset from historical Kaggle competitions, including 251 "golden" features used by top-performing teams. ELF-Gym then quantitatively evaluates LLM-generated features by measuring their impact on downstream model performance as well as their alignment with expert-crafted features through semantic and functional similarity assessments. This approach provides a more comprehensive evaluation of disparities between LLMs and human experts, while offering valuable insights into specific areas where LLMs may have room for improvement. For example, using ELF-Gym we empirically demonstrate that, in the best-case scenario, LLMs can semantically capture approximately 56% of the golden features, but at the more demanding implementation level this overlap drops to 13%. Moreover, in other cases LLMs may fail completely, particularly on datasets that require complex features, indicating broad potential pathways for improvement.</li>
</ul>

<h3>Title: Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate Speech Correction and Multimodal Emotion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Kaushal Attaluri, Anirudh CHVS, Sireesha Chittepu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12867">https://arxiv.org/abs/2410.12867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12867">https://arxiv.org/pdf/2410.12867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12867]] Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate Speech Correction and Multimodal Emotion Analysis(https://arxiv.org/abs/2410.12867)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dysarthria is a motor speech disorder caused by neurological damage that affects the muscles used for speech production, leading to slurred, slow, or difficult-to-understand speech. It affects millions of individuals worldwide, including those with conditions such as stroke, traumatic brain injury, cerebral palsy, Parkinsons disease, and multiple sclerosis. Dysarthria presents a major communication barrier, impacting quality of life and social interaction. This paper introduces a novel approach to recognizing and translating dysarthric speech, empowering individuals with this condition to communicate more effectively. We leverage advanced large language models for accurate speech correction and multimodal emotion analysis. Dysarthric speech is first converted to text using OpenAI Whisper model, followed by sentence prediction using fine-tuned open-source models and benchmark models like GPT-4.o, LLaMA 3.1 70B and Mistral 8x7B on Groq AI accelerators. The dataset used combines the TORGO dataset with Google speech data, manually labeled for emotional context. Our framework identifies emotions such as happiness, sadness, neutrality, surprise, anger, and fear, while reconstructing intended sentences from distorted speech with high accuracy. This approach demonstrates significant advancements in the recognition and interpretation of dysarthric speech.</li>
</ul>

<h3>Title: Language Model Preference Evaluation with Multiple Weak Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Zhengyu Hu, Jieyu Zhang, Zhihan Xiong, Alexander Ratner, Hui Xiong, Ranjay Krishna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12869">https://arxiv.org/abs/2410.12869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12869">https://arxiv.org/pdf/2410.12869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12869]] Language Model Preference Evaluation with Multiple Weak Evaluators(https://arxiv.org/abs/2410.12869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding preference remains a critical challenge. Existing works usually leverage a powerful LLM (e.g., GPT4) as the judge for comparing LLMs' output pairwisely, yet such model-based evaluator is vulnerable to conflicting preference, i.e., output A is better than B, B than C, but C than A, causing contradictory evaluation results. To improve model-based preference evaluation, we introduce GED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensemble and denoise these graphs for better, non-contradictory evaluation results. In particular, our method consists of two primary stages: aggregating evaluations into a unified graph and applying a denoising process to eliminate cyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments across ten benchmark datasets show that GED outperforms baseline methods in model ranking, response selection, and model alignment tasks. Notably, GED combines weaker evaluators like Llama3-8B, Mistral-7B, and Qwen2-7B to surpass the performance of stronger evaluators like Qwen2-72B, highlighting its ability to enhance evaluation reliability and improve model performance.</li>
</ul>

<h3>Title: Skill Learning Using Process Mining for Large Language Model Plan Generation</h3>
<ul>
<li><strong>Authors: </strong>Andrei Cosmin Redis, Mohammadreza Fani Sani, Bahram Zarrin, Andrea Burattin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12870">https://arxiv.org/abs/2410.12870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12870">https://arxiv.org/pdf/2410.12870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12870]] Skill Learning Using Process Mining for Large Language Model Plan Generation(https://arxiv.org/abs/2410.12870)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold promise for generating plans for complex tasks, but their effectiveness is limited by sequential execution, lack of control flow models, and difficulties in skill retrieval. Addressing these issues is crucial for improving the efficiency and interpretability of plan generation as LLMs become more central to automation and decision-making. We introduce a novel approach to skill learning in LLMs by integrating process mining techniques, leveraging process discovery for skill acquisition, process models for skill storage, and conformance checking for skill retrieval. Our methods enhance text-based plan generation by enabling flexible skill discovery, parallel execution, and improved interpretability. Experimental results suggest the effectiveness of our approach, with our skill retrieval method surpassing state-of-the-art accuracy baselines under specific conditions.</li>
</ul>

<h3>Title: Beyond Right and Wrong: Mitigating Cold Start in Knowledge Tracing Using Large Language Model and Option Weight</h3>
<ul>
<li><strong>Authors: </strong>JongWoo Kim, SeongYeub Chu, Bryan Wong, Mun Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12872">https://arxiv.org/abs/2410.12872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12872">https://arxiv.org/pdf/2410.12872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12872]] Beyond Right and Wrong: Mitigating Cold Start in Knowledge Tracing Using Large Language Model and Option Weight(https://arxiv.org/abs/2410.12872)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Tracing (KT) is vital in educational data mining, enabling personalized learning by tracking learners' knowledge states and forecasting their academic outcomes. This study introduces the LOKT (Large Language Model Option-weighted Knowledge Tracing) model to address the cold start problem where limited historical data available using large language models (LLMs). While traditional KT models have incorporated option weights, our research extends this by integrating these weights into an LLM-based KT framework. Moving beyond the binary classification of correct and incorrect responses, we emphasize that different types of incorrect answers offer valuable insights into a learner's knowledge state. By converting these responses into text-based ordinal categories, we enable LLMs to assess learner understanding with greater clarity, although our approach focuses on the final knowledge state rather than the progression of learning over time. Using five public datasets, we demonstrate that the LOKT model sustains high predictive accuracy even with limited data, effectively addressing both "learner cold-start" and "system cold-start" scenarios. These findings showcase LOKT's potential to enhance LLM-based learning tools and support early-stage personalization.</li>
</ul>

<h3>Title: In-context KV-Cache Eviction for LLMs via Attention-Gate</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zeng, Bokai Lin, Tianqi Hou, Hao Zhang, Zhijie Deng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12876">https://arxiv.org/abs/2410.12876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12876">https://arxiv.org/pdf/2410.12876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12876]] In-context KV-Cache Eviction for LLMs via Attention-Gate(https://arxiv.org/abs/2410.12876)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The KV-Cache technique has become the standard for the inference of large language models (LLMs). It caches states of self-attention to avoid recomputation. Yet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system, especially when confronted with ultra-large models and long-context queries. A natural remedy is to discard the KV-Cache for less important tokens, with StreamingLLM as an example, but the used static eviction strategies cannot flexibly adapt to varying contexts. Remedies like H2O leverage accumulative attention scores to perform dynamic eviction but suffer from the attention bias issue in capturing contextual information. This paper bridges this gap by devising a parameterized KV-Cache eviction mechanism, dubbed as Attention-Gate, which accepts the whole context as input and yields eviction flags for each token to realize in-context eviction. The subsequent self-attention module proceeds according to the flags and only the KV states for the remaining tokens need to be cached. The Attention-Gates can vary among different heads and layers and be trivially plugged into pre-trained LLMs, tuned by cost-effective continual pre-training or supervised fine-tuning objectives to acquire what to discard. The computational and memory overhead introduced by Attention-Gates is minimal. Our method is validated across multiple tasks, demonstrating both efficiency and adaptability. After a highly efficient continual pre-training, it achieves higher average accuracy and evicts more tokens compared to traditional training-free methods. In supervised fine-tuning, it not only evicts many tokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE, where it improves accuracy by 13.9% while evicting 62.8% of tokens, showing that effective eviction of redundant tokens can even enhance performance.</li>
</ul>

<h3>Title: Towards More Effective Table-to-Text Generation: Assessing In-Context Learning and Self-Evaluation with Open-Source Models</h3>
<ul>
<li><strong>Authors: </strong>Sahar Iravani, Tim .O .F Conrad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12878">https://arxiv.org/abs/2410.12878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12878">https://arxiv.org/pdf/2410.12878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12878]] Towards More Effective Table-to-Text Generation: Assessing In-Context Learning and Self-Evaluation with Open-Source Models(https://arxiv.org/abs/2410.12878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Table processing, a key task in natural language processing, has significantly benefited from recent advancements in language models (LMs). However, the capabilities of LMs in table-to-text generation, which transforms structured data into coherent narrative text, require an in-depth investigation, especially with current open-source models. This study explores the effectiveness of various in-context learning strategies in LMs across benchmark datasets, focusing on the impact of providing examples to the model. More importantly, we examine a real-world use case, offering valuable insights into practical applications. To complement traditional evaluation metrics, we employ a large language model (LLM) self-evaluation approach using chain-of-thought reasoning and assess its correlation with human-aligned metrics like BERTScore. Our findings highlight the significant impact of examples in improving table-to-text generation and suggest that, while LLM self-evaluation has potential, its current alignment with human judgment could be enhanced. This points to the need for more reliable evaluation methods.</li>
</ul>

<h3>Title: Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Somnath Banerjee, Sayan Layek, Hari Shrawgi, Rajarshi Mandal, Avik Halder, Shanu Kumar, Sagnik Basu, Parag Agrawal, Rima Hazra, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12880">https://arxiv.org/abs/2410.12880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12880">https://arxiv.org/pdf/2410.12880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12880]] Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models(https://arxiv.org/abs/2410.12880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As LLMs are increasingly deployed in global applications, the importance of cultural sensitivity becomes paramount, ensuring that users from diverse backgrounds feel respected and understood. Cultural harm can arise when these models fail to align with specific cultural norms, resulting in misrepresentations or violations of cultural values. This work addresses the challenges of ensuring cultural sensitivity in LLMs, especially in small-parameter models that often lack the extensive training data needed to capture global cultural nuances. We present two key contributions: (1) A cultural harm test dataset, created to assess model outputs across different cultural contexts through scenarios that expose potential cultural insensitivities, and (2) A culturally aligned preference dataset, aimed at restoring cultural sensitivity through fine-tuning based on feedback from diverse annotators. These datasets facilitate the evaluation and enhancement of LLMs, ensuring their ethical and safe deployment across different cultural landscapes. Our results show that integrating culturally aligned feedback leads to a marked improvement in model behavior, significantly reducing the likelihood of generating culturally insensitive or harmful content. Ultimately, this work paves the way for more inclusive and respectful AI systems, fostering a future where LLMs can safely and ethically navigate the complexities of diverse cultural landscapes.</li>
</ul>

<h3>Title: REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models</h3>
<ul>
<li><strong>Authors: </strong>Ambuje Gupta, Mrinal Rawat, Andreas Stolcke, Roberto Pieraccini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12890">https://arxiv.org/abs/2410.12890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12890">https://arxiv.org/pdf/2410.12890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12890]] REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models(https://arxiv.org/abs/2410.12890)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.</li>
</ul>

<h3>Title: Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants</h3>
<ul>
<li><strong>Authors: </strong>Rafael Ferreira, David Semedo, João Magalhães</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12891">https://arxiv.org/abs/2410.12891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12891">https://arxiv.org/pdf/2410.12891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12891]] Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants(https://arxiv.org/abs/2410.12891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conversational systems must be robust to user interactions that naturally exhibit diverse conversational traits. Capturing and simulating these diverse traits coherently and efficiently presents a complex challenge. This paper introduces Multi-Trait Adaptive Decoding (mTAD), a method that generates diverse user profiles at decoding-time by sampling from various trait-specific Language Models (LMs). mTAD provides an adaptive and scalable approach to user simulation, enabling the creation of multiple user profiles without the need for additional fine-tuning. By analyzing real-world dialogues from the Conversational Task Assistant (CTA) domain, we identify key conversational traits and developed a framework to generate profile-aware dialogues that enhance conversational diversity. Experimental results validate the effectiveness of our approach in modeling single-traits using specialized LMs, which can capture less common patterns, even in out-of-domain tasks. Furthermore, the results demonstrate that mTAD is a robust and flexible framework for combining diverse user simulators.</li>
</ul>

<h3>Title: MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Aniket Deroy, Subhankar Maity, Sudeshna Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12893">https://arxiv.org/abs/2410.12893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12893">https://arxiv.org/pdf/2410.12893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12893]] MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation(https://arxiv.org/abs/2410.12893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic question generation is a critical task that involves evaluating question quality by considering factors such as engagement, pedagogical value, and the ability to stimulate critical thinking. These aspects require human-like understanding and judgment, which automated systems currently lack. However, human evaluations are costly and impractical for large-scale samples of generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating), which leverages large language models (LLMs) to automate the evaluation process for questions generated by automated question generation systems. We experimented with several state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We observed that the scores of human evaluation metrics, namely relevance, appropriateness, novelty, complexity, and grammaticality, improved when using the feedback-based approach called MIRROR, tending to be closer to the human baseline scores. Furthermore, we observed that Pearson's correlation coefficient between GPT-4 and human experts improved when using our proposed feedback-based approach, MIRROR, compared to direct prompting for evaluation. Error analysis shows that our proposed approach, MIRROR, significantly helps to improve relevance and appropriateness.</li>
</ul>

<h3>Title: Large Language Models and the Rationalist Empiricist Debate</h3>
<ul>
<li><strong>Authors: </strong>David King</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12895">https://arxiv.org/abs/2410.12895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12895">https://arxiv.org/pdf/2410.12895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12895]] Large Language Models and the Rationalist Empiricist Debate(https://arxiv.org/abs/2410.12895)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To many Chomsky's debates with Quine and Skinner are an updated version of the Rationalist Empiricist debates of the 17th century. The consensus being that Chomsky's Rationalism was victorious. This dispute has reemerged with the advent of Large Language Models. With some arguing that LLMs vindicate rationalism because of the necessity of building in innate biases to make them work. The necessity of building in innate biases is taken to prove that empiricism hasn't got the conceptual resources to explain linguistic competence. Such claims depend on the nature of the empiricism one is endorsing. Externalized Empiricism has no difficulties with innate apparatus once they are determined empirically (Quine 1969). Thus, externalized empiricism is not refuted because of the need to build in innate biases in LLMs. Furthermore, the relevance of LLMs to the rationalist empiricist debate in relation to humans is dubious. For any claim about whether LLMs learn in an empiricist manner to be relevant to humans it needs to be shown that LLMs and humans learn in the same way. Two key features distinguish humans and LLMs. Humans learn despite a poverty of stimulus and LLMs learn because of an incredibly rich stimulus. Human linguistic outputs are grounded in sensory experience and LLMs are not. These differences in how the two learn indicates that they both use different underlying competencies to produce their output. Therefore, any claims about whether LLMs learn in an empiricist manner are not relevant to whether humans learn in an empiricist manner.</li>
</ul>

<h3>Title: A Survey on Data Synthesis and Augmentation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12896">https://arxiv.org/abs/2410.12896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12896">https://arxiv.org/pdf/2410.12896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12896]] A Survey on Data Synthesis and Augmentation for Large Language Models(https://arxiv.org/abs/2410.12896)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration.</li>
</ul>

<h3>Title: Fair Clustering for Data Summarization: Improved Approximation Algorithms and Complexity Insights</h3>
<ul>
<li><strong>Authors: </strong>Ameet Gadekar, Aristides Gionis, Suhas Thejaswi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12913">https://arxiv.org/abs/2410.12913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12913">https://arxiv.org/pdf/2410.12913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12913]] Fair Clustering for Data Summarization: Improved Approximation Algorithms and Complexity Insights(https://arxiv.org/abs/2410.12913)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Data summarization tasks are often modeled as $k$-clustering problems, where the goal is to choose $k$ data points, called cluster centers, that best represent the dataset by minimizing a clustering objective. A popular objective is to minimize the maximum distance between any data point and its nearest center, which is formalized as the $k$-center problem. While in some applications all data points can be chosen as centers, in the general setting, centers must be chosen from a predefined subset of points, referred as facilities or suppliers; this is known as the $k$-supplier problem. In this work, we focus on fair data summarization modeled as the fair $k$-supplier problem, where data consists of several groups, and a minimum number of centers must be selected from each group while minimizing the $k$-supplier objective. The groups can be disjoint or overlapping, leading to two distinct problem variants each with different computational complexity. We present $3$-approximation algorithms for both variants, improving the previously known factor of $5$. For disjoint groups, our algorithm runs in polynomial time, while for overlapping groups, we present a fixed-parameter tractable algorithm, where the exponential runtime depends only on the number of groups and centers. We show that these approximation factors match the theoretical lower bounds, assuming standard complexity theory conjectures. Finally, using an open-source implementation, we demonstrate the scalability of our algorithms on large synthetic datasets and assess the price of fairness on real-world data, comparing solution quality with and without fairness constraints.</li>
</ul>

<h3>Title: MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation</h3>
<ul>
<li><strong>Authors: </strong>Satya Krishna Gorti, Ilan Gofman, Zhaoyan Liu, Jiapeng Wu, Noël Vouitsis, Guangwei Yu, Jesse C. Cresswell, Rasa Hosseinzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12916">https://arxiv.org/abs/2410.12916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12916">https://arxiv.org/pdf/2410.12916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12916]] MSc-SQL: Multi-Sample Critiquing Small Language Models For Text-To-SQL Translation(https://arxiv.org/abs/2410.12916)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we focus on developing small, efficient, and open-source text-to-SQL models. We demonstrate the benefits of sampling multiple candidate SQL generations and propose our method, MSc-SQL, to critique them using associated metadata. Our sample critiquing model evaluates multiple outputs simultaneously, achieving state-of-the-art performance compared to other open-source models while remaining competitive with larger models at a much lower cost. Full code can be found at this http URL.</li>
</ul>

<h3>Title: Interpreting token compositionality in LLMs: A robustness analysis</h3>
<ul>
<li><strong>Authors: </strong>Nura Aljaafari, Danilo S. Carvalho, André Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12924">https://arxiv.org/abs/2410.12924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12924">https://arxiv.org/pdf/2410.12924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12924]] Interpreting token compositionality in LLMs: A robustness analysis(https://arxiv.org/abs/2410.12924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the internal mechanisms of large language models (LLMs) is integral to enhancing their reliability, interpretability, and inference processes. We present Constituent-Aware Pooling (CAP), a methodology designed to analyse how LLMs process compositional linguistic structures. Grounded in principles of compositionality, mechanistic interpretability, and information gain theory, CAP systematically intervenes in model activations through constituent-based pooling at various model levels. Our experiments on inverse definition modelling, hypernym and synonym prediction reveal critical insights into transformers' limitations in handling compositional abstractions. No specific layer integrates tokens into unified semantic representations based on their constituent parts. We observe fragmented information processing, which intensifies with model size, suggesting that larger models struggle more with these interventions and exhibit greater information dispersion. This fragmentation likely stems from transformers' training objectives and architectural design, preventing systematic and cohesive representations. Our findings highlight fundamental limitations in current transformer architectures regarding compositional semantics processing and model interpretability, underscoring the critical need for novel approaches in LLM design to address these challenges.</li>
</ul>

<h3>Title: DEeR: Deviation Eliminating and Noise Regulating for Privacy-preserving Federated Low-rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Meilu Zhu, Axiu Mao, Jun Liu, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12926">https://arxiv.org/abs/2410.12926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12926">https://arxiv.org/pdf/2410.12926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12926]] DEeR: Deviation Eliminating and Noise Regulating for Privacy-preserving Federated Low-rank Adaptation(https://arxiv.org/abs/2410.12926)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>Integrating low-rank adaptation (LoRA) with federated learning (FL) has received widespread attention recently, aiming to adapt pretrained foundation models (FMs) to downstream medical tasks via privacy-preserving decentralized training. However, owing to the direct combination of LoRA and FL, current methods generally undergo two problems, i.e., aggregation deviation, and differential privacy (DP) noise amplification effect. To address these problems, we propose a novel privacy-preserving federated finetuning framework called \underline{D}eviation \underline{E}liminating and Nois\underline{e} \underline{R}egulating (DEeR). Specifically, we firstly theoretically prove that the necessary condition to eliminate aggregation deviation is guaranteing the equivalence between LoRA parameters of clients. Based on the theoretical insight, a deviation eliminator is designed to utilize alternating minimization algorithm to iteratively optimize the zero-initialized and non-zero-initialized parameter matrices of LoRA, ensuring that aggregation deviation always be zeros during training. Furthermore, we also conduct an in-depth analysis of the noise amplification effect and find that this problem is mainly caused by the ``linear relationship'' between DP noise and LoRA parameters. To suppress the noise amplification effect, we propose a noise regulator that exploits two regulator factors to decouple relationship between DP and LoRA, thereby achieving robust privacy protection and excellent finetuning performance. Additionally, we perform comprehensive ablated experiments to verify the effectiveness of the deviation eliminator and noise regulator. DEeR shows better performance on public medical datasets in comparison with state-of-the-art approaches. The code is available at this https URL.</li>
</ul>

<h3>Title: SoK: On Finding Common Ground in Loss Landscapes Using Deep Model Merging Techniques</h3>
<ul>
<li><strong>Authors: </strong>Arham Khan, Todd Nief, Nathaniel Hudson, Mansi Sakarvadia, Daniel Grzenda, Aswathy Ajith, Jordan Pettyjohn, Kyle Chard, Ian Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12927">https://arxiv.org/abs/2410.12927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12927">https://arxiv.org/pdf/2410.12927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12927]] SoK: On Finding Common Ground in Loss Landscapes Using Deep Model Merging Techniques(https://arxiv.org/abs/2410.12927)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding neural networks is crucial to creating reliable and trustworthy deep learning models. Most contemporary research in interpretability analyzes just one model at a time via causal intervention or activation analysis. Yet despite successes, these methods leave significant gaps in our understanding of the training behaviors of neural networks, how their inner representations emerge, and how we can predictably associate model components with task-specific behaviors. Seeking new insights from work in related fields, here we survey literature in the field of model merging, a field that aims to combine the abilities of various neural networks by merging their parameters and identifying task-specific model components in the process. We analyze the model merging literature through the lens of loss landscape geometry, an approach that enables us to connect observations from empirical studies on interpretability, security, model merging, and loss landscape analysis to phenomena that govern neural network training and the emergence of their inner representations. To systematize knowledge in this area, we present a novel taxonomy of model merging techniques organized by their core algorithmic principles. Additionally, we distill repeated empirical observations from the literature in these fields into characterizations of four major aspects of loss landscape geometry: mode convexity, determinism, directedness, and connectivity. We argue that by improving our understanding of the principles underlying model merging and loss landscape geometry, this work contributes to the goal of ensuring secure and trustworthy machine learning in practice.</li>
</ul>

<h3>Title: Enhancing Mathematical Reasoning in LLMs by Stepwise Correction</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12934">https://arxiv.org/abs/2410.12934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12934">https://arxiv.org/pdf/2410.12934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12934]] Enhancing Mathematical Reasoning in LLMs by Stepwise Correction(https://arxiv.org/abs/2410.12934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Best-of-N decoding methods instruct large language models (LLMs) to generate multiple solutions, score each using a scoring function, and select the highest scored as the final answer to mathematical reasoning problems. However, this repeated independent process often leads to the same mistakes, making the selected solution still incorrect. We propose a novel prompting method named Stepwise Correction (StepCo) that helps LLMs identify and revise incorrect steps in their generated reasoning paths. It iterates verification and revision phases that employ a process-supervised verifier. The verify-then-revise process not only improves answer correctness but also reduces token consumption with fewer paths needed to generate. With StepCo, a series of LLMs demonstrate exceptional performance. Notably, using GPT-4o as the backend LLM, StepCo achieves an average accuracy of 94.1 across eight datasets, significantly outperforming the state-of-the-art Best-of-N method by +2.4, while reducing token consumption by 77.8%.</li>
</ul>

<h3>Title: UMambaAdj: Advancing GTV Segmentation for Head and Neck Cancer in MRI-Guided RT with UMamba and nnU-Net ResEnc Planner</h3>
<ul>
<li><strong>Authors: </strong>Jintao Ren, Kim Hochreuter, Jesper Folsted Kallehauge, Stine Sofia Korreman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12940">https://arxiv.org/abs/2410.12940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12940">https://arxiv.org/pdf/2410.12940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12940]] UMambaAdj: Advancing GTV Segmentation for Head and Neck Cancer in MRI-Guided RT with UMamba and nnU-Net ResEnc Planner(https://arxiv.org/abs/2410.12940)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) plays a crucial role in MRI-guided adaptive radiotherapy for head and neck cancer (HNC) due to its superior soft-tissue contrast. However, accurately segmenting the gross tumor volume (GTV), which includes both the primary tumor (GTVp) and lymph nodes (GTVn), remains challenging. Recently, two deep learning segmentation innovations have shown great promise: UMamba, which effectively captures long-range dependencies, and the nnU-Net Residual Encoder (ResEnc), which enhances feature extraction through multistage residual blocks. In this study, we integrate these strengths into a novel approach, termed 'UMambaAdj'. Our proposed method was evaluated on the HNTS-MRG 2024 challenge test set using pre-RT T2-weighted MRI images, achieving an aggregated Dice Similarity Coefficient (DSCagg) of 0.751 for GTVp and 0.842 for GTVn, with a mean DSCagg of 0.796. This approach demonstrates potential for more precise tumor delineation in MRI-guided adaptive radiotherapy, ultimately improving treatment outcomes for HNC patients. Team: DCPT-Stine's group.</li>
</ul>

<h3>Title: Gradient Map-Assisted Head and Neck Tumor Segmentation: A Pre-RT to Mid-RT Approach in MRI-Guided Radiotherapy</h3>
<ul>
<li><strong>Authors: </strong>Jintao Ren, Kim Hochreuter, Mathis Ersted Rasmussen, Jesper Folsted Kallehauge, Stine Sofia Korreman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12941">https://arxiv.org/abs/2410.12941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12941">https://arxiv.org/pdf/2410.12941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12941]] Gradient Map-Assisted Head and Neck Tumor Segmentation: A Pre-RT to Mid-RT Approach in MRI-Guided Radiotherapy(https://arxiv.org/abs/2410.12941)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Radiation therapy (RT) is a vital part of treatment for head and neck cancer, where accurate segmentation of gross tumor volume (GTV) is essential for effective treatment planning. This study investigates the use of pre-RT tumor regions and local gradient maps to enhance mid-RT tumor segmentation for head and neck cancer in MRI-guided adaptive radiotherapy. By leveraging pre-RT images and their segmentations as prior knowledge, we address the challenge of tumor localization in mid-RT segmentation. A gradient map of the tumor region from the pre-RT image is computed and applied to mid-RT images to improve tumor boundary delineation. Our approach demonstrated improved segmentation accuracy for both primary GTV (GTVp) and nodal GTV (GTVn), though performance was limited by data constraints. The final DSCagg scores from the challenge's test set evaluation were 0.534 for GTVp, 0.867 for GTVn, and a mean score of 0.70. This method shows potential for enhancing segmentation and treatment planning in adaptive radiotherapy. Team: DCPT-Stine's group.</li>
</ul>

<h3>Title: What Do Speech Foundation Models Not Learn About Speech?</h3>
<ul>
<li><strong>Authors: </strong>Abdul Waheed, Hanin Atwany, Bhiksha Raj, Rita Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12948">https://arxiv.org/abs/2410.12948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12948">https://arxiv.org/pdf/2410.12948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12948]] What Do Speech Foundation Models Not Learn About Speech?(https://arxiv.org/abs/2410.12948)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how speech foundation models capture non-verbal cues is crucial for improving their interpretability and adaptability across diverse tasks. In our work, we analyze several prominent models such as Whisper, Seamless, Wav2Vec, HuBERT, and Qwen2-Audio focusing on their learned representations in both paralinguistic and non-paralinguistic tasks from the Dynamic-SUPERB benchmark. Our study addresses three key questions: (1) What non-verbal cues (e.g., speaker intent, emotion, environmental context) are captured? (2) How are these cues represented across different layers of the models? and (3) To what extent can these representations be effectively adapted to downstream tasks? To answer these questions, we first evaluate the models in a zero-shot setting, followed by fine-tuning on layer-wise features extracted from these models. Our results provide insights into the models' capacity for generalization, the characteristics of their layer-wise representations, and the degree of transformation required for downstream task adaptation. Our findings suggest that some of these models perform well on various tasks in zero-shot settings, despite not being explicitly trained for those tasks. We also observe that zero-shot performance correlates with better-learned representations. The analysis of layer-wise features demonstrates that some models exhibit a convex relationship between the separability of the learned representations and model depth, with different layers capturing task-specific features.</li>
</ul>

<h3>Title: Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization</h3>
<ul>
<li><strong>Authors: </strong>Phillip Guo, Aaquib Syed, Abhay Sheshadri, Aidan Ewart, Gintare Karolina Dziugaite</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12949">https://arxiv.org/abs/2410.12949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12949">https://arxiv.org/pdf/2410.12949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12949]] Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization(https://arxiv.org/abs/2410.12949)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.</li>
</ul>

<h3>Title: Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12952">https://arxiv.org/abs/2410.12952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12952">https://arxiv.org/pdf/2410.12952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12952]] Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning(https://arxiv.org/abs/2410.12952)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited significant potential in performing diverse tasks, including the ability to call functions or use external tools to enhance their performance. While current research on function calling by LLMs primarily focuses on single-turn interactions, this paper addresses the overlooked necessity for LLMs to engage in multi-turn function calling--critical for handling compositional, real-world queries that require planning with functions but not only use functions. To facilitate this, we introduce an approach, BUTTON, which generates synthetic compositional instruction tuning data via bottom-up instruction construction and top-down trajectory generation. In the bottom-up phase, we generate simple atomic tasks based on real-world scenarios and build compositional tasks using heuristic strategies based on atomic tasks. Corresponding functions are then developed for these compositional tasks. The top-down phase features a multi-agent environment where interactions among simulated humans, assistants, and tools are utilized to gather multi-turn function calling trajectories. This approach ensures task compositionality and allows for effective function and trajectory generation by examining atomic tasks within compositional tasks. We produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its effectiveness through extensive experiments across various LLMs.</li>
</ul>

<h3>Title: Syn2Real Domain Generalization for Underwater Mine-like Object Detection Using Side-Scan Sonar</h3>
<ul>
<li><strong>Authors: </strong>Aayush Agrawal, Aniruddh Sikdar, Rajini Makam, Suresh Sundaram, Suresh Kumar Besai, Mahesh Gopi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12953">https://arxiv.org/abs/2410.12953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12953">https://arxiv.org/pdf/2410.12953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12953]] Syn2Real Domain Generalization for Underwater Mine-like Object Detection Using Side-Scan Sonar(https://arxiv.org/abs/2410.12953)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Underwater mine detection with deep learning suffers from limitations due to the scarcity of real-world data. This scarcity leads to overfitting, where models perform well on training data but poorly on unseen data. This paper proposes a Syn2Real (Synthetic to Real) domain generalization approach using diffusion models to address this challenge. We demonstrate that synthetic data generated with noise by DDPM and DDIM models, even if not perfectly realistic, can effectively augment real-world samples for training. The residual noise in the final sampled images improves the model's ability to generalize to real-world data with inherent noise and high variation. The baseline Mask-RCNN model when trained on a combination of synthetic and original training datasets, exhibited approximately a 60% increase in Average Precision (AP) compared to being trained solely on the original training data. This significant improvement highlights the potential of Syn2Real domain generalization for underwater mine detection tasks.</li>
</ul>

<h3>Title: A Note on Shumailov et al. (2024): `AI Models Collapse When Trained on Recursively Generated Data'</h3>
<ul>
<li><strong>Authors: </strong>Ali Borji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12954">https://arxiv.org/abs/2410.12954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12954">https://arxiv.org/pdf/2410.12954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12954]] A Note on Shumailov et al. (2024): `AI Models Collapse When Trained on Recursively Generated Data'(https://arxiv.org/abs/2410.12954)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The study conducted by Shumailov et al. (2024) demonstrates that repeatedly training a generative model on synthetic data leads to model collapse. This finding has generated considerable interest and debate, particularly given that current models have nearly exhausted the available data. In this work, we investigate the effects of fitting a distribution (through Kernel Density Estimation, or KDE) or a model to the data, followed by repeated sampling from it. Our objective is to develop a theoretical understanding of the phenomenon observed by Shumailov et al. (2024). Our results indicate that the outcomes reported are a statistical phenomenon and may be unavoidable.</li>
</ul>

<h3>Title: Long-Tailed Backdoor Attack Using Dynamic Data Augmentation Operations</h3>
<ul>
<li><strong>Authors: </strong>Lu Pang, Tao Sun, Weimin Lyu, Haibin Ling, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12955">https://arxiv.org/abs/2410.12955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12955">https://arxiv.org/pdf/2410.12955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12955]] Long-Tailed Backdoor Attack Using Dynamic Data Augmentation Operations(https://arxiv.org/abs/2410.12955)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Recently, backdoor attack has become an increasing security threat to deep neural networks and drawn the attention of researchers. Backdoor attacks exploit vulnerabilities in third-party pretrained models during the training phase, enabling them to behave normally for clean samples and mispredict for samples with specific triggers. Existing backdoor attacks mainly focus on balanced datasets. However, real-world datasets often follow long-tailed distributions. In this paper, for the first time, we explore backdoor attack on such datasets. Specifically, we first analyze the influence of data imbalance on backdoor attack. Based on our analysis, we propose an effective backdoor attack named Dynamic Data Augmentation Operation (D$^2$AO). We design D$^2$AO selectors to select operations depending jointly on the class, sample type (clean vs. backdoored) and sample features. Meanwhile, we develop a trigger generator to generate sample-specific triggers. Through simultaneous optimization of the backdoored model and trigger generator, guided by dynamic data augmentation operation selectors, we achieve significant advancements. Extensive experiments demonstrate that our method can achieve the state-of-the-art attack performance while preserving the clean accuracy.</li>
</ul>

<h3>Title: Super-resolving Real-world Image Illumination Enhancement: A New Dataset and A Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Yaofang Liu, Jinshan Pan, Yuxiang Hui, Fan Jia, Raymond H. Chan, Tieyong Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12961">https://arxiv.org/abs/2410.12961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12961">https://arxiv.org/pdf/2410.12961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12961]] Super-resolving Real-world Image Illumination Enhancement: A New Dataset and A Conditional Diffusion Model(https://arxiv.org/abs/2410.12961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Most existing super-resolution methods and datasets have been developed to improve the image quality in well-lighted conditions. However, these methods do not work well in real-world low-light conditions as the images captured in such conditions lose most important information and contain significant unknown noises. To solve this problem, we propose a SRRIIE dataset with an efficient conditional diffusion probabilistic models-based method. The proposed dataset contains 4800 paired low-high quality images. To ensure that the dataset are able to model the real-world image degradation in low-illumination environments, we capture images using an ILDC camera and an optical zoom lens with exposure levels ranging from -6 EV to 0 EV and ISO levels ranging from 50 to 12800. We comprehensively evaluate with various reconstruction and perceptual metrics and demonstrate the practicabilities of the SRRIIE dataset for deep learning-based methods. We show that most existing methods are less effective in preserving the structures and sharpness of restored images from complicated noises. To overcome this problem, we revise the condition for Raw sensor data and propose a novel time-melding condition for diffusion probabilistic model. Comprehensive quantitative and qualitative experimental results on the real-world benchmark datasets demonstrate the feasibility and effectivenesses of the proposed conditional diffusion probabilistic model on Raw sensor data. Code and dataset will be available at this https URL</li>
</ul>

<h3>Title: Self-Pluralising Culture Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shaoyang Xu, Yongqi Leng, Linhao Yu, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12971">https://arxiv.org/abs/2410.12971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12971">https://arxiv.org/pdf/2410.12971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12971]] Self-Pluralising Culture Alignment for Large Language Models(https://arxiv.org/abs/2410.12971)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly accessible in many countries, it is essential to align them to serve pluralistic human values across cultures. However, pluralistic culture alignment in LLMs remain an open problem. In this paper, we propose CultureSPA, a Self-Pluralising Culture Alignment framework that allows LLMs to simultaneously align to pluralistic cultures. The framework first generates questions on various culture topics, then yields LLM outputs in response to these generated questions under both culture-aware and culture-unaware settings. By comparing culture-aware/unaware outputs, we are able to detect and collect culture-related instances. These instances are employed to fine-tune LLMs to serve pluralistic cultures in either a culture-joint or culture-specific way. Extensive experiments demonstrate that CultureSPA significantly improves the alignment of LLMs to diverse cultures without compromising general abilities. And further improvements can be achieved if CultureSPA is combined with advanced prompt engineering techniques. Comparisons between culture-joint and culture-specific tuning strategies, along with variations in data quality and quantity, illustrate the robustness of our method. We also explore the mechanisms underlying CultureSPA and the relations between different cultures it reflects.</li>
</ul>

<h3>Title: Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks</h3>
<ul>
<li><strong>Authors: </strong>Rudra Murthy, Prince Kumar, Praveen Venkateswaran, Danish Contractor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12972">https://arxiv.org/abs/2410.12972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12972">https://arxiv.org/pdf/2410.12972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12972]] Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks(https://arxiv.org/abs/2410.12972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we focus our attention on developing a benchmark for instruction-following where it is easy to verify both task performance as well as instruction-following capabilities. We adapt existing knowledge benchmarks and augment them with instructions that are a) conditional on correctly answering the knowledge task or b) use the space of candidate options in multiple-choice knowledge-answering tasks. This allows us to study model characteristics, such as their change in performance on the knowledge tasks in the presence of answer-modifying instructions and distractor instructions. In contrast to existing benchmarks for instruction following, we not only measure instruction-following capabilities but also use LLM-free methods to study task performance. We study a series of openly available large language models of varying parameter sizes (1B-405B) and closed source models namely GPT-4o-mini, GPT-4o. We find that even large-scale instruction-tuned LLMs fail to follow simple instructions in zero-shot settings. We release our dataset, the benchmark, code, and results for future work.</li>
</ul>

<h3>Title: BenchmarkCards: Large Language Model and Risk Reporting</h3>
<ul>
<li><strong>Authors: </strong>Anna Sokol, Nuno Moniz, Elizabeth Daly, Michael Hind, Nitesh Chawla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12974">https://arxiv.org/abs/2410.12974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12974">https://arxiv.org/pdf/2410.12974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12974]] BenchmarkCards: Large Language Model and Risk Reporting(https://arxiv.org/abs/2410.12974)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) offer powerful capabilities but also introduce significant risks. One way to mitigate these risks is through comprehensive pre-deployment evaluations using benchmarks designed to test for specific vulnerabilities. However, the rapidly expanding body of LLM benchmark literature lacks a standardized method for documenting crucial benchmark details, hindering consistent use and informed selection. BenchmarkCards addresses this gap by providing a structured framework specifically for documenting LLM benchmark properties rather than defining the entire evaluation process itself. BenchmarkCards do not prescribe how to measure or interpret benchmark results (e.g., defining ``correctness'') but instead offer a standardized way to capture and report critical characteristics like targeted risks and evaluation methodologies, including properties such as bias and fairness. This structured metadata facilitates informed benchmark selection, enabling researchers to choose appropriate benchmarks and promoting transparency and reproducibility in LLM evaluation.</li>
</ul>

<h3>Title: Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Costin-Andrei Oncescu, Sanket Purandare, Stratos Idreos, Sham Kakade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12982">https://arxiv.org/abs/2410.12982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12982">https://arxiv.org/pdf/2410.12982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12982]] Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond(https://arxiv.org/abs/2410.12982)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length. Several subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution sequence models (LCSMs), such as Hyena, address this issue at training time but remain quadratic during inference. We propose a method for speeding up LCSMs' exact inference to quasilinear $O(L\log^2L)$ time, identify the key properties that make this possible, and propose a general framework that exploits these. Our approach, inspired by previous work on relaxed polynomial interpolation, is based on a tiling which helps decrease memory movement and share computation. It has the added benefit of allowing for almost complete parallelization across layers of the position-mixing part of the architecture. Empirically, we provide a proof of concept implementation for Hyena, which gets up to $1.6\times$ end-to-end improvement over standard inference by improving $50\times$ within the position-mixing part.</li>
</ul>

<h3>Title: Double-Bayesian Learning</h3>
<ul>
<li><strong>Authors: </strong>Stefan Jaeger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12984">https://arxiv.org/abs/2410.12984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12984">https://arxiv.org/pdf/2410.12984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12984]] Double-Bayesian Learning(https://arxiv.org/abs/2410.12984)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Contemporary machine learning methods will try to approach the Bayes error, as it is the lowest possible error any model can achieve. This paper postulates that any decision is composed of not one but two Bayesian decisions and that decision-making is, therefore, a double-Bayesian process. The paper shows how this duality implies intrinsic uncertainty in decisions and how it incorporates explainability. The proposed approach understands that Bayesian learning is tantamount to finding a base for a logarithmic function measuring uncertainty, with solutions being fixed points. Furthermore, following this approach, the golden ratio describes possible solutions satisfying Bayes' theorem. The double-Bayesian framework suggests using a learning rate and momentum weight with values similar to those used in the literature to train neural networks with stochastic gradient descent.</li>
</ul>

<h3>Title: Leveraging LLMs for Translating and Classifying Mental Health Data</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Skianis, A. Seza Doğruöz, John Pavlopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12985">https://arxiv.org/abs/2410.12985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12985">https://arxiv.org/pdf/2410.12985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12985]] Leveraging LLMs for Translating and Classifying Mental Health Data(https://arxiv.org/abs/2410.12985)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in medical fields. In mental health support, the early identification of linguistic markers associated with mental health conditions can provide valuable support to mental health professionals, and reduce long waiting times for patients. Despite the benefits of LLMs for mental health support, there is limited research on their application in mental health systems for languages other than English. Our study addresses this gap by focusing on the detection of depression severity in Greek through user-generated posts which are automatically translated from English. Our results show that GPT3.5-turbo is not very successful in identifying the severity of depression in English, and it has a varying performance in Greek as well. Our study underscores the necessity for further research, especially in languages with less resources. Also, careful implementation is necessary to ensure that LLMs are used effectively in mental health platforms, and human supervision remains crucial to avoid misdiagnosis.</li>
</ul>

<h3>Title: Qtok: A Comprehensive Framework for Evaluating Multilingual Tokenizer Quality in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Iaroslav Chelombitko, Egor Safronov, Aleksey Komissarov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12989">https://arxiv.org/abs/2410.12989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12989">https://arxiv.org/pdf/2410.12989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12989]] Qtok: A Comprehensive Framework for Evaluating Multilingual Tokenizer Quality in Large Language Models(https://arxiv.org/abs/2410.12989)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the development of Large Language Models (LLMs), considerable attention has been given to the quality of training datasets. However, the role of tokenizers in the LLM training pipeline, particularly for multilingual models, has received less focus. The quality of tokenization can significantly impact a model's ability to handle diverse languages effectively. We introduce Qtok, a tool designed to assess tokenizer quality with a specific emphasis on their performance in multilingual contexts. Our research proposes a set of metrics for evaluating tokenizer quality, including measures of language coverage, token completeness, and distribution across languages and linguistic categories. Qtok applies these metrics to evaluate 13 distinct tokenizers from 58 publicly available models, analyzing their output across different linguistic contexts. Our analysis revealed significant variations in token distribution across languages and categories, highlighting potential biases and areas for improvement in current tokenization strategies. This research contributes to the field of tokenizer evaluation within multilingual LLM development by providing a systematic approach to assessing tokenizer quality. Our findings highlight the critical role of tokenization in multilingual LLM capability. The Qtok tool and our analysis methodology offer practical means for researchers to evaluate and improve tokenization strategies for multilingual applications. We offer a method to compare tokenizer quality across these metrics, which may be useful when selecting or adjusting tokenizers for specific multilingual LLM applications.</li>
</ul>

<h3>Title: Explainable Binary Classification of Separable Shape Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Zachary Grey, Nicholas Fisher, Andrew Glaws, Gunay Dogan</a></li>
<li><strong>Subjects: </strong>cs.CV, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12994">https://arxiv.org/abs/2410.12994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12994">https://arxiv.org/pdf/2410.12994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12994]] Explainable Binary Classification of Separable Shape Ensembles(https://arxiv.org/abs/2410.12994)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Materials scientists utilize image segmentation of micrographs to create large curve ensembles representing grain boundaries of material microstructures. Observations of these collections of shapes can facilitate inferences about material properties and manufacturing processes. We seek to bolster this application, and related engineering/scientific tasks, using novel pattern recognition formalisms and inference over large ensembles of segmented curves -- i.e., facilitate principled assessments for quantifying differences in distributions of shapes. To this end, we apply a composite integral operator to motivate accurate and efficient numerical representations of discrete planar curves over matrix manifolds. The main result is a rigid-invariant orthonormal decomposition of curve component functions into separable forms of scale variations and complementary features of undulation. We demonstrate how these separable shape tensors -- given thousands of curves in an ensemble -- can inform explainable binary classification of segmented images by utilizing a product maximum mean discrepancy to distinguish the shape distributions; absent labelled data, building interpretable feature spaces in seconds without high performance computation, and detecting discrepancies below cursory visual inspections.</li>
</ul>

<h3>Title: SSET: Swapping-Sliding Explanation for Time Series Classifiers in Affect Detection</h3>
<ul>
<li><strong>Authors: </strong>Nazanin Fouladgar, Marjan Alirezaie, Kary Främling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12996">https://arxiv.org/abs/2410.12996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12996">https://arxiv.org/pdf/2410.12996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12996]] SSET: Swapping-Sliding Explanation for Time Series Classifiers in Affect Detection(https://arxiv.org/abs/2410.12996)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Local explanation of machine learning (ML) models has recently received significant attention due to its ability to reduce ambiguities about why the models make specific decisions. Extensive efforts have been invested to address explainability for different data types, particularly images. However, the work on multivariate time series data is limited. A possible reason is that the conflation of time and other variables in time series data can cause the generated explanations to be incomprehensible to humans. In addition, some efforts on time series fall short of providing accurate explanations as they either ignore a context in the time domain or impose differentiability requirements on the ML models. Such restrictions impede their ability to provide valid explanations in real-world applications and non-differentiable ML settings. In this paper, we propose a swapping--sliding decision explanation for multivariate time series classifiers, called SSET. The proposal consists of swapping and sliding stages, by which salient sub-sequences causing significant drops in the prediction score are presented as explanations. In the former stage, the important variables are detected by swapping the series of interest with close train data from target classes. In the latter stage, the salient observations of these variables are explored by sliding a window over each time step. Additionally, the model measures the importance of different variables over time in a novel way characterized by multiple factors. We leverage SSET on affect detection domain where evaluations are performed on two real-world physiological time series datasets, WESAD and MAHNOB-HCI, and a deep convolutional classifier, CN-Waterfall. This classifier has shown superior performance to prior models to detect human affective states. Comparing SSET with several benchmarks, including LIME, integrated gradients, and Dynamask, we found..</li>
</ul>

<h3>Title: "Let's Argue Both Sides": Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Kaveh Eskandari Miandoab, Vasanth Sarathy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12997">https://arxiv.org/abs/2410.12997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12997">https://arxiv.org/pdf/2410.12997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12997]] "Let's Argue Both Sides": Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities(https://arxiv.org/abs/2410.12997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite achieving state-of-the-art results in a number of evaluation tasks, struggle to maintain their performance when logical reasoning is strictly required to correctly infer a prediction. In this work, we propose Argument Generation as a method of forcing models to utilize their reasoning capabilities when other approaches such as chain-of-thought reasoning prove insufficient. Our method involves the generation of arguments for each possible inference result, and asking the end model to rank the generated arguments. We show that Argument Generation can serve as an appropriate substitute for zero-shot prompting techniques without the requirement to add layers of complexity. Furthermore, we argue that knowledge-probing techniques such as chain-of-thought reasoning and Argument Generation are only useful when further reasoning is required to infer a prediction, making them auxiliary to more common zero-shot approaches. Finally, we demonstrate that our approach forces larger gains in smaller language models, showcasing a complex relationship between model size and prompting methods in foundation models.</li>
</ul>

<h3>Title: POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Batuhan K. Karaman, Ishmam Zabir, Alon Benhaim, Vishrav Chaudhary, Mert R. Sabuncu, Xia Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.12999">https://arxiv.org/abs/2410.12999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.12999">https://arxiv.org/pdf/2410.12999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.12999]] POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization(https://arxiv.org/abs/2410.12999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Balancing safety and usefulness in large language models has become a critical challenge in recent years. Models often exhibit unsafe behavior or adopt an overly cautious approach, leading to frequent overrefusal of benign prompts, which reduces their usefulness. Addressing these issues requires methods that maintain safety while avoiding overrefusal. In this work, we examine how the overgeneration of training data using advanced teacher models (e.g., GPT-4o), including responses to both general-purpose and toxic prompts, influences the safety and overrefusal balance of instruction-following language models. Additionally, we present POROver, a strategy to use preference optimization methods in order to reduce overrefusal, via employing a superior teacher model's completions. Our results show that overgenerating completions for general-purpose prompts significantly improves the balance between safety and usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 70.8% to 88.3%. Moreover, overgeneration for toxic prompts substantially reduces overrefusal, decreasing it from 94.4% to 45.2%. Furthermore, preference optimization algorithms, when applied with carefully curated preference data, can effectively reduce a model's overrefusal from 45.2% to 15.0% while maintaining comparable safety levels. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: LLM Chain Ensembles for Scalable and Accurate Data Annotation</h3>
<ul>
<li><strong>Authors: </strong>David Farr, Nico Manzonelli, Iain Cruickshank, Kate Starbird, Jevin West</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13006">https://arxiv.org/abs/2410.13006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13006">https://arxiv.org/pdf/2410.13006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13006]] LLM Chain Ensembles for Scalable and Accurate Data Annotation(https://arxiv.org/abs/2410.13006)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The ability of large language models (LLMs) to perform zero-shot classification makes them viable solutions for data annotation in rapidly evolving domains where quality labeled data is often scarce and costly to obtain. However, the large-scale deployment of LLMs can be prohibitively expensive. This paper introduces an LLM chain ensemble methodology that aligns multiple LLMs in a sequence, routing data subsets to subsequent models based on classification uncertainty. This approach leverages the strengths of individual LLMs within a broader system, allowing each model to handle data points where it exhibits the highest confidence, while forwarding more complex cases to potentially more robust models. Our results show that the chain ensemble method often exceeds the performance of the best individual model in the chain and achieves substantial cost savings, making LLM chain ensembles a practical and efficient solution for large-scale data annotation challenges.</li>
</ul>

<h3>Title: Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from Images</h3>
<ul>
<li><strong>Authors: </strong>Arka Daw, Megan Hong-Thanh Chung, Maria Mahbub, Amir Sadovnik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13010">https://arxiv.org/abs/2410.13010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13010">https://arxiv.org/pdf/2410.13010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13010]] Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from Images(https://arxiv.org/abs/2410.13010)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Machine learning models are known to be vulnerable to adversarial attacks, but traditional attacks have mostly focused on single-modalities. With the rise of large multi-modal models (LMMs) like CLIP, which combine vision and language capabilities, new vulnerabilities have emerged. However, prior work in multimodal targeted attacks aim to completely change the model's output to what the adversary wants. In many realistic scenarios, an adversary might seek to make only subtle modifications to the output, so that the changes go unnoticed by downstream models or even by humans. We introduce Hiding-in-Plain-Sight (HiPS) attacks, a novel class of adversarial attacks that subtly modifies model predictions by selectively concealing target object(s), as if the target object was absent from the scene. We propose two HiPS attack variants, HiPS-cls and HiPS-cap, and demonstrate their effectiveness in transferring to downstream image captioning models, such as CLIP-Cap, for targeted object removal from image captions.</li>
</ul>

<h3>Title: Sample Compression Scheme Reductions</h3>
<ul>
<li><strong>Authors: </strong>Idan Attias, Steve Hanneke, Arvind Ramaswami</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13012">https://arxiv.org/abs/2410.13012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13012">https://arxiv.org/pdf/2410.13012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13012]] Sample Compression Scheme Reductions(https://arxiv.org/abs/2410.13012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present novel reductions from sample compression schemes in multiclass classification, regression, and adversarially robust learning settings to binary sample compression schemes. Assuming we have a compression scheme for binary classes of size $f(d_\mathrm{VC})$, where $d_\mathrm{VC}$ is the VC dimension, then we have the following results: (1) If the binary compression scheme is a majority-vote or a stable compression scheme, then there exists a multiclass compression scheme of size $O(f(d_\mathrm{G}))$, where $d_\mathrm{G}$ is the graph dimension. Moreover, for general binary compression schemes, we obtain a compression of size $O(f(d_\mathrm{G})\log|Y|)$, where $Y$ is the label space. (2) If the binary compression scheme is a majority-vote or a stable compression scheme, then there exists an $\epsilon$-approximate compression scheme for regression over $[0,1]$-valued functions of size $O(f(d_\mathrm{P}))$, where $d_\mathrm{P}$ is the pseudo-dimension. For general binary compression schemes, we obtain a compression of size $O(f(d_\mathrm{P})\log(1/\epsilon))$. These results would have significant implications if the sample compression conjecture, which posits that any binary concept class with a finite VC dimension admits a binary compression scheme of size $O(d_\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995; Warmuth, 2003). Our results would then extend the proof of the conjecture immediately to other settings. We establish similar results for adversarially robust learning and also provide an example of a concept class that is robustly learnable but has no bounded-size compression scheme, demonstrating that learnability is not equivalent to having a compression scheme independent of the sample size, unlike in binary classification, where compression of size $2^{O(d_\mathrm{VC})}$ is attainable (Moran and Yehudayoff, 2016).</li>
</ul>

<h3>Title: LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Faizan Faisal, Umair Yousaf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13013">https://arxiv.org/abs/2410.13013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13013">https://arxiv.org/pdf/2410.13013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13013]] LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question Answering(https://arxiv.org/abs/2410.13013)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We present LEGAL-UQA, the first Urdu legal question-answering dataset derived from Pakistan's constitution. This parallel English-Urdu dataset includes 619 question-answer pairs, each with corresponding legal article contexts, addressing the need for domain-specific NLP resources in low-resource languages. We describe the dataset creation process, including OCR extraction, manual refinement, and GPT-4-assisted translation and generation of QA pairs. Our experiments evaluate the latest generalist language and embedding models on LEGAL-UQA, with Claude-3.5-Sonnet achieving 99.19% human-evaluated accuracy. We fine-tune mt5-large-UQA-1.0, highlighting the challenges of adapting multilingual models to specialized domains. Additionally, we assess retrieval performance, finding OpenAI's text-embedding-3-large outperforms Mistral's mistral-embed. LEGAL-UQA bridges the gap between global NLP advancements and localized applications, particularly in constitutional law, and lays the foundation for improved legal information access in Pakistan.</li>
</ul>

<h3>Title: LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks</h3>
<ul>
<li><strong>Authors: </strong>Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, Samy Jelassi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13025">https://arxiv.org/abs/2410.13025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13025">https://arxiv.org/pdf/2410.13025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13025]] LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks(https://arxiv.org/abs/2410.13025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient fine-tuning of Large Language Models (LLMs). We study how different LoRA modules can be merged to achieve skill composition -- testing the performance of the merged model on a target task that involves combining multiple skills, each skill coming from a single LoRA. This setup is favorable when it is difficult to obtain training data for the target task and when it can be decomposed into multiple skills. First, we identify practically occurring use-cases that can be studied under the realm of skill composition, e.g. solving hard math-word problems with code, creating a bot to answer questions on proprietary manuals or about domain-specialized corpora. Our main contribution is to show that concatenation of LoRAs (CAT), which optimally averages LoRAs that were individually trained on different skills, outperforms existing model- and data- merging techniques; for instance on math-word problems, CAT beats these methods by an average of 43% and 12% respectively. Thus, this paper advocates model merging as an efficient way to solve compositional tasks and underscores CAT as a simple, compute-friendly and effective procedure. To our knowledge, this is the first work demonstrating the superiority of model merging over data mixing for binary skill composition tasks.</li>
</ul>

<h3>Title: Geometric Trajectory Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13027">https://arxiv.org/abs/2410.13027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13027">https://arxiv.org/pdf/2410.13027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13027]] Geometric Trajectory Diffusion Models(https://arxiv.org/abs/2410.13027)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.</li>
</ul>

<h3>Title: When Not to Answer: Evaluating Prompts on GPT Models for Effective Abstention in Unanswerable Math Word Problems</h3>
<ul>
<li><strong>Authors: </strong>Asir Saadat, Tasmia Binte Sogir, Md Taukir Azam Chowdhury, Syem Aziz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13029">https://arxiv.org/abs/2410.13029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13029">https://arxiv.org/pdf/2410.13029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13029]] When Not to Answer: Evaluating Prompts on GPT Models for Effective Abstention in Unanswerable Math Word Problems(https://arxiv.org/abs/2410.13029)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly relied upon to solve complex mathematical word problems. However, being susceptible to hallucination, they may generate inaccurate results when presented with unanswerable questions, raising concerns about their potential harm. While GPT models are now widely used and trusted, the exploration of how they can effectively abstain from answering unanswerable math problems and the enhancement of their abstention capabilities has not been rigorously investigated. In this paper, we investigate whether GPTs can appropriately respond to unanswerable math word problems by applying prompts typically used in solvable mathematical scenarios. Our experiments utilize the Unanswerable Word Math Problem (UWMP) dataset, directly leveraging GPT model APIs. Evaluation metrics are introduced, which integrate three key factors: abstention, correctness and confidence. Our findings reveal critical gaps in GPT models and the hallucination it suffers from for unsolvable problems, highlighting the need for improved models capable of better managing uncertainty and complex reasoning in math word problem-solving contexts.</li>
</ul>

<h3>Title: Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts</h3>
<ul>
<li><strong>Authors: </strong>Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13030">https://arxiv.org/abs/2410.13030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13030">https://arxiv.org/pdf/2410.13030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13030]] Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts(https://arxiv.org/abs/2410.13030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the significant influx of prompt-tuning techniques for generative vision-language models (VLMs), it remains unclear how sensitive these models are to lexical and semantic alterations in prompts. In this paper, we evaluate the ability of generative VLMs to understand lexical and semantic changes in text using the SugarCrepe++ dataset. We analyze the sensitivity of VLMs to lexical alterations in prompts without corresponding semantic changes. Our findings demonstrate that generative VLMs are highly sensitive to such alterations. Additionally, we show that this vulnerability affects the performance of techniques aimed at achieving consistency in their outputs.</li>
</ul>

<h3>Title: A Location Validation Technique to Mitigate GPS Spoofing Attacks in IEEE 802.11p based Fleet Operator's Network of Electric Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Ankita Samaddar, Arvind Easwaran</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13031">https://arxiv.org/abs/2410.13031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13031">https://arxiv.org/pdf/2410.13031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13031]] A Location Validation Technique to Mitigate GPS Spoofing Attacks in IEEE 802.11p based Fleet Operator's Network of Electric Vehicles(https://arxiv.org/abs/2410.13031)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Most vehicular applications in electric vehicles use IEEE 802.11p protocol for vehicular communications. Vehicle rebalancing application is one such application that has been used by many car rental service providers to overcome the disparity between vehicle demand and vehicle supply at different charging stations. Vehicle rebalancing application uses the GPS location data of the vehicles periodically to determine the vehicle(s) to be moved to a different charging station for rebalancing. However, a malicious attacker residing in the network can spoof the GPS location data packets of the target vehicle(s) resulting in misinterpretation of the location of the vehicle(s). This can result in wrong rebalancing decision leading to unmet demands of the customers and under utilization of the system. To detect and prevent this attack, we propose a location tracking technique that can validate the current location of a vehicle based on its previous location and roadmaps. We used OpenStreetMap and SUMO simulator to generate the roadmap data from the roadmaps of Singapore. Extensive experiments on the generated datasets show the efficacy of our proposed technique.</li>
</ul>

<h3>Title: LFOSum: Summarizing Long-form Opinions with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mir Tafseer Nayeem, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13037">https://arxiv.org/abs/2410.13037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13037">https://arxiv.org/pdf/2410.13037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13037]] LFOSum: Summarizing Long-form Opinions with Large Language Models(https://arxiv.org/abs/2410.13037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online reviews play a pivotal role in influencing consumer decisions across various domains, from purchasing products to selecting hotels or restaurants. However, the sheer volume of reviews -- often containing repetitive or irrelevant content -- leads to information overload, making it challenging for users to extract meaningful insights. Traditional opinion summarization models face challenges in handling long inputs and large volumes of reviews, while newer Large Language Model (LLM) approaches often fail to generate accurate and faithful summaries. To address those challenges, this paper introduces (1) a new dataset of long-form user reviews, each entity comprising over a thousand reviews, (2) two training-free LLM-based summarization approaches that scale to long inputs, and (3) automatic evaluation metrics. Our dataset of user reviews is paired with in-depth and unbiased critical summaries by domain experts, serving as a reference for evaluation. Additionally, our novel reference-free evaluation metrics provide a more granular, context-sensitive assessment of summary faithfulness. We benchmark several open-source and closed-source LLMs using our methods. Our evaluation reveals that LLMs still face challenges in balancing sentiment and format adherence in long-form summaries, though open-source models can narrow the gap when relevant information is retrieved in a focused manner.</li>
</ul>

<h3>Title: FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning</h3>
<ul>
<li><strong>Authors: </strong>Evelyn Ma, Chao Pan, Rasoul Etesami, Han Zhao, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13045">https://arxiv.org/abs/2410.13045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13045">https://arxiv.org/pdf/2410.13045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13045]] FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning(https://arxiv.org/abs/2410.13045)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>The performance of Transfer Learning (TL) heavily relies on effective pretraining, which demands large datasets and substantial computational resources. As a result, executing TL is often challenging for individual model developers. Federated Learning (FL) addresses these issues by facilitating collaborations among clients, expanding the dataset indirectly, distributing computational costs, and preserving privacy. However, key challenges remain unresolved. First, existing FL methods tend to optimize transferability only within local domains, neglecting the global learning domain. Second, most approaches rely on indirect transferability metrics, which do not accurately reflect the final target loss or true degree of transferability. To address these gaps, we propose two enhancements to FL. First, we introduce a client-server exchange protocol that leverages cross-client Jacobian (gradient) norms to boost transferability. Second, we increase the average Jacobian norm across clients at the server, using this as a local regularizer to reduce cross-client Jacobian variance. Our transferable federated algorithm, termed FedGTST (Federated Global Transferability via Statistics Tuning), demonstrates that increasing the average Jacobian and reducing its variance allows for tighter control of the target loss. This leads to an upper bound on the target loss in terms of the source loss and source-target domain discrepancy. Extensive experiments on datasets such as MNIST to MNIST-M and CIFAR10 to SVHN show that FedGTST outperforms relevant baselines, including FedSR. On the second dataset pair, FedGTST improves accuracy by 9.8% over FedSR and 7.6% over FedIIR when LeNet is used as the backbone.</li>
</ul>

<h3>Title: Supply Chain Network Extraction and Entity Classification Leveraging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tong Liu, Hadi Meidani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13051">https://arxiv.org/abs/2410.13051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13051">https://arxiv.org/pdf/2410.13051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13051]] Supply Chain Network Extraction and Entity Classification Leveraging Large Language Models(https://arxiv.org/abs/2410.13051)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Supply chain networks are critical to the operational efficiency of industries, yet their increasing complexity presents significant challenges in mapping relationships and identifying the roles of various entities. Traditional methods for constructing supply chain networks rely heavily on structured datasets and manual data collection, limiting their scope and efficiency. In contrast, recent advancements in Natural Language Processing (NLP) and large language models (LLMs) offer new opportunities for discovering and analyzing supply chain networks using unstructured text data. This paper proposes a novel approach that leverages LLMs to extract and process raw textual information from publicly available sources to construct a comprehensive supply chain graph. We focus on the civil engineering sector as a case study, demonstrating how LLMs can uncover hidden relationships among companies, projects, and other entities. Additionally, we fine-tune an LLM to classify entities within the supply chain graph, providing detailed insights into their roles and relationships. The results show that domain-specific fine-tuning improves classification accuracy, highlighting the potential of LLMs for industry-specific supply chain analysis. Our contributions include the development of a supply chain graph for the civil engineering sector, as well as a fine-tuned LLM model that enhances entity classification and understanding of supply chain networks.</li>
</ul>

<h3>Title: Channel-Wise Mixed-Precision Quantization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Chen, Bike Xie, Jundong Li, Cong Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13056">https://arxiv.org/abs/2410.13056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13056">https://arxiv.org/pdf/2410.13056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13056]] Channel-Wise Mixed-Precision Quantization for Large Language Models(https://arxiv.org/abs/2410.13056)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable success across a wide range of language tasks, but their deployment on edge devices remains challenging due to the substantial memory requirements imposed by their large parameter sizes. Weight-only quantization presents a promising solution to reduce the memory footprint of LLMs. However, existing approaches primarily focus on integer-bit quantization, limiting their adaptability to fractional-bit quantization tasks and preventing the full utilization of available storage space on devices. In this paper, we introduce Channel-Wise Mixed-Precision Quantization (CMPQ), a novel mixed-precision quantization method that allocates quantization precision in a channel-wise pattern based on activation distributions. By assigning different precision levels to different weight channels, CMPQ can adapt to any bit-width constraint. CMPQ employs a non-uniform quantization strategy and incorporates two outlier extraction techniques that collaboratively preserve the critical information, thereby minimizing the quantization loss. Experiments on different sizes of LLMs demonstrate that CMPQ not only enhances performance in integer-bit quantization tasks but also achieves significant performance gains with a modest increase in memory usage. CMPQ thus represents an adaptive and effective approach to LLM quantization, offering substantial benefits across diverse device capabilities.</li>
</ul>

<h3>Title: ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological Garden Path Errors</h3>
<ul>
<li><strong>Authors: </strong>Qinchan Li, Sophie Hao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13057">https://arxiv.org/abs/2410.13057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13057">https://arxiv.org/pdf/2410.13057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13057]] ERAS: Evaluating the Robustness of Chinese NLP Models to Morphological Garden Path Errors(https://arxiv.org/abs/2410.13057)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In languages without orthographic word boundaries, NLP models perform word segmentation, either as an explicit preprocessing step or as an implicit step in an end-to-end computation. This paper shows that Chinese NLP models are vulnerable to morphological garden path errors: errors caused by a failure to resolve local word segmentation ambiguities using sentence-level morphosyntactic context. We propose a benchmark, ERAS, that tests a model's vulnerability to morphological garden path errors by comparing its behavior on sentences with and without local segmentation ambiguities. Using ERAS, we show that word segmentation models make garden path errors on locally ambiguous sentences, but do not make equivalent errors on unambiguous sentences. We further show that sentiment analysis models with character-level tokenization make implicit garden path errors, even without an explicit word segmentation step in the pipeline. Our results indicate that models' segmentation of Chinese text often fails to account for morphosyntactic context.</li>
</ul>

<h3>Title: AERO: Softmax-Only LLMs for Efficient Private Inference</h3>
<ul>
<li><strong>Authors: </strong>Nandan Kumar Jha, Brandon Reagen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13060">https://arxiv.org/abs/2410.13060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13060">https://arxiv.org/pdf/2410.13060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13060]] AERO: Softmax-Only LLMs for Efficient Private Inference(https://arxiv.org/abs/2410.13060)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. In this paper, we present a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. We introduce AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, we propose a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, we devise a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23$\times$ communication and 1.94$\times$ latency reduction. We validate the effectiveness of AERO by benchmarking it against the state-of-the-art.</li>
</ul>

<h3>Title: PromptExp: Multi-granularity Prompt Explanation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ximing Dong, Shaowei Wang, Dayi Lin, Gopi Krishnan Rajbahadur, Boquan Zhou, Shichao Liu, Ahmed E. Hassan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13073">https://arxiv.org/abs/2410.13073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13073">https://arxiv.org/pdf/2410.13073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13073]] PromptExp: Multi-granularity Prompt Explanation of Large Language Models(https://arxiv.org/abs/2410.13073)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models excel in tasks like natural language understanding and text generation. Prompt engineering plays a critical role in leveraging LLM effectively. However, LLMs black-box nature hinders its interpretability and effective prompting engineering. A wide range of model explanation approaches have been developed for deep learning models, However, these local explanations are designed for single-output tasks like classification and regression,and cannot be directly applied to LLMs, which generate sequences of tokens. Recent efforts in LLM explanation focus on natural language explanations, but they are prone to hallucinations and inaccuracies. To address this, we introduce OurTool, a framework for multi-granularity prompt explanations by aggregating token-level insights. OurTool introduces two token-level explanation approaches: this http URL aggregation-based approach combining local explanation techniques, and 2. a perturbation-based approach with novel techniques to evaluate token masking impact. OurTool supports both white-box and black-box explanations and extends explanations to higher granularity levels, enabling flexible analysis. We evaluate OurTool in case studies such as sentiment analysis, showing the perturbation-based approach performs best using semantic similarity to assess perturbation impact. Furthermore, we conducted a user study to confirm OurTool's accuracy and practical value, and demonstrate its potential to enhance LLM interpretability.</li>
</ul>

<h3>Title: Tuning Language Models by Mixture-of-Depths Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Luo, Lucia Specia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13077">https://arxiv.org/abs/2410.13077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13077">https://arxiv.org/pdf/2410.13077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13077]] Tuning Language Models by Mixture-of-Depths Ensemble(https://arxiv.org/abs/2410.13077)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models (LLMs) traditionally rely on final-layer loss for training and final-layer representations for predictions, potentially overlooking the predictive power embedded in intermediate layers. Surprisingly, we find that focusing training efforts on these intermediate layers can yield training losses comparable to those of final layers, with complementary test-time performance. We introduce a novel tuning framework, Mixture-of-Depths (MoD), which trains late layers as ensembles contributing to the final logits through learned routing weights. With the auxiliary distillation loss and additional normalization modules, we ensure that the outputs of the late layers adapt to language modeling. Our MoD framework, which can be integrated with any existing tuning method, shows consistent improvement on various language modelling tasks. Furthermore, by replacing traditional trainable modules with MoD, our approach achieves similar performance with significantly fewer trainable parameters, demonstrating the potential of leveraging predictive power from intermediate representations during training.</li>
</ul>

<h3>Title: Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13080">https://arxiv.org/abs/2410.13080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13080">https://arxiv.org/pdf/2410.13080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13080]] Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models(https://arxiv.org/abs/2410.13080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.</li>
</ul>

<h3>Title: FedCAP: Robust Federated Learning via Customized Aggregation and Personalization</h3>
<ul>
<li><strong>Authors: </strong>Youpeng Li, Xinda Wang, Fuxun Yu, Lichao Sun, Wenbin Zhang, Xuyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13083">https://arxiv.org/abs/2410.13083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13083">https://arxiv.org/pdf/2410.13083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13083]] FedCAP: Robust Federated Learning via Customized Aggregation and Personalization(https://arxiv.org/abs/2410.13083)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL), an emerging distributed machine learning paradigm, has been applied to various privacy-preserving scenarios. However, due to its distributed nature, FL faces two key issues: the non-independent and identical distribution (non-IID) of user data and vulnerability to Byzantine threats. To address these challenges, in this paper, we propose FedCAP, a robust FL framework against both data heterogeneity and Byzantine attacks. The core of FedCAP is a model update calibration mechanism to help a server capture the differences in the direction and magnitude of model updates among clients. Furthermore, we design a customized model aggregation rule that facilitates collaborative training among similar clients while accelerating the model deterioration of malicious clients. With a Euclidean norm-based anomaly detection mechanism, the server can quickly identify and permanently remove malicious clients. Moreover, the impact of data heterogeneity and Byzantine attacks can be further mitigated through personalization on the client side. We conduct extensive experiments, comparing multiple state-of-the-art baselines, to demonstrate that FedCAP performs well in several non-IID settings and shows strong robustness under a series of poisoning attacks.</li>
</ul>

<h3>Title: Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jie Ren, Kangrui Chen, Chen Chen, Vikash Sehwag, Yue Xing, Jiliang Tang, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13088">https://arxiv.org/abs/2410.13088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13088">https://arxiv.org/pdf/2410.13088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13088]] Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Models(https://arxiv.org/abs/2410.13088)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Vision-Language Models (VLMs) have made significant advancements in a wide range of natural language processing and vision-language tasks. Access to large web-scale datasets has been a key factor in their success. However, concerns have been raised about the unauthorized use of copyrighted materials and potential copyright infringement. Existing methods, such as sample-level Membership Inference Attacks (MIA) and distribution-based dataset inference, distinguish member data (data used for training) and non-member data by leveraging the common observation that models tend to memorize and show greater confidence in member data. Nevertheless, these methods face challenges when applied to LLMs and VLMs, such as the requirement for ground-truth member data or non-member data that shares the same distribution as the test data. In this paper, we propose a novel dataset-level membership inference method based on Self-Comparison. We find that a member prefix followed by a non-member suffix (paraphrased from a member suffix) can further trigger the model's memorization on training data. Instead of directly comparing member and non-member data, we introduce paraphrasing to the second half of the sequence and evaluate how the likelihood changes before and after paraphrasing. Unlike prior approaches, our method does not require access to ground-truth member data or non-member data in identical distribution, making it more practical. Extensive experiments demonstrate that our proposed method outperforms traditional MIA and dataset inference techniques across various datasets and models, including including public models, fine-tuned models, and API-based commercial models.</li>
</ul>

<h3>Title: Task Consistent Prototype Learning for Incremental Few-shot Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Xu, Yanan Wu, Haoran Jiang, Yang Wang, Qiang Wu, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13094">https://arxiv.org/abs/2410.13094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13094">https://arxiv.org/pdf/2410.13094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13094]] Task Consistent Prototype Learning for Incremental Few-shot Semantic Segmentation(https://arxiv.org/abs/2410.13094)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Incremental Few-Shot Semantic Segmentation (iFSS) tackles a task that requires a model to continually expand its segmentation capability on novel classes using only a few annotated examples. Typical incremental approaches encounter a challenge that the objective of the base training phase (fitting base classes with sufficient instances) does not align with the incremental learning phase (rapidly adapting to new classes with less forgetting). This disconnect can result in suboptimal performance in the incremental setting. This study introduces a meta-learning-based prototype approach that encourages the model to learn how to adapt quickly while preserving previous knowledge. Concretely, we mimic the incremental evaluation protocol during the base training session by sampling a sequence of pseudo-incremental tasks. Each task in the simulated sequence is trained using a meta-objective to enable rapid adaptation without forgetting. To enhance discrimination among class prototypes, we introduce prototype space redistribution learning, which dynamically updates class prototypes to establish optimal inter-prototype boundaries within the prototype space. Extensive experiments on iFSS datasets built upon PASCAL and COCO benchmarks show the advanced performance of the proposed approach, offering valuable insights for addressing iFSS challenges.</li>
</ul>

<h3>Title: Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Ghiasvand, Yifan Yang, Zhiyu Xue, Mahnoosh Alizadeh, Zheng Zhang, Ramtin Pedarsani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13097">https://arxiv.org/abs/2410.13097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13097">https://arxiv.org/pdf/2410.13097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13097]] Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models(https://arxiv.org/abs/2410.13097)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods typically assume that Large Language Models (LLMs) are trained on data from a single device or client. However, real-world scenarios often require fine-tuning these models on private data distributed across multiple devices. Federated Learning (FL) offers an appealing solution by preserving user privacy, as sensitive data remains on local devices during training. Nonetheless, integrating PEFT methods into FL introduces two main challenges: communication overhead and data heterogeneity. In this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by integrating tensorized adapters into client-side models' encoder/decoder blocks. FedTT is versatile and can be applied to both cross-silo FL and large-scale cross-device FL. FedTT+, an extension of FedTT tailored for cross-silo FL, enhances robustness against data heterogeneity by adaptively freezing portions of tensor factors, further reducing the number of trainable parameters. Experiments on BERT and LLaMA models demonstrate that our proposed methods successfully address data heterogeneity challenges and perform on par or even better than existing federated PEFT approaches while achieving up to 10$\times$ reduction in communication cost.</li>
</ul>

<h3>Title: Cliqueformer: Model-Based Optimization with Structured Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jakub Grudzien Kuba, Pieter Abbeel, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13106">https://arxiv.org/abs/2410.13106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13106">https://arxiv.org/pdf/2410.13106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13106]] Cliqueformer: Model-Based Optimization with Structured Transformers(https://arxiv.org/abs/2410.13106)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Expressive large-scale neural networks enable training powerful models for prediction tasks. However, in many engineering and science domains, such models are intended to be used not just for prediction, but for design -- e.g., creating new proteins that serve as effective therapeutics, or creating new materials or chemicals that maximize a downstream performance measure. Thus, researchers have recently grown an interest in building deep learning methods that solve offline \emph{model-based optimization} (MBO) problems, in which design candidates are optimized with respect to surrogate models learned from offline data. However, straightforward application of predictive models that are effective at predicting in-distribution properties of a design are not necessarily the best suited for use in creating new designs. Thus, the most successful algorithms that tackle MBO draw inspiration from reinforcement learning and generative modeling to meet the in-distribution constraints. Meanwhile, recent theoretical works have observed that exploiting the structure of the target black-box function is an effective strategy for solving MBO from offline data. Unfortunately, discovering such structure remains an open problem. In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce \emph{Cliqueformer} -- a scalable transformer-based architecture that learns the black-box function's structure in the form of its \emph{functional graphical model} (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.</li>
</ul>

<h3>Title: Trust but Verify: Programmatic VLM Evaluation in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Viraj Prabhu, Senthil Purushwalkam, An Yan, Caiming Xiong, Ran Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13121">https://arxiv.org/abs/2410.13121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13121">https://arxiv.org/pdf/2410.13121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13121]] Trust but Verify: Programmatic VLM Evaluation in the Wild(https://arxiv.org/abs/2410.13121)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) often generate plausible but incorrect responses to visual queries. However, reliably quantifying the effect of such hallucinations in free-form responses to open-ended queries is challenging as it requires visually verifying each claim within the response. We propose Programmatic VLM Evaluation (PROVE), a new benchmarking paradigm for evaluating VLM responses to open-ended queries. To construct PROVE, we provide a large language model (LLM) with a high-fidelity scene-graph representation constructed from a hyper-detailed image caption, and prompt it to generate diverse question-answer (QA) pairs, as well as programs that can be executed over the scene graph object to verify each QA pair. We thus construct a benchmark of 10.5k challenging but visually grounded QA pairs. Next, to evaluate free-form model responses to queries in PROVE, we propose a programmatic evaluation strategy that measures both the helpfulness and truthfulness of a response within a unified scene graph-based framework. We benchmark the helpfulness-truthfulness trade-offs of a range of VLMs on PROVE, finding that very few are in-fact able to achieve a good balance between the two. Project page: \url{this https URL}.</li>
</ul>

<h3>Title: Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples Generation with Momentum</h3>
<ul>
<li><strong>Authors: </strong>Nashrah Haque, Xiang Li, Zhehui Chen, Yanzhao Wu, Lei Yu, Arun Iyengar, Wenqi Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13122">https://arxiv.org/abs/2410.13122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13122">https://arxiv.org/pdf/2410.13122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13122]] Boosting Imperceptibility of Stable Diffusion-based Adversarial Examples Generation with Momentum(https://arxiv.org/abs/2410.13122)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We propose a novel framework, Stable Diffusion-based Momentum Integrated Adversarial Examples (SD-MIAE), for generating adversarial examples that can effectively mislead neural network classifiers while maintaining visual imperceptibility and preserving the semantic similarity to the original class label. Our method leverages the text-to-image generation capabilities of the Stable Diffusion model by manipulating token embeddings corresponding to the specified class in its latent space. These token embeddings guide the generation of adversarial images that maintain high visual fidelity. The SD-MIAE framework consists of two phases: (1) an initial adversarial optimization phase that modifies token embeddings to produce misclassified yet natural-looking images and (2) a momentum-based optimization phase that refines the adversarial perturbations. By introducing momentum, our approach stabilizes the optimization of perturbations across iterations, enhancing both the misclassification rate and visual fidelity of the generated adversarial examples. Experimental results demonstrate that SD-MIAE achieves a high misclassification rate of 79%, improving by 35% over the state-of-the-art method while preserving the imperceptibility of adversarial perturbations and the semantic similarity to the original class label, making it a practical method for robust adversarial evaluation.</li>
</ul>

<h3>Title: Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jiwan Hur, Dong-Jae Lee, Gyojin Han, Jaehyun Choi, Yunho Jeon, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13136">https://arxiv.org/abs/2410.13136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13136">https://arxiv.org/pdf/2410.13136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13136]] Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance(https://arxiv.org/abs/2410.13136)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Masked generative models (MGMs) have shown impressive generative ability while providing an order of magnitude efficient sampling steps compared to continuous diffusion models. However, MGMs still underperform in image synthesis compared to recent well-developed continuous diffusion models with similar size in terms of quality and diversity of generated samples. A key factor in the performance of continuous diffusion models stems from the guidance methods, which enhance the sample quality at the expense of diversity. In this paper, we extend these guidance methods to generalized guidance formulation for MGMs and propose a self-guidance sampling method, which leads to better generation quality. The proposed approach leverages an auxiliary task for semantic smoothing in vector-quantized token space, analogous to the Gaussian blur in continuous pixel space. Equipped with the parameter-efficient fine-tuning method and high-temperature sampling, MGMs with the proposed self-guidance achieve a superior quality-diversity trade-off, outperforming existing sampling methods in MGMs with more efficient training and sampling costs. Extensive experiments with the various sampling hyperparameters confirm the effectiveness of the proposed self-guidance.</li>
</ul>

<h3>Title: Data Defenses Against Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>William Agnew, Harry H. Jiang, Cella Sum, Maarten Sap, Sauvik Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13138">https://arxiv.org/abs/2410.13138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13138">https://arxiv.org/pdf/2410.13138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13138]] Data Defenses Against Large Language Models(https://arxiv.org/abs/2410.13138)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel at performing inference over text to extract information, summarize information, or generate additional text. These inference capabilities are implicated in a variety of ethical harms spanning surveillance, labor displacement, and IP/copyright theft. While many policy, legal, and technical mitigations have been proposed to counteract these harms, these mitigations typically require cooperation from institutions that move slower than technical advances (i.e., governments) or that have few incentives to act to counteract these harms (i.e., the corporations that create and profit from these LLMs). In this paper, we define and build "data defenses" -- a novel strategy that directly empowers data owners to block LLMs from performing inference on their data. We create data defenses by developing a method to automatically generate adversarial prompt injections that, when added to input text, significantly reduce the ability of LLMs to accurately infer personally identifying information about the subject of the input text or to use copyrighted text in inference. We examine the ethics of enabling such direct resistance to LLM inference, and argue that making data defenses that resist and subvert LLMs enables the realization of important values such as data ownership, data sovereignty, and democratic control over AI systems. We verify that our data defenses are cheap and fast to generate, work on the latest commercial and open-source LLMs, resistance to countermeasures, and are robust to several different attack settings. Finally, we consider the security implications of LLM data defenses and outline several future research directions in this area. Our code is available at this https URL and a tool for using our defenses to protect text against LLM inference is at this https URL.</li>
</ul>

<h3>Title: Federated scientific machine learning for approximating functions and solving differential equations with data heterogeneity</h3>
<ul>
<li><strong>Authors: </strong>Handi Zhang, Langchen Liu, Lu Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13141">https://arxiv.org/abs/2410.13141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13141">https://arxiv.org/pdf/2410.13141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13141]] Federated scientific machine learning for approximating functions and solving differential equations with data heterogeneity(https://arxiv.org/abs/2410.13141)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>By leveraging neural networks, the emerging field of scientific machine learning (SciML) offers novel approaches to address complex problems governed by partial differential equations (PDEs). In practical applications, challenges arise due to the distributed essence of data, concerns about data privacy, or the impracticality of transferring large volumes of data. Federated learning (FL), a decentralized framework that enables the collaborative training of a global model while preserving data privacy, offers a solution to the challenges posed by isolated data pools and sensitive data issues. Here, this paper explores the integration of FL and SciML to approximate complex functions and solve differential equations. We propose two novel models: federated physics-informed neural networks (FedPINN) and federated deep operator networks (FedDeepONet). We further introduce various data generation methods to control the degree of non-independent and identically distributed (non-iid) data and utilize the 1-Wasserstein distance to quantify data heterogeneity in function approximation and PDE learning. We systematically investigate the relationship between data heterogeneity and federated model performance. Additionally, we propose a measure of weight divergence and develop a theoretical framework to establish growth bounds for weight divergence in federated learning compared to traditional centralized learning. To demonstrate the effectiveness of our methods, we conducted 10 experiments, including 2 on function approximation, 5 PDE problems on FedPINN, and 3 PDE problems on FedDeepONet. These experiments demonstrate that proposed federated methods surpass the models trained only using local data and achieve competitive accuracy of centralized models trained using all data.</li>
</ul>

<h3>Title: Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead</h3>
<ul>
<li><strong>Authors: </strong>Kuleen Sasse, Shan Chen, Jackson Pond, Danielle Bitterman, John Osborne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13146">https://arxiv.org/abs/2410.13146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13146">https://arxiv.org/pdf/2410.13146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13146]] Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead(https://arxiv.org/abs/2410.13146)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>As Vision Language Models (VLMs) gain widespread use, their fairness remains under-explored. In this paper, we analyze demographic biases across five models and six datasets. We find that portrait datasets like UTKFace and CelebA are the best tools for bias detection, finding gaps in performance and fairness between LLaVa and CLIP models. However, scene based datasets like PATA, VLStereoSet fail to be useful benchmarks for bias due to their construction. As for pronoun based datasets like VisoGender, we receive mixed signals as only some subsets of the data are useful in providing insights. To alleviate this problem, we introduce a more difficult version of VisoGender to serve as a more rigorous evaluation. Based on these results, we call for more effective and carefully designed datasets to ensure VLMs are both fair and reliable.</li>
</ul>

<h3>Title: Utilizing Large Language Models in An Iterative Paradigm with Domain Feedback for Molecule Optimization</h3>
<ul>
<li><strong>Authors: </strong>Khiem Le, Nitesh V. Chawla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13147">https://arxiv.org/abs/2410.13147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13147">https://arxiv.org/pdf/2410.13147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13147]] Utilizing Large Language Models in An Iterative Paradigm with Domain Feedback for Molecule Optimization(https://arxiv.org/abs/2410.13147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Molecule optimization is a critical task in drug discovery to optimize desired properties of a given molecule through chemical modification. Despite Large Language Models (LLMs) holding the potential to efficiently simulate this task by using natural language to direct the optimization, straightforwardly utilizing shows limited performance. In this work, we facilitate utilizing LLMs in an iterative paradigm by proposing a simple yet highly effective domain feedback provider, namely $\text{Re}^2$DF. In detail, $\text{Re}^2$DF harnesses an external toolkit, RDKit, to handle the molecule hallucination, if the modified molecule is chemically invalid. Otherwise, its desired properties are computed and compared to the original one, establishing reliable domain feedback with correct direction and distance towards the objective, followed by a retrieved example, to explicitly guide the LLM to refine the modified molecule. We conduct experiments across both single- and multi-property objectives with 2 thresholds, where $\text{Re}^2$DF shows significant improvements. Particularly, for 20 single-property objectives, $\text{Re}^2$DF enhances the Hit ratio by 16.95\% and 20.76\% under loose and strict thresholds, respectively. For 32 multi-property objectives, $\text{Re}^2$DF enhances the Hit ratio by 6.04\% and 5.25\%.</li>
</ul>

<h3>Title: Better to Ask in English: Evaluation of Large Language Models on English, Low-resource and Cross-Lingual Settings</h3>
<ul>
<li><strong>Authors: </strong>Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Imran Razzak, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13153">https://arxiv.org/abs/2410.13153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13153">https://arxiv.org/pdf/2410.13153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13153]] Better to Ask in English: Evaluation of Large Language Models on English, Low-resource and Cross-Lingual Settings(https://arxiv.org/abs/2410.13153)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on massive amounts of data, enabling their application across diverse domains and tasks. Despite their remarkable performance, most LLMs are developed and evaluated primarily in English. Recently, a few multi-lingual LLMs have emerged, but their performance in low-resource languages, especially the most spoken languages in South Asia, is less explored. To address this gap, in this study, we evaluate LLMs such as GPT-4, Llama 2, and Gemini to analyze their effectiveness in English compared to other low-resource languages from South Asia (e.g., Bangla, Hindi, and Urdu). Specifically, we utilized zero-shot prompting and five different prompt settings to extensively investigate the effectiveness of the LLMs in cross-lingual translated prompts. The findings of the study suggest that GPT-4 outperformed Llama 2 and Gemini in all five prompt settings and across all languages. Moreover, all three LLMs performed better for English language prompts than other low-resource language prompts. This study extensively investigates LLMs in low-resource language contexts to highlight the improvements required in LLMs and language-specific resources to develop more generally purposed NLP applications.</li>
</ul>

<h3>Title: SLM-Mod: Small Language Models Surpass LLMs at Content Moderation</h3>
<ul>
<li><strong>Authors: </strong>Xianyang Zhan, Agam Goyal, Yilun Chen, Eshwar Chandrasekharan, Koustuv Saha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13155">https://arxiv.org/abs/2410.13155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13155">https://arxiv.org/pdf/2410.13155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13155]] SLM-Mod: Small Language Models Surpass LLMs at Content Moderation(https://arxiv.org/abs/2410.13155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against much larger open- and closed-sourced models. Using 150K comments from 15 popular Reddit communities, we find that SLMs outperform LLMs at content moderation -- 11.5% higher accuracy and 25.7% higher recall on average across all communities. We further show the promise of cross-community content moderation, which has implications for new communities and the development of cross-platform moderation techniques. Finally, we outline directions for future work on language model based content moderation. Code and links to HuggingFace models can be found at this https URL.</li>
</ul>

<h3>Title: FAMSeC: A Few-shot-sample-based General AI-generated Image Detection Method</h3>
<ul>
<li><strong>Authors: </strong>Juncong Xu, Yang Yang, Han Fang, Honggu Liu, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13156">https://arxiv.org/abs/2410.13156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13156">https://arxiv.org/pdf/2410.13156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13156]] FAMSeC: A Few-shot-sample-based General AI-generated Image Detection Method(https://arxiv.org/abs/2410.13156)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>The explosive growth of generative AI has saturated the internet with AI-generated images, raising security concerns and increasing the need for reliable detection methods. The primary requirement for such detection is generalizability, typically achieved by training on numerous fake images from various models. However, practical limitations, such as closed-source models and restricted access, often result in limited training samples. Therefore, training a general detector with few-shot samples is essential for modern detection mechanisms. To address this challenge, we propose FAMSeC, a general AI-generated image detection method based on LoRA-based Forgery Awareness Module and Semantic feature-guided Contrastive learning strategy. To effectively learn from limited samples and prevent overfitting, we developed a Forgery Awareness Module (FAM) based on LoRA, maintaining the generalization of pre-trained features. Additionally, to cooperate with FAM, we designed a Semantic feature-guided Contrastive learning strategy (SeC), making the FAM focus more on the differences between real/fake image than on the features of the samples themselves. Experiments show that FAMSeC outperforms state-of-the-art method, enhancing classification accuracy by 14.55% with just 0.56% of the training samples.</li>
</ul>

<h3>Title: An Evolved Universal Transformer Memory</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13166">https://arxiv.org/abs/2410.13166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13166">https://arxiv.org/pdf/2410.13166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13166]] An Evolved Universal Transformer Memory(https://arxiv.org/abs/2410.13166)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention this http URL are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.</li>
</ul>

<h3>Title: TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness</h3>
<ul>
<li><strong>Authors: </strong>Cheng Huang, Pan Mu, Cong Bai, Peter AG Watson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13175">https://arxiv.org/abs/2410.13175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13175">https://arxiv.org/pdf/2410.13175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13175]] TCP-Diffusion: A Multi-modal Diffusion Model for Global Tropical Cyclone Precipitation Forecasting with Change Awareness(https://arxiv.org/abs/2410.13175)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Precipitation from tropical cyclones (TCs) can cause disasters such as flooding, mudslides, and landslides. Predicting such precipitation in advance is crucial, giving people time to prepare and defend against these precipitation-induced disasters. Developing deep learning (DL) rainfall prediction methods offers a new way to predict potential disasters. However, one problem is that most existing methods suffer from cumulative errors and lack physical consistency. Second, these methods overlook the importance of meteorological factors in TC rainfall and their integration with the numerical weather prediction (NWP) model. Therefore, we propose Tropical Cyclone Precipitation Diffusion (TCP-Diffusion), a multi-modal model for global tropical cyclone precipitation forecasting. It forecasts TC rainfall around the TC center for the next 12 hours at 3 hourly resolution based on past rainfall observations and multi-modal environmental variables. Adjacent residual prediction (ARP) changes the training target from the absolute rainfall value to the rainfall trend and gives our model the ability of rainfall change awareness, reducing cumulative errors and ensuring physical consistency. Considering the influence of TC-related meteorological factors and the useful information from NWP model forecasts, we propose a multi-model framework with specialized encoders to extract richer information from environmental variables and results provided by NWP models. The results of extensive experiments show that our method outperforms other DL methods and the NWP method from the European Centre for Medium-Range Weather Forecasts (ECMWF).</li>
</ul>

<h3>Title: GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Yang, Zheng Chen, Xin Liu, Rikuto Kotoge, Peng Chen, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13178">https://arxiv.org/abs/2410.13178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13178">https://arxiv.org/pdf/2410.13178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13178]] GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation(https://arxiv.org/abs/2410.13178)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from databases or explicitly learn subtype-specific interactions. To address this mismatch, we propose GeSubNet, which learns a unified representation capable of predicting gene interactions while distinguishing between different disease subtypes. Graphs generated by such representations can be considered subtype-specific networks. GeSubNet is a multi-step representation learning framework with three modules: First, a deep generative model learns distinct disease subtypes from patient gene expression profiles. Second, a graph neural network captures representations of prior gene networks from knowledge databases, ensuring accurate physical gene interactions. Finally, we integrate these two representations using an inference loss that leverages graph generation capabilities, conditioned on the patient separation loss, to refine subtype-specific information in the learned representation. GeSubNet consistently outperforms traditional methods, with average improvements of 30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged over four cancer datasets. Particularly, we conduct a biological simulation experiment to assess how the behavior of selected genes from over 11,000 candidates affects subtypes or patient distributions. The results show that the generated network has the potential to identify subtype-specific genes with an 83% likelihood of impacting patient distribution shifts. The GeSubNet resource is available: this https URL</li>
</ul>

<h3>Title: AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Sun, Jiayi Wu, Hengyi Cai, Xiaochi Wei, Yue Feng, Bo Wang, Shuaiqiang Wang, Yan Zhang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13181">https://arxiv.org/abs/2410.13181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13181">https://arxiv.org/pdf/2410.13181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13181]] AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning(https://arxiv.org/abs/2410.13181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have been remarkable. Users face a choice between using cloud-based LLMs for generation quality and deploying local-based LLMs for lower computational cost. The former option is typically costly and inefficient, while the latter usually fails to deliver satisfactory performance for reasoning steps requiring deliberate thought processes. In this work, we propose a novel LLM utilization paradigm that facilitates the collaborative operation of large cloud-based LLMs and smaller local-deployed LLMs. Our framework comprises two primary modules: the local agent instantiated with a relatively smaller LLM, handling less complex reasoning steps, and the cloud agent equipped with a larger LLM, managing more intricate reasoning steps. This collaborative processing is enabled through an adaptive mechanism where the local agent introspectively identifies errors and proactively seeks assistance from the cloud agent, thereby effectively integrating the strengths of both locally-deployed and cloud-based LLMs, resulting in significant enhancements in task completion performance and efficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from mathematical reasoning and complex question answering, using various types of LLMs to instantiate the local and cloud agents. The empirical results show that AdaSwitch effectively improves the performance of the local agent, and sometimes achieves competitive results compared to the cloud agent while utilizing much less computational overhead.</li>
</ul>

<h3>Title: Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shwai He, Tao Ge, Guoheng Sun, Bowei Tian, Xiaoyang Wang, Ang Li, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13184">https://arxiv.org/abs/2410.13184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13184">https://arxiv.org/pdf/2410.13184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13184]] Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers(https://arxiv.org/abs/2410.13184)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) \textit{high training costs due to the need to train the entire model along with the routers that determine which layers to skip}, and (2) \textit{the risk of performance degradation when important layers are bypassed}. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys \textit{Attention with Dynamic Depths}. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\% speedup and only a 0.2\% performance drop. The code is released at \url{this https URL}.</li>
</ul>

<h3>Title: aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Ge Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13187">https://arxiv.org/abs/2410.13187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13187">https://arxiv.org/pdf/2410.13187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13187]] aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion(https://arxiv.org/abs/2410.13187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been widely used in code completion, and researchers are focusing on scaling up LLMs to improve their accuracy. However, larger LLMs will increase the response time of code completion and decrease the developers' productivity. In this paper, we propose a lightweight and effective LLM for code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B achieves higher code completion accuracy while having smaller scales (i.e., 7 billion parameters). We attribute the superiority of aiXcoder-7B to three key factors: (1) Multi-objective training. We employ three training objectives, one of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers the syntax structures in code and effectively improves the performance of LLMs for code. (2) Diverse data sampling strategies. They consider inter-file relationships and enhance the capability of LLMs in understanding cross-file contexts. (3) Extensive high-quality data. We establish a rigorous data collection pipeline and consume a total of 1.2 trillion unique tokens for training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a broad distribution of code. We evaluate aiXcoder-7B in five popular code completion benchmarks and a new benchmark collected by this paper. The results show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B), positioning aiXcoder-7B as a lightweight and effective LLM for academia and industry. Finally, we summarize three valuable insights for helping practitioners train the next generations of LLMs for code. aiXcoder-7B has been open-souced and gained significant attention. As of the submission date, aiXcoder-7B has received 2,193 GitHub Stars.</li>
</ul>

<h3>Title: MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13191">https://arxiv.org/abs/2410.13191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13191">https://arxiv.org/pdf/2410.13191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13191]] MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback(https://arxiv.org/abs/2410.13191)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.</li>
</ul>

<h3>Title: Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiatao Li, Xinyu Hu, Xunjian Yin, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13192">https://arxiv.org/abs/2410.13192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13192">https://arxiv.org/pdf/2410.13192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13192]] Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models(https://arxiv.org/abs/2410.13192)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In retrieval-augmented generation systems, the integration of self-generated documents (SGDs) alongside retrieved content has emerged as a promising strategy for enhancing the performance of large language model. However, previous research primarily focuses on optimizing the use of SGDs, with the inherent properties of SGDs remaining underexplored. Therefore, this paper conducts a comprehensive analysis of different types of SGDs and experiments on various knowledge-intensive tasks. We develop a taxonomy of SGDs grounded in Systemic Functional Linguistics (SFL) to compare the influence of different SGD categories. Our findings offer key insights into what kinds of SGDs most effectively contribute to improving LLM's performance. The results and further fusion methods based on SGD categories also provide practical guidelines for taking better advantage of SGDs to achieve significant advancements in knowledge-driven QA tasks with RAG.</li>
</ul>

<h3>Title: Golyadkin's Torment: Doppelg\"angers and Adversarial Vulnerability</h3>
<ul>
<li><strong>Authors: </strong>George I. Kamberov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13193">https://arxiv.org/abs/2410.13193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13193">https://arxiv.org/pdf/2410.13193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13193]] Golyadkin's Torment: Doppelg\"angers and Adversarial Vulnerability(https://arxiv.org/abs/2410.13193)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Many machine learning (ML) classifiers are claimed to outperform humans, but they still make mistakes that humans do not. The most notorious examples of such mistakes are adversarial visual metamers. This paper aims to define and investigate the phenomenon of adversarial Doppelgangers (AD), which includes adversarial visual metamers, and to compare the performance and robustness of ML classifiers to human performance. We find that AD are inputs that are close to each other with respect to a perceptual metric defined in this paper. AD are qualitatively different from the usual adversarial examples. The vast majority of classifiers are vulnerable to AD and robustness-accuracy trade-offs may not improve them. Some classification problems may not admit any AD robust classifiers because the underlying classes are ambiguous. We provide criteria that can be used to determine whether a classification problem is well defined or not; describe the structure and attributes of an AD-robust classifier; introduce and explore the notions of conceptual entropy and regions of conceptual ambiguity for classifiers that are vulnerable to AD attacks, along with methods to bound the AD fooling rate of an attack. We define the notion of classifiers that exhibit hypersensitive behavior, that is, classifiers whose only mistakes are adversarial Doppelgangers. Improving the AD robustness of hyper-sensitive classifiers is equivalent to improving accuracy. We identify conditions guaranteeing that all classifiers with sufficiently high accuracy are hyper-sensitive. Our findings are aimed at significant improvements in the reliability and security of machine learning systems.</li>
</ul>

<h3>Title: The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Oumar El-Shangiti, Tatsuya Hiraoka, Hilal AlQuabeh, Benjamin Heinzerling, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13194">https://arxiv.org/abs/2410.13194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13194">https://arxiv.org/pdf/2410.13194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13194]] The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces(https://arxiv.org/abs/2410.13194)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates whether large language models (LLMs) utilize numerical attributes encoded in a low-dimensional subspace of the embedding space when answering logical comparison questions (e.g., Was Cristiano born before Messi?). We first identified these subspaces using partial least squares regression, which effectively encodes the numerical attributes associated with the entities in comparison prompts. Further, we demonstrate causality by intervening in these subspaces to manipulate hidden states, thereby altering the LLM's comparison outcomes. Experimental results show that our findings hold for different numerical attributes, indicating that LLMs utilize the linearly encoded information for numerical reasoning.</li>
</ul>

<h3>Title: UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Wu, Kenkun Liu, Yukai Shi, Xiaoke Jiang, Yuan Yao, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13195">https://arxiv.org/abs/2410.13195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13195">https://arxiv.org/pdf/2410.13195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13195]] UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction(https://arxiv.org/abs/2410.13195)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work, we present UniG, a view-consistent 3D reconstruction and novel view synthesis model that generates a high-fidelity representation of 3D Gaussians from sparse images. Existing 3D Gaussians-based methods usually regress Gaussians per-pixel of each view, create 3D Gaussians per view separately, and merge them through point concatenation. Such a view-independent reconstruction approach often results in a view inconsistency issue, where the predicted positions of the same 3D point from different views may have discrepancies. To address this problem, we develop a DETR (DEtection TRansformer)-like framework, which treats 3D Gaussians as decoder queries and updates their parameters layer by layer by performing multi-view cross-attention (MVDFA) over multiple input images. In this way, multiple views naturally contribute to modeling a unitary representation of 3D Gaussians, thereby making 3D reconstruction more view-consistent. Moreover, as the number of 3D Gaussians used as decoder queries is irrespective of the number of input views, allow an arbitrary number of input images without causing memory explosion. Extensive experiments validate the advantages of our approach, showcasing superior performance over existing methods quantitatively (improving PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and qualitatively.</li>
</ul>

<h3>Title: Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration</h3>
<ul>
<li><strong>Authors: </strong>Yun-Yen Chuang, Hung-Min Hsu, Kevin Lin, Chen-Sheng Gu, Ling Zhen Li, Ray-I Chang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13201">https://arxiv.org/abs/2410.13201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13201">https://arxiv.org/pdf/2410.13201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13201]] Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration(https://arxiv.org/abs/2410.13201)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-DiffuB framework - a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-DiffuB achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-DiffuB's noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a "plug-and-play" model to enhance DiffuSeq without the need for fine-tuning during the inference stage.</li>
</ul>

<h3>Title: Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations</h3>
<ul>
<li><strong>Authors: </strong>Aryan Shrivastava, Jessica Hullman, Max Lamparth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13204">https://arxiv.org/abs/2410.13204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13204">https://arxiv.org/pdf/2410.13204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13204]] Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations(https://arxiv.org/abs/2410.13204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>There is an increasing interest in using language models (LMs) for automated decision-making, with multiple countries actively testing LMs to aid in military crisis decision-making. To scrutinize relying on LM decision-making in high-stakes settings, we examine the inconsistency of responses in a crisis simulation ("wargame"), similar to reported tests conducted by the US military. Prior work illustrated escalatory tendencies and varying levels of aggression among LMs but were constrained to simulations with pre-defined actions. This was due to the challenges associated with quantitatively measuring semantic differences and evaluating natural language decision-making without relying on pre-defined actions. In this work, we query LMs for free form responses and use a metric based on BERTScore to measure response inconsistency quantitatively. Leveraging the benefits of BERTScore, we show that the inconsistency metric is robust to linguistic variations that preserve semantic meaning in a question-answering setting across text lengths. We show that all five tested LMs exhibit levels of inconsistency that indicate semantic differences, even when adjusting the wargame setting, anonymizing involved conflict countries, or adjusting the sampling temperature parameter $T$. Further qualitative evaluation shows that models recommend courses of action that share few to no similarities. We also study the impact of different prompt sensitivity variations on inconsistency at temperature $T = 0$. We find that inconsistency due to semantically equivalent prompt variations can exceed response inconsistency from temperature sampling for most studied models across different levels of ablations. Given the high-stakes nature of military deployment, we recommend further consideration be taken before using LMs to inform military decisions or other cases of high-stakes decision-making.</li>
</ul>

<h3>Title: BQA: Body Language Question Answering Dataset for Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shintaro Ozaki, Kazuki Hayashi, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13206">https://arxiv.org/abs/2410.13206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13206">https://arxiv.org/pdf/2410.13206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13206]] BQA: Body Language Question Answering Dataset for Video Large Language Models(https://arxiv.org/abs/2410.13206)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding. Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent. To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language. We evaluated various VideoLLMs on BQA and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made significantly biased answers depending on the age group and ethnicity of the individuals in the video. The dataset is available.</li>
</ul>

<h3>Title: FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs</h3>
<ul>
<li><strong>Authors: </strong>Forrest Sheng Bao, Miaoran Li, Renyi Qu, Ge Luo, Erana Wan, Yujia Tang, Weisi Fan, Manveer Singh Tamber, Suleman Kazi, Vivek Sourabh, Mike Qi, Ruixuan Tu, Chenyu Xu, Matthew Gonzales, Ofer Mendelevitch, Amin Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13210">https://arxiv.org/abs/2410.13210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13210">https://arxiv.org/pdf/2410.13210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13210]] FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs(https://arxiv.org/abs/2410.13210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10 modern LLMs from 8 different families, with ground truth annotations by human experts. ``Challenging'' here means summaries on which popular, state-of-the-art hallucination detection models, including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and GPT-3.5-Turbo produce the least hallucinations. However, even the best hallucination detection models have near 50\% accuracies on FaithBench, indicating lots of room for future improvement. The repo is this https URL</li>
</ul>

<h3>Title: Estimating the Probabilities of Rare Outputs in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Wu, Jacob Hilton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13211">https://arxiv.org/abs/2410.13211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13211">https://arxiv.org/pdf/2410.13211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13211]] Estimating the Probabilities of Rare Outputs in Language Models(https://arxiv.org/abs/2410.13211)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We consider the problem of low probability estimation: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance.</li>
</ul>

<h3>Title: AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations</h3>
<ul>
<li><strong>Authors: </strong>Qian Tao, Wenyuan Yu, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13212">https://arxiv.org/abs/2410.13212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13212">https://arxiv.org/pdf/2410.13212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13212]] AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations(https://arxiv.org/abs/2410.13212)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have shown exceptional capabilities in a wide range of tasks, such as text generation and video generation, among others. However, due to their massive parameter count, these models often require substantial storage space, imposing significant constraints on the machines deploying LLMs. To overcome this limitation, one research direction proposes to compress the models using integer replacements for floating-point numbers, in a process known as Quantization. Some recent studies suggest quantizing the key and value cache (KV Cache) of LLMs, and designing quantization techniques that treat the key and value matrices equivalently. This work delves deeper into the asymmetric structural roles of KV Cache, a phenomenon where the transformer's output loss is more sensitive to the quantization of key matrices. We conduct a systematic examination of the attention output error resulting from key and value quantization. The phenomenon inspires us to propose an asymmetric quantization strategy. Our approach allows for 1-bit quantization of the KV cache by implementing distinct configurations for key and value matrices. We carry out experiments across a variety of datasets, demonstrating that our proposed model allows for the quantization of up to 75% decoder layers with 1 bit, while simultaneously maintaining performance levels comparable to those of the models with floating parameters.</li>
</ul>

<h3>Title: A Comprehensive Analysis of Routing Vulnerabilities and Defense Strategies in IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Kim Jae-Dong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13214">https://arxiv.org/abs/2410.13214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13214">https://arxiv.org/pdf/2410.13214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13214]] A Comprehensive Analysis of Routing Vulnerabilities and Defense Strategies in IoT Networks(https://arxiv.org/abs/2410.13214)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) has revolutionized various domains, offering significant benefits through enhanced interconnectivity and data exchange. However, the security challenges associated with IoT networks have become increasingly prominent owing to their inherent vulnerability. This paper provides an in-depth analysis of the network layer in IoT architectures, highlighting the potential risks posed by routing attacks, such as blackholes, wormholes, sinkholes, Sybil, and selective forwarding attacks. This study explores the unique challenges posed by the constrained resources, heterogeneity, and dynamic topology of IoT networks, which complicate the implementation of robust security measures. Various countermeasures, including trust-based mechanisms, Intrusion Detection Systems (IDS), and routing protocols, are evaluated for their effectiveness in mitigating these threats. This study also emphasizes the importance of considering misbehavior observation, trust management, and lightweight defense strategies in the design of secure IoT networks. These findings contribute to the development of comprehensive defense mechanisms tailored to the specific challenges of IoT environments.</li>
</ul>

<h3>Title: MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ruohan Wang, Zilong Wang, Ziyang Song, David Buckeridge, Yue Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13217">https://arxiv.org/abs/2410.13217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13217">https://arxiv.org/pdf/2410.13217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13217]] MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling(https://arxiv.org/abs/2410.13217)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Automatic subphenotyping from electronic health records (EHRs)provides numerous opportunities to understand diseases with unique subgroups and enhance personalized medicine for patients. However, existing machine learning algorithms either focus on specific diseases for better interpretability or produce coarse-grained phenotype topics without considering nuanced disease patterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer sub-phenotype topics from thousands of disease using multi-modal EHR data. Specifically, MixEHR-Nest detects multiple subtopics from each phenotype topic, whose prior is guided by the expert-curated phenotype concepts such as Phenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We evaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting of over 38 thousand patients from intensive care unit (ICU) from Beth Israel Deaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare administrative database PopHR, comprising 1.3 million patients from Montreal, Canada. Experimental results demonstrate that MixEHR-Nest can identify subphenotypes with distinct patterns within each phenotype, which are predictive for disease progression and severity. Consequently, MixEHR-Nest distinguishes between type 1 and type 2 diabetes by inferring subphenotypes using CCS codes, which do not differentiate these two subtype concepts. Additionally, MixEHR-Nest not only improved the prediction accuracy of short-term mortality of ICU patients and initial insulin treatment in diabetic patients but also revealed the contributions of subphenotypes. For longitudinal analysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under the same phenotypes, such as asthma, leukemia, epilepsy, and depression. The MixEHR-Nest software is available at GitHub: this https URL.</li>
</ul>

<h3>Title: CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</h3>
<ul>
<li><strong>Authors: </strong>Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang, William Yang Wang, Zhiyu Zoey Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13218">https://arxiv.org/abs/2410.13218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13218">https://arxiv.org/pdf/2410.13218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13218]] CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy(https://arxiv.org/abs/2410.13218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.</li>
</ul>

<h3>Title: Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13224">https://arxiv.org/abs/2410.13224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13224">https://arxiv.org/pdf/2410.13224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13224]] Proof Flow: Preliminary Study on Generative Flow Network Language Model Tuning for Formal Reasoning(https://arxiv.org/abs/2410.13224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reasoning is a fundamental substrate for solving novel and complex problems. Deliberate efforts in learning and developing frameworks around System 2 reasoning have made great strides, yet problems of sufficient complexity remain largely out of reach for open models. To address this gap, we examine the potential of Generative Flow Networks as a fine-tuning method for LLMs to unlock advanced reasoning capabilities. In this paper, we present a proof of concept in the domain of formal reasoning, specifically in the Neural Theorem Proving (NTP) setting, where proofs specified in a formal language such as Lean can be deterministically and objectively verified. Unlike classical reward-maximization reinforcement learning, which frequently over-exploits high-reward actions and fails to effectively explore the state space, GFlowNets have emerged as a promising approach for sampling compositional objects, improving generalization, and enabling models to maintain diverse hypotheses. Our early results demonstrate GFlowNet fine-tuning's potential for enhancing model performance in a search setting, which is especially relevant given the paradigm shift towards inference time compute scaling and "thinking slowly."</li>
</ul>

<h3>Title: Quamba: A Post-Training Quantization Recipe for Selective State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13229">https://arxiv.org/abs/2410.13229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13229">https://arxiv.org/pdf/2410.13229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13229]] Quamba: A Post-Training Quantization Recipe for Selective State Space Models(https://arxiv.org/abs/2410.13229)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of SSMs in long sequence modeling positions them favorably over Transformers in many scenarios. However, improving the efficiency of SSMs on request-intensive cloud-serving and resource-limited edge applications is still a formidable task. SSM quantization is a possible solution to this problem, making SSMs more suitable for wide deployment, while still maintaining their accuracy. Quantization is a common technique to reduce the model size and to utilize the low bit-width acceleration features on modern computing units, yet existing quantization techniques are poorly suited for SSMs. Most notably, SSMs have highly sensitive feature maps within the selective scan mechanism (i.e., linear recurrence) and massive outliers in the output activations which are not present in the output of token-mixing in the self-attention modules. To address this issue, we propose a static 8-bit per-tensor SSM quantization method which suppresses the maximum values of the input activations to the selective SSM for finer quantization precision and quantizes the output activations in an outlier-free space with Hadamard transform. Our 8-bit weight-activation quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a 1.72x lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9% drop in average accuracy on zero-shot tasks. The experiments demonstrate the effectiveness and practical applicability of our approach for deploying SSM-based models of all sizes on both cloud and edge platforms.</li>
</ul>

<h3>Title: Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</h3>
<ul>
<li><strong>Authors: </strong>Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13232">https://arxiv.org/abs/2410.13232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13232">https://arxiv.org/pdf/2410.13232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13232]] Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation(https://arxiv.org/abs/2410.13232)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents.</li>
</ul>

<h3>Title: SPIN: Self-Supervised Prompt INjection</h3>
<ul>
<li><strong>Authors: </strong>Leon Zhou, Junfeng Yang, Chengzhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13236">https://arxiv.org/abs/2410.13236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13236">https://arxiv.org/pdf/2410.13236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13236]] SPIN: Self-Supervised Prompt INjection(https://arxiv.org/abs/2410.13236)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain as major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. As our self-supervised prompt defense is done at inference-time, it is also compatible with existing alignment and adds an additional layer of safety for defense. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.</li>
</ul>

<h3>Title: Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yiyi Chen, Qiongxiu Li, Russa Biswas, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13237">https://arxiv.org/abs/2410.13237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13237">https://arxiv.org/pdf/2410.13237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13237]] Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis(https://arxiv.org/abs/2410.13237)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Language Confusion is a phenomenon where Large Language Models (LLMs) generate text that is neither in the desired language, nor in a contextually appropriate language. This phenomenon presents a critical challenge in text generation by LLMs, often appearing as erratic and unpredictable behavior. We hypothesize that there are linguistic regularities to this inherent vulnerability in LLMs and shed light on patterns of language confusion across LLMs. We introduce a novel metric, Language Confusion Entropy, designed to directly measure and quantify this confusion, based on language distributions informed by linguistic typology and lexical variation. Comprehensive comparisons with the Language Confusion Benchmark (Marchisio et al., 2024) confirm the effectiveness of our metric, revealing patterns of language confusion across LLMs. We further link language confusion to LLM security, and find patterns in the case of multilingual embedding inversion attacks. Our analysis demonstrates that linguistic typology offers theoretically grounded interpretation, and valuable insights into leveraging language similarities as a prior for LLM alignment and security.</li>
</ul>

<h3>Title: Fundus to Fluorescein Angiography Video Generation as a Retinal Generative Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Weiyi Zhang, Jiancheng Yang, Ruoyu Chen, Siyu Huang, Pusheng Xu, Xiaolan Chen, Shanfu Lu, Hongyu Cao, Mingguang He, Danli Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13242">https://arxiv.org/abs/2410.13242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13242">https://arxiv.org/pdf/2410.13242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13242]] Fundus to Fluorescein Angiography Video Generation as a Retinal Generative Foundation Model(https://arxiv.org/abs/2410.13242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring retinal vascular issues but is limited by its invasive nature and restricted accessibility compared to color fundus (CF) imaging. Existing methods that convert CF images to FFA are confined to static image generation, missing the dynamic lesional changes. We introduce Fundus2Video, an autoregressive generative adversarial network (GAN) model that generates dynamic FFA videos from single CF images. Fundus2Video excels in video generation, achieving an FVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the fidelity of the generated videos. Additionally, the model's generator demonstrates remarkable downstream transferability across ten external public datasets, including blood vessel segmentation, retinal disease diagnosis, systemic disease prediction, and multimodal retrieval, showcasing impressive zero-shot and few-shot capabilities. These findings position Fundus2Video as a powerful, non-invasive alternative to FFA exams and a versatile retinal generative foundation model that captures both static and temporal retinal features, enabling the representation of complex inter-modality relationships.</li>
</ul>

<h3>Title: Atomic Calibration of LLMs in Long-Form Generations</h3>
<ul>
<li><strong>Authors: </strong>Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting Huang, Sen Yang, Dong Yu, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13246">https://arxiv.org/abs/2410.13246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13246">https://arxiv.org/pdf/2410.13246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13246]] Atomic Calibration of LLMs in Long-Form Generations(https://arxiv.org/abs/2410.13246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often suffer from hallucinations, posing significant challenges for real-world applications. Confidence calibration, which estimates the underlying uncertainty of model predictions, is essential to enhance the LLMs' trustworthiness. Existing research on LLM calibration has primarily focused on short-form tasks, providing a single confidence score at the response level (macro calibration). However, this approach is insufficient for long-form generations, where responses often contain more complex statements and may include both accurate and inaccurate information. Therefore, we introduce atomic calibration, a novel approach that evaluates factuality calibration at a fine-grained level by breaking down long responses into atomic claims. We classify confidence elicitation methods into discriminative and generative types and demonstrate that their combination can enhance calibration. Our extensive experiments on various LLMs and datasets show that atomic calibration is well-suited for long-form generation and can also improve macro calibration results. Additionally, atomic calibration reveals insightful patterns in LLM confidence throughout the generation process.</li>
</ul>

<h3>Title: Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Ryotaro Shimizu, Takashi Wada, Yu Wang, Johannes Kruse, Sean O'Brien, Sai HtaungKham, Linxin Song, Yuya Yoshikawa, Yuki Saito, Fugee Tsung, Masayuki Goto, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13248">https://arxiv.org/abs/2410.13248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13248">https://arxiv.org/pdf/2410.13248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13248]] Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation(https://arxiv.org/abs/2410.13248)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. We will release our code and datasets upon acceptance.</li>
</ul>

<h3>Title: FDF: Flexible Decoupled Framework for Time Series Forecasting with Conditional Denoising and Polynomial Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jintao Zhang, Mingyue Cheng, Xiaoyu Tao, Zhiding Liu, Daoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13253">https://arxiv.org/abs/2410.13253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13253">https://arxiv.org/pdf/2410.13253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13253]] FDF: Flexible Decoupled Framework for Time Series Forecasting with Conditional Denoising and Polynomial Modeling(https://arxiv.org/abs/2410.13253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series forecasting is vital in numerous web applications, influencing critical decision-making across industries. While diffusion models have recently gained increasing popularity for this task, we argue they suffer from a significant drawback: indiscriminate noise addition to the original time series followed by denoising, which can obscure underlying dynamic evolving trend and complicate forecasting. To address this limitation, we propose a novel flexible decoupled framework (FDF) that learns high-quality time series representations for enhanced forecasting performance. A key characteristic of our approach leverages the inherent inductive bias of time series data by decomposing it into trend and seasonal components, each modeled separately to enable decoupled analysis and modeling. Specifically, we propose an innovative Conditional Denoising Seasonal Module (CDSM) within the diffusion model, which leverages statistical information from the historical window to conditionally model the complex seasonal component. Notably, we incorporate a Polynomial Trend Module (PTM) to effectively capture the smooth trend component, thereby enhancing the model's ability to represent temporal dependencies. Extensive experiments validate the effectiveness of our framework, demonstrating superior performance over existing methods and higlighting its flexibility in time series forecasting. We hope our work can bring a new perspective for time series forecasting. We intend to make our code publicly available as open-source in the future.</li>
</ul>

<h3>Title: From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Yang, Pengda Wang, Luke D. Plonsky, Frederick L. Oswald, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13259">https://arxiv.org/abs/2410.13259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13259">https://arxiv.org/pdf/2410.13259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13259]] From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition(https://arxiv.org/abs/2410.13259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition. Notably, in generation tasks, LMs are more similar to human performance in areas where information is easier to extract from the corpus, such as average word length, clauses, and auxiliary verbs. Newer LMs did not exhibit significant progress in terms of specific dimensions, such as clauses and auxiliary verbs, where the variation across corpora is relatively limited. Register theory offers a plausible explanation for these observations, suggesting that the linguistic features of the training data have a substantial impact on the models' abilities.</li>
</ul>

<h3>Title: Cyber Attacks Prevention Towards Prosumer-based EV Charging Stations: An Edge-assisted Federated Prototype Knowledge Distillation Approach</h3>
<ul>
<li><strong>Authors: </strong>Luyao Zou, Quang Hieu Vo, Kitae Kim, Huy Q. Le, Chu Myaet Thwal, Chaoning Zhang, Choong Seon Hong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13260">https://arxiv.org/abs/2410.13260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13260">https://arxiv.org/pdf/2410.13260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13260]] Cyber Attacks Prevention Towards Prosumer-based EV Charging Stations: An Edge-assisted Federated Prototype Knowledge Distillation Approach(https://arxiv.org/abs/2410.13260)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>In this paper, cyber-attack prevention for the prosumer-based electric vehicle (EV) charging stations (EVCSs) is investigated, which covers two aspects: 1) cyber-attack detection on prosumers' network traffic (NT) data, and 2) cyber-attack intervention. To establish an effective prevention mechanism, several challenges need to be tackled, for instance, the NT data per prosumer may be non-independent and identically distributed (non-IID), and the boundary between benign and malicious traffic becomes blurred. To this end, we propose an edge-assisted federated prototype knowledge distillation (E-FPKD) approach, where each client is deployed on a dedicated local edge server (DLES) and can report its availability for joining the federated learning (FL) process. Prior to the E-FPKD approach, to enhance accuracy, the Pearson Correlation Coefficient is adopted for feature selection. Regarding the proposed E-FPKD approach, we integrate the knowledge distillation and prototype aggregation technique into FL to deal with the non-IID challenge. To address the boundary issue, instead of directly calculating the distance between benign and malicious traffic, we consider maximizing the overall detection correctness of all prosumers (ODC), which can mitigate the computational cost compared with the former way. After detection, a rule-based method will be triggered at each DLES for cyber-attack intervention. Experimental analysis demonstrates that the proposed E-FPKD can achieve the largest ODC on NSL-KDD, UNSW-NB15, and IoTID20 datasets in both binary and multi-class classification, compared with baselines. For instance, the ODC for IoTID20 obtained via the proposed method is separately 0.3782% and 4.4471% greater than FedProto and FedAU in multi-class classification.</li>
</ul>

<h3>Title: The Latent Road to Atoms: Backmapping Coarse-grained Protein Structures with Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xu Han, Yuancheng Sun, Kai Chen, Kang Liu, Qiwei Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13264">https://arxiv.org/abs/2410.13264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13264">https://arxiv.org/pdf/2410.13264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13264]] The Latent Road to Atoms: Backmapping Coarse-grained Protein Structures with Latent Diffusion(https://arxiv.org/abs/2410.13264)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Coarse-grained(CG) molecular dynamics simulations offer computational efficiency for exploring protein conformational ensembles and thermodynamic properties. Though coarse representations enable large-scale simulations across extended temporal and spatial ranges, the sacrifice of atomic-level details limits their utility in tasks such as ligand docking and protein-protein interaction prediction. Backmapping, the process of reconstructing all-atom structures from coarse-grained representations, is crucial for recovering these fine details. While recent machine learning methods have made strides in protein structure generation, challenges persist in reconstructing diverse atomistic conformations that maintain geometric accuracy and chemical validity. In this paper, we present Latent Diffusion Backmapping (LDB), a novel approach leveraging denoising diffusion within latent space to address these challenges. By combining discrete latent encoding with diffusion, LDB bypasses the need for equivariant and internal coordinate manipulation, significantly simplifying the training and sampling processes as well as facilitating better and wider exploration in configuration space. We evaluate LDB's state-of-the-art performance on three distinct protein datasets, demonstrating its ability to efficiently reconstruct structures with high structural accuracy and chemical validity. Moreover, LDB shows exceptional versatility in capturing diverse protein ensembles, highlighting its capability to explore intricate conformational spaces. Our results position LDB as a powerful and scalable approach for backmapping, effectively bridging the gap between CG simulations and atomic-level analyses in computational biology.</li>
</ul>

<h3>Title: Roadmap towards Superhuman Speech Understanding using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fan Bu, Yuhao Zhang, Xidong Wang, Benyou Wang, Qun Liu, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13268">https://arxiv.org/abs/2410.13268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13268">https://arxiv.org/pdf/2410.13268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13268]] Roadmap towards Superhuman Speech Understanding using Large Language Models(https://arxiv.org/abs/2410.13268)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models (LLMs) has prompted efforts to integrate speech and audio data, aiming to create general foundation models capable of processing both textual and non-textual inputs. Recent advances, such as GPT-4o, highlight the potential for end-to-end speech LLMs, which preserves non-semantic information and world knowledge for deeper speech understanding. To guide the development of speech LLMs, we propose a five-level roadmap, ranging from basic automatic speech recognition (ASR) to advanced superhuman models capable of integrating non-semantic information with abstract acoustic knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark, that standardizes critical aspects across various tasks in these five levels, uncovering challenges in using abstract acoustic knowledge and completeness of capability. Our findings reveal gaps in handling paralinguistic cues and abstract acoustic knowledge, and we offer future directions. This paper outlines a roadmap for advancing speech LLMs, introduces a benchmark for evaluation, and provides key insights into their current limitations and potential.</li>
</ul>

<h3>Title: FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13272">https://arxiv.org/abs/2410.13272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13272">https://arxiv.org/pdf/2410.13272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13272]] FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented Generation(https://arxiv.org/abs/2410.13272)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, federate</a></li>
<li><strong>Abstract: </strong>This paper introduces \textit{Federated Retrieval-Augmented Generation (FRAG)}, a novel database management paradigm tailored for the growing needs of retrieval-augmented generation (RAG) systems, which are increasingly powered by large-language models (LLMs). FRAG enables mutually-distrusted parties to collaboratively perform Approximate $k$-Nearest Neighbor (ANN) searches on encrypted query vectors and encrypted data stored in distributed vector databases, all while ensuring that no party can gain any knowledge about the queries or data of others. Achieving this paradigm presents two key challenges: (i) ensuring strong security guarantees, such as Indistinguishability under Chosen-Plaintext Attack (IND-CPA), under practical assumptions (e.g., we avoid overly optimistic assumptions like non-collusion among parties); and (ii) maintaining performance overheads comparable to traditional, non-federated RAG systems. To address these challenges, FRAG employs a single-key homomorphic encryption protocol that simplifies key management across mutually-distrusted parties. Additionally, FRAG introduces a \textit{multiplicative caching} technique to efficiently encrypt floating-point numbers, significantly improving computational performance in large-scale federated environments. We provide a rigorous security proof using standard cryptographic reductions and demonstrate the practical scalability and efficiency of FRAG through extensive experiments on both benchmark and real-world datasets.</li>
</ul>

<h3>Title: Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Minseok Choi, ChaeHun Park, Dohyun Lee, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13274">https://arxiv.org/abs/2410.13274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13274">https://arxiv.org/pdf/2410.13274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13274]] Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning(https://arxiv.org/abs/2410.13274)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.</li>
</ul>

<h3>Title: SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, Mao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13276">https://arxiv.org/abs/2410.13276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13276">https://arxiv.org/pdf/2410.13276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13276]] SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs(https://arxiv.org/abs/2410.13276)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.</li>
</ul>

<h3>Title: BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla</h3>
<ul>
<li><strong>Authors: </strong>Fabiha Haider, Fariha Tanjim Shifat, Md Farhan Ishmam, Deeparghya Dutta Barua, Md Sakib Ul Rahman Sourove, Md Fahim, Md Farhad Alam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13281">https://arxiv.org/abs/2410.13281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13281">https://arxiv.org/pdf/2410.13281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13281]] BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla(https://arxiv.org/abs/2410.13281)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in comprehending hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset comprising 37.3k samples. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We establish novel transformer encoder-based baselines by further pre-training on transliterated Bangla corpus. We also propose a novel translation-based LLM prompting strategy for transliterated text. Experiments reveal that our further pre-trained encoders are achieving state-of-the-art performance on the BanTH dataset, while our translation-based prompting outperforms other strategies in the zero-shot setting. The introduction of BanTH not only fills a critical gap in hate speech research for Bangla but also sets the stage for future exploration into code-mixed and multi-label classification challenges in underrepresented languages.</li>
</ul>

<h3>Title: Learning to Route with Confidence Tokens</h3>
<ul>
<li><strong>Authors: </strong>Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13284">https://arxiv.org/abs/2410.13284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13284">https://arxiv.org/pdf/2410.13284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13284]] Learning to Route with Confidence Tokens(https://arxiv.org/abs/2410.13284)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.</li>
</ul>

<h3>Title: A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes</h3>
<ul>
<li><strong>Authors: </strong>Jake Robertson, Thorsten Schmidt, Frank Hutter, Noor Awad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13286">https://arxiv.org/abs/2410.13286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13286">https://arxiv.org/pdf/2410.13286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13286]] A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes(https://arxiv.org/abs/2410.13286)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness-aware Machine Learning (FairML) applications are often characterized by complex social objectives and legal requirements, frequently involving multiple, potentially conflicting notions of fairness. Despite the well-known Impossibility Theorem of Fairness and extensive theoretical research on the statistical and socio-technical trade-offs between fairness metrics, many FairML tools still optimize or constrain for a single fairness objective. However, this one-sided optimization can inadvertently lead to violations of other relevant notions of fairness. In this socio-technical and empirical study, we frame fairness as a many-objective (MaO) problem by treating fairness metrics as conflicting objectives. We introduce ManyFairHPO, a human-in-the-loop, fairness-aware model selection framework that enables practitioners to effectively navigate complex and nuanced fairness objective landscapes. ManyFairHPO aids in the identification, evaluation, and balancing of fairness metric conflicts and their related social consequences, leading to more informed and socially responsible model-selection decisions. Through a comprehensive empirical evaluation and a case study on the Law School Admissions problem, we demonstrate the effectiveness of ManyFairHPO in balancing multiple fairness objectives, mitigating risks such as self-fulfilling prophecies, and providing interpretable insights to guide stakeholders in making fairness-aware modeling decisions.</li>
</ul>

<h3>Title: An Online Learning Approach to Prompt-based Selection of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Hu, Ho-fung Leung, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13287">https://arxiv.org/abs/2410.13287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13287">https://arxiv.org/pdf/2410.13287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13287]] An Online Learning Approach to Prompt-based Selection of Generative Models(https://arxiv.org/abs/2410.13287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Selecting a sample generation scheme from multiple text-based generative models is typically addressed by choosing the model that maximizes an averaged evaluation score. However, this score-based selection overlooks the possibility that different models achieve the best generation performance for different types of text prompts. An online identification of the best generation model for various input prompts can reduce the costs associated with querying sub-optimal models. In this work, we explore the possibility of varying rankings of text-based generative models for different text prompts and propose an online learning framework to predict the best data generation model for a given input prompt. The proposed framework adapts the kernelized contextual bandit (CB) methodology to a CB setting with shared context variables across arms, utilizing the generated data to update a kernel-based function that predicts which model will achieve the highest score for unseen text prompts. Additionally, we apply random Fourier features (RFF) to the kernelized CB algorithm to accelerate the online learning process and establish a $\widetilde{\mathcal{O}}(\sqrt{T})$ regret bound for the proposed RFF-based CB algorithm over T iterations. Our numerical experiments on real and simulated text-to-image and image-to-text generative models show RFF-UCB performs successfully in identifying the best generation model across different sample types.</li>
</ul>

<h3>Title: SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Dixit, Tim Oates</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13293">https://arxiv.org/abs/2410.13293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13293">https://arxiv.org/pdf/2410.13293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13293]] SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation(https://arxiv.org/abs/2410.13293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical this http URL-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM).Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a "reasoning score" metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially providing educational benefits for students</li>
</ul>

<h3>Title: LESS: Label-Efficient and Single-Stage Referring 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xuexun Liu, Xiaoxu Xu, Jinlong Li, Qiudan Zhang, Xu Wang, Nicu Sebe, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13294">https://arxiv.org/abs/2410.13294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13294">https://arxiv.org/pdf/2410.13294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13294]] LESS: Label-Efficient and Single-Stage Referring 3D Segmentation(https://arxiv.org/abs/2410.13294)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring 3D Segmentation is a visual-language task that segments all points of the specified object from a 3D point cloud described by a sentence of query. Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query. However, the semantic concepts from text query and visual cues are separately interacted during the training, and both instance and semantic labels for each object are required, which is time consuming and human-labor intensive. To mitigate these issues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient and Single-Stage, dubbed LESS, which is only under the supervision of efficient binary mask. Specifically, we design a Point-Word Cross-Modal Alignment module for aligning the fine-grained features of points and textual embedding. Query Mask Predictor module and Query-Sentence Alignment module are introduced for coarse-grained alignment between masks and query. Furthermore, we propose an area regularization loss, which coarsely reduces irrelevant background predictions on a large scale. Besides, a point-to-point contrastive loss is proposed concentrating on distinguishing points with subtly similar features. Through extensive experiments, we achieve state-of-the-art performance on ScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only binary labels.</li>
</ul>

<h3>Title: PiLocNet: Physics-informed neural network on 3D localization with rotating point spread function</h3>
<ul>
<li><strong>Authors: </strong>Mingda Lu, Zitian Ao, Chao Wang, Sudhakar Prasad, Raymond H. Chan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13295">https://arxiv.org/abs/2410.13295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13295">https://arxiv.org/pdf/2410.13295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13295]] PiLocNet: Physics-informed neural network on 3D localization with rotating point spread function(https://arxiv.org/abs/2410.13295)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>For the 3D localization problem using point spread function (PSF) engineering, we propose a novel enhancement of our previously introduced localization neural network, LocNet. The improved network is a physics-informed neural network (PINN) that we call PiLocNet. Previous works on the localization problem may be categorized separately into model-based optimization and neural network approaches. Our PiLocNet combines the unique strengths of both approaches by incorporating forward-model-based information into the network via a data-fitting loss term that constrains the neural network to yield results that are physically sensible. We additionally incorporate certain regularization terms from the variational method, which further improves the robustness of the network in the presence of image noise, as we show for the Poisson and Gaussian noise models. This framework accords interpretability to the neural network, and the results we obtain show its superiority. Although the paper focuses on the use of single-lobe rotating PSF to encode the full 3D source location, we expect the method to be widely applicable to other PSFs and imaging problems that are constrained by known forward processes.</li>
</ul>

<h3>Title: Fairness-Enhancing Ensemble Classification in Water Distribution Networks</h3>
<ul>
<li><strong>Authors: </strong>Janine Strotherm, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13296">https://arxiv.org/abs/2410.13296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13296">https://arxiv.org/pdf/2410.13296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13296]] Fairness-Enhancing Ensemble Classification in Water Distribution Networks(https://arxiv.org/abs/2410.13296)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>As relevant examples such as the future criminal detection software [1] show, fairness of AI-based and social domain affecting decision support tools constitutes an important area of research. In this contribution, we investigate the applications of AI to socioeconomically relevant infrastructures such as those of water distribution networks (WDNs), where fairness issues have yet to gain a foothold. To establish the notion of fairness in this domain, we propose an appropriate definition of protected groups and group fairness in WDNs as an extension of existing definitions. We demonstrate that typical methods for the detection of leakages in WDNs are unfair in this sense. Further, we thus propose a remedy to increase the fairness which can be applied even to non-differentiable ensemble classification methods as used in this context.</li>
</ul>

<h3>Title: Advancing Large Language Model Attribution through Self-Improving</h3>
<ul>
<li><strong>Authors: </strong>Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13298">https://arxiv.org/abs/2410.13298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13298">https://arxiv.org/pdf/2410.13298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13298]] Advancing Large Language Model Attribution through Self-Improving(https://arxiv.org/abs/2410.13298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttRibuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further self-improve the model's attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13% on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources.</li>
</ul>

<h3>Title: LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>David Hoffmann, Kailash Budhathoki, Matthaeus Kleindessner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13299">https://arxiv.org/abs/2410.13299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13299">https://arxiv.org/pdf/2410.13299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13299]] LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models(https://arxiv.org/abs/2410.13299)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The evolving capabilities of large language models are accompanied by growing sizes and deployment costs, necessitating effective inference optimisation techniques. We propose a novel pruning method utilising centrality measures from graph theory, reducing both the computational requirements and the memory footprint of these models. Specifically, we devise a method for creating a weighted directed acyclical graph representation of multilayer perceptrons to which we apply a modified version of the weighted PageRank centrality measure to compute node importance scores. In combination with uniform pruning this leads to structured sparsity. We call this pruning method MLPRank. Furthermore we introduce an extension to decoder-only transformer models and call it LLMRank. For both variants we demonstrate a strong performance. With MLPRank on average leading to 6.09 % higher accuracy retention than three popular baselines and 13.42 % with LLMRank compared to two popular baselines.</li>
</ul>

<h3>Title: Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for Long-Term Wind Power Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chongyang Wan, Shunbo Lei, Yuan Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13303">https://arxiv.org/abs/2410.13303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13303">https://arxiv.org/pdf/2410.13303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13303]] Hiformer: Hybrid Frequency Feature Enhancement Inverted Transformer for Long-Term Wind Power Prediction(https://arxiv.org/abs/2410.13303)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The increasing severity of climate change necessitates an urgent transition to renewable energy sources, making the large-scale adoption of wind energy crucial for mitigating environmental impact. However, the inherent uncertainty of wind power poses challenges for grid stability, underscoring the need for accurate wind energy prediction models to enable effective power system planning and operation. While many existing studies on wind power prediction focus on short-term forecasting, they often overlook the importance of long-term predictions. Long-term wind power forecasting is essential for effective power grid dispatch and market transactions, as it requires careful consideration of weather features such as wind speed and direction, which directly influence power output. Consequently, methods designed for short-term predictions may lead to inaccurate results and high computational costs in long-term settings. To adress these limitations, we propose a novel approach called Hybrid Frequency Feature Enhancement Inverted Transformer (Hiformer). Hiformer introduces a unique structure that integrates signal decomposition technology with weather feature extraction technique to enhance the modeling of correlations between meteorological conditions and wind power generation. Additionally, Hiformer employs an encoder-only architecture, which reduces the computational complexity associated with long-term wind power forecasting. Compared to the state-of-the-art methods, Hiformer: (i) can improve the prediction accuracy by up to 52.5\%; and (ii) can reduce computational time by up to 68.5\%.</li>
</ul>

<h3>Title: Reference-Based Post-OCR Processing with LLM for Diacritic Languages</h3>
<ul>
<li><strong>Authors: </strong>Thao Do</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13305">https://arxiv.org/abs/2410.13305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13305">https://arxiv.org/pdf/2410.13305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13305]] Reference-Based Post-OCR Processing with LLM for Diacritic Languages(https://arxiv.org/abs/2410.13305)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Extracting fine-grained OCR text from aged documents in diacritic languages remains challenging due to unexpected artifacts, time-induced degradation, and lack of datasets. While standalone spell correction approaches have been proposed, they show limited performance for historical documents due to numerous possible OCR error combinations and differences between modern and classical corpus distributions. We propose a method utilizing available content-focused ebooks as a reference base to correct imperfect OCR-generated text, supported by large language models. This technique generates high-precision pseudo-page-to-page labels for diacritic languages, where small strokes pose significant challenges in historical conditions. The pipeline eliminates various types of noise from aged documents and addresses issues such as missing characters, words, and disordered sequences. Our post-processing method, which generated a large OCR dataset of classical Vietnamese books, achieved a mean grading score of 8.72 on a 10-point scale. This outperformed the state-of-the-art transformer-based Vietnamese spell correction model, which scored 7.03 when evaluated on a sampled subset of the dataset. We also trained a baseline OCR model to assess and compare it with well-known engines. Experimental results demonstrate the strength of our baseline model compared to widely used open-source solutions. The resulting dataset will be released publicly to support future studies.</li>
</ul>

<h3>Title: Precipitation Nowcasting Using Diffusion Transformer with Causal Attention</h3>
<ul>
<li><strong>Authors: </strong>ChaoRong Li, XuDong Ling, YiLan Xue, Wenjie Luo, LiHong Zhu, FengQing Qin, Yaodong Zhou, Yuanyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13314">https://arxiv.org/abs/2410.13314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13314">https://arxiv.org/pdf/2410.13314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13314]] Precipitation Nowcasting Using Diffusion Transformer with Causal Attention(https://arxiv.org/abs/2410.13314)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Short-term precipitation forecasting remains challenging due to the difficulty in capturing long-term spatiotemporal dependencies. Current deep learning methods fall short in establishing effective dependencies between conditions and forecast results, while also lacking interpretability. To address this issue, we propose a Precipitation Nowcasting Using Diffusion Transformer with Causal Attention model. Our model leverages Transformer and combines causal attention mechanisms to establish spatiotemporal queries between conditional information (causes) and forecast results (results). This design enables the model to effectively capture long-term dependencies, allowing forecast results to maintain strong causal relationships with input conditions over a wide range of time and space. We explore four variants of spatiotemporal information interactions for DTCA, demonstrating that global spatiotemporal labeling interactions yield the best performance. In addition, we introduce a Channel-To-Batch shift operation to further enhance the model's ability to represent complex rainfall dynamics. We conducted experiments on two datasets. Compared to state-of-the-art U-Net-based methods, our approach improved the CSI (Critical Success Index) for predicting heavy precipitation by approximately 15% and 8% respectively, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Isack Lee, Haebin Seong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13334">https://arxiv.org/abs/2410.13334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13334">https://arxiv.org/pdf/2410.13334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13334]] Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems(https://arxiv.org/abs/2410.13334)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures.</li>
</ul>

<h3>Title: DiffImp: Efficient Diffusion Model for Probabilistic Time Series Imputation with Bidirectional Mamba Backbone</h3>
<ul>
<li><strong>Authors: </strong>Hongfan Gao, Wangmeng Shen, Xiangfei Qiu, Ronghui Xu, Jilin Hu, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13338">https://arxiv.org/abs/2410.13338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13338">https://arxiv.org/pdf/2410.13338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13338]] DiffImp: Efficient Diffusion Model for Probabilistic Time Series Imputation with Bidirectional Mamba Backbone(https://arxiv.org/abs/2410.13338)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Probabilistic time series imputation has been widely applied in real-world scenarios due to its ability to estimate uncertainty of imputation results. Meanwhile, denoising diffusion probabilistic models (DDPMs) have achieved great success in probabilistic time series imputation tasks with its power to model complex distributions. However, current DDPM-based probabilistic time series imputation methodologies are confronted with two types of challenges: 1)~\textit{~The backbone modules of the denoising parts are not capable of achieving sequence modeling with low time complexity.} 2)~\textit{The architecture of denoising modules can not handle the inter-variable and bidirectional dependencies in the time series imputation problem effectively.} To address the first challenge, we integrate the computational efficient state space model, namely Mamba, as the backbone denosing module for DDPMs. To tackle the second challenge, we carefully devise several SSM-based blocks for bidirectional modeling and inter-variable relation understanding. Experimental results demonstrate that our approach can achieve state-of-the-art time series imputation results on multiple datasets, different missing scenarios and missing ratios.</li>
</ul>

<h3>Title: Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13343">https://arxiv.org/abs/2410.13343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13343">https://arxiv.org/pdf/2410.13343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13343]] Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models(https://arxiv.org/abs/2410.13343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks. However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities. This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs' performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies. Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, significantly impairing their performance. 2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while few-shot prompts generally underperform compared to zero-shot prompts. 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. 5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy. Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts. The code is available at \url {this https URL}.</li>
</ul>

<h3>Title: Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Liu, Wenyuan Li, Laizhong Cui, Hailiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13344">https://arxiv.org/abs/2410.13344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13344">https://arxiv.org/pdf/2410.13344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13344]] Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement(https://arxiv.org/abs/2410.13344)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often face a bottleneck in inference speed due to their reliance on auto-regressive decoding. Recently, parallel decoding has shown significant promise in enhancing inference efficiency. However, we have identified two key issues with existing parallel decoding frameworks: (1) decoding heads fail to balance prediction accuracy and the parallelism of execution, and (2) parallel decoding is not a universal solution, as it can bring unnecessary overheads at some challenging decoding steps. To address these issues, we propose Cerberus, an adaptive parallel decoding framework introduces the gating mechanism to enable the LLMs to adaptively choose appropriate decoding approaches at each decoding step, along with introducing a new paradigm of decoding heads that introduce the sequential knowledge while maintaining execution parallelism. The experiment results demonstrate that the Cerberus can achieve up to 2.12x speed up compared to auto-regressive decoding, and outperforms one of the leading parallel decoding frameworks, Medusa, with a 10% - 30% increase in acceleration and superior generation quality.</li>
</ul>

<h3>Title: GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Shuichang Lai, Letian Huang, Jie Guo, Kai Cheng, Bowen Pan, Xiaoxiao Long, Jiangjing Lyu, Chengfei Lv, Yanwen Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13349">https://arxiv.org/abs/2410.13349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13349">https://arxiv.org/pdf/2410.13349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13349]] GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting(https://arxiv.org/abs/2410.13349)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Reconstructing objects from posed images is a crucial and complex task in computer graphics and computer vision. While NeRF-based neural reconstruction methods have exhibited impressive reconstruction ability, they tend to be time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS) for inverse rendering, which have led to quick and effective outcomes. However, these techniques generally have difficulty in producing believable geometries and materials for glossy objects, a challenge that stems from the inherent ambiguities of inverse rendering. To address this, we introduce GlossyGS, an innovative 3D-GS-based inverse rendering framework that aims to precisely reconstruct the geometry and materials of glossy objects by integrating material priors. The key idea is the use of micro-facet geometry segmentation prior, which helps to reduce the intrinsic ambiguities and improve the decomposition of geometries and materials. Additionally, we introduce a normal map prefiltering strategy to more accurately simulate the normal distribution of reflective surfaces. These strategies are integrated into a hybrid geometry and material representation that employs both explicit and implicit methods to depict glossy objects. We demonstrate through quantitative analysis and qualitative visualization that the proposed method is effective to reconstruct high-fidelity geometries and materials of glossy objects, and performs favorably against state-of-the-arts.</li>
</ul>

<h3>Title: Representation Learning of Structured Data for Medical Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Vijay Prakash Dwivedi, Viktor Schlegel, Andy T. Liu, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Jeng Wei, Wei-Hsian Yin, Stefan Winkler, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13351">https://arxiv.org/abs/2410.13351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13351">https://arxiv.org/pdf/2410.13351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13351]] Representation Learning of Structured Data for Medical Foundation Models(https://arxiv.org/abs/2410.13351)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across various domains, including healthcare. However, their ability to effectively represent structured non-textual data, such as the alphanumeric medical codes used in records like ICD-10 or SNOMED-CT, is limited and has been particularly exposed in recent research. This paper examines the challenges LLMs face in processing medical codes due to the shortcomings of current tokenization methods. As a result, we introduce the UniStruct architecture to design a multimodal medical foundation model of unstructured text and structured data, which addresses these challenges by adapting subword tokenization techniques specifically for the structured medical codes. Our approach is validated through model pre-training on both an extensive internal medical database and a public repository of structured medical records. Trained on over 1 billion tokens on the internal medical database, the proposed model achieves up to a 23% improvement in evaluation metrics, with around 2% gain attributed to our proposed tokenization. Additionally, when evaluated on the EHRSHOT public benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model improves performance on over 42% of the downstream tasks. Our approach not only enhances the representation and generalization capabilities of patient-centric models but also bridges a critical gap in representation learning models' ability to handle complex structured medical data, alongside unstructured text.</li>
</ul>

<h3>Title: LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of the European Court of Human Rights</h3>
<ul>
<li><strong>Authors: </strong>Odysseas S. Chlapanis, Dimitrios Galanis, Ion Androutsopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13352">https://arxiv.org/abs/2410.13352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13352">https://arxiv.org/pdf/2410.13352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13352]] LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of the European Court of Human Rights(https://arxiv.org/abs/2410.13352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Legal Argument Reasoning (LAR), a novel task designed to evaluate the legal reasoning capabilities of Large Language Models (LLMs). The task requires selecting the correct next statement (from multiple choice options) in a chain of legal arguments from court proceedings, given the facts of the case. We constructed a dataset (LAR-ECHR) for this task using cases from the European Court of Human Rights (ECHR). We evaluated seven general-purpose LLMs on LAR-ECHR and found that (a) the ranking of the models is aligned with that of LegalBench, an established US-based legal reasoning benchmark, even though LAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more clearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8% accuracy on LAR-ECHR, indicating significant potential for further model improvement. The process followed to construct LAR-ECHR can be replicated with cases from other legal systems.</li>
</ul>

<h3>Title: Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and Surface Representation</h3>
<ul>
<li><strong>Authors: </strong>Xuezhi Xiang, Xi Wang, Lei Zhang, Denis Ombati, Himaloy Himu, Xiantong Zhen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13355">https://arxiv.org/abs/2410.13355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13355">https://arxiv.org/pdf/2410.13355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13355]] Self-Supervised Scene Flow Estimation with Point-Voxel Fusion and Surface Representation(https://arxiv.org/abs/2410.13355)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Scene flow estimation aims to generate the 3D motion field of points between two consecutive frames of point clouds, which has wide applications in various fields. Existing point-based methods ignore the irregularity of point clouds and have difficulty capturing long-range dependencies due to the inefficiency of point-level computation. Voxel-based methods suffer from the loss of detail information. In this paper, we propose a point-voxel fusion method, where we utilize a voxel branch based on sparse grid attention and the shifted window strategy to capture long-range dependencies and a point branch to capture fine-grained features to compensate for the information loss in the voxel branch. In addition, since xyz coordinates are difficult to describe the geometric structure of complex 3D objects in the scene, we explicitly encode the local surface information of the point cloud through the umbrella surface feature extraction (USFE) module. We verify the effectiveness of our method by conducting experiments on the Flyingthings3D and KITTI datasets. Our method outperforms all other self-supervised methods and achieves highly competitive results compared to fully supervised methods. We achieve improvements in all metrics, especially EPE, which is reduced by 8.51% and 10.52% on the KITTIo and KITTIs datasets, respectively.</li>
</ul>

<h3>Title: Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant</h3>
<ul>
<li><strong>Authors: </strong>Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13360">https://arxiv.org/abs/2410.13360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13360">https://arxiv.org/pdf/2410.13360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13360]] Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant(https://arxiv.org/abs/2410.13360)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at this https URL.</li>
</ul>

<h3>Title: Statistical testing on generative AI anomaly detection tools in Alzheimer's Disease diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Rosemary He, Ichiro Takeuchi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13363">https://arxiv.org/abs/2410.13363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13363">https://arxiv.org/pdf/2410.13363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13363]] Statistical testing on generative AI anomaly detection tools in Alzheimer's Disease diagnosis(https://arxiv.org/abs/2410.13363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease is challenging to diagnose due to our limited understanding of its mechanism and large heterogeneity among patients. Neurodegeneration is studied widely as a biomarker for clinical diagnosis, which can be measured from time series MRI progression. On the other hand, generative AI has shown promise in anomaly detection in medical imaging and used for tasks including tumor detection. However, testing the reliability of such data-driven methods is non-trivial due to the issue of double-dipping in hypothesis testing. In this work, we propose to solve this issue with selective inference and develop a reliable generative AI method for Alzheimer's prediction. We show that compared to traditional statistical methods with highly inflated p-values, selective inference successfully controls the false discovery rate under the desired alpha level while retaining statistical power. In practice, our pipeline could assist clinicians in Alzheimer's diagnosis and early intervention.</li>
</ul>

<h3>Title: MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Donghao Zhou, Jiancheng Huang, Jinbin Bai, Jiaze Wang, Hao Chen, Guangyong Chen, Xiaowei Hu, Pheng-Ann Heng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13370">https://arxiv.org/abs/2410.13370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13370">https://arxiv.org/pdf/2410.13370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13370]] MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models(https://arxiv.org/abs/2410.13370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure specific components when personalizing visual concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of the concept and component. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for desired visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation.</li>
</ul>

<h3>Title: Accurate Checkerboard Corner Detection under Defoucs</h3>
<ul>
<li><strong>Authors: </strong>Zezhun Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13371">https://arxiv.org/abs/2410.13371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13371">https://arxiv.org/pdf/2410.13371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13371]] Accurate Checkerboard Corner Detection under Defoucs(https://arxiv.org/abs/2410.13371)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Camera calibration is a critical process in 3D vision, im pacting applications in autonomous driving, robotics, ar chitecture, and so on. This paper focuses on enhancing feature extraction for chessboard corner detection, a key step in calibration. We analyze existing methods, high lighting their limitations and propose a novel sub-pixel refinement approach based on symmetry, which signifi cantly improves accuracy for visible light cameras. Un like prior symmetry based method that assume a contin uous physical pattern, our approach accounts for abrupt changes in visible light camera images and defocus ef fects. We introduce a simplified objective function that reduces computation time and mitigates overfitting risks. Furthermore, we derive an explicit expression for the pixel value of a blurred edge, providing insights into the relationship between pixel value and center intensity. Our method demonstrates superior performance, achiev ing substantial accuracy improvements over existing tech niques, particularly in the context of visible light cam era calibration. Our code is available from https: //github.com/spdfghi/Accurate-Checkerboard this http URL.</li>
</ul>

<h3>Title: Railway LiDAR semantic segmentation based on intelligent semi-automated data annotation</h3>
<ul>
<li><strong>Authors: </strong>Florian Wulff, Bernd Schaeufele, Julian Pfeifer, Ilja Radusch</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13383">https://arxiv.org/abs/2410.13383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13383">https://arxiv.org/pdf/2410.13383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13383]] Railway LiDAR semantic segmentation based on intelligent semi-automated data annotation(https://arxiv.org/abs/2410.13383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Automated vehicles rely on an accurate and robust perception of the environment. Similarly to automated cars, highly automated trains require an environmental perception. Although there is a lot of research based on either camera or LiDAR sensors in the automotive domain, very few contributions for this task exist yet for automated trains. Additionally, no public dataset or described approach for a 3D LiDAR semantic segmentation in the railway environment exists yet. Thus, we propose an approach for a point-wise 3D semantic segmentation based on the 2DPass network architecture using scans and images jointly. In addition, we present a semi-automated intelligent data annotation approach, which we use to efficiently and accurately label the required dataset recorded on a railway track in Germany. To improve performance despite a still small number of labeled scans, we apply an active learning approach to intelligently select scans for the training dataset. Our contributions are threefold: We annotate rail data including camera and LiDAR data from the railway environment, transfer label the raw LiDAR point clouds using an image segmentation network, and train a state-of-the-art 3D LiDAR semantic segmentation network efficiently leveraging active learning. The trained network achieves good segmentation results with a mean IoU of 71.48% of 9 classes.</li>
</ul>

<h3>Title: RescueADI: Adaptive Disaster Interpretation in Remote Sensing Images with Autonomous Agents</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Liu, Danpei Zhao, Bo Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13384">https://arxiv.org/abs/2410.13384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13384">https://arxiv.org/pdf/2410.13384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13384]] RescueADI: Adaptive Disaster Interpretation in Remote Sensing Images with Autonomous Agents(https://arxiv.org/abs/2410.13384)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Current methods for disaster scene interpretation in remote sensing images (RSIs) mostly focus on isolated tasks such as segmentation, detection, or visual question-answering (VQA). However, current interpretation methods often fail at tasks that require the combination of multiple perception methods and specialized tools. To fill this gap, this paper introduces Adaptive Disaster Interpretation (ADI), a novel task designed to solve requests by planning and executing multiple sequentially correlative interpretation tasks to provide a comprehensive analysis of disaster scenes. To facilitate research and application in this area, we present a new dataset named RescueADI, which contains high-resolution RSIs with annotations for three connected aspects: planning, perception, and recognition. The dataset includes 4,044 RSIs, 16,949 semantic masks, 14,483 object bounding boxes, and 13,424 interpretation requests across nine challenging request types. Moreover, we propose a new disaster interpretation method employing autonomous agents driven by large language models (LLMs) for task planning and execution, proving its efficacy in handling complex disaster interpretations. The proposed agent-based method solves various complex interpretation requests such as counting, area calculation, and path-finding without human intervention, which traditional single-task approaches cannot handle effectively. Experimental results on RescueADI demonstrate the feasibility of the proposed task and show that our method achieves an accuracy 9% higher than existing VQA methods, highlighting its advantages over conventional disaster interpretation approaches. The dataset will be publicly available.</li>
</ul>

<h3>Title: A Self-Constructing Multi-Expert Fuzzy System for High-dimensional Data Classification</h3>
<ul>
<li><strong>Authors: </strong>Yingtao Ren, Yu-Cheng Chang, Thomas Do, Zehong Cao, Chin-Teng Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13390">https://arxiv.org/abs/2410.13390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13390">https://arxiv.org/pdf/2410.13390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13390]] A Self-Constructing Multi-Expert Fuzzy System for High-dimensional Data Classification(https://arxiv.org/abs/2410.13390)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fuzzy Neural Networks (FNNs) are effective machine learning models for classification tasks, commonly based on the Takagi-Sugeno-Kang (TSK) fuzzy system. However, when faced with high-dimensional data, especially with noise, FNNs encounter challenges such as vanishing gradients, excessive fuzzy rules, and limited access to prior knowledge. To address these challenges, we propose a novel fuzzy system, the Self-Constructing Multi-Expert Fuzzy System (SOME-FS). It combines two learning strategies: mixed structure learning and multi-expert advanced learning. The former enables each base classifier to effectively determine its structure without requiring prior knowledge, while the latter tackles the issue of vanishing gradients by enabling each rule to focus on its local region, thereby enhancing the robustness of the fuzzy classifiers. The overall ensemble architecture enhances the stability and prediction performance of the fuzzy system. Our experimental results demonstrate that the proposed SOME-FS is effective in high-dimensional tabular data, especially in dealing with uncertainty. Moreover, our stable rule mining process can identify concise and core rules learned by the SOME-FS.</li>
</ul>

<h3>Title: Metacognitive Monitoring: A Human Ability Beyond Generative Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Markus Huff, Elanur Ulakçı</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13392">https://arxiv.org/abs/2410.13392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13392">https://arxiv.org/pdf/2410.13392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13392]] Metacognitive Monitoring: A Human Ability Beyond Generative Artificial Intelligence(https://arxiv.org/abs/2410.13392)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive alignment with human cognitive processes, raising questions about the extent of their similarity to human cognition. This study investigates whether LLMs, specifically ChatGPT, possess metacognitive monitoring abilities akin to humans-particularly in predicting memory performance on an item-by-item basis. We employed a cross-agent prediction model to compare the metacognitive performance of humans and ChatGPT in a language-based memory task involving garden-path sentences preceded by either fitting or unfitting context sentences. Both humans and ChatGPT rated the memorability of these sentences; humans then completed a surprise recognition memory test. Our findings reveal a significant positive relationship between humans' memorability ratings and their actual recognition performance, indicating reliable metacognitive monitoring. In contrast, ChatGPT did not exhibit a similar predictive capability. Bootstrapping analyses demonstrated that none of the GPT models tested (GPT-3.5-turbo, GPT-4-turbo, GPT-4o) could accurately predict human memory performance on a per-item basis. This suggests that, despite their advanced language processing abilities and alignment with human cognition at the object level, current LLMs lack the metacognitive mechanisms that enable humans to anticipate their memory performance. These results highlight a fundamental difference between human and AI cognition at the metacognitive level. Addressing this gap is crucial for developing AI systems capable of effective self-monitoring and adaptation to human needs, thereby enhancing human-AI interactions across domains such as education and personalized learning.</li>
</ul>

<h3>Title: Linguistically Grounded Analysis of Language Models using Shapley Head Values</h3>
<ul>
<li><strong>Authors: </strong>Marcell Fekete, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13396">https://arxiv.org/abs/2410.13396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13396">https://arxiv.org/pdf/2410.13396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13396]] Linguistically Grounded Analysis of Language Models using Shapley Head Values(https://arxiv.org/abs/2410.13396)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how linguistic knowledge is encoded in language models is crucial for improving their generalisation capabilities. In this paper, we investigate the processing of morphosyntactic phenomena, by leveraging a recently proposed method for probing language models via Shapley Head Values (SHVs). Using the English language BLiMP dataset, we test our approach on two widely used models, BERT and RoBERTa, and compare how linguistic constructions such as anaphor agreement and filler-gap dependencies are handled. Through quantitative pruning and qualitative clustering analysis, we demonstrate that attention heads responsible for processing related linguistic phenomena cluster together. Our results show that SHV-based attributions reveal distinct patterns across both models, providing insights into how language models organize and process linguistic information. These findings support the hypothesis that language models learn subnetworks corresponding to linguistic theory, with potential implications for cross-linguistic model analysis and interpretability in Natural Language Processing (NLP).</li>
</ul>

<h3>Title: Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Sirui Xia, Haoran Guo, Jiaqing Liang, Zulong Chen, Liangyue Li, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13413">https://arxiv.org/abs/2410.13413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13413">https://arxiv.org/pdf/2410.13413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13413]] Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models(https://arxiv.org/abs/2410.13413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have demonstrated that progressive refinement, rather than providing a single answer, results in more accurate and thoughtful outputs. However, existing methods often rely heavily on supervision signals to evaluate previous responses, making it difficult to assess output quality in more open-ended scenarios effectively. Additionally, these methods are typically designed for specific tasks, which limits their generalization to new domains. To address these limitations, we propose Progressive Thought Refinement (PTR), a framework that enables LLMs to refine their responses progressively. PTR operates in two phases: (1) Thought data construction stage: We propose a weak and strong model collaborative selection strategy to build a high-quality progressive refinement dataset to ensure logical consistency from thought to answers, and the answers are gradually refined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training structure to mask the "thought" and adjust loss weights to encourage LLMs to refine prior thought, teaching them to implicitly understand "how to improve" rather than "what is correct." Experimental results show that PTR significantly enhances LLM performance across ten diverse tasks (avg. from 49.6% to 53.5%) without task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also demonstrate substantial improvements in the quality of responses beyond mere accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.</li>
</ul>

<h3>Title: Solving Prior Distribution Mismatch in Diffusion Models via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Zhanpeng Wang, Shenghao Li, Chen Wang, Shuting Cao, Na Lei, Zhongxuan Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13431">https://arxiv.org/abs/2410.13431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13431">https://arxiv.org/pdf/2410.13431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13431]] Solving Prior Distribution Mismatch in Diffusion Models via Optimal Transport(https://arxiv.org/abs/2410.13431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, the knowledge surrounding diffusion models(DMs) has grown significantly, though several theoretical gaps remain. Particularly noteworthy is prior error, defined as the discrepancy between the termination distribution of the forward process and the initial distribution of the reverse process. To address these deficiencies, this paper explores the deeper relationship between optimal transport(OT) theory and DMs with discrete initial distribution. Specifically, we demonstrate that the two stages of DMs fundamentally involve computing time-dependent OT. However, unavoidable prior error result in deviation during the reverse process under quadratic transport cost. By proving that as the diffusion termination time increases, the probability flow exponentially converges to the gradient of the solution to the classical Monge-Ampère equation, we establish a vital link between these fields. Therefore, static OT emerges as the most intrinsic single-step method for bridging this theoretical potential gap. Additionally, we apply these insights to accelerate sampling in both unconditional and conditional generation scenarios. Experimental results across multiple image datasets validate the effectiveness of our approach.</li>
</ul>

<h3>Title: Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changcheng Xiao, Qiong Cao, Yujie Zhong, Xiang Zhang, Tao Wang, Canqun Yang, Long Lan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13437">https://arxiv.org/abs/2410.13437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13437">https://arxiv.org/pdf/2410.13437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13437]] Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation(https://arxiv.org/abs/2410.13437)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to locate an arbitrary number of target objects and maintain their identities referred by a language expression in a video. This intricate task involves the reasoning of linguistic and visual modalities, along with the temporal association of target objects. However, the seminal work employs only loose feature fusion and overlooks the utilization of long-term information on tracked objects. In this study, we introduce a compact Transformer-based method, termed TenRMOT. We conduct feature fusion at both encoding and decoding stages to fully exploit the advantages of Transformer architecture. Specifically, we incrementally perform cross-modal fusion layer-by-layer during the encoding phase. In the decoding phase, we utilize language-guided queries to probe memory features for accurate prediction of the desired objects. Moreover, we introduce a query update module that explicitly leverages temporal prior information of the tracked objects to enhance the consistency of their trajectories. In addition, we introduce a novel task called Referring Multi-Object Tracking and Segmentation (RMOTS) and construct a new dataset named Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818 expressions, and each expression averages 10.7 masks, which poses a greater challenge compared to the typical single mask in most existing referring video segmentation datasets. TenRMOT demonstrates superior performance on both the referring multi-object tracking and the segmentation tasks.</li>
</ul>

<h3>Title: Similarity-Dissimilarity Loss with Supervised Contrastive Learning for Multi-label Classification</h3>
<ul>
<li><strong>Authors: </strong>Guangming Huang, Yunfei Long, Cunjin Luo, Sheng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13439">https://arxiv.org/abs/2410.13439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13439">https://arxiv.org/pdf/2410.13439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13439]] Similarity-Dissimilarity Loss with Supervised Contrastive Learning for Multi-label Classification(https://arxiv.org/abs/2410.13439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Supervised contrastive learning has been explored in making use of label information for multi-label classification, but determining positive samples in multi-label scenario remains challenging. Previous studies have examined strategies for identifying positive samples, considering label overlap proportion between anchors and samples. However, they ignore various relations between given anchors and samples, as well as how to dynamically adjust the weights in contrastive loss functions based on different relations, leading to great ambiguity. In this paper, we introduce five distinct relations between multi-label samples and propose a Similarity-Dissimilarity Loss with contrastive learning for multi-label classification. Our loss function re-weights the loss by computing the similarity and dissimilarity between positive samples and a given anchor based on the introduced relations. We mainly conduct experiments for multi-label text classification on MIMIC datasets, then further extend the evaluation on MS-COCO. The Experimental results show that our proposed loss effectively improves the performance on all encoders under supervised contrastive learning paradigm, demonstrating its effectiveness and robustness.</li>
</ul>

<h3>Title: Augmentation Policy Generation for Image Classification Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ant Duru, Alptekin Temizel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13453">https://arxiv.org/abs/2410.13453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13453">https://arxiv.org/pdf/2410.13453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13453]] Augmentation Policy Generation for Image Classification Using Large Language Models(https://arxiv.org/abs/2410.13453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Automated data augmentation methods have significantly improved the performance and generalization capability of deep learning models in image classification. Yet, most state-of-the-art methods are optimized on common benchmark datasets, limiting their applicability to more diverse or domain-specific data, such as medical datasets. In this paper, we propose a strategy that uses large language models to automatically generate efficient augmentation policies, customized to fit the specific characteristics of any dataset and model architecture. The proposed method iteratively interacts with an LLM to obtain and refine the augmentation policies on model performance feedback, creating a dataset-agnostic data augmentation pipeline. The proposed method was evaluated on medical imaging datasets, showing a clear improvement over state-of-the-art methods. The proposed approach offers an adaptive and scalable solution. Although it increases computational cost, it significantly boosts model robustness, automates the process, and minimizes the need for human involvement during model development.</li>
</ul>

<h3>Title: MedINST: Meta Dataset of Biomedical Instructions</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Han, Meng Fang, Zihan Zhang, Yu Yin, Zirui Song, Ling Chen, Mykola Pechenizkiy, Qingyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13458">https://arxiv.org/abs/2410.13458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13458">https://arxiv.org/pdf/2410.13458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13458]] MedINST: Meta Dataset of Biomedical Instructions(https://arxiv.org/abs/2410.13458)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains a major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MedINST as the meta dataset, we curate MedINST32, a challenging benchmark with different task difficulties aiming to evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and evaluate on MedINST32, showcasing enhanced cross-task generalization.</li>
</ul>

<h3>Title: Breaking the Manual Annotation Bottleneck: Creating a Comprehensive Legal Case Criticality Dataset through Semi-Automated Labeling</h3>
<ul>
<li><strong>Authors: </strong>Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13460">https://arxiv.org/abs/2410.13460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13460">https://arxiv.org/pdf/2410.13460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13460]] Breaking the Manual Annotation Bottleneck: Creating a Comprehensive Legal Case Criticality Dataset through Semi-Automated Labeling(https://arxiv.org/abs/2410.13460)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predicting case criticality helps legal professionals in the court system manage large volumes of case law. This paper introduces the Criticality Prediction dataset, a new resource for evaluating the potential influence of Swiss Federal Supreme Court decisions on future jurisprudence. Unlike existing approaches that rely on resource-intensive manual annotations, we semi-automatically derive labels leading to a much larger dataset than otherwise possible. Our dataset features a two-tier labeling system: (1) the LD-Label, which identifies cases published as Leading Decisions (LD), and (2) the Citation-Label, which ranks cases by their citation frequency and recency. This allows for a more nuanced evaluation of case importance. We evaluate several multilingual models, including fine-tuned variants and large language models, and find that fine-tuned models consistently outperform zero-shot baselines, demonstrating the need for task-specific adaptation. Our contributions include the introduction of this task and the release of a multilingual dataset to the research community.</li>
</ul>

<h3>Title: Progressive Mixed-Precision Decoding for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13461">https://arxiv.org/abs/2410.13461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13461">https://arxiv.org/pdf/2410.13461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13461]] Progressive Mixed-Precision Decoding for Efficient LLM Inference(https://arxiv.org/abs/2410.13461)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In spite of the great potential of large language models (LLMs) across various tasks, their deployment on resource-constrained devices remains challenging due to their excessive computational and memory demands. Quantization has emerged as an effective solution by storing weights in reduced precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially alleviate the memory-boundedness of LLM decoding, still suffers from prohibitive performance drop. In this work, we argue that existing approaches fail to explore the diversity in computational patterns, redundancy, and sensitivity to approximations of the different phases of LLM inference, resorting to a uniform quantization policy throughout. Instead, we propose a novel phase-aware method that selectively allocates precision during different phases of LLM inference, achieving both strong context extraction during prefill and efficient memory bandwidth utilization during decoding. To further address the memory-boundedness of the decoding phase, we introduce Progressive Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering of precision deeper in the generated sequence, together with a spectrum of precision-switching schedulers that dynamically drive the precision-lowering decisions in either task-adaptive or prompt-adaptive manner. Extensive evaluation across diverse language tasks shows that when targeting Nvidia GPUs, PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$ over uniform quantization approaches while preserving the output quality.</li>
</ul>

<h3>Title: Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Poiani, Nicole Nobili, Alberto Maria Metelli, Marcello Restelli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13463">https://arxiv.org/abs/2410.13463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13463">https://arxiv.org/pdf/2410.13463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13463]] Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach(https://arxiv.org/abs/2410.13463)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy evaluation via Monte Carlo (MC) simulation is at the core of many MC Reinforcement Learning (RL) algorithms (e.g., policy gradient methods). In this context, the designer of the learning system specifies an interaction budget that the agent usually spends by collecting trajectories of fixed length within a simulator. However, is this data collection strategy the best option? To answer this question, in this paper, we propose as a quality index a surrogate of the mean squared error of a return estimator that uses trajectories of different lengths, i.e., \emph{truncated}. Specifically, this surrogate shows the sub-optimality of the fixed-length trajectory schedule. Furthermore, it suggests that adaptive data collection strategies that spend the available budget sequentially can allocate a larger portion of transitions in timesteps in which more accurate sampling is required to reduce the error of the final estimate. Building on these findings, we present an adaptive algorithm called Robust and Iterative Data collection strategy Optimization (RIDO). The main intuition behind RIDO is to split the available interaction budget into mini-batches. At each round, the agent determines the most convenient schedule of trajectories that minimizes an empirical and robust version of the surrogate of the estimator's error. After discussing the theoretical properties of our method, we conclude by assessing its performance across multiple domains. Our results show that RIDO can adapt its trajectory schedule toward timesteps where more sampling is required to increase the quality of the final estimation.</li>
</ul>

<h3>Title: IterSelectTune: An Iterative Training Framework for Efficient Instruction-Tuning Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Jielin Song, Siyu Liu, Bin Zhu, Yanghui Rao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13464">https://arxiv.org/abs/2410.13464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13464">https://arxiv.org/pdf/2410.13464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13464]] IterSelectTune: An Iterative Training Framework for Efficient Instruction-Tuning Data Selection(https://arxiv.org/abs/2410.13464)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, instruction tuning has become critical for improving their ability to generate accurate and contextually appropriate responses. Although numerous instruction-tuning datasets have been developed to enhance LLM performance, selecting high-quality instruction data from large source datasets typically demands significant human effort. In this work, we introduce $\textbf{IterSelectTune}$, an efficient, cost-effective iterative training policy for selecting high-quality instruction data with no human involvement and limited reliance on GPT-4. By fine-tuning on approximately 20\% of the source data, our method consistently outperforms models fine-tuned on the full dataset across multiple benchmarks and public test datasets. These results highlight the effectiveness of our approach in enhancing LLM performance while reducing the computational resources required for instruction tuning.</li>
</ul>

<h3>Title: Interpreting Temporal Graph Neural Networks with Koopman Theory</h3>
<ul>
<li><strong>Authors: </strong>Michele Guerra, Simone Scardapane, Filippo Maria Bianchi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13469">https://arxiv.org/abs/2410.13469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13469">https://arxiv.org/pdf/2410.13469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13469]] Interpreting Temporal Graph Neural Networks with Koopman Theory(https://arxiv.org/abs/2410.13469)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Spatiotemporal graph neural networks (STGNNs) have shown promising results in many domains, from forecasting to epidemiology. However, understanding the dynamics learned by these models and explaining their behaviour is significantly more complex than for models dealing with static data. Inspired by Koopman theory, which allows a simpler description of intricate, nonlinear dynamical systems, we introduce an explainability approach for temporal graphs. We present two methods to interpret the STGNN's decision process and identify the most relevant spatial and temporal patterns in the input for the task at hand. The first relies on dynamic mode decomposition (DMD), a Koopman-inspired dimensionality reduction method. The second relies on sparse identification of nonlinear dynamics (SINDy), a popular method for discovering governing equations, which we use for the first time as a general tool for explainability. We show how our methods can correctly identify interpretable features such as infection times and infected nodes in the context of dissemination processes.</li>
</ul>

<h3>Title: SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain Adaptation in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Bin Wang, Fei Deng, Shuang Wang, Wen Luo, Zhixuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13471">https://arxiv.org/abs/2410.13471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13471">https://arxiv.org/pdf/2410.13471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13471]] SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain Adaptation in Remote Sensing(https://arxiv.org/abs/2410.13471)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of remote sensing (RS) images is a challenging task with significant potential across various applications. Deep learning, especially supervised learning with large-scale labeled datasets, has greatly advanced this field. However, acquiring high-quality labeled data is expensive and time-consuming. Moreover, variations in ground sampling distance (GSD), imaging equipment, and geographic diversity contribute to domain shifts between datasets, which pose significant challenges to models trained solely on source domain data, leading to poor cross-domain performance. Domain shift is well-known for undermining a model's generalization ability in the target domain. To address this, unsupervised domain adaptation (UDA) has emerged as a promising solution, enabling models to learn from unlabeled target domain data while training on labeled source domain data. Recent advancements, particularly in self-supervised learning via pseudo-label generation, have shown potential in mitigating domain discrepancies. Strategies combining source and target domain images with their true and pseudo labels for self-supervised training have been effective in addressing domain bias. Despite progress in computer vision, the application of pseudo-labeling methods to RS image segmentation remains underexplored.</li>
</ul>

<h3>Title: Day-Night Adaptation: An Innovative Source-free Adaptation Framework for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13472">https://arxiv.org/abs/2410.13472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13472">https://arxiv.org/pdf/2410.13472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13472]] Day-Night Adaptation: An Innovative Source-free Adaptation Framework for Medical Image Segmentation(https://arxiv.org/abs/2410.13472)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, segmentation</a></li>
<li><strong>Abstract: </strong>Distribution shifts widely exist in medical images acquired from different medical centers, hindering the deployment of semantic segmentation models trained on data from one center (source domain) to another (target domain). While unsupervised domain adaptation (UDA) has shown significant promise in mitigating these shifts, it poses privacy risks due to sharing data between centers. To facilitate adaptation while preserving data privacy, source-free domain adaptation (SFDA) and test-time adaptation (TTA) have emerged as effective paradigms, relying solely on target domain data. However, the scenarios currently addressed by SFDA and TTA are limited, making them less suitable for clinical applications. In a more realistic clinical scenario, the pre-trained model is deployed in a medical centre to assist with clinical tasks during the day and rest at night. During the daytime process, TTA can be employed to enhance inference performance. During the nighttime process, after collecting the test data from the day, the model can be fine-tuned utilizing SFDA to further adapt to the target domain. With above insights, we propose a novel adaptation framework called Day-Night Adaptation (DyNA). This framework adapts the model to the target domain through day-night loops without requiring access to source data. Specifically, we implement distinct adaptation strategies for daytime and nighttime to better meet the demands of clinical settings. During the daytime, model parameters are frozen, and a specific low-frequency prompt is trained for each test sample. Additionally, we construct a memory bank for prompt initialization and develop a warm-up mechanism to enhance prompt training. During nighttime, we integrate a global student model into the traditional teacher-student self-training paradigm to fine-tune the model while ensuring training stability...</li>
</ul>

<h3>Title: SemSim: Revisiting Weak-to-Strong Consistency from a Semantic Similarity Perspective for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shiao Xie, Hongyi Wang, Ziwei Niu, Hao Sun, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13486">https://arxiv.org/abs/2410.13486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13486">https://arxiv.org/pdf/2410.13486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13486]] SemSim: Revisiting Weak-to-Strong Consistency from a Semantic Similarity Perspective for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2410.13486)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) for medical image segmentation is a challenging yet highly practical task, which reduces reliance on large-scale labeled dataset by leveraging unlabeled samples. Among SSL techniques, the weak-to-strong consistency framework, popularized by FixMatch, has emerged as a state-of-the-art method in classification tasks. Notably, such a simple pipeline has also shown competitive performance in medical image segmentation. However, two key limitations still persist, impeding its efficient adaptation: (1) the neglect of contextual dependencies results in inconsistent predictions for similar semantic features, leading to incomplete object segmentation; (2) the lack of exploitation of semantic similarity between labeled and unlabeled data induces considerable class-distribution discrepancy. To address these limitations, we propose a novel semi-supervised framework based on FixMatch, named SemSim, powered by two appealing designs from semantic similarity perspective: (1) rectifying pixel-wise prediction by reasoning about the intra-image pair-wise affinity map, thus integrating contextual dependencies explicitly into the final prediction; (2) bridging labeled and unlabeled data via a feature querying mechanism for compact class representation learning, which fully considers cross-image anatomical similarities. As the reliable semantic similarity extraction depends on robust features, we further introduce an effective spatial-aware fusion module (SFM) to explore distinctive information from multiple scales. Extensive experiments show that SemSim yields consistent improvements over the state-of-the-art methods across three public segmentation benchmarks.</li>
</ul>

<h3>Title: Breaking Bad: How Compilers Break Constant-Time~Implementations</h3>
<ul>
<li><strong>Authors: </strong>Moritz Schneider, Daniele Lain, Ivan Puddu, Nicolas Dutly, Srdjan Capkun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13489">https://arxiv.org/abs/2410.13489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13489">https://arxiv.org/pdf/2410.13489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13489]] Breaking Bad: How Compilers Break Constant-Time~Implementations(https://arxiv.org/abs/2410.13489)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>The implementations of most hardened cryptographic libraries use defensive programming techniques for side-channel resistance. These techniques are usually specified as guidelines to developers on specific code patterns to use or avoid. Examples include performing arithmetic operations to choose between two variables instead of executing a secret-dependent branch. However, such techniques are only meaningful if they persist across compilation. In this paper, we investigate how optimizations used by modern compilers break the protections introduced by defensive programming techniques. Specifically, how compilers break high-level constant-time implementations used to mitigate timing side-channel attacks. We run a large-scale experiment to see if such compiler-induced issues manifest in state-of-the-art cryptographic libraries. We develop a tool that can profile virtually any architecture, and we use it to run trace-based dynamic analysis on 44,604 different targets. Particularly, we focus on the most widely deployed cryptographic libraries, which aim to provide side-channel resistance. We are able to evaluate whether their claims hold across various CPU architectures, including x86-64, x86-i386, armv7, aarch64, RISC-V, and MIPS-32. Our large-scale study reveals that several compiler-induced secret-dependent operations occur within some of the most highly regarded hardened cryptographic libraries. To the best of our knowledge, such findings represent the first time these issues have been observed in the wild. One of the key takeaways of this paper is that the state-of-the-art defensive programming techniques employed for side-channel resistance are still inadequate, incomplete, and bound to fail when paired with the optimizations that compilers continuously introduce.</li>
</ul>

<h3>Title: Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum Learning, Semi-Supervised Training, and Advanced Optimization Techniques</h3>
<ul>
<li><strong>Authors: </strong>Rahimanuddin Shaik, Katikela Sreeharsha Kishore</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13498">https://arxiv.org/abs/2410.13498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13498">https://arxiv.org/pdf/2410.13498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13498]] Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum Learning, Semi-Supervised Training, and Advanced Optimization Techniques(https://arxiv.org/abs/2410.13498)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Text generation is the automated process of producing written or spoken language using computational methods. It involves generating coherent and contextually relevant text based on predefined rules or learned patterns. However, challenges in text generation arise from maintaining coherence, ensuring diversity and creativity, and avoiding biases or inappropriate content. This research paper developed a novel approach to improve text generation in the context of joint Natural Language Generation (NLG) and Natural Language Understanding (NLU) learning. The data is prepared by gathering and preprocessing annotated datasets, including cleaning, tokenization, stemming, and stop-word removal. Feature extraction techniques such as POS tagging, Bag of words, and Term Frequency-Inverse Document Frequency (TF-IDF) are applied. Transformer-based encoders and decoders, capturing long range dependencies and improving source-target sequence modelling. Pre-trained language models like Optimized BERT are incorporated, along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA). Reinforcement learning with policy gradient techniques, semi-supervised training, improved attention mechanisms, and differentiable approximations like straight-through Gumbel SoftMax estimator are employed to fine-tune the models and handle complex linguistic tasks effectively. The proposed model is implemented using Python.</li>
</ul>

<h3>Title: Integrating Large Language Models and Reinforcement Learning for Non-Linear Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yoav Alon, Cristina David</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13501">https://arxiv.org/abs/2410.13501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13501">https://arxiv.org/pdf/2410.13501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13501]] Integrating Large Language Models and Reinforcement Learning for Non-Linear Reasoning(https://arxiv.org/abs/2410.13501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) were shown to struggle with long-term planning, which may be caused by the limited way in which they explore the space of possible solutions. We propose an architecture where a Reinforcement Learning (RL) Agent guides an LLM's space exploration: (1) the Agent has access to domain-specific information, and can therefore make decisions about the quality of candidate solutions based on specific and relevant metrics, which were not explicitly considered by the LLM's training objective; (2) the LLM can focus on generating immediate next steps, without the need for long-term planning. We allow non-linear reasoning by exploring alternative paths and backtracking. We evaluate this architecture on the program equivalence task, and compare it against Chain of Thought (CoT) and Tree of Thoughts (ToT). We assess both the downstream task, denoting the binary classification, and the intermediate reasoning steps. Our approach compares positively against CoT and ToT.</li>
</ul>

<h3>Title: MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs</h3>
<ul>
<li><strong>Authors: </strong>Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13502">https://arxiv.org/abs/2410.13502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13502">https://arxiv.org/pdf/2410.13502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13502]] MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs(https://arxiv.org/abs/2410.13502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can solve arithmetic word problems with high accuracy, but little is known about how well they generalize to problems that are more complex than the ones on which they have been trained. Empirical investigations of such questions are impeded by two major flaws of current evaluations: (i) much of the evaluation data is contaminated, in the sense that it has already been seen during training, and (ii) benchmark datasets do not capture how problem proofs may be arbitrarily complex in various ways. As a step towards addressing these issues, we present a framework for evaluating LLMs on problems that have arbitrarily complex arithmetic proofs, called MathGAP. MathGAP generates problems that follow fixed proof specifications -- along with chain-of-thought reasoning annotations -- enabling systematic studies on generalization with respect to arithmetic proof complexity. We apply MathGAP to analyze how in-context learning interacts with generalization to problems that have more complex proofs. We find that among the models tested, most show a significant decrease in performance as proofs get deeper and wider. This effect is more pronounced in complex, nonlinear proof structures, which are challenging even for GPT-4o. Surprisingly, providing in-context examples from the same distribution as the test set is not always beneficial for performance. In particular, zero-shot prompting as well as demonstrating a diverse range of examples that are less complex than the test data sometimes yield similar or higher accuracies.</li>
</ul>

<h3>Title: RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards</h3>
<ul>
<li><strong>Authors: </strong>Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13509">https://arxiv.org/abs/2410.13509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13509">https://arxiv.org/pdf/2410.13509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13509]] RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards(https://arxiv.org/abs/2410.13509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for RAG pipelines, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent with a rollout method. This method prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at this https URL.</li>
</ul>

<h3>Title: Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?</h3>
<ul>
<li><strong>Authors: </strong>Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13517">https://arxiv.org/abs/2410.13517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13517">https://arxiv.org/pdf/2410.13517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13517]] Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?(https://arxiv.org/abs/2410.13517)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) inherit biases from their training data and alignment processes, influencing their responses in subtle ways. While many studies have examined these biases, little work has explored their robustness during interactions. In this paper, we introduce a novel approach where two instances of an LLM engage in self-debate, arguing opposing viewpoints to persuade a neutral version of the model. Through this, we evaluate how firmly biases hold and whether models are susceptible to reinforcing misinformation or shifting to harmful viewpoints. Our experiments span multiple LLMs of varying sizes, origins, and languages, providing deeper insights into bias persistence and flexibility across linguistic and cultural contexts.</li>
</ul>

<h3>Title: Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</h3>
<ul>
<li><strong>Authors: </strong>Che Liu, Zhongwei Wan, Haozhe Wang, Yinda Chen, Talha Qaiser, Chen Jin, Fariba Yousefi, Nikolay Burlutskiy, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13523">https://arxiv.org/abs/2410.13523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13523">https://arxiv.org/pdf/2410.13523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13523]] Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?(https://arxiv.org/abs/2410.13523)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: *Can MedVLP succeed using purely synthetic data?* To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective. Our results show that MedVLP models trained *exclusively on synthetic data* outperform those trained on real data by **3.8%** in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of **9.07%**. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks. Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions.</li>
</ul>

<h3>Title: Generative Adversarial Synthesis of Radar Point Cloud Scenes</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Saad Nawaz, Thomas Dallmann, Torsten Schoen, Dirk Heberling</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13526">https://arxiv.org/abs/2410.13526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13526">https://arxiv.org/pdf/2410.13526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13526]] Generative Adversarial Synthesis of Radar Point Cloud Scenes(https://arxiv.org/abs/2410.13526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>For the validation and verification of automotive radars, datasets of realistic traffic scenarios are required, which, how ever, are laborious to acquire. In this paper, we introduce radar scene synthesis using GANs as an alternative to the real dataset acquisition and simulation-based approaches. We train a PointNet++ based GAN model to generate realistic radar point cloud scenes and use a binary classifier to evaluate the performance of scenes generated using this model against a test set of real scenes. We demonstrate that our GAN model achieves similar performance (~87%) to the real scenes test set.</li>
</ul>

<h3>Title: A Construction of Evolving $3$-threshold Secret Sharing Scheme with Perfect Security and Smaller Share Size</h3>
<ul>
<li><strong>Authors: </strong>Qi Cheng, Hongru Cao, Sian-Jheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13529">https://arxiv.org/abs/2410.13529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13529">https://arxiv.org/pdf/2410.13529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13529]] A Construction of Evolving $3$-threshold Secret Sharing Scheme with Perfect Security and Smaller Share Size(https://arxiv.org/abs/2410.13529)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The evolving $k$-threshold secret sharing scheme allows the dealer to distribute the secret to many participants such that only no less than $k$ shares together can restore the secret. In contrast to the conventional secret sharing scheme, the evolving scheme allows the number of participants to be uncertain and even ever-growing. In this paper, we consider the evolving secret sharing scheme with $k=3$. First, we point out that the prior approach has risks in the security. To solve this issue, we then propose a new evolving $3$-threshold scheme with perfect security. Given a $\ell$-bit secret, the $t$-th share of the proposed scheme has $\lceil\log_2 t\rceil +O({\lceil \log_4 \log_2 t\rceil}^2)+\log_2 p(2\lceil \log_4 \log_2 t\rceil-1)$ bits, where $p$ is a prime. Compared with the prior result $2 \lfloor\log_2 t\rfloor+O(\lfloor\log_2 t\rfloor)+\ell$, the proposed scheme reduces the leading constant from $2$ to $1$. Finally, we propose a conventional $3$-threshold secret sharing scheme over a finite field. Based on this model of the revised scheme and the proposed conventional $3$-threshold scheme, we present a brand-new and more concise evolving $3$-threshold secret sharing scheme.</li>
</ul>

<h3>Title: L3DG: Latent 3D Gaussian Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Angela Dai, Matthias Nießner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13530">https://arxiv.org/abs/2410.13530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13530">https://arxiv.org/pdf/2410.13530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13530]] L3DG: Latent 3D Gaussian Diffusion(https://arxiv.org/abs/2410.13530)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose L3DG, the first approach for generative 3D modeling of 3D Gaussians through a latent 3D Gaussian diffusion formulation. This enables effective generative 3D modeling, scaling to generation of entire room-scale scenes which can be very efficiently rendered. To enable effective synthesis of 3D Gaussians, we propose a latent diffusion formulation, operating in a compressed latent space of 3D Gaussians. This compressed latent space is learned by a vector-quantized variational autoencoder (VQ-VAE), for which we employ a sparse convolutional architecture to efficiently operate on room-scale scenes. This way, the complexity of the costly generation process via diffusion is substantially reduced, allowing higher detail on object-level generation, as well as scalability to large scenes. By leveraging the 3D Gaussian representation, the generated scenes can be rendered from arbitrary viewpoints in real-time. We demonstrate that our approach significantly improves visual quality over prior work on unconditional object-level radiance field synthesis and showcase its applicability to room-scale scene generation.</li>
</ul>

<h3>Title: Three-Input Ciphertext Multiplication for Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Akherati, Yok Jye Tang, Xinmiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13545">https://arxiv.org/abs/2410.13545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13545">https://arxiv.org/pdf/2410.13545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13545]] Three-Input Ciphertext Multiplication for Homomorphic Encryption(https://arxiv.org/abs/2410.13545)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Homomorphic encryption (HE) allows computations to be directly carried out on ciphertexts and is essential to privacy-preserving computing, such as neural network inference, medical diagnosis, and financial data analysis. Only addition and 2-input multiplication are defined over ciphertexts in popular HE schemes. However, many HE applications involve non-linear functions and they need to be approximated using high-order polynomials to maintain precision. To reduce the complexity of these computations, this paper proposes 3-input ciphertext multiplication. One extra evaluation key is introduced to carry out the relinearization step of ciphertext multiplication, and new formulas are proposed to combine computations and share intermediate results. Compared to using two consecutive 2- input multiplications, computing the product of three ciphertexts utilizing the proposed scheme leads to almost a half of the latency, 29% smaller silicon area, and lower noise without scarifying the throughput.</li>
</ul>

<h3>Title: Integrating Temporal Representations for Dynamic Memory Retrieval and Management in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuki Hou, Haruki Tamoto, Homei Miyashita</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13553">https://arxiv.org/abs/2410.13553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13553">https://arxiv.org/pdf/2410.13553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13553]] Integrating Temporal Representations for Dynamic Memory Retrieval and Management in Large Language Models(https://arxiv.org/abs/2410.13553)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Conventional dialogue agents often struggle with effective memory recall, leading to redundant retrieval and inadequate management of unique user associations. To address this, we propose SynapticRAG, a novel approach integrating synaptic dynamics into Retrieval-Augmented Generation (RAG). SynapticRAG integrates temporal representations into memory vectors, mimicking biological synapses by differentiating events based on occurrence times and dynamically updating memory significance. This model employs temporal scoring for memory connections and a synaptic-inspired propagation control mechanism. Experiments across English, Japanese, and Chinese datasets demonstrate SynapticRAG's superiority over existing methods, including traditional RAG, with up to 14.66\% improvement in memory retrieval accuracy. Our approach advances context-aware dialogue AI systems by enhancing long-term context maintenance and specific information extraction from conversations.</li>
</ul>

<h3>Title: Generative Location Modeling for Spatially Aware Object Insertion</h3>
<ul>
<li><strong>Authors: </strong>Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amirhossein Habibian, Auke Wiggers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13564">https://arxiv.org/abs/2410.13564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13564">https://arxiv.org/pdf/2410.13564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13564]] Generative Location Modeling for Spatially Aware Object Insertion(https://arxiv.org/abs/2410.13564)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have become a powerful tool for image editing tasks, including object insertion. However, these methods often lack spatial awareness, generating objects with unrealistic locations and scales, or unintentionally altering the scene background. A key challenge lies in maintaining visual coherence, which requires both a geometrically suitable object location and a high-quality image edit. In this paper, we focus on the former, creating a location model dedicated to identifying realistic object locations. Specifically, we train an autoregressive model that generates bounding box coordinates, conditioned on the background image and the desired object class. This formulation allows to effectively handle sparse placement annotations and to incorporate implausible locations into a preference dataset by performing direct preference optimization. Our extensive experiments demonstrate that our generative location model, when paired with an inpainting method, substantially outperforms state-of-the-art instruction-tuned models and location modeling baselines in object insertion tasks, delivering accurate and visually coherent results.</li>
</ul>

<h3>Title: SDI-Paste: Synthetic Dynamic Instance Copy-Paste for Video Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sahir Shrestha, Weihao Li, Gao Zhu, Nick Barnes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13565">https://arxiv.org/abs/2410.13565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13565">https://arxiv.org/pdf/2410.13565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13565]] SDI-Paste: Synthetic Dynamic Instance Copy-Paste for Video Instance Segmentation(https://arxiv.org/abs/2410.13565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Data augmentation methods such as Copy-Paste have been studied as effective ways to expand training datasets while incurring minimal costs. While such methods have been extensively implemented for image level tasks, we found no scalable implementation of Copy-Paste built specifically for video tasks. In this paper, we leverage the recent growth in video fidelity of generative models to explore effective ways of incorporating synthetically generated objects into existing video datasets to artificially expand object instance pools. We first procure synthetic video sequences featuring objects that morph dynamically with time. Our carefully devised pipeline automatically segments then copy-pastes these dynamic instances across the frames of any target background video sequence. We name our video data augmentation pipeline Synthetic Dynamic Instance Copy-Paste, and test it on the complex task of Video Instance Segmentation which combines detection, segmentation and tracking of object instances across a video sequence. Extensive experiments on the popular Youtube-VIS 2021 dataset using two separate popular networks as baselines achieve strong gains of +2.9 AP (6.5%) and +2.1 AP (4.9%). We make our code and models publicly available.</li>
</ul>

<h3>Title: 360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jack Hilliard, Adrian Hilton, Jean-Yves Guillemaut</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13566">https://arxiv.org/abs/2410.13566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13566">https://arxiv.org/pdf/2410.13566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13566]] 360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision Transformers(https://arxiv.org/abs/2410.13566)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent illumination estimation methods have focused on enhancing the resolution and improving the quality and diversity of the generated textures. However, few have explored tailoring the neural network architecture to the Equirectangular Panorama (ERP) format utilised in image-based lighting. Consequently, high dynamic range images (HDRI) results usually exhibit a seam at the side borders and textures or objects that are warped at the poles. To address this shortcoming we propose a novel architecture, 360U-Former, based on a U-Net style Vision-Transformer which leverages the work of PanoSWIN, an adapted shifted window attention tailored to the ERP format. To the best of our knowledge, this is the first purely Vision-Transformer model used in the field of illumination estimation. We train 360U-Former as a GAN to generate HDRI from a limited field of view low dynamic range image (LDRI). We evaluate our method using current illumination estimation evaluation protocols and datasets, demonstrating that our approach outperforms existing and state-of-the-art methods without the artefacts typically associated with the use of the ERP format.</li>
</ul>

<h3>Title: Towards Better Performance in Incomplete LDL: Addressing Data Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Kou, Haoyuan Xuan, Jing Wang, Yuheng Jia, Xin Geng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13579">https://arxiv.org/abs/2410.13579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13579">https://arxiv.org/pdf/2410.13579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13579]] Towards Better Performance in Incomplete LDL: Addressing Data Imbalance(https://arxiv.org/abs/2410.13579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Label Distribution Learning (LDL) is a novel machine learning paradigm that addresses the problem of label ambiguity and has found widespread applications. Obtaining complete label distributions in real-world scenarios is challenging, which has led to the emergence of Incomplete Label Distribution Learning (InLDL). However, the existing InLDL methods overlook a crucial aspect of LDL data: the inherent imbalance in label distributions. To address this limitation, we propose \textbf{Incomplete and Imbalance Label Distribution Learning (I\(^2\)LDL)}, a framework that simultaneously handles incomplete labels and imbalanced label distributions. Our method decomposes the label distribution matrix into a low-rank component for frequent labels and a sparse component for rare labels, effectively capturing the structure of both head and tail labels. We optimize the model using the Alternating Direction Method of Multipliers (ADMM) and derive generalization error bounds via Rademacher complexity, providing strong theoretical guarantees. Extensive experiments on 15 real-world datasets demonstrate the effectiveness and robustness of our proposed framework compared to existing InLDL methods.</li>
</ul>

<h3>Title: Co-Segmentation without any Pixel-level Supervision with Application to Large-Scale Sketch Classification</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos-Antonios Ypsilantis, Ondřej Chum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13582">https://arxiv.org/abs/2410.13582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13582">https://arxiv.org/pdf/2410.13582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13582]] Co-Segmentation without any Pixel-level Supervision with Application to Large-Scale Sketch Classification(https://arxiv.org/abs/2410.13582)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This work proposes a novel method for object co-segmentation, i.e. pixel-level localization of a common object in a set of images, that uses no pixel-level supervision for training. Two pre-trained Vision Transformer (ViT) models are exploited: ImageNet classification-trained ViT, whose features are used to estimate rough object localization through intra-class token relevance, and a self-supervised DINO-ViT for intra-image token relevance. On recent challenging benchmarks, the method achieves state-of-the-art performance among methods trained with the same level of supervision (image labels) while being competitive with methods trained with pixel-level supervision (binary masks). The benefits of the proposed co-segmentation method are further demonstrated in the task of large-scale sketch recognition, that is, the classification of sketches into a wide range of categories. The limited amount of hand-drawn sketch training data is leveraged by exploiting readily available image-level-annotated datasets of natural images containing a large number of classes. To bridge the domain gap, the classifier is trained on a sketch-like proxy domain derived from edges detected on natural images. We show that sketch recognition significantly benefits when the classifier is trained on sketch-like structures extracted from the co-segmented area rather than from the full image. Code: this https URL .</li>
</ul>

<h3>Title: Text-Guided Multi-Property Molecular Optimization with a Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yida Xiong, Kun Li, Weiwei Liu, Jia Wu, Bo Du, Shirui Pan, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13597">https://arxiv.org/abs/2410.13597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13597">https://arxiv.org/pdf/2410.13597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13597]] Text-Guided Multi-Property Molecular Optimization with a Diffusion Language Model(https://arxiv.org/abs/2410.13597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby preventing error propagation during diffusion process. Guided by physically and chemically detailed textual descriptions, TransDLM samples and optimizes encoded source molecules, retaining core scaffolds of source molecules and ensuring structural similarities. Moreover, TransDLM enables simultaneous sampling of multiple molecules, making it ideal for scalable, efficient large-scale optimization through distributed computation on web platforms. Furthermore, our approach surpasses state-of-the-art methods in optimizing molecular structural similarity and enhancing chemical properties on the benchmark dataset. The code is available at: this https URL.</li>
</ul>

<h3>Title: Transformer-Based Approaches for Sensor-Based Human Activity Recognition: Opportunities and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Clayton Souza Leite, Henry Mauranen, Aziza Zhanabatyrova, Yu Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13605">https://arxiv.org/abs/2410.13605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13605">https://arxiv.org/pdf/2410.13605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13605]] Transformer-Based Approaches for Sensor-Based Human Activity Recognition: Opportunities and Challenges(https://arxiv.org/abs/2410.13605)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformers have excelled in natural language processing and computer vision, paving their way to sensor-based Human Activity Recognition (HAR). Previous studies show that transformers outperform their counterparts exclusively when they harness abundant data or employ compute-intensive optimization algorithms. However, neither of these scenarios is viable in sensor-based HAR due to the scarcity of data in this field and the frequent need to perform training and inference on resource-constrained devices. Our extensive investigation into various implementations of transformer-based versus non-transformer-based HAR using wearable sensors, encompassing more than 500 experiments, corroborates these concerns. We observe that transformer-based solutions pose higher computational demands, consistently yield inferior performance, and experience significant performance degradation when quantized to accommodate resource-constrained devices. Additionally, transformers demonstrate lower robustness to adversarial attacks, posing a potential threat to user trust in HAR.</li>
</ul>

<h3>Title: All models are wrong, some are useful: Model Selection with Limited Labels</h3>
<ul>
<li><strong>Authors: </strong>Patrik Okanovic, Andreas Kirsch, Jannes Kasper, Torsten Hoefler, Andreas Krause, Nezihe Merve Gürel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13609">https://arxiv.org/abs/2410.13609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13609">https://arxiv.org/pdf/2410.13609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13609]] All models are wrong, some are useful: Model Selection with Limited Labels(https://arxiv.org/abs/2410.13609)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the multitude of pretrained models available thanks to the advancements in large-scale supervised and self-supervised learning, choosing the right model is becoming increasingly pivotal in the machine learning lifecycle. However, much like the training process, choosing the best pretrained off-the-shelf model for raw, unlabeled data is a labor-intensive task. To overcome this, we introduce MODEL SELECTOR, a framework for label-efficient selection of pretrained classifiers. Given a pool of unlabeled target data, MODEL SELECTOR samples a small subset of highly informative examples for labeling, in order to efficiently identify the best pretrained model for deployment on this target dataset. Through extensive experiments, we demonstrate that MODEL SELECTOR drastically reduces the need for labeled data while consistently picking the best or near-best performing model. Across 18 model collections on 16 different datasets, comprising over 1,500 pretrained models, MODEL SELECTOR reduces the labeling cost by up to 94.15% to identify the best model compared to the cost of the strongest baseline. Our results further highlight the robustness of MODEL SELECTOR in model selection, as it reduces the labeling cost by up to 72.41% when selecting a near-best model, whose accuracy is only within 1% of the best model.</li>
</ul>

<h3>Title: H2OVL-Mississippi Vision Language Models Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, Sri Satish Ambati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13611">https://arxiv.org/abs/2410.13611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13611">https://arxiv.org/pdf/2410.13611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13611]] H2OVL-Mississippi Vision Language Models Technical Report(https://arxiv.org/abs/2410.13611)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Smaller vision-language models (VLMs) are becoming increasingly important for privacy-focused, on-device applications due to their ability to run efficiently on consumer hardware for processing enterprise commercial documents and images. These models require strong language understanding and visual capabilities to enhance human-machine interaction. To address this need, we present H2OVL-Mississippi, a pair of small VLMs trained on 37 million image-text pairs using 240 hours of compute on 8 x H100 GPUs. H2OVL-Mississippi-0.8B is a tiny model with 0.8 billion parameters that specializes in text recognition, achieving state of the art performance on the Text Recognition portion of OCRBench and surpassing much larger models in this area. Additionally, we are releasing H2OVL-Mississippi-2B, a 2 billion parameter model for general use cases, exhibiting highly competitive metrics across various academic benchmarks. Both models build upon our prior work with H2O-Danube language models, extending their capabilities into the visual domain. We release them under the Apache 2.0 license, making VLMs accessible to everyone, democratizing document AI and visual LLMs.</li>
</ul>

<h3>Title: LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yiming Shi, Jiwei Wei, Yujia Wu, Ran Ran, Chengwei Sun, Shiyuan He, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13618">https://arxiv.org/abs/2410.13618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13618">https://arxiv.org/pdf/2410.13618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13618]] LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2410.13618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing a comprehensive and detailed analysis. Our open-source code can be accessed at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on Segment Anything</h3>
<ul>
<li><strong>Authors: </strong>Joonhyeon Song, Seohwan Yun, Seongho Yoon, Joohyeok Kim, Sangmin Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13621">https://arxiv.org/abs/2410.13621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13621">https://arxiv.org/pdf/2410.13621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13621]] Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on Segment Anything(https://arxiv.org/abs/2410.13621)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This work proposes a novel approach beyond supervised learning for effective pathological image analysis, addressing the challenge of limited robust labeled data. Pathological diagnosis of diseases like cancer has conventionally relied on the evaluation of morphological features by physicians and pathologists. However, recent advancements in compute-aided diagnosis (CAD) systems are gaining significant attention as diagnostic support tools. Although the advancement of deep learning has improved CAD significantly, segmentation models typically require large pixel-level annotated dataset, and such labeling is expensive. Existing studies not based on supervised approaches still struggle with limited generalization, and no practical approach has emerged yet. To address this issue, we present a weakly supervised semantic segmentation (WSSS) model by combining class activation map and Segment Anything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt the SAM-a foundation model that is pretrained on large datasets and operates in zero-shot configurations using only coarse prompts. The proposed approach transfer enhanced Attention Dropout Layer's knowledge to SAM, thereby generating pseudo-labels. To demonstrate the superiority of the proposed method, experimental studies are conducted on histopathological breast cancer datasets. The proposed method outperformed other WSSS methods across three datasets, demonstrating its efficiency by achieving this with only 12GB of GPU memory during training. Our code is available at : this https URL</li>
</ul>

<h3>Title: Normalizing self-supervised learning for provably reliable Change Point Detection</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Bazarova, Evgenia Romanenkova, Alexey Zaytsev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13637">https://arxiv.org/abs/2410.13637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13637">https://arxiv.org/pdf/2410.13637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13637]] Normalizing self-supervised learning for provably reliable Change Point Detection(https://arxiv.org/abs/2410.13637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Change point detection (CPD) methods aim to identify abrupt shifts in the distribution of input data streams. Accurate estimators for this task are crucial across various real-world scenarios. Yet, traditional unsupervised CPD techniques face significant limitations, often relying on strong assumptions or suffering from low expressive power due to inherent model simplicity. In contrast, representation learning methods overcome these drawbacks by offering flexibility and the ability to capture the full complexity of the data without imposing restrictive assumptions. However, these approaches are still emerging in the CPD field and lack robust theoretical foundations to ensure their reliability. Our work addresses this gap by integrating the expressive power of representation learning with the groundedness of traditional CPD techniques. We adopt spectral normalization (SN) for deep representation learning in CPD tasks and prove that the embeddings after SN are highly informative for CPD. Our method significantly outperforms current state-of-the-art methods during the comprehensive evaluation via three standard CPD datasets.</li>
</ul>

<h3>Title: Scaling Wearable Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam Tailor, Jake Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Samy Abdel-Ghaffar, Daniel McDuff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13638">https://arxiv.org/abs/2410.13638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13638">https://arxiv.org/pdf/2410.13638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13638]] Scaling Wearable Foundation Models(https://arxiv.org/abs/2410.13638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data; however, making sense of these observations for scientific and actionable insights is non-trivial. Inspired by the empirical success of generative modeling, where large neural networks learn powerful representations from vast amounts of text, image, video, or audio data, we investigate the scaling properties of sensor foundation models across compute, data, and model size. Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people, we create LSM, a multimodal foundation model built on the largest wearable-signals dataset with the most extensive range of sensor modalities to date. Our results establish the scaling laws of LSM for tasks such as imputation, interpolation and extrapolation, both across time and sensor modalities. Moreover, we highlight how LSM enables sample-efficient downstream learning for tasks like exercise and activity recognition.</li>
</ul>

<h3>Title: A Comparative Study on Reasoning Patterns of OpenAI's o1 Model</h3>
<ul>
<li><strong>Authors: </strong>Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J.H. Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13639">https://arxiv.org/abs/2410.13639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13639">https://arxiv.org/pdf/2410.13639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13639]] A Comparative Study on Reasoning Patterns of OpenAI's o1 Model(https://arxiv.org/abs/2410.13639)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.</li>
</ul>

<h3>Title: An Active Learning Framework for Inclusive Generation by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sabit Hassan, Anthony Sicilia, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13641">https://arxiv.org/abs/2410.13641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13641">https://arxiv.org/pdf/2410.13641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13641]] An Active Learning Framework for Inclusive Generation by Large Language Models(https://arxiv.org/abs/2410.13641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring that Large Language Models (LLMs) generate text representative of diverse sub-populations is essential, particularly when key concepts related to under-represented groups are scarce in the training data. We address this challenge with a novel clustering-based active learning framework, enhanced with knowledge distillation. The proposed framework transforms the intermediate outputs of the learner model, enabling effective active learning for generative tasks for the first time. Integration of clustering and knowledge distillation yields more representative models without prior knowledge of underlying data distribution and overbearing human efforts. We validate our approach in practice through case studies in counter-narration and style transfer. We construct two new datasets in tandem with model training, showing a performance improvement of 2%-10% over baseline models. Our results also show more consistent performance across various data subgroups and increased lexical diversity, underscoring our model's resilience to skewness in available data. Further, our results show that the data acquired via our approach improves the performance of secondary models not involved in the learning loop, showcasing practical utility of the framework.</li>
</ul>

<h3>Title: Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, Aviv Regev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13643">https://arxiv.org/abs/2410.13643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13643">https://arxiv.org/pdf/2410.13643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13643]] Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design(https://arxiv.org/abs/2410.13643)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences across domains from natural language to biological sequence generation. For example, in the protein inverse folding task, conditional diffusion models have achieved impressive results in generating natural-like sequences that fold back into the original structure. However, practical design tasks often require not only modeling a conditional distribution but also optimizing specific task objectives. For instance, we may prefer protein sequences with high stability. To address this, we consider the scenario where we have pre-trained discrete diffusion models that can generate natural-like sequences, as well as reward models that map sequences to task objectives. We then formulate the reward maximization problem within discrete diffusion models, analogous to reinforcement learning (RL), while minimizing the KL divergence against pretrained diffusion models to preserve naturalness. To solve this RL problem, we propose a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models, by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick. Our theoretical analysis indicates that our approach can generate sequences that are both natural-like and yield high rewards. While similar tasks have been recently explored in diffusion models for continuous domains, our work addresses unique algorithmic and theoretical challenges specific to discrete diffusion models, which arise from their foundation in continuous-time Markov chains rather than Brownian motion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA and protein sequences that optimize enhancer activity and protein stability, respectively, important tasks for gene therapies and protein-based therapeutics.</li>
</ul>

<h3>Title: SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM Application in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuling Gu, Oyvind Tafjord, Hyunwoo Kim, Jared Moore, Ronan Le Bras, Peter Clark, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13648">https://arxiv.org/abs/2410.13648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13648">https://arxiv.org/pdf/2410.13648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13648]] SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM Application in LLMs(https://arxiv.org/abs/2410.13648)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While prior work has explored whether large language models (LLMs) possess a "theory of mind" (ToM) - the ability to attribute mental states to oneself and others - there has been little work testing whether LLMs can implicitly apply such knowledge to predict behavior, or to judge whether an observed behavior is rational. Such skills are critical for appropriate interaction in social environments. We create a new dataset, SimpleTom, containing concise, diverse stories (e.g., "The can of Pringles has moldy chips in it. Mary picks up the can in the supermarket and walks to the cashier."), each with three questions that test different degrees of ToM reasoning, asking models to predict (a) mental state ("Is Mary aware of the mold?"), (b) behavior ("Will Mary pay for the chips or report the mold?"), and (c) judgment ("Mary paid for the chips. Was that reasonable?"). To our knowledge, SimpleToM is the first dataset to systematically explore downstream reasoning requiring knowledge of mental states in realistic scenarios. Our experimental results are intriguing: While most models can reliably predict mental state on our dataset (a), they often fail to correctly predict the behavior (b), and fare even worse at judging whether given behaviors are reasonable (c), despite being correctly aware of the protagonist's mental state should make such secondary predictions obvious. We further show that we can help models do better at (b) and (c) via interventions such as reminding the model of its earlier mental state answer and mental-state-specific chain-of-thought prompting, raising the action prediction accuracies (e.g., from 49.5% to 93.5% for GPT-4o) and judgment accuracies (e.g., from 15.3% to 94.7% in GPT-4o). While this shows that models can be coaxed to perform well, it requires task-specific interventions, and the natural model performances remain low, a cautionary tale for LLM deployment.</li>
</ul>

<h3>Title: A new approach for fine-tuning sentence transformers for intent classification and out-of-scope detection tasks</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zhang, Atta Norouzian, Aanchan Mohan, Frederick Ducatelle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13649">https://arxiv.org/abs/2410.13649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13649">https://arxiv.org/pdf/2410.13649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13649]] A new approach for fine-tuning sentence transformers for intent classification and out-of-scope detection tasks(https://arxiv.org/abs/2410.13649)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In virtual assistant (VA) systems it is important to reject or redirect user queries that fall outside the scope of the system. One of the most accurate approaches for out-of-scope (OOS) rejection is to combine it with the task of intent classification on in-scope queries, and to use methods based on the similarity of embeddings produced by transformer-based sentence encoders. Typically, such encoders are fine-tuned for the intent-classification task, using cross-entropy loss. Recent work has shown that while this produces suitable embeddings for the intent-classification task, it also tends to disperse in-scope embeddings over the full sentence embedding space. This causes the in-scope embeddings to potentially overlap with OOS embeddings, thereby making OOS rejection difficult. This is compounded when OOS data is unknown. To mitigate this issue our work proposes to regularize the cross-entropy loss with an in-scope embedding reconstruction loss learned using an auto-encoder. Our method achieves a 1-4% improvement in the area under the precision-recall curve for rejecting out-of-sample (OOS) instances, without compromising intent classification performance.</li>
</ul>

<h3>Title: Help Me Identify: Is an LLM+VQA System All We Need to Identify Visual Concepts?</h3>
<ul>
<li><strong>Authors: </strong>Shailaja Keyur Sampat, Maitreya Patel, Yezhou Yang, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13651">https://arxiv.org/abs/2410.13651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13651">https://arxiv.org/pdf/2410.13651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13651]] Help Me Identify: Is an LLM+VQA System All We Need to Identify Visual Concepts?(https://arxiv.org/abs/2410.13651)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>An ability to learn about new objects from a small amount of visual data and produce convincing linguistic justification about the presence/absence of certain concepts (that collectively compose the object) in novel scenarios is an important characteristic of human cognition. This is possible due to abstraction of attributes/properties that an object is composed of e.g. an object `bird' can be identified by the presence of a beak, feathers, legs, wings, etc. Inspired by this aspect of human reasoning, in this work, we present a zero-shot framework for fine-grained visual concept learning by leveraging large language model and Visual Question Answering (VQA) system. Specifically, we prompt GPT-3 to obtain a rich linguistic description of visual objects in the dataset. We convert the obtained concept descriptions into a set of binary questions. We pose these questions along with the query image to a VQA system and aggregate the answers to determine the presence or absence of an object in the test images. Our experiments demonstrate comparable performance with existing zero-shot visual classification methods and few-shot concept learning approaches, without substantial computational overhead, yet being fully explainable from the reasoning perspective.</li>
</ul>

<h3>Title: DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster Recognition</h3>
<ul>
<li><strong>Authors: </strong>Demetris Shianios, Panayiotis Kolios, Christos Kyrkou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13663">https://arxiv.org/abs/2410.13663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13663">https://arxiv.org/pdf/2410.13663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13663]] DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster Recognition(https://arxiv.org/abs/2410.13663)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>The integration of Unmanned Aerial Vehicles (UAVs) with artificial intelligence (AI) models for aerial imagery processing in disaster assessment, necessitates models that demonstrate exceptional accuracy, computational efficiency, and real-time processing capabilities. Traditionally Convolutional Neural Networks (CNNs), demonstrate efficiency in local feature extraction but are limited by their potential for global context interpretation. On the other hand, Vision Transformers (ViTs) show promise for improved global context interpretation through the use of attention mechanisms, although they still remain underinvestigated in UAV-based disaster response applications. Bridging this research gap, we introduce DiRecNetV2, an improved hybrid model that utilizes convolutional and transformer layers. It merges the inductive biases of CNNs for robust feature extraction with the global context understanding of Transformers, maintaining a low computational load ideal for UAV applications. Additionally, we introduce a new, compact multi-label dataset of disasters, to set an initial benchmark for future research, exploring how models trained on single-label data perform in a multi-label test set. The study assesses lightweight CNNs and ViTs on the AIDERSv2 dataset, based on the frames per second (FPS) for efficiency and the weighted F1 scores for classification performance. DiRecNetV2 not only achieves a weighted F1 score of 0.964 on a single-label test set but also demonstrates adaptability, with a score of 0.614 on a complex multi-label test set, while functioning at 176.13 FPS on the Nvidia Orin Jetson device.</li>
</ul>

<h3>Title: VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shailaja Keyur Sampat, Mutsumi Nakamura, Shankar Kailas, Kartik Aggarwal, Mandy Zhou, Yezhou Yang, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13666">https://arxiv.org/abs/2410.13666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13666">https://arxiv.org/pdf/2410.13666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13666]] VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic Reasoning Tasks(https://arxiv.org/abs/2410.13666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deriving inference from heterogeneous inputs (such as images, text, and audio) is an important skill for humans to perform day-to-day tasks. A similar ability is desirable for the development of advanced Artificial Intelligence (AI) systems. While state-of-the-art models are rapidly closing the gap with human-level performance on diverse computer vision and NLP tasks separately, they struggle to solve tasks that require joint reasoning over visual and textual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask benchmark for natural language understanding, we propose VL-GLUE in this paper. VL-GLUE consists of over 100k samples spanned across seven different tasks, which at their core require visuo-linguistic reasoning. Moreover, our benchmark comprises of diverse image types (from synthetically rendered figures, and day-to-day scenes to charts and complex diagrams) and includes a broad variety of domain-specific text (from cooking, politics, and sports to high-school curricula), demonstrating the need for multi-modal understanding in the real-world. We show that this benchmark is quite challenging for existing large-scale vision-language models and encourage development of systems that possess robust visuo-linguistic reasoning capabilities.</li>
</ul>

<h3>Title: ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization</h3>
<ul>
<li><strong>Authors: </strong>Xiutian Zhao, Ke Wang, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13667">https://arxiv.org/abs/2410.13667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13667">https://arxiv.org/pdf/2410.13667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13667]] ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization(https://arxiv.org/abs/2410.13667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.</li>
</ul>

<h3>Title: HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings</h3>
<ul>
<li><strong>Authors: </strong>Varun Gumma, Anandhita Raghunath, Mohit Jain, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13671">https://arxiv.org/abs/2410.13671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13671">https://arxiv.org/pdf/2410.13671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13671]] HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings(https://arxiv.org/abs/2410.13671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Assessing the capabilities and limitations of large language models (LLMs) has garnered significant interest, yet the evaluation of multiple models in real-world scenarios remains rare. Multilingual evaluation often relies on translated benchmarks, which typically do not capture linguistic and cultural nuances present in the source language. This study provides an extensive assessment of 24 LLMs on real world data collected from Indian patients interacting with a medical chatbot in Indian English and 4 other Indic languages. We employ a uniform Retrieval Augmented Generation framework to generate responses, which are evaluated using both automated techniques and human evaluators on four specific metrics relevant to our application. We find that models vary significantly in their performance and that instruction tuned Indic models do not always perform well on Indic language queries. Further, we empirically show that factual correctness is generally lower for responses to Indic queries compared to English queries. Finally, our qualitative work shows that code-mixed and culturally relevant queries in our dataset pose challenges to evaluated models.</li>
</ul>

<h3>Title: Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yijun Liang, Shweta Bhardwaj, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13674">https://arxiv.org/abs/2410.13674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13674">https://arxiv.org/pdf/2410.13674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13674]] Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion(https://arxiv.org/abs/2410.13674)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy.</li>
</ul>

<h3>Title: Pose-Based Sign Language Appearance Transfer</h3>
<ul>
<li><strong>Authors: </strong>Amit Moryossef, Gerard Sant, Zifan Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13675">https://arxiv.org/abs/2410.13675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13675">https://arxiv.org/pdf/2410.13675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13675]] Pose-Based Sign Language Appearance Transfer(https://arxiv.org/abs/2410.13675)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Label-free prediction of fluorescence markers in bovine satellite cells using deep learning</h3>
<ul>
<li><strong>Authors: </strong>Sania Sinha, Aarham Wasit, Won Seob Kim, Jongkyoo Kim, Jiyoon Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13685">https://arxiv.org/abs/2410.13685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13685">https://arxiv.org/pdf/2410.13685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13685]] Label-free prediction of fluorescence markers in bovine satellite cells using deep learning(https://arxiv.org/abs/2410.13685)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Assessing the quality of bovine satellite cells (BSCs) is essential for the cultivated meat industry, which aims to address global food sustainability challenges. This study aims to develop a label-free method for predicting fluorescence markers in isolated BSCs using deep learning. We employed a U-Net-based CNN model to predict multiple fluorescence signals from a single bright-field microscopy image of cell culture. Two key biomarkers, DAPI and Pax7, were used to determine the abundance and quality of BSCs. The image pre-processing pipeline included fluorescence denoising to improve prediction performance and consistency. A total of 48 biological replicates were used, with statistical performance metrics such as Pearson correlation coefficient and SSIM employed for model evaluation. The model exhibited better performance with DAPI predictions due to uniform staining. Pax7 predictions were more variable, reflecting biological heterogeneity. Enhanced visualization techniques, including color mapping and image overlay, improved the interpretability of the predictions by providing better contextual and perceptual information. The findings highlight the importance of data pre-processing and demonstrate the potential of deep learning to advance non-invasive, label-free assessment techniques in the cultivated meat industry, paving the way for reliable and actionable AI-driven evaluations.</li>
</ul>

<h3>Title: Exploring the Design Space of Visual Context Representation in Video MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Yifan Du, Yuqi Huo, Kun Zhou, Zijia Zhao, Haoyu Lu, Han Huang, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13694">https://arxiv.org/abs/2410.13694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13694">https://arxiv.org/pdf/2410.13694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13694]] Exploring the Design Space of Visual Context Representation in Video MLLMs(https://arxiv.org/abs/2410.13694)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models (MLLMs) have shown remarkable capability of understanding the video semantics on various downstream tasks. Despite the advancements, there is still a lack of systematic research on visual context representation, which refers to the scheme to select frames from a video and further select the tokens from a frame. In this paper, we explore the design space for visual context representation, and aim to improve the performance of video MLLMs by finding more effective representation schemes. Firstly, we formulate the task of visual context representation as a constrained optimization problem, and model the language modeling loss as a function of the number of frames and the number of embeddings (or tokens) per frame, given the maximum visual context window size. Then, we explore the scaling effects in frame selection and token selection respectively, and fit the corresponding function curve by conducting extensive empirical experiments. We examine the effectiveness of typical selection strategies and present empirical findings to determine the two factors. Furthermore, we study the joint effect of frame selection and token selection, and derive the optimal formula for determining the two factors. We demonstrate that the derived optimal settings show alignment with the best-performed results of empirical experiments. Our code and model are available at: this https URL.</li>
</ul>

<h3>Title: Unconstrained Model Merging for Enhanced LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Baoyi He, Shengyu Zhang, Yuhao Fu, Qi Zhou, Zhijie Sang, Zijin Hong, Kejing Yang, Wenjun Wang, Jianbo Yuan, Guangning Han, Linyi Li, Chunlin Ji, Fei Wu, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13699">https://arxiv.org/abs/2410.13699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13699">https://arxiv.org/pdf/2410.13699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13699]] Unconstrained Model Merging for Enhanced LLM Reasoning(https://arxiv.org/abs/2410.13699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in building domain-specific large language models (LLMs) have shown remarkable success, especially in tasks requiring reasoning abilities like logical inference over complex relationships and multi-step problem solving. However, creating a powerful all-in-one LLM remains challenging due to the need for proprietary data and vast computational resources. As a resource-friendly alternative, we explore the potential of merging multiple expert models into a single LLM. Existing studies on model merging mainly focus on generalist LLMs instead of domain experts, or the LLMs under the same architecture and size. In this work, we propose an unconstrained model merging framework that accommodates both homogeneous and heterogeneous model architectures with a focus on reasoning tasks. A fine-grained layer-wise weight merging strategy is designed for homogeneous models merging, while heterogeneous model merging is built upon the probabilistic distribution knowledge derived from instruction-response fine-tuning data. Across 7 benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that combinatorial reasoning emerges from merging which surpasses simple additive effects. We propose that unconstrained model merging could serve as a foundation for decentralized LLMs, marking a notable progression from the existing centralized LLM framework. This evolution could enhance wider participation and stimulate additional advancement in the field of artificial intelligence, effectively addressing the constraints posed by centralized models.</li>
</ul>

<h3>Title: On the Role of Attention Heads in Large Language Model Safety</h3>
<ul>
<li><strong>Authors: </strong>Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13708">https://arxiv.org/abs/2410.13708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13708">https://arxiv.org/pdf/2410.13708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13708]] On the Role of Attention Heads in Large Language Model Safety(https://arxiv.org/abs/2410.13708)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose a novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Based on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that the special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries, while only modifying 0.006% of the parameters, in contrast to the ~ 5% modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms within large models.</li>
</ul>

<h3>Title: On-device Federated Learning in Smartphones for Detecting Depression from Reddit Posts</h3>
<ul>
<li><strong>Authors: </strong>Mustofa Ahmed, Abdul Muntakim, Nawrin Tabassum, Mohammad Asifur Rahim, Faisal Muhammad Shah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13709">https://arxiv.org/abs/2410.13709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13709">https://arxiv.org/pdf/2410.13709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13709]] On-device Federated Learning in Smartphones for Detecting Depression from Reddit Posts(https://arxiv.org/abs/2410.13709)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Depression detection using deep learning models has been widely explored in previous studies, especially due to the large amounts of data available from social media posts. These posts provide valuable information about individuals' mental health conditions and can be leveraged to train models and identify patterns in the data. However, distributed learning approaches have not been extensively explored in this domain. In this study, we adopt Federated Learning (FL) to facilitate decentralized training on smartphones while protecting user data privacy. We train three neural network architectures--GRU, RNN, and LSTM on Reddit posts to detect signs of depression and evaluate their performance under heterogeneous FL settings. To optimize the training process, we leverage a common tokenizer across all client devices, which reduces the computational load. Additionally, we analyze resource consumption and communication costs on smartphones to assess their impact in a real-world FL environment. Our experimental results demonstrate that the federated models achieve comparable performance to the centralized models. This study highlights the potential of FL for decentralized mental health prediction by providing a secure and efficient model training process on edge devices.</li>
</ul>

<h3>Title: CrystalX: Ultra-Precision Crystal Structure Resolution and Error Correction Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaipeng Zheng, Weiran Huang, Wanli Ouyang, Han-Sen Zhong, Yuqiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13713">https://arxiv.org/abs/2410.13713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13713">https://arxiv.org/pdf/2410.13713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13713]] CrystalX: Ultra-Precision Crystal Structure Resolution and Error Correction Using Deep Learning(https://arxiv.org/abs/2410.13713)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>Atomic structure analysis of crystalline materials is a paramount endeavor in both chemical and material sciences. This sophisticated technique necessitates not only a solid foundation in crystallography but also a profound comprehension of the intricacies of the accompanying software, posing a significant challenge in meeting the rigorous daily demands. For the first time, we confront this challenge head-on by harnessing the power of deep learning for ultra-precise structural analysis at the full-atom level. To validate the performance of the model, named CrystalX, we employed a vast dataset comprising over 50,000 X-ray diffraction measurements derived from authentic experiments, demonstrating performance that is commensurate with human experts and adept at deciphering intricate geometric patterns. Remarkably, CrystalX revealed that even peer-reviewed publications can harbor errors that are stealthy to human scrutiny, yet CrystalX adeptly rectifies them. This deep learning model revolutionizes the time frame for crystal structure analysis, slashing it down to seconds. It has already been successfully applied in the structure analysis of newly discovered compounds in the latest research without human intervention. Overall, CrystalX marks the beginning of a new era in automating routine structural analysis within self-driving laboratories.</li>
</ul>

<h3>Title: MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Nandan Thakur, Suleman Kazi, Ge Luo, Jimmy Lin, Amin Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13716">https://arxiv.org/abs/2410.13716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13716">https://arxiv.org/pdf/2410.13716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13716]] MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2410.13716)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional Retrieval-Augmented Generation (RAG) benchmarks rely on different heuristic-based metrics for evaluation, but these require human preferences as ground truth for reference. In contrast, arena-based benchmarks, where two models compete each other, require an expensive Large Language Model (LLM) as a judge for a reliable evaluation. We present an easy and efficient technique to get the best of both worlds. The idea is to train a learning to rank model as a "surrogate" judge using RAG-based evaluation heuristics as input, to produce a synthetic arena-based leaderboard. Using this idea, We develop MIRAGE-Bench, a standardized arena-based multilingual RAG benchmark for 18 diverse languages on Wikipedia. The benchmark is constructed using MIRACL, a retrieval dataset, and extended for multilingual generation evaluation. MIRAGE-Bench evaluates RAG extensively coupling both heuristic features and LLM as a judge evaluator. In our work, we benchmark 19 diverse multilingual-focused LLMs, and achieve a high correlation (Kendall Tau ($\tau$) = 0.909) using our surrogate judge learned using heuristic features with pairwise evaluations and between GPT-4o as a teacher on the MIRAGE-Bench leaderboard using the Bradley-Terry framework. We observe proprietary and large open-source LLMs currently dominate in multilingual RAG. MIRAGE-Bench is available at: this https URL.</li>
</ul>

<h3>Title: Movie Gen: A Cast of Media Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, Yuming Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13720">https://arxiv.org/abs/2410.13720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13720">https://arxiv.org/pdf/2410.13720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13720]] Movie Gen: A Cast of Media Foundation Models(https://arxiv.org/abs/2410.13720)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at this https URL.</li>
</ul>

<h3>Title: Persistent Pre-Training Poisoning of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tramèr, Daphne Ippolito</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13722">https://arxiv.org/abs/2410.13722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13722">https://arxiv.org/pdf/2410.13722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13722]] Persistent Pre-Training Poisoning of LLMs(https://arxiv.org/abs/2410.13722)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web. Prior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets. Our work evaluates for the first time whether language models can also be compromised during pre-training, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B). Our main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%.</li>
</ul>

<h3>Title: DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13726">https://arxiv.org/abs/2410.13726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13726">https://arxiv.org/pdf/2410.13726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13726]] DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation(https://arxiv.org/abs/2410.13726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at this https URL.</li>
</ul>

<h3>Title: Reducing the Transformer Architecture to a Minimum</h3>
<ul>
<li><strong>Authors: </strong>Bernhard Bermeitinger, Tomas Hrycej, Massimo Pavone, Julianus Kath, Siegfried Handschuh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13732">https://arxiv.org/abs/2410.13732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13732">https://arxiv.org/pdf/2410.13732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13732]] Reducing the Transformer Architecture to a Minimum(https://arxiv.org/abs/2410.13732)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers are a widespread and successful model architecture, particularly in Natural Language Processing (NLP) and Computer Vision (CV). The essential innovation of this architecture is the Attention Mechanism, which solves the problem of extracting relevant context information from long sequences in NLP and realistic scenes in CV. A classical neural network component, a Multi-Layer Perceptron (MLP), complements the attention mechanism. Its necessity is frequently justified by its capability of modeling nonlinear relationships. However, the attention mechanism itself is nonlinear through its internal use of similarity measures. A possible hypothesis is that this nonlinearity is sufficient for modeling typical application problems. As the MLPs usually contain the most trainable parameters of the whole model, their omission would substantially reduce the parameter set size. Further components can also be reorganized to reduce the number of parameters. Under some conditions, query and key matrices can be collapsed into a single matrix of the same size. The same is true about value and projection matrices, which can also be omitted without eliminating the substance of the attention mechanism. Initially, the similarity measure was defined asymmetrically, with peculiar properties such as that a token is possibly dissimilar to itself. A possible symmetric definition requires only half of the parameters. We have laid the groundwork by testing widespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that simplified transformer architectures (a) without MLP, (b) with collapsed matrices, and (c) symmetric similarity matrices exhibit similar performance as the original architecture, saving up to 90% of parameters without hurting the classification performance.</li>
</ul>

<h3>Title: Improving Multi-modal Large Language Model through Boosting Vision Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Yanpeng Sun, Huaxin Zhang, Qiang Chen, Xinyu Zhang, Nong Sang, Gang Zhang, Jingdong Wang, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13733">https://arxiv.org/abs/2410.13733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13733">https://arxiv.org/pdf/2410.13733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13733]] Improving Multi-modal Large Language Model through Boosting Vision Capabilities(https://arxiv.org/abs/2410.13733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We focus on improving the visual understanding capability for boosting the vision-language models. We propose \textbf{Arcana}, a multiModal language model, which introduces two crucial techniques. First, we present Multimodal LoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional language-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for vision and one for language -- each with its own parameters. This disentangled parameters design allows for more specialized learning in each modality and better integration of multimodal information. Second, we introduce the Query Ladder adapter (QLadder) to improve the visual encoder. QLadder employs a learnable ``\textit{ladder}'' structure to deeply aggregates the intermediate representations from the frozen pretrained visual encoder (e.g., CLIP image encoder). This enables the model to learn new and informative visual features, as well as remaining the powerful capabilities of the pretrained visual encoder. These techniques collectively enhance Arcana's visual perception power, enabling it to leverage improved visual information for more accurate and contextually relevant outputs across various multimodal scenarios. Extensive experiments and ablation studies demonstrate the effectiveness and generalization capability of our Arcana. The code and re-annotated data are available at \url{this https URL}.</li>
</ul>

<h3>Title: Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores</h3>
<ul>
<li><strong>Authors: </strong>Minxing Zheng, Shixiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13735">https://arxiv.org/abs/2410.13735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13735">https://arxiv.org/pdf/2410.13735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13735]] Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores(https://arxiv.org/abs/2410.13735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have shown significant promise in critical domains such as medical diagnosis, autonomous driving, and climate science, where reliable decision-making hinges on accurate uncertainty quantification. While probabilistic conformal prediction (PCP) offers a powerful framework for this purpose, its coverage efficiency -- the size of the uncertainty set -- is limited when dealing with complex underlying distributions and a finite number of generated samples. In this paper, we propose a novel PCP framework that enhances efficiency by first vectorizing the non-conformity scores with ranked samples and then optimizing the shape of the prediction set by varying the quantiles for samples at the same rank. Our method delivers valid coverage while producing discontinuous and more efficient prediction sets, making it particularly suited for high-stakes applications. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.</li>
</ul>

<h3>Title: Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13746">https://arxiv.org/abs/2410.13746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13746">https://arxiv.org/pdf/2410.13746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13746]] Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers(https://arxiv.org/abs/2410.13746)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The denoising diffusion model has recently emerged as a powerful generative technique, capable of transforming noise into meaningful data. While theoretical convergence guarantees for diffusion models are well established when the target distribution aligns with the training distribution, practical scenarios often present mismatches. One common case is in zero-shot conditional diffusion sampling, where the target conditional distribution is different from the (unconditional) training distribution. These score-mismatched diffusion models remain largely unexplored from a theoretical perspective. In this paper, we present the first performance guarantee with explicit dimensional dependencies for general score-mismatched diffusion samplers, focusing on target distributions with finite second moments. We show that score mismatches result in an asymptotic distributional bias between the target and sampling distributions, proportional to the accumulated mismatch between the target and training distributions. This result can be directly applied to zero-shot conditional samplers for any conditional model, irrespective of measurement noise. Interestingly, the derived convergence upper bound offers useful guidance for designing a novel bias-optimal zero-shot sampler in linear conditional models that minimizes the asymptotic bias. For such bias-optimal samplers, we further establish convergence guarantees with explicit dependencies on dimension and conditioning, applied to several interesting target distributions, including those with bounded support and Gaussian mixtures. Our findings are supported by numerical studies.</li>
</ul>

<h3>Title: Privacy-Preserving Decentralized AI with Confidential Computing</h3>
<ul>
<li><strong>Authors: </strong>Dayeol Lee, Jorge Antonio, Hisham Khan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13752">https://arxiv.org/abs/2410.13752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13752">https://arxiv.org/pdf/2410.13752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13752]] Privacy-Preserving Decentralized AI with Confidential Computing(https://arxiv.org/abs/2410.13752)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>This paper addresses privacy protection in decentralized Artificial Intelligence (AI) using Confidential Computing (CC) within the Atoma Network, a decentralized AI platform designed for the Web3 domain. Decentralized AI distributes AI services among multiple entities without centralized oversight, fostering transparency and robustness. However, this structure introduces significant privacy challenges, as sensitive assets such as proprietary models and personal data may be exposed to untrusted participants. Cryptography-based privacy protection techniques such as zero-knowledge machine learning (zkML) suffers prohibitive computational overhead. To address the limitation, we propose leveraging Confidential Computing (CC). Confidential Computing leverages hardware-based Trusted Execution Environments (TEEs) to provide isolation for processing sensitive data, ensuring that both model parameters and user data remain secure, even in decentralized, potentially untrusted environments. While TEEs face a few limitations, we believe they can bridge the privacy gap in decentralized AI. We explore how we can integrate TEEs into Atoma's decentralized framework.</li>
</ul>

<h3>Title: GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning</h3>
<ul>
<li><strong>Authors: </strong>Guibin Zhang, Haonan Dong, Yuchen Zhang, Zhixun Li, Dingshuo Chen, Kai Wang, Tianlong Chen, Yuxuan Liang, Dawei Cheng, Kun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13761">https://arxiv.org/abs/2410.13761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13761">https://arxiv.org/pdf/2410.13761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13761]] GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning(https://arxiv.org/abs/2410.13761)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Training high-quality deep models necessitates vast amounts of data, resulting in overwhelming computational and memory demands. Recently, data pruning, distillation, and coreset selection have been developed to streamline data volume by retaining, synthesizing, or selecting a small yet informative subset from the full set. Among these methods, data pruning incurs the least additional training cost and offers the most practical acceleration benefits. However, it is the most vulnerable, often suffering significant performance degradation with imbalanced or biased data schema, thus raising concerns about its accuracy and reliability in on-device deployment. Therefore, there is a looming need for a new data pruning paradigm that maintains the efficiency of previous practices while ensuring balance and robustness. Unlike the fields of computer vision and natural language processing, where mature solutions have been developed to address these issues, graph neural networks (GNNs) continue to struggle with increasingly large-scale, imbalanced, and noisy datasets, lacking a unified dataset pruning solution. To achieve this, we introduce a novel dynamic soft-pruning method, GDeR, designed to update the training ``basket'' during the process using trainable prototypes. GDeR first constructs a well-modeled graph embedding hypersphere and then samples \textit{representative, balanced, and unbiased subsets} from this embedding space, which achieves the goal we called Graph Training Debugging. Extensive experiments on five datasets across three GNN backbones, demonstrate that GDeR (I) achieves or surpasses the performance of the full dataset with 30%~50% fewer training samples, (II) attains up to a 2.81x lossless training speedup, and (III) outperforms state-of-the-art pruning methods in imbalanced training and noisy training scenarios by 0.3%~4.3% and 3.6%~7.8%, respectively.</li>
</ul>

<h3>Title: Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yu Xia, Junda Wu, Sungchul Kim, Tong Yu, Ryan A. Rossi, Haoliang Wang, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13765">https://arxiv.org/abs/2410.13765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13765">https://arxiv.org/pdf/2410.13765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13765]] Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval(https://arxiv.org/abs/2410.13765)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been used to generate query expansions augmenting original queries for improving information search. Recent studies also explore providing LLMs with initial retrieval results to generate query expansions more grounded to document corpus. However, these methods mostly focus on enhancing textual similarities between search queries and target documents, overlooking document relations. For queries like "Find me a highly rated camera for wildlife photography compatible with my Nikon F-Mount lenses", existing methods may generate expansions that are semantically similar but structurally unrelated to user intents. To handle such semi-structured queries with both textual and relational requirements, in this paper we propose a knowledge-aware query expansion framework, augmenting LLMs with structured document relations from knowledge graph (KG). To further address the limitation of entity-based scoring in existing KG-based methods, we leverage document texts as rich KG node representations and use document-based relation filtering for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three datasets of diverse domains show the advantages of our method compared against state-of-the-art baselines on textual and relational semi-structured retrieval.</li>
</ul>

<h3>Title: Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?</h3>
<ul>
<li><strong>Authors: </strong>Argyrios Gerogiannis, Yu-Han Huang, Venugopal V. Veeravalli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13772">https://arxiv.org/abs/2410.13772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13772">https://arxiv.org/pdf/2410.13772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13772]] Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?(https://arxiv.org/abs/2410.13772)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without prior knowledge about the system's non-stationarity. A state-of-the-art, black-box algorithm, known as MASTER, is considered, with a focus on identifying the conditions under which it can achieve its stated goals. Specifically, we prove that MASTER's non-stationarity detection mechanism is not triggered for practical choices of horizon, leading to performance akin to a random restarting algorithm. Moreover, we show that the regret bound for MASTER, while being order optimal, stays above the worst-case linear regret until unreasonably large values of the horizon. To validate these observations, MASTER is tested for the special case of piecewise stationary multi-armed bandits, along with methods that employ random restarting, and others that use quickest change detection to restart. A simple, order optimal random restarting algorithm, that has prior knowledge of the non-stationarity is proposed as a baseline. The behavior of the MASTER algorithm is validated in simulations, and it is shown that methods employing quickest change detection are more robust and consistently outperform MASTER and other random restarting approaches.</li>
</ul>

<h3>Title: Enhancing Retail Sales Forecasting with Optimized Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Priyam Ganguly, Isha Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13773">https://arxiv.org/abs/2410.13773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13773">https://arxiv.org/pdf/2410.13773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13773]] Enhancing Retail Sales Forecasting with Optimized Machine Learning Models(https://arxiv.org/abs/2410.13773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In retail sales forecasting, accurately predicting future sales is crucial for inventory management and strategic planning. Traditional methods like LR often fall short due to the complexity of sales data, which includes seasonality and numerous product families. Recent advancements in machine learning (ML) provide more robust alternatives. This research benefits from the power of ML, particularly Random Forest (RF), Gradient Boosting (GB), Support Vector Regression (SVR), and XGBoost, to improve prediction accuracy. Despite advancements, a significant gap exists in handling complex datasets with high seasonality and multiple product families. The proposed solution involves implementing and optimizing a RF model, leveraging hyperparameter tuning through randomized search cross-validation. This approach addresses the complexities of the dataset, capturing intricate patterns that traditional methods miss. The optimized RF model achieved an R-squared value of 0.945, substantially higher than the initial RF model and traditional LR, which had an R-squared of 0.531. The model reduced the root mean squared logarithmic error (RMSLE) to 1.172, demonstrating its superior predictive capability. The optimized RF model did better than cutting-edge models like Gradient Boosting (R-squared: 0.942), SVR (R-squared: 0.940), and XGBoost (R-squared: 0.939), with more minor mean squared error (MSE) and mean absolute error (MAE) numbers. The results demonstrate that the optimized RF model excels in forecasting retail sales, handling the datasets complexity with higher accuracy and reliability. This research highlights the importance of advanced ML techniques in predictive analytics, offering a significant improvement over traditional methods and other contemporary models.</li>
</ul>

<h3>Title: Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors</h3>
<ul>
<li><strong>Authors: </strong>Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13776">https://arxiv.org/abs/2410.13776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13776">https://arxiv.org/pdf/2410.13776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13776]] Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors(https://arxiv.org/abs/2410.13776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than "learning" to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified.</li>
</ul>

<h3>Title: DPLM-2: A Multimodal Diffusion Protein Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13782">https://arxiv.org/abs/2410.13782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13782">https://arxiv.org/pdf/2410.13782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13782]] DPLM-2: A Multimodal Diffusion Protein Language Model(https://arxiv.org/abs/2410.13782)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.</li>
</ul>

<h3>Title: PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zekun Moore Wang, Shawn Wang, Kang Zhu, Jiaheng Liu, Ke Xu, Jie Fu, Wangchunshu Zhou, Wenhao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13785">https://arxiv.org/abs/2410.13785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13785">https://arxiv.org/pdf/2410.13785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13785]] PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment(https://arxiv.org/abs/2410.13785)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model variants or decoding temperatures. This singularity leads to two issues: (1) alignment is not comprehensive; and thereby (2) models are susceptible to jailbreaking attacks. To address these issues, we investigate how to construct more comprehensive and diversified contrasting patterns to enhance preference data (RQ1) and verify the impact of the diversification of contrasting patterns on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that integrates diversified contrasting patterns across the prompt, model, and pipeline levels, introducing six contrasting strategies that do not require additional feedback labeling procedures. Regarding RQ2, we conduct thorough experiments demonstrating that PopAlign significantly outperforms existing methods, leading to more comprehensive alignment.</li>
</ul>

<h3>Title: Looking Inward: Language Models Can Learn About Themselves by Introspection</h3>
<ul>
<li><strong>Authors: </strong>Felix J Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13787">https://arxiv.org/abs/2410.13787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13787">https://arxiv.org/pdf/2410.13787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13787]] Looking Inward: Language Models Can Learn About Themselves by Introspection(https://arxiv.org/abs/2410.13787)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data. We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger). In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.</li>
</ul>

<h3>Title: Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions</h3>
<ul>
<li><strong>Authors: </strong>Michael J.Q. Zhang, W. Bradley Knox, Eunsol Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13788">https://arxiv.org/abs/2410.13788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13788">https://arxiv.org/pdf/2410.13788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13788]] Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions(https://arxiv.org/abs/2410.13788)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) must often respond to highly ambiguous user requests. In such cases, the LLM's best response may be to ask a clarifying question to elicit more information. We observe existing LLMs often respond by presupposing a single interpretation of such ambiguous requests, frustrating users who intended a different interpretation. We speculate this is caused by current preference data labeling practice, where LLM responses are evaluated only on their prior contexts. To address this, we propose to assign preference labels by simulating their expected outcomes in the future turns. This allows LLMs to learn to ask clarifying questions when it can generate responses that are tailored to each user interpretation in future turns. In experiments on open-domain QA, we compare systems that trained using our proposed preference labeling methods against standard methods, which assign preferences based on only prior context. We evaluate systems based on their ability to ask clarifying questions that can recover each user's interpretation and expected answer, and find that our training with our proposed method trains LLMs to ask clarifying questions with a 5% improvement in F1 measured against the answer set from different interpretations of each query</li>
</ul>

<h3>Title: MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations</h3>
<ul>
<li><strong>Authors: </strong>Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13790">https://arxiv.org/abs/2410.13790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13790">https://arxiv.org/pdf/2410.13790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13790]] MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations(https://arxiv.org/abs/2410.13790)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the problem of how to build and benchmark a large motion model (LMM). The ultimate goal of LMM is to serve as a foundation model for versatile motion-related tasks, e.g., human motion generation, with interpretability and generalizability. Though advanced, recent LMM-related works are still limited by small-scale motion data and costly text descriptions. Besides, previous motion benchmarks primarily focus on pure body movements, neglecting the ubiquitous motions in context, i.e., humans interacting with humans, objects, and scenes. To address these limitations, we consolidate large-scale video action datasets as knowledge banks to build MotionBank, which comprises 13 video action datasets, 1.24M motion sequences, and 132.9M frames of natural and diverse human motions. Different from laboratory-captured motions, in-the-wild human-centric videos contain abundant motions in context. To facilitate better motion text alignment, we also meticulously devise a motion caption generation algorithm to automatically produce rule-based, unbiased, and disentangled text descriptions via the kinematic characteristics for each motion. Extensive experiments show that our MotionBank is beneficial for general motion-related tasks of human motion generation, motion in-context generation, and motion understanding. Video motions together with the rule-based text annotations could serve as an efficient alternative for larger LMMs. Our dataset, codes, and benchmark will be publicly available at this https URL.</li>
</ul>

<h3>Title: Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning</h3>
<ul>
<li><strong>Authors: </strong>Ilya Kaufman, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13792">https://arxiv.org/abs/2410.13792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13792">https://arxiv.org/pdf/2410.13792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13792]] Analyzing Deep Transformer Models for Time Series Forecasting via Manifold Learning(https://arxiv.org/abs/2410.13792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have consistently achieved remarkable results in various domains such as natural language processing and computer vision. However, despite ongoing research efforts to better understand these models, the field still lacks a comprehensive understanding. This is particularly true for deep time series forecasting methods, where analysis and understanding work is relatively limited. Time series data, unlike image and text information, can be more challenging to interpret and analyze. To address this, we approach the problem from a manifold learning perspective, assuming that the latent representations of time series forecasting models lie next to a low-dimensional manifold. In our study, we focus on analyzing the geometric features of these latent data manifolds, including intrinsic dimension and principal curvatures. Our findings reveal that deep transformer models exhibit similar geometric behavior across layers, and these geometric features are correlated with model performance. Additionally, we observe that untrained models initially have different structures, but they rapidly converge during training. By leveraging our geometric analysis and differentiable tools, we can potentially design new and improved deep forecasting neural networks. This approach complements existing analysis studies and contributes to a better understanding of transformer models in the context of time series forecasting. Code is released at this https URL.</li>
</ul>

<h3>Title: Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation</h3>
<ul>
<li><strong>Authors: </strong>Da Long, Zhitong Xu, Guang Yang, Akil Narayan, Shandian Zhe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13794">https://arxiv.org/abs/2410.13794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13794">https://arxiv.org/pdf/2410.13794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13794]] Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation(https://arxiv.org/abs/2410.13794)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modern physics simulation often involves multiple functions of interests, and traditional numerical approaches are known to be complex and computationally costly. While machine learning-based surrogate models can offer significant cost reductions, most focus on a single task, such as forward prediction, and typically lack uncertainty quantification -- an essential component in many applications. To overcome these limitations, we propose Arbitrarily-Conditioned Multi-Functional Diffusion (ACMFD), a versatile probabilistic surrogate model for multi-physics emulation. ACMFD can perform a wide range of tasks within a single framework, including forward prediction, various inverse problems, and simulating data for entire systems or subsets of quantities conditioned on others. Specifically, we extend the standard Denoising Diffusion Probabilistic Model (DDPM) for multi-functional generation by modeling noise as Gaussian processes (GP). We then introduce an innovative denoising loss. The training involves randomly sampling the conditioned part and fitting the corresponding predicted noise to zero, enabling ACMFD to flexibly generate function values conditioned on any other functions or quantities. To enable efficient training and sampling, and to flexibly handle irregularly sampled data, we use GPs to interpolate function samples onto a grid, inducing a Kronecker product structure for efficient computation. We demonstrate the advantages of ACMFD across several fundamental multi-physics systems.</li>
</ul>

<h3>Title: Adversarial Testing as a Tool for Interpretability: Length-based Overfitting of Elementary Functions in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Patrik Zavoral, Dušan Variš, Ondřej Bojar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13802">https://arxiv.org/abs/2410.13802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13802">https://arxiv.org/pdf/2410.13802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13802]] Adversarial Testing as a Tool for Interpretability: Length-based Overfitting of Elementary Functions in Transformers(https://arxiv.org/abs/2410.13802)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The Transformer model has a tendency to overfit various aspects of the training data, such as the overall sequence length. We study elementary string edit functions using a defined set of error indicators to interpret the behaviour of the sequence-to-sequence Transformer. We show that generalization to shorter sequences is often possible, but confirm that longer sequences are highly problematic, although partially correct answers are often obtained. Additionally, we find that other structural characteristics of the sequences, such as subsegment length, may be equally important. We hypothesize that the models learn algorithmic aspects of the tasks simultaneously with structural aspects but adhering to the structural aspects is unfortunately often preferred by Transformer when they come into conflict.</li>
</ul>

<h3>Title: BenTo: Benchmark Task Reduction with In-Context Transferability</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13804">https://arxiv.org/abs/2410.13804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13804">https://arxiv.org/pdf/2410.13804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13804]] BenTo: Benchmark Task Reduction with In-Context Transferability(https://arxiv.org/abs/2410.13804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a <4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only.</li>
</ul>

<h3>Title: A Watermark for Order-Agnostic Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13805">https://arxiv.org/abs/2410.13805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13805">https://arxiv.org/pdf/2410.13805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13805]] A Watermark for Order-Agnostic Language Models(https://arxiv.org/abs/2410.13805)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>Statistical watermarking techniques are well-established for sequentially decoded language models (LMs). However, these techniques cannot be directly applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not generated sequentially. In this work, we introduce Pattern-mark, a pattern-based watermarking framework specifically designed for order-agnostic LMs. We develop a Markov-chain-based watermark generator that produces watermark key sequences with high-frequency key patterns. Correspondingly, we propose a statistical pattern-based detection algorithm that recovers the key sequence during detection and conducts statistical tests based on the count of high-frequency patterns. Our extensive evaluations on order-agnostic LMs, such as ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection efficiency, generation quality, and robustness, positioning it as a superior watermarking technique for order-agnostic LMs.</li>
</ul>

<h3>Title: ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13807">https://arxiv.org/abs/2410.13807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13807">https://arxiv.org/pdf/2410.13807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13807]] ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution(https://arxiv.org/abs/2410.13807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (Real-ISR) aims at restoring high-quality (HQ) images from low-quality (LQ) inputs corrupted by unknown and complex degradations. In particular, pretrained text-to-image (T2I) diffusion models provide strong generative priors to reconstruct credible and intricate details. However, T2I generation focuses on semantic consistency while Real-ISR emphasizes pixel-level reconstruction, which hinders existing methods from fully exploiting diffusion priors. To address this challenge, we introduce ConsisSR to handle both semantic and pixel-level consistency. Specifically, compared to coarse-grained text prompts, we exploit the more powerful CLIP image embedding and effectively leverage both modalities through our Hybrid Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent inputs, our model not only handle timestep-specific diffusion noise but also refine the accumulated latent representations. Last but not least, our GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the diffusion start point. This accelerates the inference process to 10 steps while preserving sampling quality, in a training-free this http URL method demonstrates state-of-the-art performance among both full-scale and accelerated models. The code will be made publicly available.</li>
</ul>

<h3>Title: De-mark: Watermark Removal in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13808">https://arxiv.org/abs/2410.13808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13808">https://arxiv.org/pdf/2410.13808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13808]] De-mark: Watermark Removal in Large Language Models(https://arxiv.org/abs/2410.13808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models (LMs). However, the robustness of the watermarking schemes has not been well explored. In this paper, we present De-mark, an advanced framework designed to remove n-gram-based watermarks effectively. Our method utilizes a novel querying strategy, termed random selection probing, which aids in assessing the strength of the watermark and identifying the red-green list within the n-gram watermark. Experiments on popular LMs, such as Llama3 and ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark removal and exploitation tasks.</li>
</ul>

<h3>Title: Artificial Kuramoto Oscillatory Neurons</h3>
<ul>
<li><strong>Authors: </strong>Takeru Miyato, Sindy Löwe, Andreas Geiger, Max Welling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13821">https://arxiv.org/abs/2410.13821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13821">https://arxiv.org/pdf/2410.13821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13821]] Artificial Kuramoto Oscillatory Neurons(https://arxiv.org/abs/2410.13821)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.</li>
</ul>

<h3>Title: Multi-style conversion for semantic segmentation of lesions in fundus images by adversarial attacks</h3>
<ul>
<li><strong>Authors: </strong>Clément Playout, Renaud Duval, Marie Carole Boucher, Farida Cheriet</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13822">https://arxiv.org/abs/2410.13822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13822">https://arxiv.org/pdf/2410.13822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13822]] Multi-style conversion for semantic segmentation of lesions in fundus images by adversarial attacks(https://arxiv.org/abs/2410.13822)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>The diagnosis of diabetic retinopathy, which relies on fundus images, faces challenges in achieving transparency and interpretability when using a global classification approach. However, segmentation-based databases are significantly more expensive to acquire and combining them is often problematic. This paper introduces a novel method, termed adversarial style conversion, to address the lack of standardization in annotation styles across diverse databases. By training a single architecture on combined databases, the model spontaneously modifies its segmentation style depending on the input, demonstrating the ability to convert among different labeling styles. The proposed methodology adds a linear probe to detect dataset origin based on encoder features and employs adversarial attacks to condition the model's segmentation style. Results indicate significant qualitative and quantitative through dataset combination, offering avenues for improved model generalization, uncertainty estimation and continuous interpolation between annotation styles. Our approach enables training a segmentation model with diverse databases while controlling and leveraging annotation styles for improved retinopathy diagnosis.</li>
</ul>

<h3>Title: Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Xiaodan Xing, Junzhi Ning, Yang Nan, Guang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13823">https://arxiv.org/abs/2410.13823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13823">https://arxiv.org/pdf/2410.13823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13823]] Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning(https://arxiv.org/abs/2410.13823)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Deep generative models have significantly advanced medical imaging analysis by enhancing dataset size and quality. Beyond mere data augmentation, our research in this paper highlights an additional, significant capacity of deep generative models: their ability to reveal and demonstrate patterns in medical images. We employ a generative structure with hybrid conditions, combining clinical data and segmentation masks to guide the image synthesis process. Furthermore, we innovatively transformed the tabular clinical data into textual descriptions. This approach simplifies the handling of missing values and also enables us to leverage large pre-trained vision-language models that investigate the relations between independent clinical entries and comprehend general terms, such as gender and smoking status. Our approach differs from and presents a more challenging task than traditional medical report-guided synthesis due to the less visual correlation of our clinical information with the images. To overcome this, we introduce a text-visual embedding mechanism that strengthens the conditions, ensuring the network effectively utilizes the provided information. Our pipeline is generalizable to both GAN-based and diffusion models. Experiments on chest CT, particularly focusing on the smoking status, demonstrated a consistent intensity shift in the lungs which is in agreement with clinical observations, indicating the effectiveness of our method in capturing and visualizing the impact of specific attributes on medical image patterns. Our methods offer a new avenue for the early detection and precise visualization of complex clinical conditions with deep generative models. All codes are this https URL.</li>
</ul>

<h3>Title: Harnessing Webpage UIs for Text-Rich Visual Understanding</h3>
<ul>
<li><strong>Authors: </strong>Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13824">https://arxiv.org/abs/2410.13824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13824">https://arxiv.org/pdf/2410.13824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13824]] Harnessing Webpage UIs for Text-Rich Visual Understanding(https://arxiv.org/abs/2410.13824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimodal instructions from webpage UIs using text-based large language models (LLMs). Despite lacking direct visual input, text-based LLMs are able to process structured text representations from webpage accessibility trees. These instructions are then paired with UI screenshots to train multimodal models. We introduce MultiUI, a dataset containing 7.3 million samples from 1 million websites, covering diverse multimodal tasks and UI layouts. Models trained on MultiUI not only excel in web UI tasks-achieving up to a 48\% improvement on VisualWebBench and a 19.1\% boost in action accuracy on a web agent dataset Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to non-UI domains, such as document understanding, OCR, and chart interpretation. These results highlight the broad applicability of web UI data for advancing text-rich visual understanding across various scenarios.</li>
</ul>

<h3>Title: DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</h3>
<ul>
<li><strong>Authors: </strong>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13830">https://arxiv.org/abs/2410.13830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13830">https://arxiv.org/pdf/2410.13830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13830]] DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control(https://arxiv.org/abs/2410.13830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.</li>
</ul>

<h3>Title: The Disparate Benefits of Deep Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Kajetan Schweighofer, Adrian Arnaiz-Rodriguez, Sepp Hochreiter, Nuria Oliver</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13831">https://arxiv.org/abs/2410.13831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13831">https://arxiv.org/pdf/2410.13831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13831]] The Disparate Benefits of Deep Ensembles(https://arxiv.org/abs/2410.13831)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness investigates how a model's performance varies across different groups, typically defined by protected attributes such as age, gender, or race. In this work, we investigate the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups in what we refer to as a disparate benefits effect. We empirically investigate this effect with Deep Ensembles applied to popular facial analysis and medical imaging datasets, where protected group attributes are given and find that it occurs for multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify the per-group difference in predictive diversity of ensemble members as the potential cause of the disparate benefits effect. Finally, we evaluate different approaches to reduce unfairness due to the disparate benefits effect. Our findings show that post-processing is an effective method to mitigate this unfairness while preserving the improved performance of Deep Ensembles.</li>
</ul>

<h3>Title: VidPanos: Generative Panoramic Videos from Casual Panning Videos</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, Forrester Cole</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13832">https://arxiv.org/abs/2410.13832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13832">https://arxiv.org/pdf/2410.13832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13832]] VidPanos: Generative Panoramic Videos from Casual Panning Videos(https://arxiv.org/abs/2410.13832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture the scene. We present a method for synthesizing a panoramic video from a casually-captured panning video, as if the original video were captured with a wide-angle camera. We pose panorama synthesis as a space-time outpainting problem, where we aim to create a full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires a powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as a component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for a range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features.</li>
</ul>

<h3>Title: Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13835">https://arxiv.org/abs/2410.13835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13835">https://arxiv.org/pdf/2410.13835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13835]] Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs(https://arxiv.org/abs/2410.13835)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called "sink tokens" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability. We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures -- transformers with one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.</li>
</ul>

<h3>Title: SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction</h3>
<ul>
<li><strong>Authors: </strong>Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13846">https://arxiv.org/abs/2410.13846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13846">https://arxiv.org/pdf/2410.13846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13846]] SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction(https://arxiv.org/abs/2410.13846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have extended their capabilities to handle long contexts. However, increasing the number of model layers and the length of input sequences significantly escalates the memory required to store key-value (KV) cache, posing challenges for efficient inference. To mitigate this issue, we present SimLayerKV, a simple yet effective method that reduces inter-layer KV cache redundancies by selectively dropping cache in identified lazy layers. Our approach is based on the observation that certain layers in long-context LLMs exhibit "lazy" behavior, contributing less to modeling long-range dependencies compared to non-lazy layers. By analyzing attention weight patterns, we find that the behavior of these lazy layers is consistent across tokens during generation for a given input. This insight motivates our SimLayerKV, which identifies lazy layers and reduces their KV cache accordingly. SimLayerKV is training-free, generalizable, and can be implemented with only seven lines of code. We conduct extensive experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and Mistral-7B across 16 tasks from the LongBench benchmark. The results demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$ with only a 1.2% performance drop when combined with 4-bit quantization. Our code is available at this https URL.</li>
</ul>

<h3>Title: Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13848">https://arxiv.org/abs/2410.13848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13848">https://arxiv.org/pdf/2410.13848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13848]] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation(https://arxiv.org/abs/2410.13848)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.</li>
</ul>

<h3>Title: Influence Functions for Scalable Data Attribution in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David Krueger, Richard Turner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13850">https://arxiv.org/abs/2410.13850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13850">https://arxiv.org/pdf/2410.13850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13850]] Influence Functions for Scalable Data Attribution in Diffusion Models(https://arxiv.org/abs/2410.13850)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by developing an \textit{influence functions} framework. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we systematically develop K-FAC approximations based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We recast previously proposed methods as specific design choices in our framework and show that our recommended method outperforms previous data attribution approaches on common evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning.</li>
</ul>

<h3>Title: Retrospective Learning from Interactions</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13852">https://arxiv.org/abs/2410.13852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13852">https://arxiv.org/pdf/2410.13852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13852]] Retrospective Learning from Interactions(https://arxiv.org/abs/2410.13852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.</li>
</ul>

<h3>Title: Can MLLMs Understand the Deep Implication Behind Chinese Images?</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Zhang, Xi Feng, Yuelin Bai, Xinrun Du, Jinchang Hou, Kaixin Deng, Guangzeng Han, Qinrui Li, Bingli Wang, Jiaheng Liu, Xingwei Qu, Yifei Zhang, Qixuan Zhao, Yiming Liang, Ziqiang Liu, Feiteng Fang, Min Yang, Wenhao Huang, Chenghua Lin, Ge Zhang, Shiwen Ni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13854">https://arxiv.org/abs/2410.13854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13854">https://arxiv.org/pdf/2410.13854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13854]] Can MLLMs Understand the Deep Implication Behind Chinese Images?(https://arxiv.org/abs/2410.13854)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we introduce the **C**hinese **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which aims to assess the higher-order perception and understanding capabilities of MLLMs for Chinese images. CII-Bench stands out in several ways compared to existing benchmarks. Firstly, to ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model's understanding of Chinese traditional culture. Through extensive experiments on CII-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on CII-Bench. The highest accuracy of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional culture images, suggesting limitations in their ability to understand high-level semantics and lack a deep knowledge base of Chinese traditional culture. Finally, it is observed that most models exhibit enhanced accuracy when image emotion hints are incorporated into the prompts. We believe that CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics and Chinese-specific images, advancing the journey towards expert artificial general intelligence (AGI). Our project is publicly available at this https URL.</li>
</ul>

<h3>Title: Diffusing States and Matching Scores: A New Framework for Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Runzhe Wu, Yiding Chen, Gokul Swamy, Kianté Brantley, Wen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13855">https://arxiv.org/abs/2410.13855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13855">https://arxiv.org/pdf/2410.13855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13855]] Diffusing States and Matching Scores: A New Framework for Imitation Learning(https://arxiv.org/abs/2410.13855)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Adversarial Imitation Learning is traditionally framed as a two-player zero-sum game between a learner and an adversarially chosen cost function, and can therefore be thought of as the sequential generalization of a Generative Adversarial Network (GAN). A prominent example of this framework is Generative Adversarial Imitation Learning (GAIL). However, in recent years, diffusion models have emerged as a non-adversarial alternative to GANs that merely require training a score function via regression, yet produce generations of a higher quality. In response, we investigate how to lift insights from diffusion modeling to the sequential setting. We propose diffusing states and performing score-matching along diffused states to measure the discrepancy between the expert's and learner's states. Thus, our approach only requires training score functions to predict noises via standard regression, making it significantly easier and more stable to train than adversarial methods. Theoretically, we prove first- and second-order instance-dependent bounds with linear scaling in the horizon, proving that our approach avoids the compounding errors that stymie offline approaches to imitation learning. Empirically, we show our approach outperforms GAN-style imitation learning baselines across various continuous control problems, including complex tasks like controlling humanoids to walk, sit, and crawl.</li>
</ul>

<h3>Title: How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13857">https://arxiv.org/abs/2410.13857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13857">https://arxiv.org/pdf/2410.13857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13857]] How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs(https://arxiv.org/abs/2410.13857)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Transformer-based Large Language Models (LLMs) across various domains, understanding and enhancing their mathematical capabilities remains a significant challenge. In this paper, we conduct a rigorous theoretical analysis of LLMs' mathematical abilities, with a specific focus on their arithmetic performances. We identify numerical precision as a key factor that influences their effectiveness in mathematical tasks. Our results show that Transformers operating with low numerical precision fail to address arithmetic tasks, such as iterated addition and integer multiplication, unless the model size grows super-polynomially with respect to the input length. In contrast, Transformers with standard numerical precision can efficiently handle these tasks with significantly smaller model sizes. We further support our theoretical findings through empirical experiments that explore the impact of varying numerical precision on arithmetic tasks, providing valuable insights for improving the mathematical reasoning capabilities of LLMs.</li>
</ul>

<h3>Title: $\gamma-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13859">https://arxiv.org/abs/2410.13859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13859">https://arxiv.org/pdf/2410.13859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13859]] $\gamma-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models(https://arxiv.org/abs/2410.13859)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called $\gamma$-MoD. In $\gamma$-MoD, a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of $\gamma$-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with a minor performance drop, i.e., -1.5%, $\gamma$-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.</li>
</ul>

<h3>Title: PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13861">https://arxiv.org/abs/2410.13861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13861">https://arxiv.org/pdf/2410.13861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13861]] PUMA: Empowering Unified MLLM with Multi-granular Visual Generation(https://arxiv.org/abs/2410.13861)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in this https URL.</li>
</ul>

<h3>Title: DepthSplat: Connecting Gaussian Splatting and Depth</h3>
<ul>
<li><strong>Authors: </strong>Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13862">https://arxiv.org/abs/2410.13862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13862">https://arxiv.org/pdf/2410.13862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13862]] DepthSplat: Connecting Gaussian Splatting and Depth(https://arxiv.org/abs/2410.13862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gaussian splatting and single/multi-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabelled datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. Our code, models, and video results are available at this https URL.</li>
</ul>

<h3>Title: Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</h3>
<ul>
<li><strong>Authors: </strong>Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13863">https://arxiv.org/abs/2410.13863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13863">https://arxiv.org/pdf/2410.13863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13863]] Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens(https://arxiv.org/abs/2410.13863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
