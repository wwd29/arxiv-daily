<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: SmartBugs 2.0: An Execution Framework for Weakness Detection in Ethereum Smart Contracts. (arXiv:2306.05057v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05057">http://arxiv.org/abs/2306.05057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05057] SmartBugs 2](http://arxiv.org/abs/2306.05057) #secure</code></li>
<li>Summary: <p>Smart contracts are blockchain programs that often handle valuable assets.
Writing secure smart contracts is far from trivial, and any vulnerability may
lead to significant financial losses. To support developers in identifying and
eliminating vulnerabilities, methods and tools for the automated analysis have
been proposed. However, the lack of commonly accepted benchmark suites and
performance metrics makes it difficult to compare and evaluate such tools.
Moreover, the tools are heterogeneous in their interfaces and reports as well
as their runtime requirements, and installing several tools is time-consuming.
</p></li>
</ul>

<p>In this paper, we present SmartBugs 2.0, a modular execution framework. It
provides a uniform interface to 19 tools aimed at smart contract analysis and
accepts both Solidity source code and EVM bytecode as input. After describing
its architecture, we highlight the features of the framework. We evaluate the
framework via its reception by the community and illustrate its scalability by
describing its role in a study involving 3.25 million analyses.
</p>

<h2>security</h2>
<h3>Title: From Data to Action: Exploring AI and IoT-driven Solutions for Smarter Cities. (arXiv:2306.04653v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04653">http://arxiv.org/abs/2306.04653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04653] From Data to Action: Exploring AI and IoT-driven Solutions for Smarter Cities](http://arxiv.org/abs/2306.04653) #security</code></li>
<li>Summary: <p>The emergence of smart cities demands harnessing advanced technologies like
the Internet of Things (IoT) and Artificial Intelligence (AI) and promises to
unlock cities' potential to become more sustainable, efficient, and ultimately
livable for their inhabitants. This work introduces an intelligent city
management system that provides a data-driven approach to three use cases: (i)
analyze traffic information to reduce the risk of traffic collisions and
improve driver and pedestrian safety, (ii) identify when and where energy
consumption can be reduced to improve cost savings, and (iii) detect
maintenance issues like potholes in the city's roads and sidewalks, as well as
the beginning of hazards like floods and fires. A case study in Aveiro City
demonstrates the system's effectiveness in generating actionable insights that
enhance security, energy efficiency, and sustainability, while highlighting the
potential of AI and IoT-driven solutions for smart city development.
</p></li>
</ul>

<h3>Title: Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04877">http://arxiv.org/abs/2306.04877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04877] Trojan Model Detection Using Activation Optimization](http://arxiv.org/abs/2306.04877) #security</code></li>
<li>Summary: <p>Due to data's unavailability or large size, and the high computational and
human labor costs of training machine learning models, it is a common practice
to rely on open source pre-trained models whenever possible. However, this
practice is worry some from the security perspective. Pre-trained models can be
infected with Trojan attacks, in which the attacker embeds a trigger in the
model such that the model's behavior can be controlled by the attacker when the
trigger is present in the input. In this paper, we present our preliminary work
on a novel method for Trojan model detection. Our method creates a signature
for a model based on activation optimization. A classifier is then trained to
detect a Trojan model given its signature. Our method achieves state of the art
performance on two public datasets.
</p></li>
</ul>

<h3>Title: A GDPR-compliant Risk Management Approach based on Threat Modelling and ISO 27005. (arXiv:2306.04783v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04783">http://arxiv.org/abs/2306.04783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04783] A GDPR-compliant Risk Management Approach based on Threat Modelling and ISO 27005](http://arxiv.org/abs/2306.04783) #security</code></li>
<li>Summary: <p>Computer systems process, store and transfer sensitive information which
makes them a valuable asset. Despite the existence of standards such as ISO
27005 for managing information risk, cyber threats are increasing, exposing
such systems to security breaches, and at the same time, compromising users'
privacy. However, threat modelling has also emerged as an alternative to
identify and analyze them, reducing the attack landscape by discarding low-risk
attack vectors, and mitigating high-risk ones. In this work, we introduce a
novel threat-modelling-based approach for risk management, using ISO 27005 as a
baseline for integrating ISO 27001/27002 security controls with privacy
regulations outlined in the European General Data Protection Regulation (GDPR).
In our proposal, risk estimation and mitigation is enhanced by combining STRIDE
and attack trees as a threat modelling strategy. Our approach is applied to an
IoT case study, where different attacks are analyzed to determine their risk
levels and potential countermeasures.
</p></li>
</ul>

<h3>Title: FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04959">http://arxiv.org/abs/2306.04959</a></li>
<li>Code URL: <a href="https://github.com/FedML-AI/FedML">https://github.com/FedML-AI/FedML</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04959] FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs](http://arxiv.org/abs/2306.04959) #security</code></li>
<li>Summary: <p>This paper introduces FedMLSecurity, a benchmark that simulates adversarial
attacks and corresponding defense mechanisms in Federated Learning (FL). As an
integral module of the open-sourced library FedML that facilitates FL algorithm
development and performance comparison, FedMLSecurity enhances the security
assessment capacity of FedML. FedMLSecurity comprises two principal components:
FedMLAttacker, which simulates attacks injected into FL training, and
FedMLDefender, which emulates defensive strategies designed to mitigate the
impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to
a wide range of machine learning models (e.g., Logistic Regression, ResNet,
GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.).
Experimental evaluations in this paper also demonstrate the ease of application
of FedMLSecurity to Large Language Models (LLMs), further reinforcing its
versatility and practical utility in various scenarios.
</p></li>
</ul>

<h3>Title: Machine Learning in Digital Forensics: A Systematic Literature Review. (arXiv:2306.04965v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04965">http://arxiv.org/abs/2306.04965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04965] Machine Learning in Digital Forensics: A Systematic Literature Review](http://arxiv.org/abs/2306.04965) #security</code></li>
<li>Summary: <p>Development and exploitation of technology have led to the further expansion
and complexity of digital crimes. On the other hand, the growing volume of data
and, subsequently, evidence is a severe challenge in digital forensics. In
recent years, the application of machine learning techniques to identify and
analyze evidence has been on the rise in different digital forensics domains.
This paper offers a systematic literature review of the research published in
major academic databases from January 2010 to December 2021 on the application
of machine learning in digital forensics, which was not presented yet to the
best of our knowledge as comprehensive as this. The review also identifies the
domains of digital forensics and machine learning methods that have received
the most attention in the previous papers and finally introduces remaining
research gaps. Our findings demonstrate that image forensics has obtained the
greatest benefit from using machine learning methods, compared to other
forensic domains. Moreover, CNN-based models are the most important machine
learning methods that are increasingly being used in digital forensics. We
present a comprehensive mind map to provide a proper perspective for valuable
analytical results. Furthermore, visual analysis has been conducted based on
the keywords of the papers, providing different thematic relevance topics. This
research will give digital forensics investigators, machine learning
developers, security researchers, and enthusiasts a broad view of the
application of machine learning in digital forensics.
</p></li>
</ul>

<h3>Title: Detecting Neural Trojans Through Merkle Trees. (arXiv:2306.05368v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05368">http://arxiv.org/abs/2306.05368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05368] Detecting Neural Trojans Through Merkle Trees](http://arxiv.org/abs/2306.05368) #security</code></li>
<li>Summary: <p>Deep neural networks are utilized in a growing number of industries. Much of
the current literature focuses on the applications of deep neural networks
without discussing the security of the network itself. One security issue
facing deep neural networks is neural trojans. Through a neural trojan, a
malicious actor may force the deep neural network to act in unintended ways.
Several potential defenses have been proposed, but they are computationally
expensive, complex, or unusable in commercial applications. We propose Merkle
trees as a novel way to detect and isolate neural trojans.
</p></li>
</ul>

<h3>Title: Sequential Graph Neural Networks for Source Code Vulnerability Identification. (arXiv:2306.05375v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05375">http://arxiv.org/abs/2306.05375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05375] Sequential Graph Neural Networks for Source Code Vulnerability Identification](http://arxiv.org/abs/2306.05375) #security</code></li>
<li>Summary: <p>Vulnerability identification constitutes a task of high importance for cyber
security. It is quite helpful for locating and fixing vulnerable functions in
large applications. However, this task is rather challenging owing to the
absence of reliable and adequately managed datasets and learning models.
Existing solutions typically rely on human expertise to annotate datasets or
specify features, which is prone to error. In addition, the learning models
have a high rate of false positives. To bridge this gap, in this paper, we
present a properly curated C/C++ source code vulnerability dataset, denoted as
CVEFunctionGraphEmbeddings (CVEFGE), to aid in developing models. CVEFGE is
automatically crawled from the CVE database, which contains authentic and
publicly disclosed source code vulnerabilities. We also propose a learning
framework based on graph neural networks, denoted SEquential Graph Neural
Network (SEGNN) for learning a large number of code semantic representations.
SEGNN consists of a sequential learning module, graph convolution, pooling, and
fully connected layers. Our evaluations on two datasets and four baseline
methods in a graph classification setting demonstrate state-of-the-art results.
</p></li>
</ul>

<h3>Title: Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique. (arXiv:2306.04646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04646">http://arxiv.org/abs/2306.04646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04646] Improve State-Level Wheat Yield Forecasts in Kazakhstan on GEOGLAM's EO Data by Leveraging A Simple Spatial-Aware Technique](http://arxiv.org/abs/2306.04646) #security</code></li>
<li>Summary: <p>Accurate yield forecasting is essential for making informed policies and
long-term decisions for food security. Earth Observation (EO) data and machine
learning algorithms play a key role in providing a comprehensive and timely
view of crop conditions from field to national scales. However, machine
learning algorithms' prediction accuracy is often harmed by spatial
heterogeneity caused by exogenous factors not reflected in remote sensing data,
such as differences in crop management strategies. In this paper, we propose
and investigate a simple technique called state-wise additive bias to
explicitly address the cross-region yield heterogeneity in Kazakhstan. Compared
to baseline machine learning models (Random Forest, CatBoost, XGBoost), our
method reduces the overall RMSE by 8.9\% and the highest state-wise RMSE by
28.37\%. The effectiveness of state-wise additive bias indicates machine
learning's performance can be significantly improved by explicitly addressing
the spatial heterogeneity, motivating future work on spatial-aware machine
learning algorithms for yield forecasts as well as for general geospatial
forecasting problems.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Does Image Anonymization Impact Computer Vision Training?. (arXiv:2306.05135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05135">http://arxiv.org/abs/2306.05135</a></li>
<li>Code URL: <a href="https://github.com/hukkelas/deep_privacy2">https://github.com/hukkelas/deep_privacy2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05135] Does Image Anonymization Impact Computer Vision Training?](http://arxiv.org/abs/2306.05135) #privacy</code></li>
<li>Summary: <p>Image anonymization is widely adapted in practice to comply with privacy
regulations in many regions. However, anonymization often degrades the quality
of the data, reducing its utility for computer vision development. In this
paper, we investigate the impact of image anonymization for training computer
vision models on key computer vision tasks (detection, instance segmentation,
and pose estimation). Specifically, we benchmark the recognition drop on common
detection datasets, where we evaluate both traditional and realistic
anonymization for faces and full bodies. Our comprehensive experiments reflect
that traditional image anonymization substantially impacts final model
performance, particularly when anonymizing the full body. Furthermore, we find
that realistic anonymization can mitigate this decrease in performance, where
our experiments reflect a minimal performance drop for face anonymization. Our
study demonstrates that realistic anonymization can enable privacy-preserving
computer vision development with minimal performance degradation across a range
of important computer vision benchmarks.
</p></li>
</ul>

<h3>Title: Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose. (arXiv:2306.05147v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05147">http://arxiv.org/abs/2306.05147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05147] Human Action Recognition in Egocentric Perspective Using 2D Object and Hands Pose](http://arxiv.org/abs/2306.05147) #privacy</code></li>
<li>Summary: <p>Egocentric action recognition is essential for healthcare and assistive
technology that relies on egocentric cameras because it allows for the
automatic and continuous monitoring of activities of daily living (ADLs)
without requiring any conscious effort from the user. This study explores the
feasibility of using 2D hand and object pose information for egocentric action
recognition. While current literature focuses on 3D hand pose information, our
work shows that using 2D skeleton data is a promising approach for hand-based
action classification, might offer privacy enhancement, and could be less
computationally demanding. The study uses a state-of-the-art transformer-based
method to classify sequences and achieves validation results of 94%,
outperforming other existing solutions. The accuracy of the test subset drops
to 76%, indicating the need for further generalization improvement. This
research highlights the potential of 2D hand and object pose information for
action recognition tasks and offers a promising alternative to 3D-based
methods.
</p></li>
</ul>

<h3>Title: Privately generating tabular data using language models. (arXiv:2306.04803v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04803">http://arxiv.org/abs/2306.04803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04803] Privately generating tabular data using language models](http://arxiv.org/abs/2306.04803) #privacy</code></li>
<li>Summary: <p>Privately generating synthetic data from a table is an important brick of a
privacy-first world. We propose and investigate a simple approach of treating
each row in a table as a sentence and training a language model with
differential privacy. We show this approach obtains competitive results in
modelling tabular data across multiple datasets, even at small scales that
favor alternative methods based on marginal distributions.
</p></li>
</ul>

<h3>Title: PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05087">http://arxiv.org/abs/2306.05087</a></li>
<li>Code URL: <a href="https://github.com/weopenml/pandalm">https://github.com/weopenml/pandalm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05087] PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](http://arxiv.org/abs/2306.05087) #privacy</code></li>
<li>Summary: <p>Instruction tuning large language models (LLMs) remains a challenging task,
owing to the complexity of hyperparameter selection and the difficulty involved
in evaluating the tuned models. To determine the optimal hyperparameters, an
automatic, robust, and reliable evaluation benchmark is essential. However,
establishing such a benchmark is not a trivial task due to the challenges
associated with evaluation accuracy and privacy protection. In response to
these challenges, we introduce a judge large language model, named PandaLM,
which is trained to distinguish the superior model given several LLMs.
PandaLM's focus extends beyond just the objective correctness of responses,
which is the main focus of traditional evaluation datasets. It addresses vital
subjective factors such as relative conciseness, clarity, adherence to
instructions, comprehensiveness, and formality. To ensure the reliability of
PandaLM, we collect a diverse human-annotated test dataset, where all contexts
are generated by humans and labels are aligned with human preferences. Our
results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation
ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM
enables the evaluation of LLM to be fairer but with less cost, evidenced by
significant improvements achieved by models tuned through PandaLM compared to
their counterparts trained with default Alpaca's hyperparameters. In addition,
PandaLM does not depend on API-based evaluations, thus avoiding potential data
leakage. All resources of PandaLM are released at
https://github.com/WeOpenML/PandaLM.
</p></li>
</ul>

<h3>Title: Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation. (arXiv:2306.04924v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04924">http://arxiv.org/abs/2306.04924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04924] Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation](http://arxiv.org/abs/2306.04924) #privacy</code></li>
<li>Summary: <p>We study the mean estimation problem under communication and local
differential privacy constraints. While previous work has proposed
\emph{order}-optimal algorithms for the same problem (i.e., asymptotically
optimal as we spend more bits), \emph{exact} optimality (in the non-asymptotic
setting) still has not been achieved. In this work, we take a step towards
characterizing the \emph{exact}-optimal approach in the presence of shared
randomness (a random variable shared between the server and the user) and
identify several necessary conditions for \emph{exact} optimality. We prove
that one of the necessary conditions is to utilize a rotationally symmetric
shared random codebook. Based on this, we propose a randomization mechanism
where the codebook is a randomly rotated simplex -- satisfying the necessary
properties of the \emph{exact}-optimal codebook. The proposed mechanism is
based on a $k$-closest encoding which we prove to be \emph{exact}-optimal for
the randomly rotated simplex codebook.
</p></li>
</ul>

<h3>Title: Federated Linear Contextual Bandits with User-level Differential Privacy. (arXiv:2306.05275v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05275">http://arxiv.org/abs/2306.05275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05275] Federated Linear Contextual Bandits with User-level Differential Privacy](http://arxiv.org/abs/2306.05275) #privacy</code></li>
<li>Summary: <p>This paper studies federated linear contextual bandits under the notion of
user-level differential privacy (DP). We first introduce a unified federated
bandits framework that can accommodate various definitions of DP in the
sequential decision-making setting. We then formally introduce user-level
central DP (CDP) and local DP (LDP) in the federated bandits framework, and
investigate the fundamental trade-offs between the learning regrets and the
corresponding DP guarantees in a federated linear contextual bandits model. For
CDP, we propose a federated algorithm termed as \robin and show that it is
near-optimal in terms of the number of clients $M$ and the privacy budget
$\varepsilon$ by deriving nearly-matching upper and lower regret bounds when
user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating
that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret
blow-up factor at least {$\min{1/\varepsilon,M}$ or
$\min{1/\sqrt{\varepsilon},\sqrt{M}}$} under different conditions.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04642">http://arxiv.org/abs/2306.04642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04642] DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models](http://arxiv.org/abs/2306.04642) #protect</code></li>
<li>Summary: <p>Recently, Generative Diffusion Models (GDMs) have showcased their remarkable
capabilities in learning and generating images. A large community of GDMs has
naturally emerged, further promoting the diversified applications of GDMs in
various fields. However, this unrestricted proliferation has raised serious
concerns about copyright protection. For example, artists including painters
and photographers are becoming increasingly concerned that GDMs could
effortlessly replicate their unique creative works without authorization. In
response to these challenges, we introduce a novel watermarking scheme,
DiffusionShield, tailored for GDMs. DiffusionShield protects images from
copyright infringement by GDMs through encoding the ownership information into
an imperceptible watermark and injecting it into the images. Its watermark can
be easily learned by GDMs and will be reproduced in their generated images. By
detecting the watermark from generated images, copyright infringement can be
exposed with evidence. Benefiting from the uniformity of the watermarks and the
joint optimization method, DiffusionShield ensures low distortion of the
original image, high watermark detection performance, and the ability to embed
lengthy messages. We conduct rigorous and comprehensive experiments to show the
effectiveness of DiffusionShield in defending against infringement by GDMs and
its superiority over traditional watermarking methods.
</p></li>
</ul>

<h3>Title: Ownership Protection of Generative Adversarial Networks. (arXiv:2306.05233v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05233">http://arxiv.org/abs/2306.05233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05233] Ownership Protection of Generative Adversarial Networks](http://arxiv.org/abs/2306.05233) #protect</code></li>
<li>Summary: <p>Generative adversarial networks (GANs) have shown remarkable success in image
synthesis, making GAN models themselves commercially valuable to legitimate
model owners. Therefore, it is critical to technically protect the intellectual
property of GANs. Prior works need to tamper with the training set or training
process, and they are not robust to emerging model extraction attacks. In this
paper, we propose a new ownership protection method based on the common
characteristics of a target model and its stolen models. Our method can be
directly applicable to all well-trained GANs as it does not require retraining
target models. Extensive experimental results show that our new method can
achieve the best protection performance, compared to the state-of-the-art
methods. Finally, we demonstrate the effectiveness of our method with respect
to the number of generations of model extraction attacks, the number of
generated samples, different datasets, as well as adaptive attacks.
</p></li>
</ul>

<h3>Title: Parallel and Asynchronous Smart Contract Execution. (arXiv:2306.05007v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05007">http://arxiv.org/abs/2306.05007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05007] Parallel and Asynchronous Smart Contract Execution](http://arxiv.org/abs/2306.05007) #protect</code></li>
<li>Summary: <p>Today's blockchains suffer from low throughput and high latency, which
impedes their widespread adoption of more complex applications like smart
contracts. In this paper, we propose a novel paradigm for smart contract
execution. It distinguishes between consensus nodes and execution nodes:
different groups of execution nodes can execute transactions in parallel;
meanwhile, consensus nodes can asynchronously order transactions and process
execution results. Moreover, it requires no coordination among execution nodes
and can effectively prevent livelocks. We show two ways of applying this
paradigm to blockchains. First, we show how we can make Ethereum support
parallel and asynchronous contract execution \emph{without hard-forks}. Then,
we propose a new public, permissionless blockchain. Our benchmark shows that,
with a fast consensus layer, it can provide a high throughput even for complex
transactions like Cryptokitties gene mixing. It can also protect simple
transactions from being starved by complex transactions.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions. (arXiv:2306.04756v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04756">http://arxiv.org/abs/2306.04756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04756] A Linearly Convergent GAN Inversion-based Algorithm for Reverse Engineering of Deceptions](http://arxiv.org/abs/2306.04756) #defense</code></li>
<li>Summary: <p>An important aspect of developing reliable deep learning systems is devising
strategies that make these systems robust to adversarial attacks. There is a
long line of work that focuses on developing defenses against these attacks,
but recently, researchers have began to study ways to reverse engineer the
attack process. This allows us to not only defend against several attack
models, but also classify the threat model. However, there is still a lack of
theoretical guarantees for the reverse engineering process. Current approaches
that give any guarantees are based on the assumption that the data lies in a
union of linear subspaces, which is not a valid assumption for more complex
datasets. In this paper, we build on prior work and propose a novel framework
for reverse engineering of deceptions which supposes that the clean data lies
in the range of a GAN. To classify the signal and attack, we jointly solve a
GAN inversion problem and a block-sparse recovery problem. For the first time
in the literature, we provide deterministic linear convergence guarantees for
this problem. We also empirically demonstrate the merits of the proposed
approach on several nonlinear datasets as compared to state-of-the-art methods.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04955">http://arxiv.org/abs/2306.04955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04955] Degraded Polygons Raise Fundamental Questions of Neural Network Perception](http://arxiv.org/abs/2306.04955) #attack</code></li>
<li>Summary: <p>It is well-known that modern computer vision systems often exhibit behaviors
misaligned with those of humans: from adversarial attacks to image corruptions,
deep learning vision models suffer in a variety of settings that humans capably
handle. In light of these phenomena, here we introduce another, orthogonal
perspective studying the human-machine vision gap. We revisit the task of
recovering images under degradation, first introduced over 30 years ago in the
Recognition-by-Components theory of human vision. Specifically, we study the
performance and behavior of neural networks on the seemingly simple task of
classifying regular polygons at varying orders of degradation along their
perimeters. To this end, we implement the Automated Shape Recoverability Test
for rapidly generating large-scale datasets of perimeter-degraded regular
polygons, modernizing the historically manual creation of image recoverability
experiments. We then investigate the capacity of neural networks to recognize
and recover such degraded shapes when initialized with different priors.
Ultimately, we find that neural networks' behavior on this simple task
conflicts with human behavior, raising a fundamental question of the robustness
and learning capabilities of modern computer vision models.
</p></li>
</ul>

<h3>Title: Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05225">http://arxiv.org/abs/2306.05225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05225] Boosting Adversarial Transferability by Achieving Flat Local Maxima](http://arxiv.org/abs/2306.05225) #attack</code></li>
<li>Summary: <p>Transfer-based attack adopts the adversarial examples generated on the
surrogate model to attack various models, making it applicable in the physical
world and attracting increasing interest. Recently, various adversarial attacks
have emerged to boost adversarial transferability from different perspectives.
In this work, inspired by the fact that flat local minima are correlated with
good generalization, we assume and empirically validate that adversarial
examples at a flat local region tend to have good transferability by
introducing a penalized gradient norm to the original loss function. Since
directly optimizing the gradient regularization norm is computationally
expensive and intractable for generating adversarial examples, we propose an
approximation optimization method to simplify the gradient update of the
objective function. Specifically, we randomly sample an example and adopt the
first-order gradient to approximate the second-order Hessian matrix, which
makes computing more efficient by interpolating two Jacobian matrices.
Meanwhile, in order to obtain a more stable gradient direction, we randomly
sample multiple examples and average the gradients of these examples to reduce
the variance due to random sampling during the iterative process. Extensive
experimental results on the ImageNet-compatible dataset show that the proposed
method can generate adversarial examples at flat local regions, and
significantly improve the adversarial transferability on either normally
trained models or adversarially trained models than the state-of-the-art
attacks.
</p></li>
</ul>

<h3>Title: Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04874">http://arxiv.org/abs/2306.04874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04874] Expanding Scope: Adapting English Adversarial Attacks to Chinese](http://arxiv.org/abs/2306.04874) #attack</code></li>
<li>Summary: <p>Recent studies have revealed that NLP predictive models are vulnerable to
adversarial attacks. Most existing studies focused on designing attacks to
evaluate the robustness of NLP models in the English language alone. Literature
has seen an increasing need for NLP solutions for other languages. We,
therefore, ask one natural question: whether state-of-the-art (SOTA) attack
methods generalize to other languages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English to the Chinese language. Our
experiments show that attack methods previously applied to English NLP can
generate high-quality adversarial examples in Chinese when combined with proper
text segmentation and linguistic constraints. In addition, we demonstrate that
the generated adversarial examples can achieve high fluency and semantic
consistency by focusing on the Chinese language's morphology and phonology,
which in turn can be used to improve the adversarial robustness of Chinese NLP
models.
</p></li>
</ul>

<h3>Title: Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks. (arXiv:2306.04859v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04859">http://arxiv.org/abs/2306.04859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04859] Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks](http://arxiv.org/abs/2306.04859) #attack</code></li>
<li>Summary: <p>In this paper, we describe and analyze an island-based random dynamic voltage
scaling (iRDVS) approach to thwart power side-channel attacks. We first analyze
the impact of the number of independent voltage islands on the resulting
signal-to-noise ratio and trace misalignment. As part of our analysis of
misalignment, we propose a novel unsupervised machine learning (ML) based
attack that is effective on systems with three or fewer independent voltages.
Our results show that iRDVS with four voltage islands, however, cannot be
broken with 200k encryption traces, suggesting that iRDVS can be effective. We
finish the talk by describing an iRDVS test chip in a 12nm FinFet process that
incorporates three variants of an AES-256 accelerator, all originating from the
same RTL. This included a synchronous core, an asynchronous core with no
protection, and a core employing the iRDVS technique using asynchronous logic.
Lab measurements from the chips indicated that both unprotected variants failed
the test vector leakage assessment (TVLA) security metric test, while the iRDVS
was proven secure in a variety of configurations.
</p></li>
</ul>

<h3>Title: G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering. (arXiv:2306.04984v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04984">http://arxiv.org/abs/2306.04984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04984] G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering](http://arxiv.org/abs/2306.04984) #attack</code></li>
<li>Summary: <p>As a collaborative paradigm, Federated Learning (FL) empowers clients to
engage in collective model training without exchanging their respective local
data. Nevertheless, FL remains vulnerable to backdoor attacks in which an
attacker compromises malicious clients, and injects poisoned model weights into
the aggregation process to yield attacker-chosen predictions for particular
samples. Existing countermeasures, mainly based on anomaly detection, may
erroneously reject legitimate weights while accepting malicious ones, which is
due to inadequacies in quantifying client model similarities. Other defense
mechanisms prove effective exclusively when confronted with a restricted number
of malicious clients, e.g., less than 10%. To address these vulnerabilities, we
present G$^2$uardFL, a protective framework that reframes the detection of
malicious clients as an attributed graph clustering problem, thereby
safeguarding FL systems. This framework employs a client graph clustering
technique to identify malicious clients and incorporates an adaptive method to
amplify the disparity between the aggregated model and poisoned client models,
thereby eliminating previously embedded backdoors. A theoretical analysis of
convergence is also performed to demonstrate that the global model closely
approximates the model untouched by any backdoor. Through empirical evaluation
compared to cutting-edge defenses and against various backdoor attacks, our
experimental results indicate that G$^2$uardFL considerably undermines the
effectiveness of backdoor attacks while maintaining a negligible impact on the
benign sample performance.
</p></li>
</ul>

<h3>Title: Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (arXiv:2306.05093v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05093">http://arxiv.org/abs/2306.05093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05093] Re-aligning Shadow Models can Improve White-box Membership Inference Attacks](http://arxiv.org/abs/2306.05093) #attack</code></li>
<li>Summary: <p>Machine learning models have been shown to leak sensitive information about
their training datasets. As models are being increasingly used, on devices, to
automate tasks and power new applications, there have been concerns that such
white-box access to its parameters, as opposed to the black-box setting which
only provides query access to the model, increases the attack surface. Directly
extending the shadow modelling technique from the black-box to the white-box
setting has been shown, in general, not to perform better than black-box only
attacks. A key reason is misalignment, a known characteristic of deep neural
networks. We here present the first systematic analysis of the causes of
misalignment in shadow models and show the use of a different weight
initialisation to be the main cause of shadow model misalignment. Second, we
extend several re-alignment techniques, previously developed in the model
fusion literature, to the shadow modelling context, where the goal is to
re-align the layers of a shadow model to those of the target model.We show
re-alignment techniques to significantly reduce the measured misalignment
between the target and shadow models. Finally, we perform a comprehensive
evaluation of white-box membership inference attacks (MIA). Our analysis
reveals that (1) MIAs suffer from misalignment between shadow models, but that
(2) re-aligning the shadow models improves, sometimes significantly, MIA
performance. On the CIFAR10 dataset with a false positive rate of 1\%,
white-box MIA using re-aligned shadow models improves the true positive rate by
4.5\%.Taken together, our results highlight that on-device deployment increase
the attack surface and that the newly available information can be used by an
attacker.
</p></li>
</ul>

<h3>Title: Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System. (arXiv:2306.05358v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05358">http://arxiv.org/abs/2306.05358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05358] Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System](http://arxiv.org/abs/2306.05358) #attack</code></li>
<li>Summary: <p>There are increasing concerns about malicious attacks on autonomous vehicles.
In particular, inaudible voice command attacks pose a significant threat as
voice commands become available in autonomous driving systems. How to
empirically defend against these inaudible attacks remains an open question.
Previous research investigates utilizing deep learning-based multimodal fusion
for defense, without considering the model uncertainty in trustworthiness. As
deep learning has been applied to increasingly sensitive tasks, uncertainty
measurement is crucial in helping improve model robustness, especially in
mission-critical scenarios. In this paper, we propose the Multimodal Fusion
Framework (MFF) as an intelligent security system to defend against inaudible
voice command attacks. MFF fuses heterogeneous audio-vision modalities using
VGG family neural networks and achieves the detection accuracy of 92.25% in the
comparative fusion method empirical study. Additionally, extensive experiments
on audio-vision tasks reveal the model's uncertainty. Using Expected
Calibration Errors, we measure calibration errors and Monte-Carlo Dropout to
estimate the predictive distribution for the proposed models. Our findings show
empirically to train robust multimodal models, improve standard accuracy and
provide a further step toward interpretability. Finally, we discuss the pros
and cons of our approach and its applicability for Advanced Driver Assistance
Systems.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks. (arXiv:2306.04701v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04701">http://arxiv.org/abs/2306.04701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04701] Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks](http://arxiv.org/abs/2306.04701) #robust</code></li>
<li>Summary: <p>Point cloud registration is a fundamental problem in computer vision that
aims to estimate the transformation between corresponding sets of points.
Non-rigid registration, in particular, involves addressing challenges including
various levels of deformation, noise, outliers, and data incompleteness. This
paper introduces Robust-DefReg, a robust non-rigid point cloud registration
method based on graph convolutional networks (GCNNs). Robust-DefReg is a
coarse-to-fine registration approach within an end-to-end pipeline, leveraging
the advantages of both coarse and fine methods. The method learns global
features to find correspondences between source and target point clouds, to
enable appropriate initial alignment, and subsequently fine registration. The
simultaneous achievement of high accuracy and robustness across all challenges
is reported less frequently in existing studies, making it a key objective of
the Robust-DefReg method. The proposed method achieves high accuracy in large
deformations while maintaining computational efficiency. This method possesses
three primary attributes: high accuracy, robustness to different challenges,
and computational efficiency. The experimental results show that the proposed
Robust-DefReg holds significant potential as a foundational architecture for
future investigations in non-rigid point cloud registration. The source code of
Robust-DefReg is available.
</p></li>
</ul>

<h3>Title: Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04940">http://arxiv.org/abs/2306.04940</a></li>
<li>Code URL: <a href="https://github.com/layeract/layeract">https://github.com/layeract/layeract</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04940] Layer-level activation mechanism](http://arxiv.org/abs/2306.04940) #robust</code></li>
<li>Summary: <p>In this work, we propose a novel activation mechanism aimed at establishing
layer-level activation (LayerAct) functions. These functions are designed to be
more noise-robust compared to traditional element-level activation functions by
reducing the layer-level fluctuation of the activation outputs due to shift in
inputs. Moreover, the LayerAct functions achieve a zero-like mean activation
output without restricting the activation output space. We present an analysis
and experiments demonstrating that LayerAct functions exhibit superior
noise-robustness compared to element-level activation functions, and
empirically show that these functions have a zero-like mean activation.
Experimental results on three benchmark image classification tasks show that
LayerAct functions excel in handling noisy image datasets, outperforming
element-level activation functions, while the performance on clean datasets is
also superior in most cases.
</p></li>
</ul>

<h3>Title: Focus for Free in Density-Based Counting. (arXiv:2306.05129v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05129">http://arxiv.org/abs/2306.05129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05129] Focus for Free in Density-Based Counting](http://arxiv.org/abs/2306.05129) #robust</code></li>
<li>Summary: <p>This work considers supervised learning to count from images and their
corresponding point annotations. Where density-based counting methods typically
use the point annotations only to create Gaussian-density maps, which act as
the supervision signal, the starting point of this work is that point
annotations have counting potential beyond density map generation. We introduce
two methods that repurpose the available point annotations to enhance counting
performance. The first is a counting-specific augmentation that leverages point
annotations to simulate occluded objects in both input and density images to
enhance the network's robustness to occlusions. The second method, foreground
distillation, generates foreground masks from the point annotations, from which
we train an auxiliary network on images with blacked-out backgrounds. By doing
so, it learns to extract foreground counting knowledge without interference
from the background. These methods can be seamlessly integrated with existing
counting advances and are adaptable to different loss functions. We demonstrate
complementary effects of the approaches, allowing us to achieve robust counting
results even in challenging scenarios such as background clutter, occlusion,
and varying crowd densities. Our proposed approach achieves strong counting
results on multiple datasets, including ShanghaiTech Part_A and Part_B,
UCF_QNRF, JHU-Crowd++, and NWPU-Crowd.
</p></li>
</ul>

<h3>Title: SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth. (arXiv:2306.05238v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05238">http://arxiv.org/abs/2306.05238</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05238] SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth](http://arxiv.org/abs/2306.05238) #robust</code></li>
<li>Summary: <p>Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.
</p></li>
</ul>

<h3>Title: Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04723">http://arxiv.org/abs/2306.04723</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04723] Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts](http://arxiv.org/abs/2306.04723) #robust</code></li>
<li>Summary: <p>Rapidly increasing quality of AI-generated content makes it difficult to
distinguish between human and AI-generated texts, which may lead to undesirable
consequences for society. Therefore, it becomes increasingly important to study
the properties of human texts that are invariant over text domains and various
proficiency of human writers, can be easily calculated for any language, and
can robustly separate natural and AI-generated texts regardless of the
generation model and sampling method. In this work, we propose such an
invariant of human texts, namely the intrinsic dimensionality of the manifold
underlying the set of embeddings of a given text sample. We show that the
average intrinsic dimensionality of fluent texts in natural language is
hovering around the value $9$ for several alphabet-based languages and around
$7$ for Chinese, while the average intrinsic dimensionality of AI-generated
texts for each language is $\approx 1.5$ lower, with a clear statistical
separation between human-generated and AI-generated distributions. This
property allows us to build a score-based artificial text detector. The
proposed detector's accuracy is stable over text domains, generator models, and
human writer proficiency levels, outperforming SOTA detectors in model-agnostic
and cross-domain scenarios by a significant margin.
</p></li>
</ul>

<h3>Title: Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation. (arXiv:2306.04724v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04724">http://arxiv.org/abs/2306.04724</a></li>
<li>Code URL: <a href="https://github.com/cuthalionn/prompter">https://github.com/cuthalionn/prompter</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04724] Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation](http://arxiv.org/abs/2306.04724) #robust</code></li>
<li>Summary: <p>A challenge in the Dialogue State Tracking (DST) field is adapting models to
new domains without using any supervised data, zero-shot domain adaptation.
Parameter-Efficient Transfer Learning (PETL) has the potential to address this
problem due to its robustness. However, it has yet to be applied to the
zero-shot scenarios, as it is not clear how to apply it unsupervisedly.
</p></li>
</ul>

<p>Our method, Prompter, uses descriptions of target domain slots to generate
dynamic prefixes that are concatenated to the key and values at each layer's
self-attention mechanism. This allows for the use of prefix-tuning in
zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD
benchmarks. In generating prefixes, our analyses find that Prompter not only
utilizes the semantics of slot descriptions but also how often the slots appear
together in conversation. Moreover, Prompter's gains are due to its improved
ability to distinguish "none"-valued dialogue slots, compared against
baselines.
</p>

<h3>Title: Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems. (arXiv:2306.04823v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04823">http://arxiv.org/abs/2306.04823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04823] Data Augmentation for Improving Tail-traffic Robustness in Skill-routing for Dialogue Systems](http://arxiv.org/abs/2306.04823) #robust</code></li>
<li>Summary: <p>Large-scale conversational systems typically rely on a skill-routing
component to route a user request to an appropriate skill and interpretation to
serve the request. In such system, the agent is responsible for serving
thousands of skills and interpretations which create a long-tail distribution
due to the natural frequency of requests. For example, the samples related to
play music might be a thousand times more frequent than those asking for
theatre show times. Moreover, inputs used for ML-based skill routing are often
a heterogeneous mix of strings, embedding vectors, categorical and scalar
features which makes employing augmentation-based long-tail learning approaches
challenging. To improve the skill-routing robustness, we propose an
augmentation of heterogeneous skill-routing data and training targeted for
robust operation in long-tail data regimes. We explore a variety of conditional
encoder-decoder generative frameworks to perturb original data fields and
create synthetic training data. To demonstrate the effectiveness of the
proposed method, we conduct extensive experiments using real-world data from a
commercial conversational system. Based on the experiment results, the proposed
approach improves more than 80% (51 out of 63) of intents with less than 10K of
traffic instances in the skill-routing replication task.
</p></li>
</ul>

<h3>Title: InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding. (arXiv:2306.04933v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04933">http://arxiv.org/abs/2306.04933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04933] InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding](http://arxiv.org/abs/2306.04933) #robust</code></li>
<li>Summary: <p>Soft prompt tuning achieves superior performances across a wide range of
few-shot tasks. However, the performances of prompt tuning can be highly
sensitive to the initialization of the prompts. We also empirically observe
that conventional prompt tuning methods cannot encode and learn sufficient
task-relevant information from prompt tokens. In this work, we develop an
information-theoretic framework that formulates soft prompt tuning as
maximizing mutual information between prompts and other model parameters (or
encoded representations). This novel view helps us to develop a more efficient,
accurate and robust soft prompt tuning method InfoPrompt. With this framework,
we develop two novel mutual information based loss functions, to (i) discover
proper prompt initialization for the downstream tasks and learn sufficient
task-relevant information from prompt tokens and (ii) encourage the output
representation from the pretrained language model to be more aware of the
task-relevant information captured in the learnt prompt. Extensive experiments
validate that InfoPrompt can significantly accelerate the convergence of the
prompt tuning and outperform traditional prompt tuning methods. Finally, we
provide a formal theoretical result for showing to show that gradient descent
type algorithm can be used to train our mutual information loss.
</p></li>
</ul>

<h3>Title: Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05079">http://arxiv.org/abs/2306.05079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05079] Enhancing Robustness of AI Offensive Code Generators via Data Augmentation](http://arxiv.org/abs/2306.05079) #robust</code></li>
<li>Summary: <p>In this work, we present a method to add perturbations to the code
descriptions, i.e., new inputs in natural language (NL) from well-intentioned
developers, in the context of security-oriented code, and analyze how and to
what extent perturbations affect the performance of AI offensive code
generators. Our experiments show that the performance of the code generators is
highly affected by perturbations in the NL descriptions. To enhance the
robustness of the code generators, we use the method to perform data
augmentation, i.e., to increase the variability and diversity of the training
data, proving its effectiveness against both perturbed and non-perturbed code
descriptions.
</p></li>
</ul>

<h3>Title: Robust Learning with Progressive Data Expansion Against Spurious Correlation. (arXiv:2306.04949v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04949">http://arxiv.org/abs/2306.04949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04949] Robust Learning with Progressive Data Expansion Against Spurious Correlation](http://arxiv.org/abs/2306.04949) #robust</code></li>
<li>Summary: <p>While deep learning models have shown remarkable performance in various
tasks, they are susceptible to learning non-generalizable spurious features
rather than the core features that are genuinely correlated to the true label.
In this paper, beyond existing analyses of linear models, we theoretically
examine the learning process of a two-layer nonlinear convolutional neural
network in the presence of spurious features. Our analysis suggests that
imbalanced data groups and easily learnable spurious features can lead to the
dominance of spurious features during the learning process. In light of this,
we propose a new training algorithm called PDE that efficiently enhances the
model's robustness for a better worst-group performance. PDE begins with a
group-balanced subset of training data and progressively expands it to
facilitate the learning of the core features. Experiments on synthetic and
real-world benchmark datasets confirm the superior performance of our method on
models such as ResNets and Transformers. On average, our method achieves a 2.8%
improvement in worst-group accuracy compared with the state-of-the-art method,
while enjoying up to 10x faster training efficiency.
</p></li>
</ul>

<h3>Title: Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations. (arXiv:2306.05031v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05031">http://arxiv.org/abs/2306.05031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05031] Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations](http://arxiv.org/abs/2306.05031) #robust</code></li>
<li>Summary: <p>Recent neural architecture search (NAS) frameworks have been successful in
finding optimal architectures for given conditions (e.g., performance or
latency). However, they search for optimal architectures in terms of their
performance on clean images only, while robustness against various types of
perturbations or corruptions is crucial in practice. Although there exist
several robust NAS frameworks that tackle this issue by integrating adversarial
training into one-shot NAS, however, they are limited in that they only
consider robustness against adversarial attacks and require significant
computational resources to discover optimal architectures for a single task,
which makes them impractical in real-world scenarios. To address these
challenges, we propose a novel lightweight robust zero-cost proxy that
considers the consistency across features, parameters, and gradients of both
clean and perturbed images at the initialization state. Our approach
facilitates an efficient and rapid search for neural architectures capable of
learning generalizable features that exhibit robustness across diverse
perturbations. The experimental results demonstrate that our proxy can rapidly
and efficiently search for neural architectures that are consistently robust
against various perturbations on multiple benchmark datasets and diverse search
spaces, largely outperforming existing clean zero-shot NAS and robust NAS with
reduced search cost.
</p></li>
</ul>

<h3>Title: A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels. (arXiv:2306.05046v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05046">http://arxiv.org/abs/2306.05046</a></li>
<li>Code URL: <a href="https://github.com/anonymoussubmission100/ogrs_neurips">https://github.com/anonymoussubmission100/ogrs_neurips</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05046] A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels](http://arxiv.org/abs/2306.05046) #robust</code></li>
<li>Summary: <p>Learning with noisy labels is an important topic for scalable training in
many real-world scenarios. However, few previous research considers this
problem in the online setting, where the arrival of data is streaming. In this
paper, we propose a novel gradient-based approach to enable the detection of
noisy labels for the online learning of model parameters, named Online
Gradient-based Robust Selection (OGRS). In contrast to the previous sample
selection approach for the offline training that requires the estimation of a
clean ratio of the dataset before each epoch of training, OGRS can
automatically select clean samples by steps of gradient update from datasets
with varying clean ratios without changing the parameter setting. During the
training process, the OGRS method selects clean samples at each iteration and
feeds the selected sample to incrementally update the model parameters. We
provide a detailed theoretical analysis to demonstrate data selection process
is converging to the low-loss region of the sample space, by introducing and
proving the sub-linear local Lagrangian regret of the non-convex constrained
optimization problem. Experimental results show that it outperforms
state-of-the-art methods in different settings.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: GaitMPL: Gait Recognition with Memory-Augmented Progressive Learning. (arXiv:2306.04650v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04650">http://arxiv.org/abs/2306.04650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04650] GaitMPL: Gait Recognition with Memory-Augmented Progressive Learning](http://arxiv.org/abs/2306.04650) #biometric</code></li>
<li>Summary: <p>Gait recognition aims at identifying the pedestrians at a long distance by
their biometric gait patterns. It is inherently challenging due to the various
covariates and the properties of silhouettes (textureless and colorless), which
result in two kinds of pair-wise hard samples: the same pedestrian could have
distinct silhouettes (intra-class diversity) and different pedestrians could
have similar silhouettes (inter-class similarity). In this work, we propose to
solve the hard sample issue with a Memory-augmented Progressive Learning
network (GaitMPL), including Dynamic Reweighting Progressive Learning module
(DRPL) and Global Structure-Aligned Memory bank (GSAM). Specifically, DRPL
reduces the learning difficulty of hard samples by easy-to-hard progressive
learning. GSAM further augments DRPL with a structure-aligned memory mechanism,
which maintains and models the feature distribution of each ID. Experiments on
two commonly used datasets, CASIA-B and OU-MVLP, demonstrate the effectiveness
of GaitMPL. On CASIA-B, we achieve the state-of-the-art performance, i.e.,
88.0% on the most challenging condition (Clothing) and 93.3% on the average
condition, which outperforms the other methods by at least 3.8% and 1.4%,
respectively.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Neighborhood Attention Makes the Encoder of ResUNet Stronger for Accurate Road Extraction. (arXiv:2306.04947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04947">http://arxiv.org/abs/2306.04947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04947] Neighborhood Attention Makes the Encoder of ResUNet Stronger for Accurate Road Extraction](http://arxiv.org/abs/2306.04947) #extraction</code></li>
<li>Summary: <p>In the domain of remote sensing image interpretation, road extraction from
high-resolution aerial imagery has already been a hot research topic. Although
deep CNNs have presented excellent results for semantic segmentation, the
efficiency and capabilities of vision transformers are yet to be fully
researched. As such, for accurate road extraction, a deep semantic segmentation
neural network that utilizes the abilities of residual learning, HetConvs,
UNet, and vision transformers, which is called \texttt{ResUNetFormer}, is
proposed in this letter. The developed \texttt{ResUNetFormer} is evaluated on
various cutting-edge deep learning-based road extraction techniques on the
public Massachusetts road dataset. Statistical and visual results demonstrate
the superiority of the \texttt{ResUNetFormer} over the state-of-the-art CNNs
and vision transformers for segmentation. The code will be made available
publicly at \url{https://github.com/aj1365/ResUNetFormer}.
</p></li>
</ul>

<h3>Title: Open Set Relation Extraction via Unknown-Aware Training. (arXiv:2306.04950v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04950">http://arxiv.org/abs/2306.04950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04950] Open Set Relation Extraction via Unknown-Aware Training](http://arxiv.org/abs/2306.04950) #extraction</code></li>
<li>Summary: <p>The existing supervised relation extraction methods have achieved impressive
performance in a closed-set setting, where the relations during both training
and testing remain the same. In a more realistic open-set setting, unknown
relations may appear in the test set. Due to the lack of supervision signals
from unknown relations, a well-performing closed-set relation extractor can
still confidently misclassify them into known relations. In this paper, we
propose an unknown-aware training method, regularizing the model by dynamically
synthesizing negative instances. To facilitate a compact decision boundary,
``difficult'' negative instances are necessary. Inspired by text adversarial
attacks, we adaptively apply small but critical perturbations to original
training instances and thus synthesizing negative instances that are more
likely to be mistaken by the model as known relations. Experimental results
show that this method achieves SOTA unknown relation detection without
compromising the classification of known relations.
</p></li>
</ul>

<h3>Title: RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction. (arXiv:2306.04954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04954">http://arxiv.org/abs/2306.04954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04954] RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction](http://arxiv.org/abs/2306.04954) #extraction</code></li>
<li>Summary: <p>Semantic matching is a mainstream paradigm of zero-shot relation extraction,
which matches a given input with a corresponding label description. The
entities in the input should exactly match their hypernyms in the description,
while the irrelevant contexts should be ignored when matching. However, general
matching methods lack explicit modeling of the above matching pattern. In this
work, we propose a fine-grained semantic matching method tailored for zero-shot
relation extraction. Following the above matching pattern, we decompose the
sentence-level similarity score into entity and context matching scores. Due to
the lack of explicit annotations of the redundant components, we design a
feature distillation module to adaptively identify the relation-irrelevant
features and reduce their negative impact on context matching. Experimental
results show that our method achieves higher matching $F_1$ score and has an
inference speed 10 times faster, when compared with the state-of-the-art
methods.
</p></li>
</ul>

<h3>Title: Actively Supervised Clustering for Open Relation Extraction. (arXiv:2306.04968v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04968">http://arxiv.org/abs/2306.04968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04968] Actively Supervised Clustering for Open Relation Extraction](http://arxiv.org/abs/2306.04968) #extraction</code></li>
<li>Summary: <p>Current clustering-based Open Relation Extraction (OpenRE) methods usually
adopt a two-stage pipeline. The first stage simultaneously learns relation
representations and assignments. The second stage manually labels several
instances and thus names the relation for each cluster. However, unsupervised
objectives struggle to optimize the model to derive accurate clustering
assignments, and the number of clusters has to be supplied in advance. In this
paper, we present a novel setting, named actively supervised clustering for
OpenRE. Our insight lies in that clustering learning and relation labeling can
be alternately performed, providing the necessary guidance for clustering
without a significant increase in human effort. The key to the setting is
selecting which instances to label. Instead of using classical active labeling
strategies designed for fixed known classes, we propose a new strategy, which
is applicable to dynamically discover clusters of unknown relations.
Experimental results show that our method is able to discover almost all
relational clusters in the data and improve the SOTA methods by 10.3\% and
5.2\%, on two datasets respectively.
</p></li>
</ul>

<h3>Title: Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models. (arXiv:2306.05052v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05052">http://arxiv.org/abs/2306.05052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05052] Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models](http://arxiv.org/abs/2306.05052) #extraction</code></li>
<li>Summary: <p>Tabular data is often hidden in text, particularly in medical diagnostic
reports. Traditional machine learning (ML) models designed to work with tabular
data, cannot effectively process information in such form. On the other hand,
large language models (LLMs) which excel at textual tasks, are probably not the
best tool for modeling tabular data. Therefore, we propose a novel, simple, and
effective methodology for extracting structured tabular data from textual
medical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of
LLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately
inferring tabular features, even when their names are not explicitly mentioned
in the text. This is achieved by combining domain-specific reasoning guidelines
with a proposed data validation and reasoning correction feedback loop. By
applying interpretable ML models such as decision trees and logistic regression
over the extracted and validated data, we obtain end-to-end interpretable
predictions. We demonstrate that our approach significantly outperforms
state-of-the-art text classification models in medical diagnostics. Given its
predictive performance, simplicity, and interpretability, TEMED-LLM underscores
the potential of leveraging LLMs to improve the performance and trustworthiness
of ML models in medical applications.
</p></li>
</ul>

<h3>Title: Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05276">http://arxiv.org/abs/2306.05276</a></li>
<li>Code URL: <a href="https://github.com/ailabudinegit/ade-detection-survey">https://github.com/ailabudinegit/ade-detection-survey</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05276] Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction](http://arxiv.org/abs/2306.05276) #extraction</code></li>
<li>Summary: <p>Adverse Event (ADE) extraction is one of the core tasks in digital
pharmacovigilance, especially when applied to informal texts. This task has
been addressed by the Natural Language Processing community using large
pre-trained language models, such as BERT. Despite the great number of
Transformer-based architectures used in the literature, it is unclear which of
them has better performances and why. Therefore, in this paper we perform an
extensive evaluation and analysis of 19 Transformer-based models for ADE
extraction on informal texts. We compare the performance of all the considered
models on two datasets with increasing levels of informality (forums posts and
tweets). We also combine the purely Transformer-based models with two
commonly-used additional processing layers (CRF and LSTM), and analyze their
effect on the models performance. Furthermore, we use a well-established
feature importance technique (SHAP) to correlate the performance of the models
with a set of features that describe them: model category (AutoEncoding,
AutoRegressive, Text-to-Text), pretraining domain, training from scratch, and
model size in number of parameters. At the end of our analyses, we identify a
list of take-home messages that can be derived from the experimental data.
</p></li>
</ul>

<h3>Title: Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05323">http://arxiv.org/abs/2306.05323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05323] Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application](http://arxiv.org/abs/2306.05323) #extraction</code></li>
<li>Summary: <p>The introduction of computerized medical records in hospitals has reduced
burdensome operations like manual writing and information fetching. However,
the data contained in medical records are still far underutilized, primarily
because extracting them from unstructured textual medical records takes time
and effort. Information Extraction, a subfield of Natural Language Processing,
can help clinical practitioners overcome this limitation, using automated
text-mining pipelines. In this work, we created the first Italian
neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to
develop a Large Language Model for this task. Moreover, we conducted several
experiments with three external independent datasets to implement an effective
multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall
86.44%. The lessons learned are: (i) the crucial role of a consistent
annotation process and (ii) a fine-tuning strategy that combines classical
methods with a "few-shot" approach. This allowed us to establish methodological
guidelines that pave the way for future implementations in this field and allow
Italian hospitals to tap into important research opportunities.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Systematic Literature Review on Client Selection in Federated Learning. (arXiv:2306.04862v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04862">http://arxiv.org/abs/2306.04862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04862] A Systematic Literature Review on Client Selection in Federated Learning](http://arxiv.org/abs/2306.04862) #federate</code></li>
<li>Summary: <p>With the arising concerns of privacy within machine learning, federated
learning (FL) was invented in 2017, in which the clients, such as mobile
devices, train a model and send the update to the centralized server. Choosing
clients randomly for FL can harm learning performance due to different reasons.
Many studies have proposed approaches to address the challenges of client
selection of FL. However, no systematic literature review (SLR) on this topic
existed. This SLR investigates the state of the art of client selection in FL
and answers the challenges, solutions, and metrics to evaluate the solutions.
We systematically reviewed 47 primary studies. The main challenges found in
client selection are heterogeneity, resource allocation, communication costs,
and fairness. The client selection schemes aim to improve the original random
selection algorithm by focusing on one or several of the aforementioned
challenges. The most common metric used is testing accuracy versus
communication rounds, as testing accuracy measures the successfulness of the
learning and preferably in as few communication rounds as possible, as they are
very expensive. Although several possible improvements can be made with the
current state of client selection, the most beneficial ones are evaluating the
impact of unsuccessful clients and gaining a more theoretical understanding of
the impact of fairness in FL.
</p></li>
</ul>

<h3>Title: FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05172">http://arxiv.org/abs/2306.05172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05172] FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems](http://arxiv.org/abs/2306.05172) #federate</code></li>
<li>Summary: <p>Federated Machine Learning (FL) has received considerable attention in recent
years. FL benchmarks are predominantly explored in either simulated systems or
data center environments, neglecting the setups of real-world systems, which
are often closely linked to edge computing. We close this research gap by
introducing FLEdge, a benchmark targeting FL workloads in edge computing
systems. We systematically study hardware heterogeneity, energy efficiency
during training, and the effect of various differential privacy levels on
training in FL systems. To make this benchmark applicable to real-world
scenarios, we evaluate the impact of client dropouts on state-of-the-art FL
strategies with failure rates as high as 50%. FLEdge provides new insights,
such as that training state-of-the-art FL workloads on older GPU-accelerated
embedded devices is up to 3x more energy efficient than on modern server-grade
GPUs.
</p></li>
</ul>

<h3>Title: Federated Learning under Covariate Shifts with Generalization Guarantees. (arXiv:2306.05325v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05325">http://arxiv.org/abs/2306.05325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05325] Federated Learning under Covariate Shifts with Generalization Guarantees](http://arxiv.org/abs/2306.05325) #federate</code></li>
<li>Summary: <p>This paper addresses intra-client and inter-client covariate shifts in
federated learning (FL) with a focus on the overall generalization performance.
To handle covariate shifts, we formulate a new global model training paradigm
and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)
along with improving density ratio matching methods without requiring perfect
knowledge of the supremum over true ratios. We also propose the
communication-efficient variant FITW-ERM with the same level of privacy
guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM
achieves smaller generalization error than classical ERM under certain
settings. Experimental results demonstrate the superiority of FTW-ERM over
existing FL baselines in challenging imbalanced federated settings in terms of
data distribution shifts across clients.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. (arXiv:2306.04675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04675">http://arxiv.org/abs/2306.04675</a></li>
<li>Code URL: <a href="https://github.com/layer6ai-labs/dgm-eval">https://github.com/layer6ai-labs/dgm-eval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04675] Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models](http://arxiv.org/abs/2306.04675) #fair</code></li>
<li>Summary: <p>We systematically study a wide variety of image-based generative models
spanning semantically-diverse datasets to understand and improve the feature
extractors and metrics used to evaluate them. Using best practices in
psychophysics, we measure human perception of image realism for generated
samples by conducting the largest experiment evaluating generative models to
date, and find that no existing metric strongly correlates with human
evaluations. Comparing to 16 modern metrics for evaluating the overall
performance, fidelity, diversity, and memorization of generative models, we
find that the state-of-the-art perceptual realism of diffusion models as judged
by humans is not reflected in commonly reported metrics such as FID. This
discrepancy is not explained by diversity in generated samples, though one
cause is over-reliance on Inception-V3. We address these flaws through a study
of alternative self-supervised feature extractors, find that the semantic
information encoded by individual networks strongly depends on their training
procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of
generative models. Next, we investigate data memorization, and find that
generative models do memorize training examples on simple, smaller datasets
like CIFAR10, but not necessarily on more complex datasets like ImageNet.
However, our experiments show that current metrics do not properly detect
memorization; none in the literature is able to separate memorization from
other phenomena such as underfitting or mode shrinkage. To facilitate further
development of generative models and their evaluation we release all generated
image datasets, human evaluation data, and a modular library to compute 16
common metrics for 8 different encoders at
https://github.com/layer6ai-labs/dgm-eval.
</p></li>
</ul>

<h3>Title: DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models. (arXiv:2306.05076v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05076">http://arxiv.org/abs/2306.05076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05076] DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models](http://arxiv.org/abs/2306.05076) #fair</code></li>
<li>Summary: <p>A few benchmarking datasets have been released to evaluate the factual
knowledge of pretrained language models. These benchmarks (e.g., LAMA, and
ParaRel) are mainly developed in English and later are translated to form new
multilingual versions (e.g., mLAMA, and mParaRel). Results on these
multilingual benchmarks suggest that using English prompts to recall the facts
from multilingual models usually yields significantly better and more
consistent performance than using non-English prompts. Our analysis shows that
mLAMA is biased toward facts from Western countries, which might affect the
fairness of probing models. We propose a new framework for curating factual
triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is
built of factual triples from three pairs of contrasting cultures having a
total of 78,259 triples from 20 relation predicates. The three pairs comprise
facts representing the (Arab and Western), (Asian and Western), and (South
American and Western) countries respectively. Having a more balanced benchmark
(DLAMA-v1) supports that mBERT performs better on Western facts than
non-Western ones, while monolingual Arabic, English, and Korean models tend to
perform better on their culturally proximate facts. Moreover, both monolingual
and multilingual models tend to make a prediction that is culturally or
geographically relevant to the correct label, even if the prediction is wrong.
</p></li>
</ul>

<h3>Title: Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media. (arXiv:2306.05115v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05115">http://arxiv.org/abs/2306.05115</a></li>
<li>Code URL: <a href="https://github.com/thalesbertaglia/chatgpt-explanations-sponsored-content">https://github.com/thalesbertaglia/chatgpt-explanations-sponsored-content</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05115] Closing the Loop: Testing ChatGPT to Generate Model Explanations to Improve Human Labelling of Sponsored Content on Social Media](http://arxiv.org/abs/2306.05115) #fair</code></li>
<li>Summary: <p>Regulatory bodies worldwide are intensifying their efforts to ensure
transparency in influencer marketing on social media through instruments like
the Unfair Commercial Practices Directive (UCPD) in the European Union, or
Section 5 of the Federal Trade Commission Act. Yet enforcing these obligations
has proven to be highly problematic due to the sheer scale of the influencer
market. The task of automatically detecting sponsored content aims to enable
the monitoring and enforcement of such regulations at scale. Current research
in this field primarily frames this problem as a machine learning task,
focusing on developing models that achieve high classification performance in
detecting ads. These machine learning tasks rely on human data annotation to
provide ground truth information. However, agreement between annotators is
often low, leading to inconsistent labels that hinder the reliability of
models. To improve annotation accuracy and, thus, the detection of sponsored
content, we propose using chatGPT to augment the annotation process with
phrases identified as relevant features and brief explanations. Our experiments
show that this approach consistently improves inter-annotator agreement and
annotation accuracy. Additionally, our survey of user experience in the
annotation task indicates that the explanations improve the annotators'
confidence and streamline the process. Our proposed methods can ultimately lead
to more transparency and alignment with regulatory requirements in sponsored
content detection.
</p></li>
</ul>

<h3>Title: Are fairness metric scores enough to assess discrimination biases in machine learning?. (arXiv:2306.05307v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05307">http://arxiv.org/abs/2306.05307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05307] Are fairness metric scores enough to assess discrimination biases in machine learning?](http://arxiv.org/abs/2306.05307) #fair</code></li>
<li>Summary: <p>This paper presents novel experiments shedding light on the shortcomings of
current metrics for assessing biases of gender discrimination made by machine
learning algorithms on textual data. We focus on the Bios dataset, and our
learning task is to predict the occupation of individuals, based on their
biography. Such prediction tasks are common in commercial Natural Language
Processing (NLP) applications such as automatic job recommendations. We address
an important limitation of theoretical discussions dealing with group-wise
fairness metrics: they focus on large datasets, although the norm in many
industrial NLP applications is to use small to reasonably large linguistic
datasets for which the main practical constraint is to get a good prediction
accuracy. We then question how reliable are different popular measures of bias
when the size of the training set is simply sufficient to learn reasonably
accurate predictions. Our experiments sample the Bios dataset and learn more
than 200 models on different sample sizes. This allows us to statistically
study our results and to confirm that common gender bias indices provide
diverging and sometimes unreliable results when applied to relatively small
training and test samples. This highlights the crucial importance of variance
calculations for providing sound results in this field.
</p></li>
</ul>

<h3>Title: Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05068">http://arxiv.org/abs/2306.05068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05068] Shedding light on underrepresentation and Sampling Bias in machine learning](http://arxiv.org/abs/2306.05068) #fair</code></li>
<li>Summary: <p>Accurately measuring discrimination is crucial to faithfully assessing
fairness of trained machine learning (ML) models. Any bias in measuring
discrimination leads to either amplification or underestimation of the existing
disparity. Several sources of bias exist and it is assumed that bias resulting
from machine learning is born equally by different groups (e.g. females vs
males, whites vs blacks, etc.). If, however, bias is born differently by
different groups, it may exacerbate discrimination against specific
sub-populations. Sampling bias, is inconsistently used in the literature to
describe bias due to the sampling procedure. In this paper, we attempt to
disambiguate this term by introducing clearly defined variants of sampling
bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We
show also how discrimination can be decomposed into variance, bias, and noise.
Finally, we challenge the commonly accepted mitigation approach that
discrimination can be addressed by collecting more samples of the
underrepresented group.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map. (arXiv:2306.04644v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04644">http://arxiv.org/abs/2306.04644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04644] Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map](http://arxiv.org/abs/2306.04644) #interpretability</code></li>
<li>Summary: <p>Interpretation of deep learning remains a very challenging problem. Although
the Class Activation Map (CAM) is widely used to interpret deep model
predictions by highlighting object location, it fails to provide insight into
the salient features used by the model to make decisions. Furthermore, existing
evaluation protocols often overlook the correlation between interpretability
performance and the model's decision quality, which presents a more fundamental
issue. This paper proposes a new two-stage interpretability method called the
Decomposition Class Activation Map (Decom-CAM), which offers a feature-level
interpretation of the model's prediction. Decom-CAM decomposes intermediate
activation maps into orthogonal features using singular value decomposition and
generates saliency maps by integrating them. The orthogonality of features
enables CAM to capture local features and can be used to pinpoint semantic
components such as eyes, noses, and faces in the input image, making it more
beneficial for deep model interpretation. To ensure a comprehensive comparison,
we introduce a new evaluation protocol by dividing the dataset into subsets
based on classification accuracy results and evaluating the interpretability
performance on each subset separately. Our experiments demonstrate that the
proposed Decom-CAM outperforms current state-of-the-art methods significantly
by generating more precise saliency maps across all levels of classification
accuracy. Combined with our feature-level interpretability approach, this paper
could pave the way for a new direction for understanding the decision-making
process of deep neural networks.
</p></li>
</ul>

<h3>Title: Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04719">http://arxiv.org/abs/2306.04719</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04719] Don't trust your eyes: on the (un)reliability of feature visualizations](http://arxiv.org/abs/2306.04719) #interpretability</code></li>
<li>Summary: <p>How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to "explain" how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
</p></li>
</ul>

<h3>Title: Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge. (arXiv:2306.04657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04657">http://arxiv.org/abs/2306.04657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04657] Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge](http://arxiv.org/abs/2306.04657) #interpretability</code></li>
<li>Summary: <p>In empathetic conversations, individuals express their empathy towards
others. Previous work has mainly focused on generating empathetic responses by
utilizing the speaker's emotion. Besides, external commonsense knowledge has
been applied to enhance the system's understandings of the speaker's situation.
However, given an event, commonsense knowledge base contains various relations,
potentially leading to confusion for the dialogue system. Consequently,
inconsistencies arise among the emotion, generated response and speaker's
contextual information. To this end, we propose a novel approach for empathetic
response generation, which incorporates an adaptive module for commonsense
knowledge selection to ensure consistency between the generated empathetic
responses and the speaker's situation. This selected knowledge is used to
refine the commonsense cognition and empathy expression for generated
responses. Experimental results show that our approach significantly
outperforms baseline models in both automatic and human evaluations, exhibiting
the generation of more coherent and empathetic responses. Moreover, case
studies highlight the interpretability of knowledge selection in the responses
and the effectiveness of adaptive module in our model. Code:
https://github.com/Hanscal/DCKS.
</p></li>
</ul>

<h3>Title: Neural Symbolic Regression using Control Variables. (arXiv:2306.04718v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04718">http://arxiv.org/abs/2306.04718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04718] Neural Symbolic Regression using Control Variables](http://arxiv.org/abs/2306.04718) #interpretability</code></li>
<li>Summary: <p>Symbolic regression (SR) is a powerful technique for discovering the
analytical mathematical expression from data, finding various applications in
natural sciences due to its good interpretability of results. However, existing
methods face scalability issues when dealing with complex equations involving
multiple variables. To address this challenge, we propose SRCV, a novel neural
symbolic regression method that leverages control variables to enhance both
accuracy and scalability. The core idea is to decompose multi-variable symbolic
regression into a set of single-variable SR problems, which are then combined
in a bottom-up manner. The proposed method involves a four-step process. First,
we learn a data generator from observed data using deep neural networks (DNNs).
Second, the data generator is used to generate samples for a certain variable
by controlling the input variables. Thirdly, single-variable symbolic
regression is applied to estimate the corresponding mathematical expression.
Lastly, we repeat steps 2 and 3 by gradually adding variables one by one until
completion. We evaluate the performance of our method on multiple benchmark
datasets. Experimental results demonstrate that the proposed SRCV significantly
outperforms state-of-the-art baselines in discovering mathematical expressions
with multiple variables. Moreover, it can substantially reduce the search space
for symbolic regression. The source code will be made publicly available upon
publication.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification. (arXiv:2306.05075v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05075">http://arxiv.org/abs/2306.05075</a></li>
<li>Code URL: <a href="https://github.com/lct-rug-2022/edos-2023">https://github.com/lct-rug-2022/edos-2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05075] LCT-1 at SemEval-2023 Task 10: Pre-training and Multi-task Learning for Sexism Detection and Classification](http://arxiv.org/abs/2306.05075) #explainability</code></li>
<li>Summary: <p>Misogyny and sexism are growing problems in social media. Advances have been
made in online sexism detection but the systems are often uninterpretable.
SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at
increasing explainability of the sexism detection, and our team participated in
all the proposed subtasks. Our system is based on further domain-adaptive
pre-training (Gururangan et al., 2020). Building on the Transformer-based
models with the domain adaptation, we compare fine-tuning with multi-task
learning and show that each subtask requires a different system configuration.
In our experiments, multi-task learning performs on par with standard
fine-tuning for sexism detection and noticeably better for coarse-grained
sexism classification, while fine-tuning is preferable for fine-grained
classification.
</p></li>
</ul>

<h3>Title: XInsight: Revealing Model Insights for GNNs with Flow-based Explanations. (arXiv:2306.04791v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04791">http://arxiv.org/abs/2306.04791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04791] XInsight: Revealing Model Insights for GNNs with Flow-based Explanations](http://arxiv.org/abs/2306.04791) #explainability</code></li>
<li>Summary: <p>Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are <code>black boxes'
with little understanding of the</code>what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
</p></li>
</ul>

<p>We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.
</p>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models. (arXiv:2306.04695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04695">http://arxiv.org/abs/2306.04695</a></li>
<li>Code URL: <a href="https://github.com/conceptbed/evaluations">https://github.com/conceptbed/evaluations</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04695] ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models](http://arxiv.org/abs/2306.04695) #diffusion</code></li>
<li>Summary: <p>The ability to understand visual concepts and replicate and compose these
concepts from images is a central goal for computer vision. Recent advances in
text-to-image (T2I) models have lead to high definition and realistic image
quality generation by learning from large databases of images and their
descriptions. However, the evaluation of T2I models has focused on photorealism
and limited qualitative measures of visual understanding. To quantify the
ability of T2I models in learning and synthesizing novel visual concepts, we
introduce ConceptBed, a large-scale dataset that consists of 284 unique visual
concepts, 5K unique concept compositions, and 33K composite text prompts. Along
with the dataset, we propose an evaluation metric, Concept Confidence Deviation
(CCD), that uses the confidence of oracle concept classifiers to measure the
alignment between concepts generated by T2I generators and concepts contained
in ground truth images. We evaluate visual concepts that are either objects,
attributes, or styles, and also evaluate four dimensions of compositionality:
counting, attributes, relations, and actions. Our human study shows that CCD is
highly correlated with human understanding of concepts. Our results point to a
trade-off between learning the concepts and preserving the compositionality
which existing approaches struggle to overcome.
</p></li>
</ul>

<h3>Title: WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models. (arXiv:2306.04744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04744">http://arxiv.org/abs/2306.04744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04744] WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models](http://arxiv.org/abs/2306.04744) #diffusion</code></li>
<li>Summary: <p>The rapid advancement of generative models, facilitating the creation of
hyper-realistic images from textual descriptions, has concurrently escalated
critical societal concerns such as misinformation. Traditional fake detection
mechanisms, although providing some mitigation, fall short in attributing
responsibility for the malicious use of synthetic images. This paper introduces
a novel approach to model fingerprinting that assigns responsibility for the
generated images, thereby serving as a potential countermeasure to model
misuse. Our method modifies generative models based on each user's unique
digital fingerprint, imprinting a unique identifier onto the resultant content
that can be traced back to the user. This approach, incorporating fine-tuning
into Text-to-Image (T2I) tasks using the Stable Diffusion Model, demonstrates
near-perfect attribution accuracy with a minimal impact on output quality. We
rigorously scrutinize our method's secrecy under two distinct scenarios: one
where a malicious user attempts to detect the fingerprint, and another where a
user possesses a comprehensive understanding of our method. We also evaluate
the robustness of our approach against various image post-processing
manipulations typically executed by end-users. Through extensive evaluation of
the Stable Diffusion models, our method presents a promising and novel avenue
for accountable model distribution and responsible use.
</p></li>
</ul>

<h3>Title: Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04848">http://arxiv.org/abs/2306.04848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04848] Interpreting and Improving Diffusion Models Using the Euclidean Distance Function](http://arxiv.org/abs/2306.04848) #diffusion</code></li>
<li>Summary: <p>Denoising is intuitively related to projection. Indeed, under the manifold
hypothesis, adding random noise is approximately equivalent to orthogonal
perturbation. Hence, learning to denoise is approximately learning to project.
In this paper, we use this observation to reinterpret denoising diffusion
models as approximate gradient descent applied to the Euclidean distance
function. We then provide straight-forward convergence analysis of the DDIM
sampler under simple assumptions on the projection-error of the denoiser.
Finally, we propose a new sampler based on two simple modifications to DDIM
using insights from our theoretical results. In as few as 5-10 function
evaluations, our sampler achieves state-of-the-art FID scores on pretrained
CIFAR-10 and CelebA models and can generate high quality samples on latent
diffusion models.
</p></li>
</ul>

<h3>Title: Multi-Architecture Multi-Expert Diffusion Models. (arXiv:2306.04990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04990">http://arxiv.org/abs/2306.04990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04990] Multi-Architecture Multi-Expert Diffusion Models](http://arxiv.org/abs/2306.04990) #diffusion</code></li>
<li>Summary: <p>Diffusion models have achieved impressive results in generating diverse and
realistic data by employing multi-step denoising processes. However, the need
for accommodating significant variations in input noise at each time-step has
led to diffusion models requiring a large number of parameters for their
denoisers. We have observed that diffusion models effectively act as filters
for different frequency ranges at each time-step noise. While some previous
works have introduced multi-expert strategies, assigning denoisers to different
noise intervals, they overlook the importance of specialized operations for
high and low frequencies. For instance, self-attention operations are effective
at handling low-frequency components (low-pass filters), while convolutions
excel at capturing high-frequency features (high-pass filters). In other words,
existing diffusion models employ denoisers with the same architecture, without
considering the optimal operations for each time-step noise. To address this
limitation, we propose a novel approach called Multi-architecturE Multi-Expert
(MEME), which consists of multiple experts with specialized architectures
tailored to the operations required at each time-step interval. Through
extensive experiments, we demonstrate that MEME outperforms large competitors
in terms of both generation performance and computational efficiency.
</p></li>
</ul>

<h3>Title: SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions. (arXiv:2306.05178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05178">http://arxiv.org/abs/2306.05178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05178] SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions](http://arxiv.org/abs/2306.05178) #diffusion</code></li>
<li>Summary: <p>The remarkable capabilities of pretrained image diffusion models have been
utilized not only for generating fixed-size images but also for creating
panoramas. However, naive stitching of multiple images often results in visible
seams. Recent techniques have attempted to address this issue by performing
joint diffusions in multiple windows and averaging latent features in
overlapping regions. However, these approaches, which focus on seamless montage
generation, often yield incoherent outputs by blending different scenes within
a single image. To overcome this limitation, we propose SyncDiffusion, a
plug-and-play module that synchronizes multiple diffusions through gradient
descent from a perceptual similarity loss. Specifically, we compute the
gradient of the perceptual loss using the predicted denoised images at each
denoising step, providing meaningful guidance for achieving coherent montages.
Our experimental results demonstrate that our method produces significantly
more coherent outputs compared to previous methods (66.35% vs. 33.65% in our
user study) while still maintaining fidelity (as assessed by GIQA) and
compatibility with the input prompt (as measured by CLIP score).
</p></li>
</ul>

<h3>Title: Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models. (arXiv:2306.05182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05182">http://arxiv.org/abs/2306.05182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05182] Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models](http://arxiv.org/abs/2306.05182) #diffusion</code></li>
<li>Summary: <p>Fashionable image generation aims to synthesize images of diverse fashion
prevalent around the globe, helping fashion designers in real-time
visualization by giving them a basic customized structure of how a specific
design preference would look in real life and what further improvements can be
made for enhanced customer satisfaction. Moreover, users can alone interact and
generate fashionable images by just giving a few simple prompts. Recently,
diffusion models have gained popularity as generative models owing to their
flexibility and generation of realistic images from Gaussian noise. Latent
diffusion models are a type of generative model that use diffusion processes to
model the generation of complex data, such as images, audio, or text. They are
called "latent" because they learn a hidden representation, or latent variable,
of the data that captures its underlying structure. We propose a method
exploiting the equivalence between diffusion models and energy-based models
(EBMs) and suggesting ways to compose multiple probability distributions. We
describe a pipeline on how our method can be used specifically for new
fashionable outfit generation and virtual try-on using LLM-guided text-to-image
generation. Our results indicate that using an LLM to refine the prompts to the
latent diffusion model assists in generating globally creative and culturally
diversified fashion styles and reducing bias.
</p></li>
</ul>

<h3>Title: PriSampler: Mitigating Property Inference of Diffusion Models. (arXiv:2306.05208v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05208">http://arxiv.org/abs/2306.05208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05208] PriSampler: Mitigating Property Inference of Diffusion Models](http://arxiv.org/abs/2306.05208) #diffusion</code></li>
<li>Summary: <p>Diffusion models have been remarkably successful in data synthesis. Such
successes have also driven diffusion models to apply to sensitive data, such as
human face data, but this might bring about severe privacy concerns. In this
work, we systematically present the first privacy study about property
inference attacks against diffusion models, in which adversaries aim to extract
sensitive global properties of the training set from a diffusion model, such as
the proportion of the training data for certain sensitive properties.
Specifically, we consider the most practical attack scenario: adversaries are
only allowed to obtain synthetic data. Under this realistic scenario, we
evaluate the property inference attacks on different types of samplers and
diffusion models. A broad range of evaluations shows that various diffusion
models and their samplers are all vulnerable to property inference attacks.
Furthermore, one case study on off-the-shelf pre-trained diffusion models also
demonstrates the effectiveness of the attack in practice. Finally, we propose a
new model-agnostic plug-in method PriSampler to mitigate the property inference
of diffusion models. PriSampler can be directly applied to well-trained
diffusion models and support both stochastic and deterministic sampling.
Extensive experiments illustrate the effectiveness of our defense and it makes
adversaries infer the proportion of properties as close as random guesses.
PriSampler also shows its significantly superior performance to diffusion
models trained with differential privacy on both model utility and defense
performance.
</p></li>
</ul>

<h3>Title: Anomaly Detection in Satellite Videos using Diffusion Models. (arXiv:2306.05376v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05376">http://arxiv.org/abs/2306.05376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05376] Anomaly Detection in Satellite Videos using Diffusion Models](http://arxiv.org/abs/2306.05376) #diffusion</code></li>
<li>Summary: <p>The definition of anomaly detection is the identification of an unexpected
event. Real-time detection of extreme events such as wildfires, cyclones, or
floods using satellite data has become crucial for disaster management.
Although several earth-observing satellites provide information about
disasters, satellites in the geostationary orbit provide data at intervals as
frequent as every minute, effectively creating a video from space. There are
many techniques that have been proposed to identify anomalies in surveillance
videos; however, the available datasets do not have dynamic behavior, so we
discuss an anomaly framework that can work on very high-frequency datasets to
find very fast-moving anomalies. In this work, we present a diffusion model
which does not need any motion component to capture the fast-moving anomalies
and outperforms the other baseline methods.
</p></li>
</ul>

<h3>Title: City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion. (arXiv:2306.04873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04873">http://arxiv.org/abs/2306.04873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04873] City-wide Origin-Destination Matrix Generation via Graph Denoising Diffusion](http://arxiv.org/abs/2306.04873) #diffusion</code></li>
<li>Summary: <p>The Origin-Destination~(OD) matrix provides an estimation of number of
individuals traveling between regions, i.e., mobility flow in the city, which
is widely-used in urban planning, transportation, etc. Given various city
characteristics of urban regions, generating the city-wide OD matrix without
using historical flow information has become increasingly appealing to both
researchers and practitioners. However, existing works are limited in
independent generation of each element, i.e., flow, in OD matrix, overlooking
the element relations within the matrix that can be well formulated as a
network. In this paper, we instead propose to generate the city-wide OD matrix
from the network perspective, and design a graph denoising diffusion method to
learn the conditional joint probability distribution of all elements in the OD
matrix given city characteristics at region level. To overcome the learning
difficulty of the city-wide OD matrix covering over thousands of regions, we
decompose the original one-shot generative modeling of the diffusion model into
two cascaded stages, corresponding to the generation of network topology and
mobility flow, respectively. To further reproduce important network properties
contained in city-wide OD matrices, we design an elaborated graph denoising
network structure including a node property augmentation module and a graph
transformer backbone. Empirical experiments on data collected in two large US
cities have verified that our method can generate OD matrices for new cities
with network statistics remarkably similar with the ground truth, further
achieving superior outperformance over competitive baselines in terms of the
generation realism.
</p></li>
</ul>

<h3>Title: Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning. (arXiv:2306.04875v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04875">http://arxiv.org/abs/2306.04875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04875] Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning](http://arxiv.org/abs/2306.04875) #diffusion</code></li>
<li>Summary: <p>Recent works have shown the potential of diffusion models in computer vision
and natural language processing. Apart from the classical supervised learning
fields, diffusion models have also shown strong competitiveness in
reinforcement learning (RL) by formulating decision-making as sequential
generation. However, incorporating temporal information of sequential data and
utilizing it to guide diffusion models to perform better generation is still an
open challenge. In this paper, we take one step forward to investigate
controllable generation with temporal conditions that are refined from temporal
information. We observe the importance of temporal conditions in sequential
generation in sufficient explorative scenarios and provide a comprehensive
discussion and comparison of different temporal conditions. Based on the
observations, we propose an effective temporally-conditional diffusion model
coined Temporally-Composable Diffuser (TCD), which extracts temporal
information from interaction sequences and explicitly guides generation with
temporal conditions. Specifically, we separate the sequences into three parts
according to time expansion and identify historical, immediate, and prospective
conditions accordingly. Each condition preserves non-overlapping temporal
information of sequences, enabling more controllable generation when we jointly
use them to guide the diffuser. Finally, we conduct extensive experiments and
analysis to reveal the favorable applicability of TCD in offline RL tasks,
where our method reaches or matches the best performance compared with prior
SOTA baselines.
</p></li>
</ul>

<h3>Title: Non-autoregressive Conditional Diffusion Models for Time Series Prediction. (arXiv:2306.05043v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05043">http://arxiv.org/abs/2306.05043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05043] Non-autoregressive Conditional Diffusion Models for Time Series Prediction](http://arxiv.org/abs/2306.05043) #diffusion</code></li>
<li>Summary: <p>Recently, denoising diffusion models have led to significant breakthroughs in
the generation of images, audio and text. However, it is still an open question
on how to adapt their strong modeling ability to model time series. In this
paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves
high-quality time series prediction with the introduction of two novel
conditioning mechanisms: future mixup and autoregressive initialization.
Similar to teacher forcing, future mixup allows parts of the ground-truth
future predictions for conditioning, while autoregressive initialization helps
better initialize the model with basic time series patterns such as short-term
trends. Extensive experiments are performed on nine real-world datasets.
Results show that TimeDiff consistently outperforms existing time series
diffusion models, and also achieves the best overall performance across a
variety of the existing strong baselines (including transformers and FiLM).
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency. (arXiv:2306.04654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04654">http://arxiv.org/abs/2306.04654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04654] DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency](http://arxiv.org/abs/2306.04654) #transformer</code></li>
<li>Summary: <p>In this paper, we propose a simple yet effective transformer framework for
self-supervised learning called DenseDINO to learn dense visual
representations. To exploit the spatial information that the dense prediction
tasks require but neglected by the existing self-supervised transformers, we
introduce point-level supervision across views in a novel token-based way.
Specifically, DenseDINO introduces some extra input tokens called reference
tokens to match the point-level features with the position prior. With the
reference token, the model could maintain spatial consistency and deal with
multi-object complex scene images, thus generalizing better on dense prediction
tasks. Compared with the vanilla DINO, our approach obtains competitive
performance when evaluated on classification in ImageNet and achieves a large
margin (+7.2% mIoU) improvement in semantic segmentation on PascalVOC under the
linear probing protocol for segmentation.
</p></li>
</ul>

<h3>Title: 2D Object Detection with Transformers: A Review. (arXiv:2306.04670v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04670">http://arxiv.org/abs/2306.04670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04670] 2D Object Detection with Transformers: A Review](http://arxiv.org/abs/2306.04670) #transformer</code></li>
<li>Summary: <p>Astounding performance of Transformers in natural language processing (NLP)
has delighted researchers to explore their utilization in computer vision
tasks. Like other computer vision tasks, DEtection TRansformer (DETR)
introduces transformers for object detection tasks by considering the detection
as a set prediction problem without needing proposal generation and
post-processing steps. It is a state-of-the-art (SOTA) method for object
detection, particularly in scenarios where the number of objects in an image is
relatively small. Despite the success of DETR, it suffers from slow training
convergence and performance drops for small objects. Therefore, many
improvements are proposed to address these issues, leading to immense
refinement in DETR. Since 2020, transformer-based object detection has
attracted increasing interest and demonstrated impressive performance. Although
numerous surveys have been conducted on transformers in vision in general, a
review regarding advancements made in 2D object detection using transformers is
still missing. This paper gives a detailed review of twenty-one papers about
recent developments in DETR. We begin with the basic modules of Transformers,
such as self-attention, object queries and input features encoding. Then, we
cover the latest advancements in DETR, including backbone modification, query
design and attention refinement. We also compare all detection transformers in
terms of performance and network design. We hope this study will increase the
researcher's interest in solving existing challenges towards applying
transformers in the object detection domain. Researchers can follow newer
improvements in detection transformers on this webpage available at:
https://github.com/mindgarage-shan/trans_object_detection_survey
</p></li>
</ul>

<h3>Title: Optimizing ViViT Training: Time and Memory Reduction for Action Recognition. (arXiv:2306.04822v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04822">http://arxiv.org/abs/2306.04822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04822] Optimizing ViViT Training: Time and Memory Reduction for Action Recognition](http://arxiv.org/abs/2306.04822) #transformer</code></li>
<li>Summary: <p>In this paper, we address the challenges posed by the substantial training
time and memory consumption associated with video transformers, focusing on the
ViViT (Video Vision Transformer) model, in particular the Factorised Encoder
version, as our baseline for action recognition tasks. The factorised encoder
variant follows the late-fusion approach that is adopted by many state of the
art approaches. Despite standing out for its favorable speed/accuracy tradeoffs
among the different variants of ViViT, its considerable training time and
memory requirements still pose a significant barrier to entry. Our method is
designed to lower this barrier and is based on the idea of freezing the spatial
transformer during training. This leads to a low accuracy model if naively
done. But we show that by (1) appropriately initializing the temporal
transformer (a module responsible for processing temporal information) (2)
introducing a compact adapter model connecting frozen spatial representations
((a module that selectively focuses on regions of the input image) to the
temporal transformer, we can enjoy the benefits of freezing the spatial
transformer without sacrificing accuracy. Through extensive experimentation
over 6 benchmarks, we demonstrate that our proposed training strategy
significantly reduces training costs (by $\sim 50\%$) and memory consumption
while maintaining or slightly improving performance by up to 1.79\% compared to
the baseline model. Our approach additionally unlocks the capability to utilize
larger image transformer models as our spatial transformer and access more
frames with the same memory consumption.
</p></li>
</ul>

<h3>Title: InvPT++: Inverted Pyramid Multi-Task Transformer for Visual Scene Understanding. (arXiv:2306.04842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04842">http://arxiv.org/abs/2306.04842</a></li>
<li>Code URL: <a href="https://github.com/prismformore/multi-task-transformer">https://github.com/prismformore/multi-task-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04842] InvPT++: Inverted Pyramid Multi-Task Transformer for Visual Scene Understanding](http://arxiv.org/abs/2306.04842) #transformer</code></li>
<li>Summary: <p>Multi-task scene understanding aims to design models that can simultaneously
predict several scene understanding tasks with one versatile model. Previous
studies typically process multi-task features in a more local way, and thus
cannot effectively learn spatially global and cross-task interactions, which
hampers the models' ability to fully leverage the consistency of various tasks
in multi-task learning. To tackle this problem, we propose an Inverted Pyramid
multi-task Transformer, capable of modeling cross-task interaction among
spatial features of different tasks in a global context. Specifically, we first
utilize a transformer encoder to capture task-generic features for all tasks.
And then, we design a transformer decoder to establish spatial and cross-task
interaction globally, and a novel UP-Transformer block is devised to increase
the resolutions of multi-task features gradually and establish cross-task
interaction at different scales. Furthermore, two types of Cross-Scale
Self-Attention modules, i.e., Fusion Attention and Selective Attention, are
proposed to efficiently facilitate cross-task interaction across different
feature scales. An Encoder Feature Aggregation strategy is further introduced
to better model multi-scale information in the decoder. Comprehensive
experiments on several 2D/3D multi-task benchmarks clearly demonstrate our
proposal's effectiveness, establishing significant state-of-the-art
performances.
</p></li>
</ul>

<h3>Title: Muti-Scale And Token Mergence: Make Your ViT More Efficient. (arXiv:2306.04897v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04897">http://arxiv.org/abs/2306.04897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04897] Muti-Scale And Token Mergence: Make Your ViT More Efficient](http://arxiv.org/abs/2306.04897) #transformer</code></li>
<li>Summary: <p>Since its inception, Vision Transformer (ViT) has emerged as a prevalent
model in the computer vision domain. Nonetheless, the multi-head self-attention
(MHSA) mechanism in ViT is computationally expensive due to its calculation of
relationships among all tokens. Although some techniques mitigate computational
overhead by discarding tokens, this also results in the loss of potential
information from those tokens. To tackle these issues, we propose a novel token
pruning method that retains information from non-crucial tokens by merging them
with more crucial tokens, thereby mitigating the impact of pruning on model
performance. Crucial and non-crucial tokens are identified by their importance
scores and merged based on similarity scores. Furthermore, multi-scale features
are exploited to represent images, which are fused prior to token pruning to
produce richer feature representations. Importantly, our method can be
seamlessly integrated with various ViTs, enhancing their adaptability.
Experimental evidence substantiates the efficacy of our approach in reducing
the influence of token pruning on model performance. For instance, on the
ImageNet dataset, it achieves a remarkable 33% reduction in computational costs
while only incurring a 0.1% decrease in accuracy on DeiT-S.
</p></li>
</ul>

<h3>Title: An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection. (arXiv:2306.04927v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04927">http://arxiv.org/abs/2306.04927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04927] An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection](http://arxiv.org/abs/2306.04927) #transformer</code></li>
<li>Summary: <p>Accurately detecting lane lines in 3D space is crucial for autonomous
driving. Existing methods usually first transform image-view features into
bird-eye-view (BEV) by aid of inverse perspective mapping (IPM), and then
detect lane lines based on the BEV features. However, IPM ignores the changes
in road height, leading to inaccurate view transformations. Additionally, the
two separate stages of the process can cause cumulative errors and increased
complexity. To address these limitations, we propose an efficient transformer
for 3D lane detection. Different from the vanilla transformer, our model
contains a decomposed cross-attention mechanism to simultaneously learn lane
and BEV representations. The mechanism decomposes the cross-attention between
image-view and BEV features into the one between image-view and lane features,
and the one between lane and BEV features, both of which are supervised with
ground-truth lane lines. Our method obtains 2D and 3D lane predictions by
applying the lane features to the image-view and BEV features, respectively.
This allows for a more accurate view transformation than IPM-based methods, as
the view transformation is learned from data with a supervised cross-attention.
Additionally, the cross-attention between lane and BEV features enables them to
adjust to each other, resulting in more accurate lane detection than the two
separate stages. Finally, the decomposed cross-attention is more efficient than
the original one. Experimental results on OpenLane and ONCE-3DLanes demonstrate
the state-of-the-art performance of our method.
</p></li>
</ul>

<h3>Title: Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification. (arXiv:2306.05029v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05029">http://arxiv.org/abs/2306.05029</a></li>
<li>Code URL: <a href="https://github.com/hustvl/mmil-transformer">https://github.com/hustvl/mmil-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05029] Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification](http://arxiv.org/abs/2306.05029) #transformer</code></li>
<li>Summary: <p>Whole slide image (WSI) refers to a type of high-resolution scanned tissue
image, which is extensively employed in computer-assisted diagnosis (CAD). The
extremely high resolution and limited availability of region-level annotations
make it challenging to employ deep learning methods for WSI-based digital
diagnosis. Multiple instance learning (MIL) is a powerful tool to address the
weak annotation problem, while Transformer has shown great success in the field
of visual tasks. The combination of both should provide new insights for deep
learning based image diagnosis. However, due to the limitations of single-level
MIL and the attention mechanism's constraints on sequence length, directly
applying Transformer to WSI-based MIL tasks is not practical. To tackle this
issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer)
approach. By introducing a hierarchical structure to MIL, this approach enables
efficient handling of MIL tasks that involve a large number of instances. To
validate its effectiveness, we conducted a set of experiments on WSIs
classification task, where MMIL-Transformer demonstrate superior performance
compared to existing state-of-the-art methods. Our proposed approach achieves
test AUC 94.74% and test accuracy 93.41% on CAMELYON16 dataset, test AUC 99.04%
and test accuracy 94.37% on TCGA-NSCLC dataset, respectively. All code and
pre-trained models are available at: https://github.com/hustvl/MMIL-Transformer
</p></li>
</ul>

<h3>Title: Improving Visual Prompt Tuning for Self-supervised Vision Transformers. (arXiv:2306.05067v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05067">http://arxiv.org/abs/2306.05067</a></li>
<li>Code URL: <a href="https://github.com/ryongithub/gatedprompttuning">https://github.com/ryongithub/gatedprompttuning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05067] Improving Visual Prompt Tuning for Self-supervised Vision Transformers](http://arxiv.org/abs/2306.05067) #transformer</code></li>
<li>Summary: <p>Visual Prompt Tuning (VPT) is an effective tuning method for adapting
pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra
learnable tokens, known as prompts, which steer the frozen pretrained ViTs.
Although VPT has demonstrated its applicability with supervised vision
transformers, it often underperforms with self-supervised ones. Through
empirical observations, we deduce that the effectiveness of VPT hinges largely
on the ViT blocks with which the prompt tokens interact. Specifically, VPT
shows improved performance on image classification tasks for MAE and MoCo v3
when the prompt tokens are inserted into later blocks rather than the first
block. These observations suggest that there exists an optimal location of
blocks for the insertion of prompt tokens. Unfortunately, identifying the
optimal blocks for prompts within each self-supervised ViT for diverse future
scenarios is a costly process. To mitigate this problem, we propose a simple
yet effective method that learns a gate for each ViT block to adjust its
intervention into the prompt tokens. With our method, prompt tokens are
selectively influenced by blocks that require steering for task adaptation. Our
method outperforms VPT variants in FGVC and VTAB image classification and
ADE20K semantic segmentation. The code is available at
https://github.com/ryongithub/GatedPromptTuning.
</p></li>
</ul>

<h3>Title: Large-scale Dataset Pruning with Dynamic Uncertainty. (arXiv:2306.05175v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05175">http://arxiv.org/abs/2306.05175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05175] Large-scale Dataset Pruning with Dynamic Uncertainty](http://arxiv.org/abs/2306.05175) #transformer</code></li>
<li>Summary: <p>The state of the art of many learning tasks, e.g., image classification, is
advanced by collecting larger datasets and then training larger models on them.
As the outcome, the increasing computational cost is becoming unaffordable. In
this paper, we investigate how to prune the large-scale datasets, and thus
produce an informative subset for training sophisticated deep models with
negligible performance drop. We propose a simple yet effective dataset pruning
method by exploring both the prediction uncertainty and training dynamics. To
our knowledge, this is the first work to study dataset pruning on large-scale
datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin
Transformer and ConvNeXt. Extensive experimental results indicate that our
method outperforms the state of the art and achieves 75% lossless compression
ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are
available at https://github.com/BAAI-DCAI/Dataset-Pruning.
</p></li>
</ul>

<h3>Title: Efficient Multi-Task Scene Analysis with RGB-D Transformers. (arXiv:2306.05242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05242">http://arxiv.org/abs/2306.05242</a></li>
<li>Code URL: <a href="https://github.com/tui-nicr/emsaformer">https://github.com/tui-nicr/emsaformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05242] Efficient Multi-Task Scene Analysis with RGB-D Transformers](http://arxiv.org/abs/2306.05242) #transformer</code></li>
<li>Summary: <p>Scene analysis is essential for enabling autonomous systems, such as mobile
robots, to operate in real-world environments. However, obtaining a
comprehensive understanding of the scene requires solving multiple tasks, such
as panoptic segmentation, instance orientation estimation, and scene
classification. Solving these tasks given limited computing and battery
capabilities on mobile platforms is challenging. To address this challenge, we
introduce an efficient multi-task scene analysis approach, called EMSAFormer,
that uses an RGB-D Transformer-based encoder to simultaneously perform the
aforementioned tasks. Our approach builds upon the previously published
EMSANet. However, we show that the dual CNN-based encoder of EMSANet can be
replaced with a single Transformer-based encoder. To achieve this, we
investigate how information from both RGB and depth data can be effectively
incorporated in a single encoder. To accelerate inference on robotic hardware,
we provide a custom NVIDIA TensorRT extension enabling highly optimization for
our EMSAFormer approach. Through extensive experiments on the commonly used
indoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approach
achieves state-of-the-art performance while still enabling inference with up to
39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.
</p></li>
</ul>

<h3>Title: Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization. (arXiv:2306.04787v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04787">http://arxiv.org/abs/2306.04787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04787] Absformer: Transformer-based Model for Unsupervised Multi-Document Abstractive Summarization](http://arxiv.org/abs/2306.04787) #transformer</code></li>
<li>Summary: <p>Multi-document summarization (MDS) refers to the task of summarizing the text
in multiple documents into a concise summary. The generated summary can save
the time of reading many documents by providing the important content in the
form of a few sentences. Abstractive MDS aims to generate a coherent and fluent
summary for multiple documents using natural language generation techniques. In
this paper, we consider the unsupervised abstractive MDS setting where there
are only documents with no groundtruh summaries provided, and we propose
Absformer, a new Transformer-based method for unsupervised abstractive summary
generation. Our method consists of a first step where we pretrain a
Transformer-based encoder using the masked language modeling (MLM) objective as
the pretraining task in order to cluster the documents into semantically
similar groups; and a second step where we train a Transformer-based decoder to
generate abstractive summaries for the clusters of documents. To our knowledge,
we are the first to successfully incorporate a Transformer-based model to solve
the unsupervised abstractive MDS task. We evaluate our approach using three
real-world datasets from different domains, and we demonstrate both substantial
improvements in terms of evaluation metrics over state-of-the-art
abstractive-based methods, and generalization to datasets from different
domains.
</p></li>
</ul>

<h3>Title: In-Context Learning through the Bayesian Prism. (arXiv:2306.04891v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04891">http://arxiv.org/abs/2306.04891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04891] In-Context Learning through the Bayesian Prism](http://arxiv.org/abs/2306.04891) #transformer</code></li>
<li>Summary: <p>In-context learning is one of the surprising and useful features of large
language models. How it works is an active area of research. Recently, stylized
meta-learning-like setups have been devised that train these models on a
sequence of input-output pairs $(x, f(x))$ from a function class using the
language modeling loss and observe generalization to unseen functions from the
same class. One of the main discoveries in this line of research has been that
for several problems such as linear regression, trained transformers learn
algorithms for learning functions in context. However, the inductive biases of
these models resulting in this behavior are not clearly understood. A model
with unlimited training data and compute is a Bayesian predictor: it learns the
pretraining distribution. It has been shown that high-capacity transformers
mimic the Bayesian predictor for linear regression. In this paper, we show
empirical evidence of transformers exhibiting the behavior of this ideal
learner across different linear and non-linear function classes. We also extend
the previous setups to work in the multitask setting and verify that
transformers can do in-context learning in this setup as well and the Bayesian
perspective sheds light on this setting also. Finally, via the example of
learning Fourier series, we study the inductive bias for in-context learning.
We find that in-context learning may or may not have simplicity bias depending
on the pretraining data distribution.
</p></li>
</ul>

<h3>Title: RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05176">http://arxiv.org/abs/2306.05176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05176] RRWKV: Capturing Long-range Dependencies in RWKV](http://arxiv.org/abs/2306.05176) #transformer</code></li>
<li>Summary: <p>Owing to the impressive dot-product attention, the Transformers have been the
dominant architectures in various natural language processing (NLP) tasks.
Recently, the Receptance Weighted Key Value (RWKV) architecture follows a
non-transformer architecture to eliminate the drawbacks of dot-product
attention, where memory and computational complexity exhibits quadratic scaling
with sequence length. Although RWKV has exploited a linearly tensor-product
attention mechanism and achieved parallelized computations by deploying the
time-sequential mode, it fails to capture long-range dependencies because of
its limitation on looking back at previous information, compared with full
information obtained by direct interactions in the standard transformer.
Therefore, the paper devises the Retrospected Receptance Weighted Key Value
(RRWKV) architecture via incorporating the retrospecting ability into the RWKV
to effectively absorb information, which maintains memory and computational
efficiency as well.
</p></li>
</ul>

<h3>Title: Flow-based Network Intrusion Detection Based on BERT Masked Language Model. (arXiv:2306.04920v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04920">http://arxiv.org/abs/2306.04920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04920] Flow-based Network Intrusion Detection Based on BERT Masked Language Model](http://arxiv.org/abs/2306.04920) #transformer</code></li>
<li>Summary: <p>A Network Intrusion Detection System (NIDS) is an important tool that
identifies potential threats to a network. Recently, different flow-based NIDS
designs utilizing Machine Learning (ML) algorithms have been proposed as
potential solutions to detect intrusions efficiently. However, conventional
ML-based classifiers have not seen widespread adoption in the real-world due to
their poor domain adaptation capability. In this research, our goal is to
explore the possibility of improve the domain adaptation capability of NIDS.
Our proposal employs Natural Language Processing (NLP) techniques and
Bidirectional Encoder Representations from Transformers (BERT) framework. The
proposed method achieved positive results when tested on data from different
domains.
</p></li>
</ul>

<h3>Title: Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition. (arXiv:2306.05021v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05021">http://arxiv.org/abs/2306.05021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05021] Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific Tensor Decomposition](http://arxiv.org/abs/2306.05021) #transformer</code></li>
<li>Summary: <p>Neural Network designs are quite diverse, from VGG-style to ResNet-style, and
from Convolutional Neural Networks to Transformers. Towards the design of
efficient accelerators, many works have adopted a dataflow-based, inter-layer
pipelined architecture, with a customised hardware towards each layer,
achieving ultra high throughput and low latency. The deployment of neural
networks to such dataflow architecture accelerators is usually hindered by the
available on-chip memory as it is desirable to preload the weights of neural
networks on-chip to maximise the system performance. To address this, networks
are usually compressed before the deployment through methods such as pruning,
quantization and tensor decomposition. In this paper, a framework for mapping
CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is
proposed. The proposed method applies layer-specific Singular Value
Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed
manner, achieving 1.73x to 10.29x throughput per DSP to state-of-the-art CNNs.
Our work is open-sourced: https://github.com/Yu-Zhewen/Mixed-TD
</p></li>
</ul>

<h3>Title: Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?. (arXiv:2306.05035v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05035">http://arxiv.org/abs/2306.05035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05035] Does Long-Term Series Forecasting Need Complex Attention and Extra Long Inputs?](http://arxiv.org/abs/2306.05035) #transformer</code></li>
<li>Summary: <p>As Transformer-based models have achieved impressive performance on various
time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received
extensive attention in recent years. However, due to the inherent computational
complexity and long sequences demanding of Transformer-based methods, its
application on LTSF tasks still has two major issues that need to be further
investigated: 1) Whether the sparse attention mechanism designed by these
methods actually reduce the running time on real devices; 2) Whether these
models need extra long input sequences to guarantee their performance? The
answers given in this paper are negative. Therefore, to better copy with these
two issues, we design a lightweight Period-Attention mechanism (Periodformer),
which renovates the aggregation of long-term subseries via explicit periodicity
and short-term subseries via built-in proximity. Meanwhile, a gating mechanism
is embedded into Periodformer to regulate the influence of the attention module
on the prediction results. Furthermore, to take full advantage of GPUs for fast
hyperparameter optimization (e.g., finding the suitable input length), a
Multi-GPU Asynchronous parallel algorithm based on Bayesian Optimization (MABO)
is presented. MABO allocates a process to each GPU via a queue mechanism, and
then creates multiple trials at a time for asynchronous parallel search, which
greatly reduces the search time. Compared with the state-of-the-art methods,
the prediction error of Periodformer reduced by 13% and 26% for multivariate
and univariate forecasting, respectively. In addition, MABO reduces the average
search time by 46% while finding better hyperparameters. As a conclusion, this
paper indicates that LTSF may not need complex attention and extra long input
sequences. The source code will be open source on Github.
</p></li>
</ul>

<h3>Title: Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer. (arXiv:2306.05143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05143">http://arxiv.org/abs/2306.05143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05143] Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer](http://arxiv.org/abs/2306.05143) #transformer</code></li>
<li>Summary: <p>Given the increasing volume and quality of genomics data, extracting new
insights requires interpretable machine-learning models. This work presents
Genomic Interpreter: a novel architecture for genomic assay prediction. This
model outperforms the state-of-the-art models for genomic assay prediction
tasks. Our model can identify hierarchical dependencies in genomic sites. This
is achieved through the integration of 1D-Swin, a novel Transformer-based block
designed by us for modelling long-range hierarchical data. Evaluated on a
dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter
demonstrates superior performance in chromatin accessibility and gene
expression prediction and unmasks the underlying `syntax' of gene regulation.
</p></li>
</ul>

<h3>Title: Decision S4: Efficient Sequence-Based RL via State Spaces Layers. (arXiv:2306.05167v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05167">http://arxiv.org/abs/2306.05167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05167] Decision S4: Efficient Sequence-Based RL via State Spaces Layers](http://arxiv.org/abs/2306.05167) #transformer</code></li>
<li>Summary: <p>Recently, sequence learning methods have been applied to the problem of
off-policy Reinforcement Learning, including the seminal work on Decision
Transformers, which employs transformers for this task. Since transformers are
parameter-heavy, cannot benefit from history longer than a fixed window size,
and are not computed using recurrence, we set out to investigate the
suitability of the S4 family of models, which are based on state-space layers
and have been shown to outperform transformers, especially in modeling
long-range dependencies. In this work we present two main algorithms: (i) an
off-policy training procedure that works with trajectories, while still
maintaining the training efficiency of the S4 model. (ii) An on-policy training
procedure that is trained in a recurrent manner, benefits from long-range
dependencies, and is based on a novel stable actor-critic mechanism. Our
results indicate that our method outperforms multiple variants of decision
transformers, as well as the other baseline methods on most tasks, while
reducing the latency, number of parameters, and training time by several orders
of magnitude, making our approach more suitable for real-world RL.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment. (arXiv:2306.04717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04717">http://arxiv.org/abs/2306.04717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04717] AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment](http://arxiv.org/abs/2306.04717) #generative</code></li>
<li>Summary: <p>With the rapid advancements of the text-to-image generative model,
AI-generated images (AGIs) have been widely applied to entertainment,
education, social media, etc. However, considering the large quality variance
among different AGIs, there is an urgent need for quality models that are
consistent with human subjective ratings. To address this issue, we extensively
consider various popular AGI models, generated AGI through different prompts
and model parameters, and collected subjective scores at the perceptual quality
and text-to-image alignment, thus building the most comprehensive AGI
subjective quality database AGIQA-3K so far. Furthermore, we conduct a
benchmark experiment on this database to evaluate the consistency between the
current Image Quality Assessment (IQA) model and human perception, while
proposing StairReward that significantly improves the assessment performance of
subjective text-to-image alignment. We believe that the fine-grained subjective
scores in AGIQA-3K will inspire subsequent AGI quality models to fit human
subjective perception mechanisms at both perception and alignment levels and to
optimize the generation result of future AGI models. The database is released
on \url{https://github.com/lcysyzxdxc/AGIQA-3k-Database}.
</p></li>
</ul>

<h3>Title: Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation. (arXiv:2306.04811v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04811">http://arxiv.org/abs/2306.04811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04811] Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation](http://arxiv.org/abs/2306.04811) #generative</code></li>
<li>Summary: <p>Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in
learning visual representations from textual descriptions of images without
annotations. Yet, effective VLP demands large-scale image-text pairs, a
resource that suffers scarcity in the medical domain. Moreover, conventional
VLP is limited to 2D images while medical images encompass diverse modalities,
often in 3D, making the learning process more challenging. To address these
challenges, we present Generative Text-Guided 3D Vision-Language Pretraining
for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP
to 3D medical images without relying on paired textual descriptions.
Specifically, GTGM utilizes large language models (LLM) to generate
medical-style text from 3D medical images. This synthetic text is then used to
supervise 3D visual representation learning. Furthermore, a negative-free
contrastive learning objective strategy is introduced to cultivate consistent
visual representations between augmented 3D medical image patches, which
effectively mitigates the biases associated with strict positive-negative
sample pairings. We evaluate GTGM on three imaging modalities - Computed
Tomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM)
over 13 datasets. GTGM's superior performance across various medical image
segmentation tasks underscores its effectiveness and versatility, by enabling
VLP extension into 3D medical imagery while bypassing the need for paired text.
</p></li>
</ul>

<h3>Title: MyStyle++: A Controllable Personalized Generative Prior. (arXiv:2306.04865v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04865">http://arxiv.org/abs/2306.04865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04865] MyStyle++: A Controllable Personalized Generative Prior](http://arxiv.org/abs/2306.04865) #generative</code></li>
<li>Summary: <p>In this paper, we propose an approach to obtain a personalized generative
prior with explicit control over a set of attributes. We build upon MyStyle, a
recently introduced method, that tunes the weights of a pre-trained StyleGAN
face generator on a few images of an individual. This system allows
synthesizing, editing, and enhancing images of the target individual with high
fidelity to their facial features. However, MyStyle does not demonstrate
precise control over the attributes of the generated images. We propose to
address this problem through a novel optimization system that organizes the
latent space in addition to tuning the generator. Our key contribution is to
formulate a loss that arranges the latent codes, corresponding to the input
images, along a set of specific directions according to their attributes. We
demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,
and enhance images of an individual with great control over the attributes,
while preserving the unique facial characteristics of that individual.
</p></li>
</ul>

<h3>Title: ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04889">http://arxiv.org/abs/2306.04889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04889] ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering](http://arxiv.org/abs/2306.04889) #generative</code></li>
<li>Summary: <p>We present ShaDDR, an example-based deep generative neural network which
produces a high-resolution textured 3D shape through geometry detailization and
conditional texture generation applied to an input coarse voxel shape. Trained
on a small set of detailed and textured exemplar shapes, our method learns to
detailize the geometry via multi-resolution voxel upsampling and generate
textures on voxel surfaces via differentiable rendering against exemplar
texture images from a few views. The generation is real-time, taking less than
1 second to produce a 3D model with voxel resolutions up to 512^3. The
generated shape preserves the overall structure of the input coarse voxel
model, while the style of the generated geometric details and textures can be
manipulated through learned latent codes. In the experiments, we show that our
method can generate higher-resolution shapes with plausible and improved
geometric details and clean textures compared to prior works. Furthermore, we
showcase the ability of our method to learn geometric details and textures from
shapes reconstructed from real-world photos. In addition, we have developed an
interactive modeling application to demonstrate the generalizability of our
method to various user inputs and the controllability it offers, allowing users
to interactively sculpt a coarse voxel shape to define the overall structure of
the detailized 3D shape.
</p></li>
</ul>

<h3>Title: Unscented Autoencoder. (arXiv:2306.05256v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05256">http://arxiv.org/abs/2306.05256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05256] Unscented Autoencoder](http://arxiv.org/abs/2306.05256) #generative</code></li>
<li>Summary: <p>The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.
</p></li>
</ul>

<h3>Title: Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05357">http://arxiv.org/abs/2306.05357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05357] Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models](http://arxiv.org/abs/2306.05357) #generative</code></li>
<li>Summary: <p>Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.
</p></li>
</ul>

<h3>Title: The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. (arXiv:2306.05360v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05360">http://arxiv.org/abs/2306.05360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05360] The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues](http://arxiv.org/abs/2306.05360) #generative</code></li>
<li>Summary: <p>This paper presents the ADAIO team's system entry in the Building Educational
Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in
Educational Dialogues. The task aims to assess the performance of
state-of-the-art generative models as AI teachers in producing suitable
responses within a student-teacher dialogue. Our system comprises evaluating
various baseline models using OpenAI GPT-3 and designing diverse prompts to
prompt the OpenAI models for teacher response generation. After the challenge,
our system achieved second place by employing a few-shot prompt-based approach
with the OpenAI text-davinci-003 model. The results highlight the few-shot
learning capabilities of large-language models, particularly OpenAI's GPT-3, in
the role of AI teachers.
</p></li>
</ul>

<h3>Title: Understanding Place Identity with Generative AI. (arXiv:2306.04662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04662">http://arxiv.org/abs/2306.04662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04662] Understanding Place Identity with Generative AI](http://arxiv.org/abs/2306.04662) #generative</code></li>
<li>Summary: <p>Researchers are constantly leveraging new forms of data with the goal of
understanding how people perceive the built environment and build the
collective place identity of cities. Latest advancements in generative
artificial intelligence (AI) models have enabled the production of realistic
representations learned from vast amounts of data. In this study, we aim to
test the potential of generative AI as the source of textual and visual
information in capturing the place identity of cities assessed by filtered
descriptions and images. We asked questions on the place identity of a set of
31 global cities to two generative AI models, ChatGPT and DALL-E2. Since
generative AI has raised ethical concerns regarding its trustworthiness, we
performed cross-validation to examine whether the results show similar patterns
to real urban settings. In particular, we compared the outputs with Wikipedia
data for text and images searched from Google for image. Our results indicate
that generative AI models have the potential to capture the collective image of
cities that can make them distinguishable. This study is among the first
attempts to explore the capabilities of generative AI in understanding human
perceptions of the built environment. It contributes to urban design literature
by discussing future research opportunities and potential limitations.
</p></li>
</ul>

<h3>Title: Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning. (arXiv:2306.04748v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04748">http://arxiv.org/abs/2306.04748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04748] Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning](http://arxiv.org/abs/2306.04748) #generative</code></li>
<li>Summary: <p>Parkinson's disease (PD) is a prevalent neurodegenerative disorder with
varying patient trajectories, yet little is understood about the underlying
causes and symptom progression. The Parkinson's Progression Markers Initiative
(PPMI) has collected comprehensive longitudinal data from diverse patient
cohorts to identify biomarkers and aid in the development of interventions.
Despite over 110 machine learning studies using the PPMI database, the majority
have focused on supervised models for diagnosis prediction, which has limited
impact on understanding patient variability and progression. This paper
addresses this gap by combining supervised and unsupervised machine learning
methods to identify subtypes that accurately predict disease progression in
Parkinson's patients. Building upon previous work, we replicate and extend the
study by integrating unsupervised patient clustering and prediction of present
and future symptoms using 5 additional years of longitudinal data from the
Progressive Parkinson's Markers Initiative (PPMI) database. Our findings
demonstrate accurate prediction of disease trajectories and symptoms at
baseline, offering valuable insights into patient heterogeneity and the
potential for personalized interventions. The integration of supervised and
unsupervised models presents a promising avenue for uncovering latent subgroups
and understanding the complexity of Parkinson's disease progression.
</p></li>
</ul>

<h3>Title: Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators. (arXiv:2306.05041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05041">http://arxiv.org/abs/2306.05041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05041] Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators](http://arxiv.org/abs/2306.05041) #generative</code></li>
<li>Summary: <p>In this paper, we introduce a novel semantic generative communication (SGC)
framework, where generative users leverage text-to-image (T2I) generators to
create images locally from downloaded text prompts, while non-generative users
directly download images from a base station (BS). Although generative users
help reduce downlink transmission energy at the BS, they consume additional
energy for image generation and for uploading their generator state information
(GSI). We formulate the problem of minimizing the total energy consumption of
the BS and the users, and devise a generative user selection algorithm.
Simulation results corroborate that our proposed algorithm reduces total energy
by up to 54% compared to a baseline with all non-generative users.
</p></li>
</ul>

<h3>Title: A Meta-Generation framework for Industrial System Generation. (arXiv:2306.05123v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05123">http://arxiv.org/abs/2306.05123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05123] A Meta-Generation framework for Industrial System Generation](http://arxiv.org/abs/2306.05123) #generative</code></li>
<li>Summary: <p>Generative design is an increasingly important tool in the industrial world.
It allows the designers and engineers to easily explore vast ranges of design
options, providing a cheaper and faster alternative to the trial and failure
approaches. Thanks to the flexibility they offer, Deep Generative Models are
gaining popularity amongst Generative Design technologies. However, developing
and evaluating these models can be challenging. The field lacks accessible
benchmarks, in order to evaluate and compare objectively different Deep
Generative Models architectures. Moreover, vanilla Deep Generative Models
appear to be unable to accurately generate multi-components industrial systems
that are controlled by latent design constraints. To address these challenges,
we propose an industry-inspired use case that incorporates actual industrial
system characteristics. This use case can be quickly generated and used as a
benchmark. We propose a Meta-VAE capable of producing multi-component
industrial systems and showcase its application on the proposed use case.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. (arXiv:2306.05179v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05179">http://arxiv.org/abs/2306.05179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05179] M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models](http://arxiv.org/abs/2306.05179) #large language model</code></li>
<li>Summary: <p>Despite the existence of various benchmarks for evaluating natural language
processing models, we argue that human exams are a more suitable means of
evaluating general intelligence for large language models (LLMs), as they
inherently demand a much wider range of abilities such as language
understanding, domain knowledge, and problem-solving skills. To this end, we
introduce M3Exam, a novel benchmark sourced from real and official human exam
questions for evaluating LLMs in a multilingual, multimodal, and multilevel
context. M3Exam exhibits three unique characteristics: (1) multilingualism,
encompassing questions from multiple countries that require strong multilingual
proficiency and cultural knowledge; (2) multimodality, accounting for the
multimodal nature of many exam questions to test the model's multimodal
understanding capability; and (3) multilevel structure, featuring exams from
three critical educational periods to comprehensively assess a model's
proficiency at different levels. In total, M3Exam contains 12,317 questions in
9 diverse languages with three educational levels, where about 23\% of the
questions require processing images for successful solving. We assess the
performance of top-performing LLMs on M3Exam and find that current models,
including GPT-4, still struggle with multilingual text, particularly in
low-resource and non-Latin script languages. Multimodal LLMs also perform
poorly with complex multimodal questions. We believe that M3Exam can be a
valuable resource for comprehensively evaluating LLMs by examining their
multilingual and multimodal abilities and tracking their development. Data and
evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.
</p></li>
</ul>

<h3>Title: Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04735">http://arxiv.org/abs/2306.04735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04735] Soft-prompt Tuning for Large Language Models to Evaluate Bias](http://arxiv.org/abs/2306.04735) #large language model</code></li>
<li>Summary: <p>Prompting large language models has gained immense popularity in recent years
due to the advantage of producing good results even without the need for
labelled data. However, this requires prompt tuning to get optimal prompts that
lead to better model performances. In this paper, we explore the use of
soft-prompt tuning on sentiment classification task to quantify the biases of
large language models (LLMs) such as Open Pre-trained Transformers (OPT) and
Galactica language model. Since these models are trained on real-world data
that could be prone to bias toward certain groups of populations, it is
important to identify these underlying issues. Using soft-prompts to evaluate
bias gives us the extra advantage of avoiding the human-bias injection that can
be caused by manually designed prompts. We check the model biases on different
sensitive attributes using the group fairness (bias) and find interesting bias
patterns. Since LLMs have been used in the industry in various applications, it
is crucial to identify the biases before deploying these models in practice. We
open-source our pipeline and encourage industry researchers to adapt our work
to their use cases.
</p></li>
</ul>

<h3>Title: INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04757">http://arxiv.org/abs/2306.04757</a></li>
<li>Code URL: <a href="https://github.com/declare-lab/instruct-eval">https://github.com/declare-lab/instruct-eval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04757] INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](http://arxiv.org/abs/2306.04757) #large language model</code></li>
<li>Summary: <p>Instruction-tuned large language models have revolutionized natural language
processing and have shown great potential in applications such as
conversational agents. These models, such as GPT-4, can not only master
language but also solve complex tasks in areas like mathematics, coding,
medicine, and law. Despite their impressive capabilities, there is still a lack
of comprehensive understanding regarding their full potential, primarily due to
the black-box nature of many models and the absence of holistic evaluation
studies. To address these challenges, we present INSTRUCTEVAL, a more
comprehensive evaluation suite designed specifically for instruction-tuned
large language models. Unlike previous works, our evaluation involves a
rigorous assessment of models based on problem-solving, writing ability, and
alignment to human values. We take a holistic approach to analyze various
factors affecting model performance, including the pretraining foundation,
instruction-tuning data, and training methods. Our findings reveal that the
quality of instruction data is the most crucial factor in scaling model
performance. While open-source models demonstrate impressive writing abilities,
there is substantial room for improvement in problem-solving and alignment. We
are encouraged by the rapid development of models by the open-source community,
but we also highlight the need for rigorous evaluation to support claims made
about these models. Through INSTRUCTEVAL, we aim to foster a deeper
understanding of instruction-tuned models and advancements in their
capabilities. INSTRUCTEVAL is publicly available at
https://github.com/declare-lab/instruct-eval.
</p></li>
</ul>

<h3>Title: Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers. (arXiv:2306.04820v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04820">http://arxiv.org/abs/2306.04820</a></li>
<li>Code URL: <a href="https://github.com/crowd-ai-lab/coda-19-exp">https://github.com/crowd-ai-lab/coda-19-exp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04820] Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers](http://arxiv.org/abs/2306.04820) #large language model</code></li>
<li>Summary: <p>The rapid growth of scientific publications, particularly during the COVID-19
pandemic, emphasizes the need for tools to help researchers efficiently
comprehend the latest advancements. One essential part of understanding
scientific literature is research aspect classification, which categorizes
sentences in abstracts to Background, Purpose, Method, and Finding. In this
study, we investigate the impact of different datasets on model performance for
the crowd-annotated CODA-19 research aspect classification task. Specifically,
we explore the potential benefits of using the large, automatically curated
PubMed 200K RCT dataset and evaluate the effectiveness of large language models
(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that
using the PubMed 200K RCT dataset does not improve performance for the CODA-19
task. We also observe that while GPT-4 performs well, it does not outperform
the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance
of a dedicated and task-aligned datasets dataset for the target task. Our code
is available at https://github.com/Crowd-AI-Lab/CODA-19-exp.
</p></li>
</ul>

<h3>Title: Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning. (arXiv:2306.04925v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04925">http://arxiv.org/abs/2306.04925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04925] Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning](http://arxiv.org/abs/2306.04925) #large language model</code></li>
<li>Summary: <p>The development of largely human-annotated benchmarks has driven the success
of deep neural networks in various NLP tasks. To enhance the effectiveness of
existing benchmarks, collecting new additional input-output pairs is often too
costly and challenging, particularly considering their marginal impact on
improving the current model accuracy. Instead, additional or complementary
annotations on the existing input texts in the benchmarks can be preferable as
an efficient way to pay the additional human cost. In this paper, we
investigate task-specific preferences between pairs of input texts as a new
alternative way for such auxiliary data annotation. From 'pair-wise'
comparisons with respect to the task, the auxiliary preference learning enables
the model to learn an additional informative training signal that cannot be
captured with 'instance-wise' task labels. To this end, we propose a novel
multi-task learning framework, called prefer-to-classify (P2C), which can enjoy
the cooperative effect of learning both the given classification task and the
auxiliary preferences. Here, we provide three different ways to collect
preference signals in practice: (a) implicitly extracting from annotation
records (for free, but often unavailable), (b) collecting explicitly from crowd
workers (high paid), or (c) pre-trained large language models such as GPT-3
(low paid). Given existing classification NLP benchmarks, we demonstrate that
the proposed auxiliary preference learning via P2C on them is effective in
improving text classifiers. Our codes are publicly available.
</p></li>
</ul>

<h3>Title: covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04926">http://arxiv.org/abs/2306.04926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04926] covLLM: Large Language Models for COVID-19 Biomedical Literature](http://arxiv.org/abs/2306.04926) #large language model</code></li>
<li>Summary: <p>The COVID-19 pandemic led to 1.1 million deaths in the United States, despite
the explosion of coronavirus research. These new findings are slow to translate
to clinical interventions, leading to poorer patient outcomes and unnecessary
deaths. One reason is that clinicians, overwhelmed by patients, struggle to
keep pace with the rate of new coronavirus literature. A potential solution is
developing a tool for evaluating coronavirus literature using large language
models (LLMs) -- neural networks that are deployed for natural language
processing. LLMs can be used to summarize and extract user-specified
information. The greater availability and advancement of LLMs and pre-processed
coronavirus literature databases provide the opportunity to assist clinicians
in evaluating coronavirus literature through a coronavirus literature specific
LLM (covLLM), a tool that directly takes an inputted research article and a
user query to return an answer. Using the COVID-19 Open Research Dataset
(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of
handwritten prompts and synthetic prompts generated using OpenAI, and (2) real
abstracts, which contains abstract and title pairs. covLLM was trained with
LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca
and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real
abstract datasets. These models were evaluated by two human evaluators and
ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract
pairs datasets performs competitively with ChatGPT and outperforms covLLM
trained primarily using the Alpaca dataset.
</p></li>
</ul>

<h3>Title: Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models. (arXiv:2306.04980v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04980">http://arxiv.org/abs/2306.04980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04980] Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models](http://arxiv.org/abs/2306.04980) #large language model</code></li>
<li>Summary: <p>This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.
</p></li>
</ul>

<h3>Title: Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05064">http://arxiv.org/abs/2306.05064</a></li>
<li>Code URL: <a href="https://github.com/davendw49/k2">https://github.com/davendw49/k2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05064] Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization](http://arxiv.org/abs/2306.05064) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs)have achieved great success in general domains of
natural language processing. In this paper, we bring LLMs to the realm of
geoscience, with the objective of advancing research and applications in this
field. To this end, we present the first-ever LLM in geoscience, K2, alongside
a suite of resources developed to further promote LLM research within
geoscience. For instance, we have curated the first geoscience instruction
tuning dataset, GeoSignal, which aims to align LLM responses to
geoscience-related user queries. Additionally, we have established the first
geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of
geoscience. In this work, we experiment with a complete recipe to adapt a
pretrained general-domain LLM to the geoscience domain. Specifically, we
further train the LLaMA-7B model on over 1 million pieces of geoscience
literature and utilize GeoSignal's supervised data to fine-tune the model.
Moreover, we share a protocol that can efficiently gather domain-specific data
and construct domain-supervised data, even in situations where manpower is
scarce. Experiments conducted on the GeoBenchmark demonstrate the the
effectiveness of our approach and datasets.
</p></li>
</ul>

<h3>Title: ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. (arXiv:2306.05301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05301">http://arxiv.org/abs/2306.05301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05301] ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases](http://arxiv.org/abs/2306.05301) #large language model</code></li>
<li>Summary: <p>Enabling large language models to effectively utilize real-world tools is
crucial for achieving embodied intelligence. Existing approaches to tool
learning have primarily relied on either extremely large language models, such
as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or
have utilized supervised learning to train limited types of tools on compact
models. However, it remains uncertain whether smaller language models can
achieve generalized tool-use abilities without specific tool-specific training.
To address this question, this paper introduces ToolAlpaca, a novel framework
designed to automatically generate a tool-use corpus and learn generalized
tool-use abilities on compact language models with minimal human intervention.
Specifically, ToolAlpaca first collects a comprehensive dataset by building a
multi-agent simulation environment, which contains 3938 tool-use instances from
more than 400 real-world tool APIs spanning 50 distinct categories.
Subsequently, the constructed corpus is employed to fine-tune compact language
models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,
respectively. Finally, we evaluate the ability of these models to utilize
previously unseen tools without specific training. Experimental results
demonstrate that ToolAlpaca achieves effective generalized tool-use
capabilities comparable to those of extremely large language models like
GPT-3.5. This validation supports the notion that learning generalized tool-use
abilities is feasible for compact language models.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Language Adaptive Weight Generation for Multi-task Visual Grounding. (arXiv:2306.04652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04652">http://arxiv.org/abs/2306.04652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04652] Language Adaptive Weight Generation for Multi-task Visual Grounding](http://arxiv.org/abs/2306.04652) #segmentation</code></li>
<li>Summary: <p>Although the impressive performance in visual grounding, the prevailing
approaches usually exploit the visual backbone in a passive way, i.e., the
visual backbone extracts features with fixed weights without expression-related
hints. The passive perception may lead to mismatches (e.g., redundant and
missing), limiting further performance improvement. Ideally, the visual
backbone should actively extract visual features since the expressions already
provide the blueprint of desired visual features. The active perception can
take expressions as priors to extract relevant visual features, which can
effectively alleviate the mismatches. Inspired by this, we propose an active
perception Visual Grounding framework based on Language Adaptive Weights,
called VG-LAW. The visual backbone serves as an expression-specific feature
extractor through dynamic weights generated for various expressions. Benefiting
from the specific and relevant visual features extracted from the
language-aware visual backbone, VG-LAW does not require additional modules for
cross-modal interaction. Along with a neat multi-task head, VG-LAW can be
competent in referring expression comprehension and segmentation jointly.
Extensive experiments on four representative datasets, i.e., RefCOCO, RefCOCO+,
RefCOCOg, and ReferItGame, validate the effectiveness of the proposed framework
and demonstrate state-of-the-art performance.
</p></li>
</ul>

<h3>Title: UniBoost: Unsupervised Unimodal Pre-training for Boosting Zero-shot Vision-Language Tasks. (arXiv:2306.04715v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04715">http://arxiv.org/abs/2306.04715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04715] UniBoost: Unsupervised Unimodal Pre-training for Boosting Zero-shot Vision-Language Tasks](http://arxiv.org/abs/2306.04715) #segmentation</code></li>
<li>Summary: <p>Large-scale joint training of multimodal models, e.g., CLIP, have
demonstrated great performance in many vision-language tasks. However,
image-text pairs for pre-training are restricted to the intersection of images
and texts, limiting their ability to cover a large distribution of real-world
data, where noise can also be introduced as misaligned pairs during
pre-processing. Conversely, unimodal models trained on text or image data alone
through unsupervised techniques can achieve broader coverage of diverse
real-world data and are not constrained by the requirement of simultaneous
presence of image and text. In this paper, we demonstrate that using
large-scale unsupervised unimodal models as pre-training can enhance the
zero-shot performance of image-text pair models. Our thorough studies validate
that models pre-trained as such can learn rich representations of both
modalities, improving their ability to understand how images and text relate to
each other. Our experiments show that unimodal pre-training outperforms
state-of-the-art CLIP-based models by 6.5% (52.3% $\rightarrow$ 58.8%) on
PASCAL-5$^i$ and 6.2% (27.2% $\rightarrow$ 33.4%) on COCO-20$^i$ semantic
segmentation under zero-shot setting respectively. By learning representations
of both modalities, unimodal pre-training offers broader coverage, reduced
misalignment errors, and the ability to capture more complex features and
patterns in the real-world data resulting in better performance especially for
zero-shot vision-language tasks.
</p></li>
</ul>

<h3>Title: RefineVIS: Video Instance Segmentation with Temporal Attention Refinement. (arXiv:2306.04774v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04774">http://arxiv.org/abs/2306.04774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04774] RefineVIS: Video Instance Segmentation with Temporal Attention Refinement](http://arxiv.org/abs/2306.04774) #segmentation</code></li>
<li>Summary: <p>We introduce a novel framework called RefineVIS for Video Instance
Segmentation (VIS) that achieves good object association between frames and
accurate segmentation masks by iteratively refining the representations using
sequence context. RefineVIS learns two separate representations on top of an
off-the-shelf frame-level image instance segmentation model: an association
representation responsible for associating objects across frames and a
segmentation representation that produces accurate segmentation masks.
Contrastive learning is utilized to learn temporally stable association
representations. A Temporal Attention Refinement (TAR) module learns
discriminative segmentation representations by exploiting temporal
relationships and a novel temporal contrastive denoising technique. Our method
supports both online and offline inference. It achieves state-of-the-art video
instance segmentation accuracy on YouTube-VIS 2019 (64.4 AP), Youtube-VIS 2021
(61.4 AP), and OVIS (46.1 AP) datasets. The visualization shows that the TAR
module can generate more accurate instance segmentation masks, particularly for
challenging cases such as highly occluded objects.
</p></li>
</ul>

<h3>Title: A Dynamic Feature Interaction Framework for Multi-task Visual Perception. (arXiv:2306.05061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05061">http://arxiv.org/abs/2306.05061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05061] A Dynamic Feature Interaction Framework for Multi-task Visual Perception](http://arxiv.org/abs/2306.05061) #segmentation</code></li>
<li>Summary: <p>Multi-task visual perception has a wide range of applications in scene
understanding such as autonomous driving. In this work, we devise an efficient
unified framework to solve multiple common perception tasks, including instance
segmentation, semantic segmentation, monocular 3D detection, and depth
estimation. Simply sharing the same visual feature representations for these
tasks impairs the performance of tasks, while independent task-specific feature
extractors lead to parameter redundancy and latency. Thus, we design two
feature-merge branches to learn feature basis, which can be useful to, and thus
shared by, multiple perception tasks. Then, each task takes the corresponding
feature basis as the input of the prediction task head to fulfill a specific
task. In particular, one feature merge branch is designed for instance-level
recognition the other for dense predictions. To enhance inter-branch
communication, the instance branch passes pixel-wise spatial information of
each instance to the dense branch using efficient dynamic convolution
weighting. Moreover, a simple but effective dynamic routing mechanism is
proposed to isolate task-specific features and leverage common properties among
tasks. Our proposed framework, termed D2BNet, demonstrates a unique approach to
parameter-efficient predictions for multi-task perception. In addition, as
tasks benefit from co-training with each other, our solution achieves on par
results on partially labeled settings on nuScenes and outperforms previous
works for 3D detection and depth estimation on the Cityscapes dataset with full
supervision.
</p></li>
</ul>

<h3>Title: Unsupervised augmentation optimization for few-shot medical image segmentation. (arXiv:2306.05107v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05107">http://arxiv.org/abs/2306.05107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05107] Unsupervised augmentation optimization for few-shot medical image segmentation](http://arxiv.org/abs/2306.05107) #segmentation</code></li>
<li>Summary: <p>The augmentation parameters matter to few-shot semantic segmentation since
they directly affect the training outcome by feeding the networks with varying
perturbated samples. However, searching optimal augmentation parameters for
few-shot segmentation models without annotations is a challenge that current
methods fail to address. In this paper, we first propose a framework to
determine the ``optimal'' parameters without human annotations by solving a
distribution-matching problem between the intra-instance and intra-class
similarity distribution, with the intra-instance similarity describing the
similarity between the original sample of a particular anatomy and its
augmented ones and the intra-class similarity representing the similarity
between the selected sample and the others in the same class. Extensive
experiments demonstrate the superiority of our optimized augmentation in
boosting few-shot segmentation models. We greatly improve the top competing
method by 1.27\% and 1.11\% on Abd-MRI and Abd-CT datasets, respectively, and
even achieve a significant improvement for SSL-ALP on the left kidney by 3.39\%
on the Abd-CT dataset.
</p></li>
</ul>

<h3>Title: Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation. (arXiv:2306.05246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05246">http://arxiv.org/abs/2306.05246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05246] Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation](http://arxiv.org/abs/2306.05246) #segmentation</code></li>
<li>Summary: <p>With the rapid development of geometric deep learning techniques, many
mesh-based convolutional operators have been proposed to bridge irregular mesh
structures and popular backbone networks. In this paper, we show that while
convolutions are helpful, a simple architecture based exclusively on
multi-layer perceptrons (MLPs) is competent enough to deal with mesh
classification and semantic segmentation. Our new network architecture, named
Mesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) and
dihedral angles as the input, replaces the convolution module of a ResNet with
Multi-layer Perceptron (MLP), and utilizes layer normalization (LN) to perform
the normalization of the layers. The all-MLP architecture operates in an
end-to-end fashion and does not include a pooling module. Extensive
experimental results on the mesh classification/segmentation tasks validate the
effectiveness of the all-MLP architecture.
</p></li>
</ul>

<h3>Title: Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation. (arXiv:2306.05254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05254">http://arxiv.org/abs/2306.05254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05254] Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation](http://arxiv.org/abs/2306.05254) #segmentation</code></li>
<li>Summary: <p>Deep learning-based medical image segmentation models suffer from performance
degradation when deployed to a new healthcare center. To address this issue,
unsupervised domain adaptation and multi-source domain generalization methods
have been proposed, which, however, are less favorable for clinical practice
due to the cost of acquiring target-domain data and the privacy concerns
associated with redistributing the data from multiple source domains. In this
paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle
\textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical
image segmentation. In C$^2$SDG, the shallower features of each image and its
style-augmented counterpart are extracted and used for contrastive training,
resulting in the disentangled style representations and structure
representations. The segmentation is performed based solely on the structure
representations. Our method is novel in the contrastive perspective that
enables channel-wise feature disentanglement using a single source domain. We
evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup
and optic disc segmentation benchmark. Our results suggest the effectiveness of
each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the
baseline and all competing methods with a large margin. The code will be
available at \url{https://github.com/ShishuaiHu/CCSDG}.
</p></li>
</ul>

<h3>Title: Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features. (arXiv:2306.05341v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05341">http://arxiv.org/abs/2306.05341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05341] Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features](http://arxiv.org/abs/2306.05341) #segmentation</code></li>
<li>Summary: <p>This paper introduces a real-time GeoAI workflow for large-scale image
analysis and the segmentation of Arctic permafrost features at a
fine-granularity. Very high-resolution (0.5m) commercial imagery is used in
this analysis. To achieve real-time prediction, our workflow employs a
lightweight, deep learning-based instance segmentation model, SparseInst, which
introduces and uses Instance Activation Maps to accurately locate the position
of objects within the image scene. Experimental results show that the model can
achieve better accuracy of prediction at a much faster inference speed than the
popular Mask-RCNN model.
</p></li>
</ul>

<h3>Title: Automatic Image Blending Algorithm Based on SAM and DINO. (arXiv:2306.05382v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.05382">http://arxiv.org/abs/2306.05382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.05382] Automatic Image Blending Algorithm Based on SAM and DINO](http://arxiv.org/abs/2306.05382) #segmentation</code></li>
<li>Summary: <p>The field of image blending has gained significant popularity in recent years
due to its ability to create visually stunning content. The main objective of
image blending is to merge an object from one image onto another seamlessly,
with minor masking adjustments. With the recent development of SAM, which can
detect and segment targets in images automatically. Our approach (1) combines
semantic object detection and segmentation with corresponding mask generation
to automatically fuse images and (2) introduces the use of PAN for further
quality enhancement during the fusion process. Our approach surpasses many
classical visual fusion models in various performance indicators such as PSNR,
SSIM, and Realism. Notably, our process is highly efficient and speedy, making
it widely applicable in industrial settings. This new process has the potential
to revolutionize visual content creation and improve productivity across
various industries.
</p></li>
</ul>

<h3>Title: Automatic retrieval of corresponding US views in longitudinal examinations. (arXiv:2306.04739v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.04739">http://arxiv.org/abs/2306.04739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.04739] Automatic retrieval of corresponding US views in longitudinal examinations](http://arxiv.org/abs/2306.04739) #segmentation</code></li>
<li>Summary: <p>Skeletal muscle atrophy is a common occurrence in critically ill patients in
the intensive care unit (ICU) who spend long periods in bed. Muscle mass must
be recovered through physiotherapy before patient discharge and ultrasound
imaging is frequently used to assess the recovery process by measuring the
muscle size over time. However, these manual measurements are subject to large
variability, particularly since the scans are typically acquired on different
days and potentially by different operators. In this paper, we propose a
self-supervised contrastive learning approach to automatically retrieve similar
ultrasound muscle views at different scan times. Three different models were
compared using data from 67 patients acquired in the ICU. Results indicate that
our contrastive model outperformed a supervised baseline model in the task of
view retrieval with an AUC of 73.52% and when combined with an automatic
segmentation model achieved 5.7%+/-0.24% error in cross-sectional area.
Furthermore, a user study survey confirmed the efficacy of our model for muscle
view retrieval.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
