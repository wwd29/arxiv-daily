<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09883">http://arxiv.org/abs/2308.09883</a></li>
<li>Code URL: https://github.com/eniac/flamingo</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09883]] Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning(http://arxiv.org/abs/2308.09883)</code></li>
<li>Summary: <p>This paper introduces Flamingo, a system for secure aggregation of data
across a large set of clients. In secure aggregation, a server sums up the
private inputs of clients and obtains the result without learning anything
about the individual inputs beyond what is implied by the final sum. Flamingo
focuses on the multi-round setting found in federated learning in which many
consecutive summations (averages) of model weights are performed to derive a
good model. Previous protocols, such as Bell et al. (CCS '20), have been
designed for a single round and are adapted to the federated learning setting
by repeating the protocol multiple times. Flamingo eliminates the need for the
per-round setup of previous protocols, and has a new lightweight dropout
resilience protocol to ensure that if clients leave in the middle of a sum the
server can still obtain a meaningful result. Furthermore, Flamingo introduces a
new way to locally choose the so-called client neighborhood introduced by Bell
et al. These techniques help Flamingo reduce the number of interactions between
clients and the server, resulting in a significant reduction in the end-to-end
runtime for a full training session over prior work. We implement and evaluate
Flamingo and show that it can securely train a neural network on the (Extended)
MNIST and CIFAR-100 datasets, and the model converges without a loss in
accuracy, compared to a non-private federated learning system.
</p></li>
</ul>

<h3>Title: East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09923">http://arxiv.org/abs/2308.09923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09923]] East: Efficient and Accurate Secure Transformer Framework for Inference(http://arxiv.org/abs/2308.09923)</code></li>
<li>Summary: <p>Transformer has been successfully used in practical applications, such as
ChatGPT, due to its powerful advantages. However, users' input is leaked to the
model provider during the service. With people's attention to privacy,
privacy-preserving Transformer inference is on the demand of such services.
Secure protocols for non-linear functions are crucial in privacy-preserving
Transformer inference, which are not well studied. Thus, designing practical
secure protocols for non-linear functions is hard but significant to model
performance. In this work, we propose a framework \emph{East} to enable
efficient and accurate secure Transformer inference. Firstly, we propose a new
oblivious piecewise polynomial evaluation algorithm and apply it to the
activation functions, which reduces the runtime and communication of GELU by
over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure
protocols for softmax and layer normalization are carefully designed to
faithfully maintain the desired functionality. Thirdly, several optimizations
are conducted in detail to enhance the overall efficiency. We applied
\emph{East} to BERT and the results show that the inference accuracy remains
consistent with the plaintext inference without fine-tuning. Compared to Iron,
we achieve about 1.8$\times$ lower communication within 1.2$\times$ lower
runtime.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Enhancing SCF with Privacy-Preserving and Splitting-Enabled E-Bills on Blockchain. (arXiv:2308.10020v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10020">http://arxiv.org/abs/2308.10020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10020]] Enhancing SCF with Privacy-Preserving and Splitting-Enabled E-Bills on Blockchain(http://arxiv.org/abs/2308.10020)</code></li>
<li>Summary: <p>Electronic Bill (E-Bill) is a rucial negotiable instrument in the form of
data messages, relying on the Electronic Bill System (EB System). Blockchain
technology offers inherent data sharing capabilities, so it is increasingly
being adopted by small and medium-sized enterprises (SMEs) in the supply chain
to build EB systems. However, the blockchain-based E-Bill still face
significant challenges: the E-Bill is difficult to split, like non-fungible
tokens (NFTs), and sensitive information such as amounts always be exposed on
the blockchain. Therefore, to address these issues, we propose a novel data
structure called Reverse-HashTree for Re-storing transactions in blockchain. In
addition, we employ a variant of the Paillier public-key cryptosystem to ensure
transaction validity without decryption, thus preserving privacy. Building upon
these innovations, we designed BillChain, an EB system that enhances supply
chain finance by providing privacy-preserving and splitting-enabled E-Bills on
the blockchain. This work offers a comprehensive and innovative solution to the
challenges faced by E-Bills applied in blockchain in the context of supply
chain finance.
</p></li>
</ul>

<h3>Title: DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.09902v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09902">http://arxiv.org/abs/2308.09902</a></li>
<li>Code URL: https://github.com/CANVOLCANO/DPMAC</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09902]] DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning(http://arxiv.org/abs/2308.09902)</code></li>
<li>Summary: <p>Communication lays the foundation for cooperation in human society and in
multi-agent reinforcement learning (MARL). Humans also desire to maintain their
privacy when communicating with others, yet such privacy concern has not been
considered in existing works in MARL. To this end, we propose the
\textit{differentially private multi-agent communication} (DPMAC) algorithm,
which protects the sensitive information of individual agents by equipping each
agent with a local message sender with rigorous $(\epsilon,
\delta)$-differential privacy (DP) guarantee. In contrast to directly
perturbing the messages with predefined DP noise as commonly done in
privacy-preserving scenarios, we adopt a stochastic message sender for each
agent respectively and incorporate the DP requirement into the sender, which
automatically adjusts the learned message distribution to alleviate the
instability caused by DP noise. Further, we prove the existence of a Nash
equilibrium in cooperative MARL with privacy-preserving communication, which
suggests that this problem is game-theoretically learnable. Extensive
experiments demonstrate a clear advantage of DPMAC over baseline methods in
privacy-preserving scenarios.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models. (arXiv:2308.09899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09899">http://arxiv.org/abs/2308.09899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09899]] Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models(http://arxiv.org/abs/2308.09899)</code></li>
<li>Summary: <p>Accurate drone detection is strongly desired in drone collision avoidance,
drone defense and autonomous Unmanned Aerial Vehicle (UAV) self-landing. With
the recent emergence of the Vision Transformer (ViT), this critical task is
reassessed in this paper using a UAV dataset composed of 1359 drone photos. We
construct various CNN and ViT-based models, demonstrating that for single-drone
detection, a basic ViT can achieve performance 4.6 times more robust than our
best CNN-based transfer learning models. By implementing the state-of-the-art
You Only Look Once (YOLO v7, 200 epochs) and the experimental ViT-based You
Only Look At One Sequence (YOLOS, 20 epochs) in multi-drone detection, we
attain impressive 98% and 96% mAP values, respectively. We find that ViT
outperforms CNN at the same epoch, but also requires more training data,
computational power, and sophisticated, performance-oriented designs to fully
surpass the capabilities of cutting-edge CNN detectors. We summarize the
distinct characteristics of ViT and CNN models to aid future researchers in
developing more efficient deep learning models.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection. (arXiv:2308.10015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10015">http://arxiv.org/abs/2308.10015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10015]] DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection(http://arxiv.org/abs/2308.10015)</code></li>
<li>Summary: <p>Automatic fingerprint recognition systems suffer from the threat of
presentation attacks due to their wide range of applications in areas including
national borders and commercial applications. Presentation attacks can be
performed by fabricating the fake fingerprint of a user with or without the
intention of the subject. This paper presents a dynamic ensemble of deep
learning and handcrafted features to detect presentation attacks in
known-material and unknown-material protocols. The proposed model is a dynamic
ensemble of deep CNN and handcrafted features empowered deep neural networks
both of which learn their parameters together. The proposed presentation attack
detection model, in this way, utilizes the capabilities of both classification
techniques and exhibits better performance than their individual results. The
proposed model's performance is validated using benchmark LivDet 2015, 2017,
and 2019 databases, with an overall accuracy of 96.10\%, 96.49\%, and 95.99\%
attained on them, respectively. The proposed model outperforms state-of-the-art
methods in benchmark protocols of presentation attack detection in terms of
classification accuracy.
</p></li>
</ul>

<h3>Title: Backdoor Mitigation by Correcting the Distribution of Neural Activations. (arXiv:2308.09850v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09850">http://arxiv.org/abs/2308.09850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09850]] Backdoor Mitigation by Correcting the Distribution of Neural Activations(http://arxiv.org/abs/2308.09850)</code></li>
<li>Summary: <p>Backdoor (Trojan) attacks are an important type of adversarial exploit
against deep neural networks (DNNs), wherein a test instance is (mis)classified
to the attacker's target class whenever the attacker's backdoor trigger is
present. In this paper, we reveal and analyze an important property of backdoor
attacks: a successful attack causes an alteration in the distribution of
internal layer activations for backdoor-trigger instances, compared to that for
clean instances. Even more importantly, we find that instances with the
backdoor trigger will be correctly classified to their original source classes
if this distribution alteration is corrected. Based on our observations, we
propose an efficient and effective method that achieves post-training backdoor
mitigation by correcting the distribution alteration using reverse-engineered
triggers. Notably, our method does not change any trainable parameters of the
DNN, but achieves generally better mitigation performance than existing methods
that do require intensive DNN parameter tuning. It also efficiently detects
test instances with the trigger, which may help to catch adversarial entities
in the act of exploiting the backdoor.
</p></li>
</ul>

<h3>Title: A Comparison of Adversarial Learning Techniques for Malware Detection. (arXiv:2308.09958v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09958">http://arxiv.org/abs/2308.09958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09958]] A Comparison of Adversarial Learning Techniques for Malware Detection(http://arxiv.org/abs/2308.09958)</code></li>
<li>Summary: <p>Machine learning has proven to be a useful tool for automated malware
detection, but machine learning models have also been shown to be vulnerable to
adversarial attacks. This article addresses the problem of generating
adversarial malware samples, specifically malicious Windows Portable Executable
files. We summarize and compare work that has focused on adversarial machine
learning for malware detection. We use gradient-based, evolutionary
algorithm-based, and reinforcement-based methods to generate adversarial
samples, and then test the generated samples against selected antivirus
products. We compare the selected methods in terms of accuracy and practical
applicability. The results show that applying optimized modifications to
previously detected malware can lead to incorrect classification of the file as
benign. It is also known that generated malware samples can be successfully
used against detection models other than those used to generate them and that
using combinations of generators can create new samples that evade detection.
Experiments show that the Gym-malware generator, which uses a reinforcement
learning approach, has the greatest practical potential. This generator
achieved an average sample generation time of 5.73 seconds and the highest
average evasion rate of 44.11%. Using the Gym-malware generator in combination
with itself improved the evasion rate to 58.35%.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Noisy-Correspondence Learning for Text-to-Image Person Re-identification. (arXiv:2308.09911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09911">http://arxiv.org/abs/2308.09911</a></li>
<li>Code URL: https://github.com/tencentyouturesearch/personretrieval-ivt</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09911]] Noisy-Correspondence Learning for Text-to-Image Person Re-identification(http://arxiv.org/abs/2308.09911)</code></li>
<li>Summary: <p>Text-to-image person re-identification (TIReID) is a compelling topic in the
cross-modal community, which aims to retrieve the target person based on a
textual query. Although numerous TIReID methods have been proposed and achieved
promising performance, they implicitly assume the training image-text pairs are
correctly aligned, which is not always the case in real-world scenarios. In
practice, the image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to the low quality of
the images and annotation errors. To address this problem, we propose a novel
Robust Dual Embedding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of two main components:
1) A Confident Consensus Division (CCD) module that leverages the dual-grained
decisions of dual embedding modules to obtain a consensus set of clean training
data, which enables the model to learn correct and reliable visual-semantic
associations. 2) A Triplet-Alignment Loss (TAL) relaxes the conventional
triplet-ranking loss with hardest negatives, which tends to rapidly overfit NC,
to a log-exponential upper bound over all negatives, thus preventing the model
from overemphasizing false image-text pairs. We conduct extensive experiments
on three public benchmarks, namely CUHK-PEDES, ICFG-PEDES, and RSTPReID, to
evaluate the performance and robustness of our RDE. Our method achieves
state-of-the-art results both with and without synthetic noisy correspondences
on all three datasets.
</p></li>
</ul>

<h3>Title: On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion. (arXiv:2308.09942v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09942">http://arxiv.org/abs/2308.09942</a></li>
<li>Code URL: https://github.com/yushu-li/owttt</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09942]] On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion(http://arxiv.org/abs/2308.09942)</code></li>
<li>Summary: <p>Generalizing deep learning models to unknown target domain distribution with
low latency has motivated research into test-time training/adaptation
(TTT/TTA). Existing approaches often focus on improving test-time training
performance under well-curated target domain data. As figured out in this work,
many state-of-the-art methods fail to maintain the performance when the target
domain is contaminated with strong out-of-distribution (OOD) data, a.k.a.
open-world test-time training (OWTTT). The failure is mainly due to the
inability to distinguish strong OOD samples from regular weak OOD samples. To
improve the robustness of OWTTT we first develop an adaptive strong OOD pruning
which improves the efficacy of the self-training TTT method. We further propose
a way to dynamically expand the prototypes to represent strong OOD samples for
an improved weak/strong OOD data separation. Finally, we regularize
self-training with distribution alignment and the combination yields the
state-of-the-art performance on 5 OWTTT benchmarks. The code is available at
https://github.com/Yushu-Li/OWTTT.
</p></li>
</ul>

<h3>Title: Scene-Aware Feature Matching. (arXiv:2308.09949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09949">http://arxiv.org/abs/2308.09949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09949]] Scene-Aware Feature Matching(http://arxiv.org/abs/2308.09949)</code></li>
<li>Summary: <p>Current feature matching methods focus on point-level matching, pursuing
better representation learning of individual features, but lacking further
understanding of the scene. This results in significant performance degradation
when handling challenging scenes such as scenes with large viewpoint and
illumination changes. To tackle this problem, we propose a novel model named
SAM, which applies attentional grouping to guide Scene-Aware feature Matching.
SAM handles multi-level features, i.e., image tokens and group tokens, with
attention layers, and groups the image tokens with the proposed token grouping
module. Our model can be trained by ground-truth matches only and produce
reasonable grouping results. With the sense-aware grouping guidance, SAM is not
only more accurate and robust but also more interpretable than conventional
feature matching models. Sufficient experiments on various applications,
including homography estimation, pose estimation, and image matching,
demonstrate that our model achieves state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Prototypical Cross-domain Knowledge Transfer for Cervical Dysplasia Visual Inspection. (arXiv:2308.09983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09983">http://arxiv.org/abs/2308.09983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09983]] Prototypical Cross-domain Knowledge Transfer for Cervical Dysplasia Visual Inspection(http://arxiv.org/abs/2308.09983)</code></li>
<li>Summary: <p>Early detection of dysplasia of the cervix is critical for cervical cancer
treatment. However, automatic cervical dysplasia diagnosis via visual
inspection, which is more appropriate in low-resource settings, remains a
challenging problem. Though promising results have been obtained by recent deep
learning models, their performance is significantly hindered by the limited
scale of the available cervix datasets. Distinct from previous methods that
learn from a single dataset, we propose to leverage cross-domain cervical
images that were collected in different but related clinical studies to improve
the model's performance on the targeted cervix dataset. To robustly learn the
transferable information across datasets, we propose a novel prototype-based
knowledge filtering method to estimate the transferability of cross-domain
samples. We further optimize the shared feature space by aligning the
cross-domain image representations simultaneously on domain level with early
alignment and class level with supervised contrastive learning, which endows
model training and knowledge transfer with stronger robustness. The empirical
results on three real-world benchmark cervical image datasets show that our
proposed method outperforms the state-of-the-art cervical dysplasia visual
inspection by an absolute improvement of 4.7% in top-1 accuracy, 7.0% in
precision, 1.4% in recall, 4.6% in F1 score, and 0.05 in ROC-AUC.
</p></li>
</ul>

<h3>Title: AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization. (arXiv:2308.10001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10001">http://arxiv.org/abs/2308.10001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10001]] AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization(http://arxiv.org/abs/2308.10001)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) have shown promise in generating realistic
novel views from sparse scene images. However, existing NeRF approaches often
encounter challenges due to the lack of explicit 3D supervision and imprecise
camera poses, resulting in suboptimal outcomes. To tackle these issues, we
propose AltNeRF -- a novel framework designed to create resilient NeRF
representations using self-supervised monocular depth estimation (SMDE) from
monocular videos, without relying on known camera poses. SMDE in AltNeRF
masterfully learns depth and pose priors to regulate NeRF training. The depth
prior enriches NeRF's capacity for precise scene geometry depiction, while the
pose prior provides a robust starting point for subsequent pose refinement.
Moreover, we introduce an alternating algorithm that harmoniously melds NeRF
outputs into SMDE through a consistence-driven mechanism, thus enhancing the
integrity of depth priors. This alternation empowers AltNeRF to progressively
refine NeRF representations, yielding the synthesis of realistic novel views.
Additionally, we curate a distinctive dataset comprising indoor videos captured
via mobile devices. Extensive experiments showcase the compelling capabilities
of AltNeRF in generating high-fidelity and robust novel views that closely
resemble reality.
</p></li>
</ul>

<h3>Title: How susceptible are LLMs to Logical Fallacies?. (arXiv:2308.09853v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09853">http://arxiv.org/abs/2308.09853</a></li>
<li>Code URL: https://github.com/Amir-pyh/LOGICOM</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09853]] How susceptible are LLMs to Logical Fallacies?(http://arxiv.org/abs/2308.09853)</code></li>
<li>Summary: <p>This paper investigates the rational thinking capability of Large Language
Models (LLMs) in multi-round argumentative debates by exploring the impact of
fallacious arguments on their logical reasoning performance. More specifically,
we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic
benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM
involves two agents: a persuader and a debater engaging in a multi-round debate
on a controversial topic, where the persuader tries to convince the debater of
the correctness of its claim. First, LOGICOM assesses the potential of LLMs to
change their opinions through reasoning. Then, it evaluates the debater's
performance in logical reasoning by contrasting the scenario where the
persuader employs logical fallacies against one where logical reasoning is
used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4
using a dataset containing controversial topics, claims, and reasons supporting
them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their
opinion through reasoning. However, when presented with logical fallacies,
GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often,
respectively, compared to when logical reasoning is used. Finally, we introduce
a new dataset containing over 5k pairs of logical vs. fallacious arguments. The
source code and dataset of this work are made publicly available.
</p></li>
</ul>

<h3>Title: A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments. (arXiv:2308.09734v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09734">http://arxiv.org/abs/2308.09734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09734]] A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments(http://arxiv.org/abs/2308.09734)</code></li>
<li>Summary: <p>Multi-objective Markov decision processes are a special kind of
multi-objective optimization problem that involves sequential decision making
while satisfying the Markov property of stochastic processes. Multi-objective
reinforcement learning methods address this problem by fusing the reinforcement
learning paradigm with multi-objective optimization techniques. One major
drawback of these methods is the lack of adaptability to non-stationary
dynamics in the environment. This is because they adopt optimization procedures
that assume stationarity to evolve a coverage set of policies that can solve
the problem. This paper introduces a developmental optimization approach that
can evolve the policy coverage set while exploring the preference space over
the defined objectives in an online manner. We propose a novel multi-objective
reinforcement learning algorithm that can robustly evolve a convex coverage set
of policies in an online manner in non-stationary environments. We compare the
proposed algorithm with two state-of-the-art multi-objective reinforcement
learning algorithms in stationary and non-stationary environments. Results
showed that the proposed algorithm significantly outperforms the existing
algorithms in non-stationary environments while achieving comparable results in
stationary environments.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations. (arXiv:2308.09916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09916">http://arxiv.org/abs/2308.09916</a></li>
<li>Code URL: https://github.com/jiehonglin/vi-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09916]] VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations(http://arxiv.org/abs/2308.09916)</code></li>
<li>Summary: <p>Rotation estimation of high precision from an RGB-D object observation is a
huge challenge in 6D object pose estimation, due to the difficulty of learning
in the non-linear space of SO(3). In this paper, we propose a novel rotation
estimation network, termed as VI-Net, to make the task easier by decoupling the
rotation as the combination of a viewpoint rotation and an in-plane rotation.
More specifically, VI-Net bases the feature learning on the sphere with two
individual branches for the estimates of two factorized rotations, where a
V-Branch is employed to learn the viewpoint rotation via binary classification
on the spherical signals, while another I-Branch is used to estimate the
in-plane rotation by transforming the signals to view from the zenith
direction. To process the spherical signals, a Spherical Feature Pyramid
Network is constructed based on a novel design of SPAtial Spherical Convolution
(SPA-SConv), which settles the boundary problem of spherical signals via
feature padding and realizesviewpoint-equivariant feature extraction by
symmetric convolutional operations. We apply the proposed VI-Net to the
challenging task of category-level 6D object pose estimation for predicting the
poses of unknown objects without available CAD models; experiments on the
benchmarking datasets confirm the efficacy of our method, which outperforms the
existing ones with a large margin in the regime of high precision.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09726">http://arxiv.org/abs/2308.09726</a></li>
<li>Code URL: https://github.com/google-research/socialgood</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09726]] Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health(http://arxiv.org/abs/2308.09726)</code></li>
<li>Summary: <p>Restless multi-armed bandits (RMABs) are a popular framework for algorithmic
decision making in sequential settings with limited resources. RMABs are
increasingly being used for sensitive decisions such as in public health,
treatment scheduling, anti-poaching, and -- the motivation for this work --
digital health. For such high stakes settings, decisions must both improve
outcomes and prevent disparities between groups (e.g., ensure health equity).
We study equitable objectives for RMABs (ERMABs) for the first time. We
consider two equity-aligned objectives from the fairness literature, minimax
reward and max Nash welfare. We develop efficient algorithms for solving each
-- a water filling algorithm for the former, and a greedy algorithm with
theoretically motivated nuance to balance disparate group sizes for the latter.
Finally, we demonstrate across three simulation domains, including a new
digital health model, that our approaches can be multiple times more equitable
than the current state of the art without drastic sacrifices to utility. Our
findings underscore our work's urgency as RMABs permeate into systems that
impact human and wildlife outcomes. Code is available at
https://github.com/google-research/socialgood/tree/equitable-rmab
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09970">http://arxiv.org/abs/2308.09970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09970]] Tackling Vision Language Tasks Through Learning Inner Monologues(http://arxiv.org/abs/2308.09970)</code></li>
<li>Summary: <p>Visual language tasks require AI models to comprehend and reason with both
visual and textual content. Driven by the power of Large Language Models
(LLMs), two prominent methods have emerged: (1) the hybrid integration between
LLMs and Vision-Language Models (VLMs), where visual inputs are firstly
converted into language descriptions by VLMs, serving as inputs for LLMs to
generate final answer(s); (2) visual feature alignment in language space, where
visual inputs are encoded as embeddings and projected to LLMs' language space
via further supervised fine-tuning. The first approach provides light training
costs and interpretability but is hard to be optimized in an end-to-end
fashion. The second approach presents decent performance, but feature alignment
usually requires large amounts of training data and lacks interpretability. To
tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal
Optimization (IMMO), to solve complex vision language problems by simulating
inner monologue processes, a cognitive process in which an individual engages
in silent verbal communication with themselves. We enable LLMs and VLMs to
interact through natural language conversation and propose to use a two-stage
training process to learn how to do the inner monologue (self-asking questions
and answering questions). IMMO is evaluated on two popular tasks and the
results suggest by emulating the cognitive phenomenon of internal dialogue, our
approach can enhance reasoning and explanation abilities, contributing to the
more effective fusion of vision and language models. More importantly, instead
of using predefined human-crafted monologues, IMMO learns this process within
the deep learning models, promising wider applicability to many different AI
problems beyond vision language tasks.
</p></li>
</ul>

<h3>Title: Causal Interpretable Progression Trajectory Analysis of Chronic Disease. (arXiv:2308.09735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09735">http://arxiv.org/abs/2308.09735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09735]] Causal Interpretable Progression Trajectory Analysis of Chronic Disease(http://arxiv.org/abs/2308.09735)</code></li>
<li>Summary: <p>Chronic disease is the leading cause of death, emphasizing the need for
accurate prediction of disease progression trajectories and informed clinical
decision-making. Machine learning (ML) models have shown promise in this domain
by capturing non-linear patterns within patient features. However, existing
ML-based models lack the ability to provide causal interpretable predictions
and estimate treatment effects, limiting their decision-assisting perspective.
In this study, we propose a novel model called causal trajectory prediction
(CTP) to tackle the limitation. The CTP model combines trajectory prediction
and causal discovery to enable accurate prediction of disease progression
trajectories and uncovering causal relationships between features. By
incorporating a causal graph into the prediction process, CTP ensures that
ancestor features are not influenced by treatment on descendant features,
thereby enhancing the interpretability of the model. By estimating the bounds
of treatment effects, even in the presence of unmeasured confounders, the CTP
provides valuable insights for clinical decision-making. We evaluate the
performance of the CTP using simulated and real medical datasets. Experimental
results demonstrate that our model achieves satisfactory performance,
highlighting its potential to assist clinical decisions.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks. (arXiv:2308.09829v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09829">http://arxiv.org/abs/2308.09829</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09829]] Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks(http://arxiv.org/abs/2308.09829)</code></li>
<li>Summary: <p>We propose a learning algorithm for local routing policies that needs only a
few data samples obtained from a single graph while generalizing to all random
graphs in a standard model of wireless networks. We thus solve the all-pairs
near-shortest path problem by training deep neural networks (DNNs) that
efficiently and scalably learn routing policies that are local, i.e., they only
consider node states and the states of neighboring nodes. Remarkably, one of
these DNNs we train learns a policy that exactly matches the performance of
greedy forwarding; another generally outperforms greedy forwarding. Our
algorithm design exploits network domain knowledge in several ways: First, in
the selection of input features and, second, in the selection of a ``seed
graph'' and subsamples from its shortest paths. The leverage of domain
knowledge provides theoretical explainability of why the seed graph and node
subsampling suffice for learning that is efficient, scalable, and
generalizable. Simulation-based results on uniform random graphs with diverse
sizes and densities empirically corroborate that using samples generated from a
few routing paths in a modest-sized seed graph quickly learns a model that is
generalizable across (almost) all random graphs in the wireless network model.
</p></li>
</ul>

<h3>Title: To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks. (arXiv:2308.09955v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09955">http://arxiv.org/abs/2308.09955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09955]] To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks(http://arxiv.org/abs/2308.09955)</code></li>
<li>Summary: <p>Reducing the size of a neural network (pruning) by removing weights without
impacting its performance is an important problem for resource-constrained
devices. In the past, pruning was typically accomplished by ranking or
penalizing weights based on criteria like magnitude and removing low-ranked
weights before retraining the remaining ones. Pruning strategies may also
involve removing neurons from the network in order to achieve the desired
reduction in network size. We formulate pruning as an optimization problem with
the objective of minimizing misclassifications by selecting specific weights.
To accomplish this, we have introduced the concept of chaos in learning
(Lyapunov exponents) via weight updates and exploiting causality to identify
the causal weights responsible for misclassification. Such a pruned network
maintains the original performance and retains feature explainability.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization. (arXiv:2308.09889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09889]] DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization(http://arxiv.org/abs/2308.09889)</code></li>
<li>Summary: <p>Stable Diffusion (SD) customization approaches enable users to personalize SD
model outputs, greatly enhancing the flexibility and diversity of AI art.
However, they also allow individuals to plagiarize specific styles or subjects
from copyrighted images, which raises significant concerns about potential
copyright infringement. To address this issue, we propose an invisible
data-free universal adversarial watermark (DUAW), aiming to protect a myriad of
copyrighted images from different customization approaches across various
versions of SD models. First, DUAW is designed to disrupt the variational
autoencoder during SD customization. Second, DUAW operates in a data-free
context, where it is trained on synthetic images produced by a Large Language
Model (LLM) and a pretrained SD model. This approach circumvents the necessity
of directly handling copyrighted images, thereby preserving their
confidentiality. Once crafted, DUAW can be imperceptibly integrated into
massive copyrighted images, serving as a protective measure by inducing
significant distortions in the images generated by customized SD models.
Experimental results demonstrate that DUAW can effectively distort the outputs
of fine-tuned SD models, rendering them discernible to both human observers and
a simple classifier.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: DiffusionTrack: Diffusion Model For Multi-Object Tracking. (arXiv:2308.09905v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09905">http://arxiv.org/abs/2308.09905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09905]] DiffusionTrack: Diffusion Model For Multi-Object Tracking(http://arxiv.org/abs/2308.09905)</code></li>
<li>Summary: <p>Multi-object tracking (MOT) is a challenging vision task that aims to detect
individual objects within a single frame and associate them across multiple
frames. Recent MOT approaches can be categorized into two-stage
tracking-by-detection (TBD) methods and one-stage joint detection and tracking
(JDT) methods. Despite the success of these approaches, they also suffer from
common problems, such as harmful global or local inconsistency, poor trade-off
between robustness and model complexity, and lack of flexibility in different
scenes within the same video. In this paper we propose a simple but robust
framework that formulates object detection and association jointly as a
consistent denoising diffusion process from paired noise boxes to paired
ground-truth boxes. This novel progressive denoising diffusion strategy
substantially augments the tracker's effectiveness, enabling it to discriminate
between various objects. During the training stage, paired object boxes diffuse
from paired ground-truth boxes to random distribution, and the model learns
detection and tracking simultaneously by reversing this noising process. In
inference, the model refines a set of paired randomly generated boxes to the
detection and tracking results in a flexible one-step or multi-step denoising
diffusion process. Extensive experiments on three widely used MOT benchmarks,
including MOT17, MOT20, and Dancetrack, demonstrate that our approach achieves
competitive performance compared to the current state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Physics-Guided Human Motion Capture with Pose Probability Modeling. (arXiv:2308.09910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09910">http://arxiv.org/abs/2308.09910</a></li>
<li>Code URL: https://github.com/me-ditto/physics-guided-mocap</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09910]] Physics-Guided Human Motion Capture with Pose Probability Modeling(http://arxiv.org/abs/2308.09910)</code></li>
<li>Summary: <p>Incorporating physics in human motion capture to avoid artifacts like
floating, foot sliding, and ground penetration is a promising direction.
Existing solutions always adopt kinematic results as reference motions, and the
physics is treated as a post-processing module. However, due to the depth
ambiguity, monocular motion capture inevitably suffers from noises, and the
noisy reference often leads to failure for physics-based tracking. To address
the obstacles, our key-idea is to employ physics as denoising guidance in the
reverse diffusion process to reconstruct physically plausible human motion from
a modeled pose probability distribution. Specifically, we first train a latent
gaussian model that encodes the uncertainty of 2D-to-3D lifting to facilitate
reverse diffusion. Then, a physics module is constructed to track the motion
sampled from the distribution. The discrepancies between the tracked motion and
image observation are used to provide explicit guidance for the reverse
diffusion model to refine the motion. With several iterations, the
physics-based tracking and kinematic denoising promote each other to generate a
physically plausible human motion. Experimental results show that our method
outperforms previous physics-based methods in both joint accuracy and success
rate. More information can be found at
\url{https://github.com/Me-Ditto/Physics-Guided-Mocap}.
</p></li>
</ul>

<h3>Title: AltDiffusion: A Multilingual Text-to-Image Diffusion Model. (arXiv:2308.09991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09991">http://arxiv.org/abs/2308.09991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09991]] AltDiffusion: A Multilingual Text-to-Image Diffusion Model(http://arxiv.org/abs/2308.09991)</code></li>
<li>Summary: <p>Large Text-to-Image(T2I) diffusion models have shown a remarkable capability
to produce photorealistic and diverse images based on text inputs. However,
existing works only support limited language input, e.g., English, Chinese, and
Japanese, leaving users beyond these languages underserved and blocking the
global expansion of T2I models. Therefore, this paper presents AltDiffusion, a
novel multilingual T2I diffusion model that supports eighteen different
languages. Specifically, we first train a multilingual text encoder based on
the knowledge distillation. Then we plug it into a pretrained English-only
diffusion model and train the model with a two-stage schema to enhance the
multilingual capability, including concept alignment and quality improvement
stage on a large-scale multilingual dataset. Furthermore, we introduce a new
benchmark, which includes Multilingual-General-18(MG-18) and
Multilingual-Cultural-18(MC-18) datasets, to evaluate the capabilities of T2I
diffusion models for generating high-quality images and capturing
culture-specific concepts in different languages. Experimental results on both
MG-18 and MC-18 demonstrate that AltDiffusion outperforms current
state-of-the-art T2I models, e.g., Stable Diffusion in multilingual
understanding, especially with respect to culture-specific concepts, while
still having comparable capability for generating high-quality images.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09891">http://arxiv.org/abs/2308.09891</a></li>
<li>Code URL: https://github.com/SongTang-x/SwinLSTM</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09891]] SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM(http://arxiv.org/abs/2308.09891)</code></li>
<li>Summary: <p>Integrating CNNs and RNNs to capture spatiotemporal dependencies is a
prevalent strategy for spatiotemporal prediction tasks. However, the property
of CNNs to learn local spatial information decreases their efficiency in
capturing spatiotemporal dependencies, thereby limiting their prediction
accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which
integrates Swin Transformer blocks and the simplified LSTM, an extension that
replaces the convolutional structure in ConvLSTM with the self-attention
mechanism. Furthermore, we construct a network with SwinLSTM cell as the core
for spatiotemporal prediction. Without using unique tricks, SwinLSTM
outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and
KTH datasets. In particular, it exhibits a significant improvement in
prediction accuracy compared to ConvLSTM. Our competitive experimental results
demonstrate that learning global spatial dependencies is more advantageous for
models to capture spatiotemporal dependencies. We hope that SwinLSTM can serve
as a solid baseline to promote the advancement of spatiotemporal prediction
accuracy. The codes are publicly available at
https://github.com/SongTang-x/SwinLSTM.
</p></li>
</ul>

<h3>Title: UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning. (arXiv:2308.09953v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09953">http://arxiv.org/abs/2308.09953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09953]] UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning(http://arxiv.org/abs/2308.09953)</code></li>
<li>Summary: <p>Animal visual perception is an important technique for automatically
monitoring animal health, understanding animal behaviors, and assisting
animal-related research. However, it is challenging to design a deep
learning-based perception model that can freely adapt to different animals
across various perception tasks, due to the varying poses of a large diversity
of animals, lacking data on rare species, and the semantic inconsistency of
different tasks. We introduce UniAP, a novel Universal Animal Perception model
that leverages few-shot learning to enable cross-species perception among
various visual tasks. Our proposed model takes support images and labels as
prompt guidance for a query image. Images and labels are processed through a
Transformer-based encoder and a lightweight label encoder, respectively. Then a
matching module is designed for aggregating information between prompt guidance
and the query image, followed by a multi-head label decoder to generate outputs
for various tasks. By capitalizing on the shared visual characteristics among
different animals and tasks, UniAP enables the transfer of knowledge from
well-studied species to those with limited labeled data or even unseen species.
We demonstrate the effectiveness of UniAP through comprehensive experiments in
pose estimation, segmentation, and classification tasks on diverse animal
species, showcasing its ability to generalize and adapt to new classes with
minimal labeled examples.
</p></li>
</ul>

<h3>Title: A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09722]] A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data(http://arxiv.org/abs/2308.09722)</code></li>
<li>Summary: <p>Social media cyberbullying has a detrimental effect on human life. As online
social networking grows daily, the amount of hate speech also increases. Such
terrible content can cause depression and actions related to suicide. This
paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection
on social media using synthetic data. We have demonstrated a cutting-edge
method to address data availability difficulties by producing
machine-translated data. However, several languages such as Hindi and Bangla
still lack adequate investigations due to a lack of datasets. We carried out
experimental identification of aggressive comments on Hindi, Bangla, and
English datasets using the proposed model and traditional models, including
Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),
LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from
Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.
We employed evaluation metrics such as f1-score, accuracy, precision, and
recall to assess the models performance. Our proposed model outperformed all
the models on all datasets, achieving the highest accuracy of 95%. Our model
achieves state-of-the-art results among all the previous works on the dataset
we used in this paper.
</p></li>
</ul>

<h3>Title: Skill Transformer: A Monolithic Policy for Mobile Manipulation. (arXiv:2308.09873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09873">http://arxiv.org/abs/2308.09873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09873]] Skill Transformer: A Monolithic Policy for Mobile Manipulation(http://arxiv.org/abs/2308.09873)</code></li>
<li>Summary: <p>We present Skill Transformer, an approach for solving long-horizon robotic
tasks by combining conditional sequence modeling and skill modularity.
Conditioned on egocentric and proprioceptive observations of a robot, Skill
Transformer is trained end-to-end to predict both a high-level skill (e.g.,
navigation, picking, placing), and a whole-body low-level action (e.g., base
and arm motion), using a transformer architecture and demonstration
trajectories that solve the full task. It retains the composability and
modularity of the overall task through a skill predictor module while reasoning
about low-level actions and avoiding hand-off errors, common in modular
approaches. We test Skill Transformer on an embodied rearrangement benchmark
and find it performs robust task planning and low-level control in new
scenarios, achieving a 2.5x higher success rate than baselines in hard
rearrangement problems.
</p></li>
</ul>

<h3>Title: A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case. (arXiv:2308.09884v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09884">http://arxiv.org/abs/2308.09884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09884]] A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case(http://arxiv.org/abs/2308.09884)</code></li>
<li>Summary: <p>In recent times, Large Language Models (LLMs) have captured a global
spotlight and revolutionized the field of Natural Language Processing. One of
the factors attributed to the effectiveness of LLMs is the model architecture
used for training, transformers. Transformer models excel at capturing
contextual features in sequential data since time series data are sequential,
transformer models can be leveraged for more efficient time series data
prediction. The field of prognostics is vital to system health management and
proper maintenance planning. A reliable estimation of the remaining useful life
(RUL) of machines holds the potential for substantial cost savings. This
includes avoiding abrupt machine failures, maximizing equipment usage, and
serving as a decision support system (DSS). This work proposed an
encoder-transformer architecture-based framework for multivariate time series
prediction for a prognostics use case. We validated the effectiveness of the
proposed framework on all four sets of the C-MAPPS benchmark dataset for the
remaining useful life prediction task. To effectively transfer the knowledge
and application of transformers from the natural language domain to time
series, three model-specific experiments were conducted. Also, to enable the
model awareness of the initial stages of the machine life and its degradation
path, a novel expanding window method was proposed for the first time in this
work, it was compared with the sliding window method, and it led to a large
improvement in the performance of the encoder transformer model. Finally, the
performance of the proposed encoder-transformer model was evaluated on the test
dataset and compared with the results from 13 other state-of-the-art (SOTA)
models in the literature and it outperformed them all with an average
performance increase of 137.65% over the next best model across all the
datasets.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning. (arXiv:2308.09915v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09915]] EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning(http://arxiv.org/abs/2308.09915)</code></li>
<li>Summary: <p>Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be
collected for training a prediction model. Accordingly, generative models
(e.g., generative adversarial network (GAN)) are typically used to synthesize
the visual samples conditioned by the class semantic vectors and achieve
remarkable progress for ZSL. However, existing GAN-based generative ZSL methods
are based on hand-crafted models, which cannot adapt to various
datasets/scenarios and fails to model instability. To alleviate these
challenges, we propose evolutionary generative adversarial network search
(termed EGANS) to automatically design the generative network with good
adaptation and stability, enabling reliable visual feature sample synthesis for
advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a
neural architecture search for both generator and discriminator under a unified
evolutionary adversarial framework. EGANS is learned by two stages: evolution
generator architecture search and evolution discriminator architecture search.
During the evolution generator architecture search, we adopt a many-to-one
adversarial training strategy to evolutionarily search for the optimal
generator. Then the optimal generator is further applied to search for the
optimal discriminator in the evolution discriminator architecture search with a
similar evolution search algorithm. Once the optimal generator and
discriminator are searched, we entail them into various generative ZSL
baselines for ZSL classification. Extensive experiments show that EGANS
consistently improve existing generative ZSL methods on the standard CUB, SUN,
AWA2 and FLO datasets. The significant performance gains indicate that the
evolutionary neural architecture search explores a virgin field in ZSL.
</p></li>
</ul>

<h3>Title: FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09723">http://arxiv.org/abs/2308.09723</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09723]] FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs(http://arxiv.org/abs/2308.09723)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have achieved state-of-the-art performance
across various language tasks but pose challenges for practical deployment due
to their substantial memory requirements. Furthermore, the latest generative
models suffer from high inference costs caused by the memory bandwidth
bottleneck in the auto-regressive decoding process. To address these issues, we
propose an efficient weight-only quantization method that reduces memory
consumption and accelerates inference for LLMs. To ensure minimal quality
degradation, we introduce a simple and effective heuristic approach that
utilizes only the model weights of a pre-trained model. This approach is
applicable to both Mixture-of-Experts (MoE) and dense models without requiring
additional fine-tuning. To demonstrate the effectiveness of our proposed
method, we first analyze the challenges and issues associated with LLM
quantization. Subsequently, we present our heuristic approach, which adaptively
finds the granularity of quantization, effectively addressing these problems.
Furthermore, we implement highly efficient GPU GEMMs that perform on-the-fly
matrix multiplication and dequantization, supporting the multiplication of fp16
or bf16 activations with int8 or int4 weights. We evaluate our approach on
large-scale open source models such as OPT-175B and internal MoE models,
showcasing minimal accuracy loss while achieving up to 3.65 times higher
throughput on the same number of GPUs.
</p></li>
</ul>

<h3>Title: Generative Adversarial Networks Unlearning. (arXiv:2308.09881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09881]] Generative Adversarial Networks Unlearning(http://arxiv.org/abs/2308.09881)</code></li>
<li>Summary: <p>As machine learning continues to develop, and data misuse scandals become
more prevalent, individuals are becoming increasingly concerned about their
personal information and are advocating for the right to remove their data.
Machine unlearning has emerged as a solution to erase training data from
trained machine learning models. Despite its success in classifiers, research
on Generative Adversarial Networks (GANs) is limited due to their unique
architecture, including a generator and a discriminator. One challenge pertains
to generator unlearning, as the process could potentially disrupt the
continuity and completeness of the latent space. This disruption might
consequently diminish the model's effectiveness after unlearning. Another
challenge is how to define a criterion that the discriminator should perform
for the unlearning images. In this paper, we introduce a substitution mechanism
and define a fake label to effectively mitigate these challenges. Based on the
substitution mechanism and fake label, we propose a cascaded unlearning
approach for both item and class unlearning within GAN models, in which the
unlearning and learning processes run in a cascaded manner. We conducted a
comprehensive evaluation of the cascaded unlearning technique using the MNIST
and CIFAR-10 datasets. Experimental results demonstrate that this approach
achieves significantly improved item and class unlearning efficiency, reducing
the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets,
respectively, in comparison to retraining from scratch. Notably, although the
model's performance experiences minor degradation after unlearning, this
reduction is negligible when dealing with a minimal number of images (e.g., 64)
and has no adverse effects on downstream tasks such as classification.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09936">http://arxiv.org/abs/2308.09936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09936]] BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions(http://arxiv.org/abs/2308.09936)</code></li>
<li>Summary: <p>Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA
benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual
Spatial Reasoning benchmark), comparing to our baseline InstructBLIP. BLIVA
demonstrates significant capability in decoding real-world images, irrespective
of text presence. To demonstrate the broad industry applications enabled by
BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails
paired with question-answer sets across 13 diverse categories. For researchers
interested in further exploration, our code and models are freely accessible at
https://github.com/mlpc-ucsd/BLIVA.git
</p></li>
</ul>

<h3>Title: YORC: Yoruba Reading Comprehension dataset. (arXiv:2308.09768v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09768">http://arxiv.org/abs/2308.09768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09768]] YORC: Yoruba Reading Comprehension dataset(http://arxiv.org/abs/2308.09768)</code></li>
<li>Summary: <p>In this paper, we create YORC: a new multi-choice Yoruba Reading
Comprehension dataset that is based on Yoruba high-school reading comprehension
examination. We provide baseline results by performing cross-lingual transfer
using existing English RACE dataset based on a pre-trained encoder-only model.
Additionally, we provide results by prompting large language models (LLMs) like
GPT-4.
</p></li>
</ul>

<h3>Title: Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>Code URL: https://github.com/fuyu-quant/iblm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09890]] Inductive-bias Learning: Generating Code Models with Large Language Model(http://arxiv.org/abs/2308.09890)</code></li>
<li>Summary: <p>Large Language Models(LLMs) have been attracting attention due to a ability
called in-context learning(ICL). ICL, without updating the parameters of a LLM,
it is possible to achieve highly accurate inference based on rules ``in the
context'' by merely inputting a training data into the prompt. Although ICL is
a developing field with many unanswered questions, LLMs themselves serves as a
inference model, seemingly realizing inference without explicitly indicate
``inductive bias''. On the other hand, a code generation is also a highlighted
application of LLMs. The accuracy of code generation has dramatically improved,
enabling even non-engineers to generate code to perform the desired tasks by
crafting appropriate prompts. In this paper, we propose a novel ``learning''
method called an ``Inductive-Bias Learning (IBL)'', which combines the
techniques of ICL and code generation. An idea of IBL is straightforward. Like
ICL, IBL inputs a training data into the prompt and outputs a code with a
necessary structure for inference (we referred to as ``Code Model'') from a
``contextual understanding''. Despite being a seemingly simple approach, IBL
encompasses both a ``property of inference without explicit inductive bias''
inherent in ICL and a ``readability and explainability'' of the code
generation. Surprisingly, generated Code Models have been found to achieve
predictive accuracy comparable to, and in some cases surpassing, ICL and
representative machine learning models. Our IBL code is open source:
https://github.com/fuyu-quant/IBLM
</p></li>
</ul>

<h3>Title: Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09954">http://arxiv.org/abs/2308.09954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09954]] Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs(http://arxiv.org/abs/2308.09954)</code></li>
<li>Summary: <p>Large language models (LLMs) possess a wealth of knowledge encoded in their
parameters. However, this knowledge may become outdated or unsuitable over
time. As a result, there has been a growing interest in knowledge editing for
LLMs and evaluating its effectiveness. Existing studies primarily focus on
knowledge editing using factual triplets, which not only incur high costs for
collection but also struggle to express complex facts. Furthermore, these
studies are often limited in their evaluation perspectives. In this paper, we
propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs.
This benchmark includes an evaluation framework and a corresponding dataset.
Under our framework, we first ask the LLM to perform knowledge editing using
raw documents, which provides a more convenient and universal approach compared
to using factual triplets. We then evaluate the updated LLM from multiple
perspectives. In addition to assessing the effectiveness of knowledge editing
and the retention of unrelated knowledge from conventional studies, we further
test the LLM's ability in two aspects: 1) Reasoning with the altered knowledge,
aiming for the LLM to genuinely learn the altered knowledge instead of simply
memorizing it. 2) Cross-lingual knowledge transfer, where the LLM updated with
raw documents in one language should be capable of handling queries from
another language. To facilitate further research, we construct and release the
corresponding dataset. Using this benchmark, we investigate the effectiveness
of several commonly-used knowledge editing methods. Experimental results
indicate that the current methods for knowledge editing using raw documents are
not effective in yielding satisfactory results, particularly when it comes to
reasoning with altered knowledge and cross-lingual knowledge transfer.
</p></li>
</ul>

<h3>Title: FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models. (arXiv:2308.09975v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09975">http://arxiv.org/abs/2308.09975</a></li>
<li>Code URL: https://github.com/sufe-aiflm-lab/fineval</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09975]] FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models(http://arxiv.org/abs/2308.09975)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated exceptional performance in
various natural language processing tasks, yet their efficacy in more
challenging and domain-specific tasks remains largely unexplored. This paper
presents FinEval, a benchmark specifically designed for the financial domain
knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice
questions covering Finance, Economy, Accounting, and Certificate. It includes
4,661 questions spanning 34 different academic subjects. To ensure a
comprehensive model performance evaluation, FinEval employs a range of prompt
types, including zero-shot and few-shot prompts, as well as answer-only and
chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs
on FinEval, the results show that only GPT-4 achieved an accuracy close to 70%
in different prompt settings, indicating significant growth potential for LLMs
in the financial domain knowledge. Our work offers a more comprehensive
financial knowledge evaluation benchmark, utilizing data of mock exams and
covering a wide range of evaluated LLMs.
</p></li>
</ul>

<h3>Title: GameEval: Evaluating LLMs on Conversational Games. (arXiv:2308.10032v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10032">http://arxiv.org/abs/2308.10032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10032]] GameEval: Evaluating LLMs on Conversational Games(http://arxiv.org/abs/2308.10032)</code></li>
<li>Summary: <p>The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09764">http://arxiv.org/abs/2308.09764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09764]] The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation(http://arxiv.org/abs/2308.09764)</code></li>
<li>Summary: <p>Fashion understanding is a hot topic in computer vision, with many
applications having great business value in the market. Fashion understanding
remains a difficult challenge for computer vision due to the immense diversity
of garments and various scenes and backgrounds. In this work, we try removing
the background from fashion images to boost data quality and increase model
performance. Having fashion images of evident persons in fully visible
garments, we can utilize Salient Object Detection to achieve the background
removal of fashion data to our expectations. A fashion image with the
background removed is claimed as the "rembg" image, contrasting with the
original one in the fashion dataset. We conducted extensive comparative
experiments with these two types of images on multiple aspects of model
training, including model architectures, model initialization, compatibility
with other training tricks and data augmentations, and target task types. Our
experiments show that background removal can effectively work for fashion data
in simple and shallow networks that are not susceptible to overfitting. It can
improve model accuracy by up to 5% in the classification on the FashionStyle14
dataset when training models from scratch. However, background removal does not
perform well in deep neural networks due to incompatibility with other
regularization techniques like batch normalization, pre-trained initialization,
and data augmentations introducing randomness. The loss of background pixels
invalidates many existing training tricks in the model training, adding the
risk of overfitting for deep models.
</p></li>
</ul>

<h3>Title: EAVL: Explicitly Align Vision and Language for Referring Image Segmentation. (arXiv:2308.09779v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09779">http://arxiv.org/abs/2308.09779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09779]] EAVL: Explicitly Align Vision and Language for Referring Image Segmentation(http://arxiv.org/abs/2308.09779)</code></li>
<li>Summary: <p>Referring image segmentation aims to segment an object mentioned in natural
language from an image. A main challenge is language-related localization,
which means locating the object with the relevant language. Previous approaches
mainly focus on the fusion of vision and language features without fully
addressing language-related localization. In previous approaches, fused
vision-language features are directly fed into a decoder and pass through a
convolution with a fixed kernel to obtain the result, which follows a similar
pattern as traditional image segmentation. This approach does not explicitly
align language and vision features in the segmentation stage, resulting in a
suboptimal language-related localization. Different from previous methods, we
propose Explicitly Align the Vision and Language for Referring Image
Segmentation (EAVL). Instead of using a fixed convolution kernel, we propose an
Aligner which explicitly aligns the vision and language features in the
segmentation stage. Specifically, a series of unfixed convolution kernels are
generated based on the input l, and then are use to explicitly align the vision
and language features. To achieve this, We generate multiple queries that
represent different emphases of the language expression. These queries are
transformed into a series of query-based convolution kernels. Then, we utilize
these kernels to do convolutions in the segmentation stage and obtain a series
of segmentation masks. The final result is obtained through the aggregation of
all masks. Our method can not only fuse vision and language features
effectively but also exploit their potential in the segmentation stage. And
most importantly, we explicitly align language features of different emphases
with the image features to achieve language-related localization. Our method
surpasses previous state-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by
large margins.
</p></li>
</ul>

<h3>Title: Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09835]] Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis(http://arxiv.org/abs/2308.09835)</code></li>
<li>Summary: <p>Current deep learning-based approaches for the segmentation of microscopy
images heavily rely on large amount of training data with dense annotation,
which is highly costly and laborious in practice. Compared to full annotation
where the complete contour of objects is depicted, point annotations,
specifically object centroids, are much easier to acquire and still provide
crucial information about the objects for subsequent segmentation. In this
paper, we assume access to point annotations only during training and develop a
unified pipeline for microscopy image segmentation using synthetically
generated training data. Our framework includes three stages: (1) it takes
point annotations and samples a pseudo dense segmentation mask constrained with
shape priors; (2) with an image generative model trained in an unpaired manner,
it translates the mask to a realistic microscopy image regularized by object
level consistency; (3) the pseudo masks along with the synthetic images then
constitute a pairwise dataset for training an ad-hoc segmentation model. On the
public MoNuSeg dataset, our synthesis pipeline produces more diverse and
realistic images than baseline models while maintaining high coherence between
input masks and generated images. When using the identical segmentation
backbones, the models trained on our synthetic dataset significantly outperform
those trained with pseudo-labels or baseline-generated images. Moreover, our
framework achieves comparable results to models trained on authentic microscopy
images with dense labels, demonstrating its potential as a reliable and highly
efficient alternative to labor-intensive manual pixel-wise annotations in
microscopy image segmentation. The code is available.
</p></li>
</ul>

<h3>Title: Scalable Video Object Segmentation with Simplified Framework. (arXiv:2308.09903v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09903">http://arxiv.org/abs/2308.09903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09903]] Scalable Video Object Segmentation with Simplified Framework(http://arxiv.org/abs/2308.09903)</code></li>
<li>Summary: <p>The current popular methods for video object segmentation (VOS) implement
feature matching through several hand-crafted modules that separately perform
feature extraction and matching. However, the above hand-crafted designs
empirically cause insufficient target interaction, thus limiting the dynamic
target-aware feature learning in VOS. To tackle these limitations, this paper
presents a scalable Simplified VOS (SimVOS) framework to perform joint feature
extraction and matching by leveraging a single transformer backbone.
Specifically, SimVOS employs a scalable ViT backbone for simultaneous feature
extraction and matching between query and reference features. This design
enables SimVOS to learn better target-ware features for accurate mask
prediction. More importantly, SimVOS could directly apply well-pretrained ViT
backbones (e.g., MAE) for VOS, which bridges the gap between VOS and
large-scale self-supervised pre-training. To achieve a better performance-speed
trade-off, we further explore within-frame attention and propose a new token
refinement module to improve the running speed and save computational cost.
Experimentally, our SimVOS achieves state-of-the-art results on popular video
object segmentation benchmarks, i.e., DAVIS-2017 (88.0% J&amp;F), DAVIS-2016 (92.9%
J&amp;F) and YouTube-VOS 2019 (84.2% J&amp;F), without applying any synthetic video or
BL30K pre-training used in previous VOS approaches.
</p></li>
</ul>

<h3>Title: Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09917">http://arxiv.org/abs/2308.09917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09917]] Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation(http://arxiv.org/abs/2308.09917)</code></li>
<li>Summary: <p>Instance segmentation in electron microscopy (EM) volumes poses a significant
challenge due to the complex morphology of instances and insufficient
annotations. Self-supervised learning has recently emerged as a promising
solution, enabling the acquisition of prior knowledge of cellular tissue
structures that are essential for EM instance segmentation. However, existing
pretraining methods often lack the ability to capture complex visual patterns
and relationships between voxels, which results in the acquired prior knowledge
being insufficient for downstream EM analysis tasks. In this paper, we propose
a novel pretraining framework that leverages multiscale visual representations
to capture both voxel-level and feature-level consistency in EM volumes.
Specifically, our framework enforces voxel-level consistency between the
outputs of a Siamese network by a reconstruction function, and incorporates a
cross-attention mechanism for soft feature matching to achieve fine-grained
feature-level consistency. Moreover, we propose a contrastive learning scheme
on the feature pyramid to extract discriminative features across multiple
scales. We extensively pretrain our method on four large-scale EM datasets,
achieving promising performance improvements in representative tasks of neuron
and mitochondria instance segmentation.
</p></li>
</ul>

<h3>Title: Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos. (arXiv:2308.09951v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09951">http://arxiv.org/abs/2308.09951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09951]] Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos(http://arxiv.org/abs/2308.09951)</code></li>
<li>Summary: <p>Self-supervised methods have shown remarkable progress in learning high-level
semantics and low-level temporal correspondence. Building on these results, we
take one step further and explore the possibility of integrating these two
features to enhance object-centric representations. Our preliminary experiments
indicate that query slot attention can extract different semantic components
from the RGB feature map, while random sampling based slot attention can
exploit temporal correspondence cues between frames to assist instance
identification. Motivated by this, we propose a novel semantic-aware masked
slot attention on top of the fused semantic features and correspondence maps.
It comprises two slot attention stages with a set of shared learnable Gaussian
distributions. In the first stage, we use the mean vectors as slot
initialization to decompose potential semantics and generate semantic
segmentation masks through iterative attention. In the second stage, for each
semantics, we randomly sample slots from the corresponding Gaussian
distribution and perform masked feature aggregation within the semantic area to
exploit temporal correspondence patterns for instance identification. We adopt
semantic- and instance-level temporal consistency as self-supervision to
encourage temporally coherent object-centric representations. Our model
effectively identifies multiple object instances with semantic structure,
reaching promising results on unsupervised video object discovery. Furthermore,
we achieve state-of-the-art performance on dense label propagation tasks,
demonstrating the potential for object-centric analysis. The code is released
at https://github.com/shvdiwnkozbw/SMTC.
</p></li>
</ul>

<h3>Title: Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09965]] Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation(http://arxiv.org/abs/2308.09965)</code></li>
<li>Summary: <p>Within the context of autonomous driving, encountering unknown objects
becomes inevitable during deployment in the open world. Therefore, it is
crucial to equip standard semantic segmentation models with anomaly awareness.
Many previous approaches have utilized synthetic out-of-distribution (OoD) data
augmentation to tackle this problem. In this work, we advance the OoD synthesis
process by reducing the domain gap between the OoD data and driving scenes,
effectively mitigating the style difference that might otherwise act as an
obvious shortcut during training. Additionally, we propose a simple fine-tuning
loss that effectively induces a pre-trained semantic segmentation model to
generate a ``none of the given classes" prediction, leveraging per-pixel OoD
scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline
enables the use of pre-trained models for anomaly segmentation while
maintaining the performance on the original task.
</p></li>
</ul>

<h3>Title: TSAR-MVS: Textureless-aware Segmentation and Correlative Refinement Guided Multi-View Stereo. (arXiv:2308.09990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09990">http://arxiv.org/abs/2308.09990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09990]] TSAR-MVS: Textureless-aware Segmentation and Correlative Refinement Guided Multi-View Stereo(http://arxiv.org/abs/2308.09990)</code></li>
<li>Summary: <p>The reconstruction of textureless areas has long been a challenging problem
in MVS due to lack of reliable pixel correspondences between images. In this
paper, we propose the Textureless-aware Segmentation And Correlative Refinement
guided Multi-View Stereo (TSAR-MVS), a novel method that effectively tackles
challenges posed by textureless areas in 3D reconstruction through filtering,
refinement and segmentation. First, we implement joint hypothesis filtering, a
technique that merges a confidence estimator with a disparity discontinuity
detector to eliminate incorrect depth estimations. Second, to spread the pixels
with confident depth, we introduce a iterative correlation refinement strategy
that leverages RANSAC to generate superpixels, succeeded by a median filter for
broadening the influence of accurately determined pixels.Finally, we present a
textureless-aware segmentation method that leverages edge detection and line
detection for accurately identify large textureless regions to be fitted using
3D planes. Experiments on extensive datasets demonstrate that our method
significantly outperforms most non-learning methods and exhibits robustness to
textureless areas while preserving fine details.
</p></li>
</ul>

<h3>Title: Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation. (arXiv:2308.10016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10016">http://arxiv.org/abs/2308.10016</a></li>
<li>Code URL: https://github.com/yanghai-1218/pseudoflow</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10016]] Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation(http://arxiv.org/abs/2308.10016)</code></li>
<li>Summary: <p>Most self-supervised 6D object pose estimation methods can only work with
additional depth information or rely on the accurate annotation of 2D
segmentation masks, limiting their application range. In this paper, we propose
a 6D object pose estimation method that can be trained with pure RGB images
without any auxiliary information. We first obtain a rough pose initialization
from networks trained on synthetic images rendered from the target's 3D mesh.
Then, we introduce a refinement strategy leveraging the geometry constraint in
synthetic-to-real image pairs from multiple different views. We formulate this
geometry constraint as pixel-level flow consistency between the training images
with dynamically generated pseudo labels. We evaluate our method on three
challenging datasets and demonstrate that it outperforms state-of-the-art
self-supervised methods significantly, with neither 2D annotations nor
additional depth images.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
