<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Secure Shapley Value for Cross-Silo Federated Learning. (arXiv:2209.04856v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04856">http://arxiv.org/abs/2209.04856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04856] Secure Shapley Value for Cross-Silo Federated Learning](http://arxiv.org/abs/2209.04856)</code></li>
<li>Summary: <p>The Shapley value (SV) is a fair and principled metric for contribution
evaluation in cross-silo federated learning (cross-silo FL), in which
organizations, i.e., clients, collaboratively train prediction models with the
coordination of a parameter server. However, existing SV calculation methods
for FL assume that the server can access the raw FL models and public test
data, which might not be a valid assumption in practice given the emerging
privacy attacks on FL models and that test data might be clients' private
assets. Hence, in this paper, we investigate the problem of secure SV
calculation for cross-silo FL. We first propose a one-server solution, HESV,
which is based solely on homomorphic encryption (HE) for privacy protection and
has some considerable limitations in efficiency. To overcome these limitations,
we further propose an efficient two-server protocol, SecSV, which has the
following novel features. First, SecSV utilizes a hybrid privacy protection
scheme to avoid ciphertext-ciphertext multiplications between test data and
models, which are extremely expensive under HE. Second, a more efficient secure
matrix multiplication method is proposed for SecSV. Third, SecSV strategically
identifies and skips some test samples without significantly affecting the
evaluation accuracy. Our experiments demonstrate that SecSV is 5.9-18.0 times
as fast as HESV while sacrificing a limited loss in the accuracy of calculated
SVs.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: IR-LPR: Large Scale of Iranian License Plate Recognition Dataset. (arXiv:2209.04680v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04680">http://arxiv.org/abs/2209.04680</a></li>
<li>Code URL: <a href="https://github.com/mut-deep/ir-lpr">https://github.com/mut-deep/ir-lpr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04680] IR-LPR: Large Scale of Iranian License Plate Recognition Dataset](http://arxiv.org/abs/2209.04680)</code></li>
<li>Summary: <p>Object detection has always been practical. There are so many things in our
world that recognizing them can not only increase our automatic knowledge of
the surroundings, but can also be lucrative for those interested in starting a
new business. One of these attractive objects is the license plate (LP). In
addition to the security uses that license plate detection can have, it can
also be used to create creative businesses. With the development of object
detection methods based on deep learning models, an appropriate and
comprehensive dataset becomes doubly important. But due to the frequent
commercial use of license plate datasets, there are limited datasets not only
in Iran but also in the world. The largest Iranian dataset for detection
license plates has 1,466 images. Also, the largest Iranian dataset for
recognizing the characters of a license plate has 5,000 images. We have
prepared a complete dataset including 20,967 car images along with all the
detection annotation of the whole license plate and its characters, which can
be useful for various purposes. Also, the total number of license plate images
for character recognition application is 27,745 images.
</p></li>
</ul>

<h3>Title: A Close Look at a Systematic Method for Analyzing Sets of Security Advice. (arXiv:2209.04502v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04502">http://arxiv.org/abs/2209.04502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04502] A Close Look at a Systematic Method for Analyzing Sets of Security Advice](http://arxiv.org/abs/2209.04502)</code></li>
<li>Summary: <p>We carry out a detailed analysis of the security advice coding method
(SAcoding) of Barrera et al. (2022), which is designed to analyze security
advice in the sense of measuring actionability and categorizing advice items as
practices, policies, principles, or outcomes. The main part of our analysis
explores the extent to which a second coder's assignment of codes to advice
items agrees with that of a first, for a dataset of 1013 security advice items
nominally addressing Internet of Things devices. More broadly, we seek a deeper
understanding of the soundness and utility of the SAcoding method, and the
degree to which it meets the design goal of reducing subjectivity in assigning
codes to security advice items. Our analysis results in suggestions for minor
changes to the coding tree methodology, and some recommendations. We believe
the coding tree approach may be of interest for analysis of qualitative data
beyond security advice datasets alone.
</p></li>
</ul>

<h3>Title: The Space of Adversarial Strategies. (arXiv:2209.04521v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04521">http://arxiv.org/abs/2209.04521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04521] The Space of Adversarial Strategies](http://arxiv.org/abs/2209.04521)</code></li>
<li>Summary: <p>Adversarial examples, inputs designed to induce worst-case behavior in
machine learning models, have been extensively studied over the past decade.
Yet, our understanding of this phenomenon stems from a rather fragmented pool
of knowledge; at present, there are a handful of attacks, each with disparate
assumptions in threat models and incomparable definitions of optimality. In
this paper, we propose a systematic approach to characterize worst-case (i.e.,
optimal) adversaries. We first introduce an extensible decomposition of attacks
in adversarial machine learning by atomizing attack components into surfaces
and travelers. With our decomposition, we enumerate over components to create
576 attacks (568 of which were previously unexplored). Next, we propose the
Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack
performance. With our new attacks, we measure performance relative to the PEA
on: both robust and non-robust models, seven datasets, and three extended
lp-based threat models incorporating compute costs, formalizing the Space of
Adversarial Strategies. From our evaluation we find that attack performance to
be highly contextual: the domain, model robustness, and threat model can have a
profound influence on attack efficacy. Our investigation suggests that future
studies measuring the security of machine learning should: (1) be
contextualized to the domain &amp; threat models, and (2) go beyond the handful of
known attacks used today.
</p></li>
</ul>

<h3>Title: Cache Refinement Type for Side-Channel Detection of Cryptographic Software. (arXiv:2209.04610v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04610">http://arxiv.org/abs/2209.04610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04610] Cache Refinement Type for Side-Channel Detection of Cryptographic Software](http://arxiv.org/abs/2209.04610)</code></li>
<li>Summary: <p>Cache side-channel attacks exhibit severe threats to software security and
privacy, especially for cryptosystems. In this paper, we propose CaType, a
novel refinement type-based tool for detecting cache side channels in crypto
software. Compared to previous works, CaType provides the following advantages:
(1) For the first time CaType analyzes cache side channels using refinement
type over x86 assembly code. It reveals several significant and effective
enhancements with refined types, including bit-level granularity tracking,
distinguishing different effects of variables, precise type inferences, and
high scalability. (2) CaType is the first static analyzer for crypto libraries
in consideration of blinding-based defenses. (3) From the perspective of
implementation, CaType uses cache layouts of potential vulnerable control-flow
branches rather than cache states to suppress false positives. We evaluate
CaType in identifying side channel vulnerabilities in real-world crypto
software, including RSA, ElGamal, and (EC)DSA from OpenSSL and Libgcrypt.
CaType captures all known defects, detects previously-unknown vulnerabilities,
and reveals several false positives of previous tools. In terms of performance,
CaType is 16X faster than CacheD and 131X faster than CacheS when analyzing the
same libraries. These evaluation results confirm the capability of CaType in
identifying side channel defects with great precision, efficiency, and
scalability.
</p></li>
</ul>

<h3>Title: A 3.3 Gbps SPAD-Based Quantum Random Number Generator. (arXiv:2209.04868v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04868">http://arxiv.org/abs/2209.04868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04868] A 3](http://arxiv.org/abs/2209.04868)</code></li>
<li>Summary: <p>Quantum random number generators are a burgeoning technology used for a
variety of applications, including modern security and encryption systems.
Typical methods exploit an entropy source combined with an extraction or bit
generation circuit in order to produce a random string. In integrated designs
there is often little modelling or analytical description of the entropy
source, circuit extraction and post-processing provided. In this work, we first
discuss theory on the quantum random flip-flop (QRFF), which elucidates the
role of circuit imperfections that manifest themselves in bias and correlation.
Then, a Verilog-AMS model is developed in order to validate the analytical
model in simulation. A novel transistor implementation of the QRFF circuit is
presented, which enables compensation of the degradation in entropy inherent to
the finite non-symmetric transitions of the random flip-flop. Finally, a full
system containing two independent arrays of the QRFF circuit is manufactured
and tested in a 55 nm Bipolar-CMOS-DMOS (BCD) technology node, demonstrating
bit generation statistics that are commensurate to the developed model. The
full chip is able to generate 3.3 Gbps of data when operated with an external
LED, whereas an individual QRFF can generate 25 Mbps each of random data while
maintaining a Shannon entropy bound > 0.997, which is one of the highest per
pixel bit generation rates to date. NIST STS is used to benchmark the generated
bit strings, thereby validating the QRFF circuit as an excellent candidate for
fully-integrated QRNGs.
</p></li>
</ul>

<h3>Title: Towards Security Enhancement of Blockchain-based Supply Chain Management. (arXiv:2209.04917v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04917">http://arxiv.org/abs/2209.04917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04917] Towards Security Enhancement of Blockchain-based Supply Chain Management](http://arxiv.org/abs/2209.04917)</code></li>
<li>Summary: <p>The cybersecurity of modern systems has dramatically increased attention from
both industrial and academia perspectives. In the recent era, the popularity of
the blockchain-based system has traditionally been emergent among various
industrials sectors especially in supply chain management due to its
streamlined nature. This reveals the importance of the quality aspects from a
supply chain management perspective. Many industries realized the importance of
having quality systems for supply chain management and logistics. The emergence
of blockchain technology has created several potential innovations in handling
and tracking business activities over the supply chain processes as specific.
This paper shed the light on the blockchain and specifically on a smart
contract technology which been used to handle the process of creation,
verification and checking data over the supply chain management process. Then,
touch upon the area of blockchain cybersecurity in the supply chain context.
More and more, since the smart contract handles the transfer of data over
different locations, then the security protection should be strong enough to
secure the data and the assets from any attacks. Finally, the paper examines
the main security attacks that affect the data on the blockchain and propose a
solution
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation. (arXiv:2209.04599v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04599">http://arxiv.org/abs/2209.04599</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04599] Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation](http://arxiv.org/abs/2209.04599)</code></li>
<li>Summary: <p>Federated Learning (FL) is a machine learning paradigm where local nodes
collaboratively train a central model while the training data remains
decentralized. Existing FL methods typically share model parameters or employ
co-distillation to address the issue of unbalanced data distribution. However,
they suffer from communication bottlenecks. More importantly, they risk privacy
leakage. In this work, we develop a privacy preserving and communication
efficient method in a FL framework with one-shot offline knowledge distillation
using unlabeled, cross-domain public data. We propose a quantized and noisy
ensemble of local predictions from completely trained local models for stronger
privacy guarantees without sacrificing accuracy. Based on extensive experiments
on image classification and text classification tasks, we show that our
privacy-preserving method outperforms baseline FL algorithms with superior
performance in both accuracy and communication efficiency.
</p></li>
</ul>

<h3>Title: Is Synthetic Dataset Reliable for Benchmarking Generalizable Person Re-Identification?. (arXiv:2209.05047v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05047">http://arxiv.org/abs/2209.05047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05047] Is Synthetic Dataset Reliable for Benchmarking Generalizable Person Re-Identification?](http://arxiv.org/abs/2209.05047)</code></li>
<li>Summary: <p>Recent studies show that models trained on synthetic datasets are able to
achieve better generalizable person re-identification (GPReID) performance than
that trained on public real-world datasets. On the other hand, due to the
limitations of real-world person ReID datasets, it would also be important and
interesting to use large-scale synthetic datasets as test sets to benchmark
person ReID algorithms. Yet this raises a critical question: is synthetic
dataset reliable for benchmarking generalizable person re-identification? In
the literature there is no evidence showing this. To address this, we design a
method called Pairwise Ranking Analysis (PRA) to quantitatively measure the
ranking similarity and perform the statistical test of identical distributions.
Specifically, we employ Kendall rank correlation coefficients to evaluate
pairwise similarity values between algorithm rankings on different datasets.
Then, a non-parametric two-sample Kolmogorov-Smirnov (KS) test is performed for
the judgement of whether algorithm ranking correlations between synthetic and
real-world datasets and those only between real-world datasets lie in identical
distributions. We conduct comprehensive experiments, with ten representative
algorithms, three popular real-world person ReID datasets, and three recently
released large-scale synthetic datasets. Through the designed pairwise ranking
analysis and comprehensive evaluations, we conclude that a recent large-scale
synthetic dataset ClonedPerson can be reliably used to benchmark GPReID,
statistically the same as real-world datasets. Therefore, this study guarantees
the usage of synthetic datasets for both source training set and target testing
set, with completely no privacy concerns from real-world surveillance data.
Besides, the study in this paper might also inspire future designs of synthetic
datasets.
</p></li>
</ul>

<h3>Title: SSOPrivateEye: Timely Disclosure of Single Sign-On Privacy Design Differences. (arXiv:2209.04490v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04490">http://arxiv.org/abs/2209.04490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04490] SSOPrivateEye: Timely Disclosure of Single Sign-On Privacy Design Differences](http://arxiv.org/abs/2209.04490)</code></li>
<li>Summary: <p>The number of login options on websites has increased since the introduction
of web single sign-on (SSO) protocols. SSO services allow users to grant
websites or relying parties (RPs) access to their personal profile information
from identity provider (IdP) accounts. When prompting users to select an SSO
login option, many websites do not provide any privacy information that could
help users make informed choices. Moreover, privacy differences in permission
requests across available login options are largely hidden from users and are
time consuming to manually extract and compare. In this paper, we present an
empirical study of popular RP implementations supporting three major IdP login
options (Facebook, Google, and Apple) and categorize RPs in the top 300 sites
into four client-side code patterns. Our findings suggest a relatively uniform
distribution in three code patterns. We select RPs in one of these patterns as
target sites for the design and implementation of SSOPrivateEye (SPEye), a
browser extension prototype that extracts comparative data on SSO login options
in RPs covering the three IdPs. Our evaluation of SPEye demonstrates the
viability of extracting privacy information that can inform SSO login choices
in the majority of our target sites.
</p></li>
</ul>

<h3>Title: Exploring privacy-enhancing technologies in the automotive value chain. (arXiv:2209.05085v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05085">http://arxiv.org/abs/2209.05085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05085] Exploring privacy-enhancing technologies in the automotive value chain](http://arxiv.org/abs/2209.05085)</code></li>
<li>Summary: <p>Privacy-enhancing technologies (PETs) are becoming increasingly crucial for
addressing customer needs, security, privacy (e.g., enhancing anonymity and
confidentiality), and regulatory requirements. However, applying PETs in
organizations requires a precise understanding of use cases, technologies, and
limitations. This paper investigates several industrial use cases, their
characteristics, and the potential applicability of PETs to these. We conduct
expert interviews to identify and classify uses cases, a gray literature review
of relevant open-source PET tools, and discuss how the use case characteristics
can be addressed using PETs' capabilities. While we focus mainly on automotive
use cases, the results also apply to other use case domains.
</p></li>
</ul>

<h3>Title: Responsible AI Pattern Catalogue: a Multivocal Literature Review. (arXiv:2209.04963v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04963">http://arxiv.org/abs/2209.04963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04963] Responsible AI Pattern Catalogue: a Multivocal Literature Review](http://arxiv.org/abs/2209.04963)</code></li>
<li>Summary: <p>Responsible AI has been widely considered as one of the greatest scientific
challenges of our time and the key to unlock the AI market and increase the
adoption. To address the responsible AI challenge, a number of AI ethics
principles frameworks have been published recently, which AI systems are
supposed to conform to. However, without further best practice guidance,
practitioners are left with nothing much beyond truisms. Also, significant
efforts have been placed at algorithm-level rather than system-level, mainly
focusing on a subset of mathematics-amenable ethical principles (such as
privacy and fairness). Nevertheless, ethical issues can occur at any step of
the development lifecycle crosscutting many AI, non-AI and data components of
systems beyond AI algorithms and models. To operationalize responsible AI from
a system perspective, in this paper, we adopt a pattern-oriented approach and
present a Responsible AI Pattern Catalogue based on the results of a systematic
Multivocal Literature Review (MLR). Rather than staying at the ethical
principle level or algorithm level, we focus on patterns that AI system
stakeholders can undertake in practice to ensure that the developed AI systems
are responsible throughout the entire governance and engineering lifecycle. The
Responsible AI Pattern Catalogue classifies patterns into three groups:
multi-level governance patterns, trustworthy process patterns, and
responsible-AI-by-design product patterns. These patterns provide a systematic
and actionable guidance for stakeholders to implement responsible AI.
</p></li>
</ul>

<h3>Title: An Investigation of Smart Contract for Collaborative Machine Learning Model Training. (arXiv:2209.05017v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05017">http://arxiv.org/abs/2209.05017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05017] An Investigation of Smart Contract for Collaborative Machine Learning Model Training](http://arxiv.org/abs/2209.05017)</code></li>
<li>Summary: <p>Machine learning (ML) has penetrated various fields in the era of big data.
The advantage of collaborative machine learning (CML) over most conventional ML
lies in the joint effort of decentralized nodes or agents that results in
better model performance and generalization. As the training of ML models
requires a massive amount of good quality data, it is necessary to eliminate
concerns about data privacy and ensure high-quality data. To solve this
problem, we cast our eyes on the integration of CML and smart contracts. Based
on blockchain, smart contracts enable automatic execution of data preserving
and validation, as well as the continuity of CML model training. In our
simulation experiments, we define incentive mechanisms on the smart contract,
investigate the important factors such as the number of features in the dataset
(num_words), the size of the training data, the cost for the data holders to
submit data, etc., and conclude how these factors impact the performance
metrics of the model: the accuracy of the trained model, the gap between the
accuracies of the model before and after simulation, and the time to use up the
balance of bad agent. For instance, the increase of the value of num_words
leads to higher model accuracy and eliminates the negative influence of
malicious agents in a shorter time from our observation of the experiment
results. Statistical analyses show that with the help of smart contracts, the
influence of invalid data is efficiently diminished and model robustness is
maintained. We also discuss the gap in existing research and put forward
possible future directions for further works.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Scattering Model Guided Adversarial Examples for SAR Target Recognition: Attack and Defense. (arXiv:2209.04779v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04779">http://arxiv.org/abs/2209.04779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04779] Scattering Model Guided Adversarial Examples for SAR Target Recognition: Attack and Defense](http://arxiv.org/abs/2209.04779)</code></li>
<li>Summary: <p>Deep Neural Networks (DNNs) based Synthetic Aperture Radar (SAR) Automatic
Target Recognition (ATR) systems have shown to be highly vulnerable to
adversarial perturbations that are deliberately designed yet almost
imperceptible but can bias DNN inference when added to targeted objects. This
leads to serious safety concerns when applying DNNs to high-stake SAR ATR
applications. Therefore, enhancing the adversarial robustness of DNNs is
essential for implementing DNNs to modern real-world SAR ATR systems. Toward
building more robust DNN-based SAR ATR models, this article explores the domain
knowledge of SAR imaging process and proposes a novel Scattering Model Guided
Adversarial Attack (SMGAA) algorithm which can generate adversarial
perturbations in the form of electromagnetic scattering response (called
adversarial scatterers). The proposed SMGAA consists of two parts: 1) a
parametric scattering model and corresponding imaging method and 2) a
customized gradient-based optimization algorithm. First, we introduce the
effective Attributed Scattering Center Model (ASCM) and a general imaging
method to describe the scattering behavior of typical geometric structures in
the SAR imaging process. By further devising several strategies to take the
domain knowledge of SAR target images into account and relax the greedy search
procedure, the proposed method does not need to be prudentially finetuned, but
can efficiently to find the effective ASCM parameters to fool the SAR
classifiers and facilitate the robust model training. Comprehensive evaluations
on the MSTAR dataset show that the adversarial scatterers generated by SMGAA
are more robust to perturbations and transformations in the SAR processing
chain than the currently studied attacks, and are effective to construct a
defensive model against the malicious scatterers.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Adaptive Perturbation Generation for Multiple Backdoors Detection. (arXiv:2209.05244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05244">http://arxiv.org/abs/2209.05244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05244] Adaptive Perturbation Generation for Multiple Backdoors Detection](http://arxiv.org/abs/2209.05244)</code></li>
<li>Summary: <p>Extensive evidence has demonstrated that deep neural networks (DNNs) are
vulnerable to backdoor attacks, which motivates the development of backdoor
detection methods. Existing backdoor detection methods are typically tailored
for backdoor attacks with individual specific types (e.g., patch-based or
perturbation-based). However, adversaries are likely to generate multiple types
of backdoor attacks in practice, which challenges the current detection
strategies. Based on the fact that adversarial perturbations are highly
correlated with trigger patterns, this paper proposes the Adaptive Perturbation
Generation (APG) framework to detect multiple types of backdoor attacks by
adaptively injecting adversarial perturbations. Since different trigger
patterns turn out to show highly diverse behaviors under the same adversarial
perturbations, we first design the global-to-local strategy to fit the multiple
types of backdoor triggers via adjusting the region and budget of attacks. To
further increase the efficiency of perturbation injection, we introduce a
gradient-guided mask generation strategy to search for the optimal regions for
adversarial attacks. Extensive experiments conducted on multiple datasets
(CIFAR-10, GTSRB, Tiny-ImageNet) demonstrate that our method outperforms
state-of-the-art baselines by large margins(+12%).
</p></li>
</ul>

<h3>Title: Semantic-Preserving Adversarial Code Comprehension. (arXiv:2209.05130v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05130">http://arxiv.org/abs/2209.05130</a></li>
<li>Code URL: <a href="https://github.com/ericlee8/space">https://github.com/ericlee8/space</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05130] Semantic-Preserving Adversarial Code Comprehension](http://arxiv.org/abs/2209.05130)</code></li>
<li>Summary: <p>Based on the tremendous success of pre-trained language models (PrLMs) for
source code comprehension tasks, current literature studies either ways to
further improve the performance (generalization) of PrLMs, or their robustness
against adversarial attacks. However, they have to compromise on the trade-off
between the two aspects and none of them consider improving both sides in an
effective and practical way. To fill this gap, we propose Semantic-Preserving
Adversarial Code Embeddings (SPACE) to find the worst-case semantic-preserving
attacks while forcing the model to predict the correct labels under these worst
cases. Experiments and analysis demonstrate that SPACE can stay robust against
state-of-the-art attacks while boosting the performance of PrLMs for code.
</p></li>
</ul>

<h3>Title: Logic and Reduction Operation based Hardware Trojans in Digital Design. (arXiv:2209.04484v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04484">http://arxiv.org/abs/2209.04484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04484] Logic and Reduction Operation based Hardware Trojans in Digital Design](http://arxiv.org/abs/2209.04484)</code></li>
<li>Summary: <p>In this paper, we will demonstrate Hardware Trojan Attacks on four different
digital designs implemented on FPGA. The hardware trojan is activated based on
special logical and reduction-based operations on vectors which makes the
trojan-activity as silent and effective as possible. In this paper, we have
introduced 5 novel trojan attack methodologies.
</p></li>
</ul>

<h3>Title: Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04547">http://arxiv.org/abs/2209.04547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04547] Defend Data Poisoning Attacks on Voice Authentication](http://arxiv.org/abs/2209.04547)</code></li>
<li>Summary: <p>With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, "vocal passwords" are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users' web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
</p></li>
</ul>

<h3>Title: Resisting Deep Learning Models Against Adversarial Attack Transferability via Feature Randomization. (arXiv:2209.04930v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04930">http://arxiv.org/abs/2209.04930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04930] Resisting Deep Learning Models Against Adversarial Attack Transferability via Feature Randomization](http://arxiv.org/abs/2209.04930)</code></li>
<li>Summary: <p>In the past decades, the rise of artificial intelligence has given us the
capabilities to solve the most challenging problems in our day-to-day lives,
such as cancer prediction and autonomous navigation. However, these
applications might not be reliable if not secured against adversarial attacks.
In addition, recent works demonstrated that some adversarial examples are
transferable across different models. Therefore, it is crucial to avoid such
transferability via robust models that resist adversarial manipulations. In
this paper, we propose a feature randomization-based approach that resists
eight adversarial attacks targeting deep learning models in the testing phase.
Our novel approach consists of changing the training strategy in the target
network classifier and selecting random feature samples. We consider the
attacker with a Limited-Knowledge and Semi-Knowledge conditions to undertake
the most prevalent types of adversarial attacks. We evaluate the robustness of
our approach using the well-known UNSW-NB15 datasets that include realistic and
synthetic attacks. Afterward, we demonstrate that our strategy outperforms the
existing state-of-the-art approach, such as the Most Powerful Attack, which
consists of fine-tuning the network model against specific adversarial attacks.
Finally, our experimental results show that our methodology can secure the
target network and resists adversarial attack transferability by over 60%.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Self-supervised Human Mesh Recovery with Cross-Representation Alignment. (arXiv:2209.04596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04596">http://arxiv.org/abs/2209.04596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04596] Self-supervised Human Mesh Recovery with Cross-Representation Alignment](http://arxiv.org/abs/2209.04596)</code></li>
<li>Summary: <p>Fully supervised human mesh recovery methods are data-hungry and have poor
generalizability due to the limited availability and diversity of 3D-annotated
benchmark datasets. Recent progress in self-supervised human mesh recovery has
been made using synthetic-data-driven training paradigms where the model is
trained from synthetic paired 2D representation (e.g., 2D keypoints and
segmentation masks) and 3D mesh. However, on synthetic dense correspondence
maps (i.e., IUV) few have been explored since the domain gap between synthetic
training data and real testing data is hard to address for 2D dense
representation. To alleviate this domain gap on IUV, we propose
cross-representation alignment utilizing the complementary information from the
robust but sparse representation (2D keypoints). Specifically, the alignment
errors between initial mesh estimation and both 2D representations are
forwarded into regressor and dynamically corrected in the following mesh
regression. This adaptive cross-representation alignment explicitly learns from
the deviations and captures complementary information: robustness from sparse
representation and richness from dense representation. We conduct extensive
experiments on multiple standard benchmark datasets and demonstrate competitive
results, helping take a step towards reducing the annotation effort needed to
produce state-of-the-art models in human mesh estimation.
</p></li>
</ul>

<h3>Title: Local-Aware Global Attention Network for Person Re-Identification. (arXiv:2209.04821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04821">http://arxiv.org/abs/2209.04821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04821] Local-Aware Global Attention Network for Person Re-Identification](http://arxiv.org/abs/2209.04821)</code></li>
<li>Summary: <p>Learning representative, robust and discriminative information from images is
essential for effective person re-identification (Re-Id). In this paper, we
propose a compound approach for end-to-end discriminative deep feature learning
for person Re-Id based on both body and hand images. We carefully design the
Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network
architecture consisting of one branch for spatial attention, one branch for
channel attention, one branch for global feature representations and another
branch for local feature representations. The attention branches focus on the
relevant features of the image while suppressing the irrelevant backgrounds. In
order to overcome the weakness of the attention mechanisms, equivariant to
pixel shuffling, we integrate relative positional encodings into the spatial
attention module to capture the spatial positions of pixels. The global branch
intends to preserve the global context or structural information. For the the
local branch, which intends to capture the fine-grained information, we perform
uniform partitioning to generate stripes on the conv-layer horizontally. We
retrieve the parts by conducting a soft partition without explicitly
partitioning the images or requiring external cues such as pose estimation. A
set of ablation study shows that each component contributes to the increased
performance of the LAGA-Net. Extensive evaluations on four popular body-based
person Re-Id benchmarks and two publicly available hand datasets demonstrate
that our proposed method consistently outperforms existing state-of-the-art
methods.
</p></li>
</ul>

<h3>Title: Vec2Face-v2: Unveil Human Faces from their Blackbox Features via Attention-based Network in Face Recognition. (arXiv:2209.04920v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04920">http://arxiv.org/abs/2209.04920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04920] Vec2Face-v2: Unveil Human Faces from their Blackbox Features via Attention-based Network in Face Recognition](http://arxiv.org/abs/2209.04920)</code></li>
<li>Summary: <p>In this work, we investigate the problem of face reconstruction given a
facial feature representation extracted from a blackbox face recognition
engine. Indeed, it is very challenging problem in practice due to the
limitations of abstracted information from the engine. We therefore introduce a
new method named Attention-based Bijective Generative Adversarial Networks in a
Distillation framework (DAB-GAN) to synthesize faces of a subject given his/her
extracted face recognition features. Given any unconstrained unseen facial
features of a subject, the DAB-GAN can reconstruct his/her faces in high
definition. The DAB-GAN method includes a novel attention-based generative
structure with the new defined Bijective Metrics Learning approach. The
framework starts by introducing a bijective metric so that the distance
measurement and metric learning process can be directly adopted in image domain
for an image reconstruction task. The information from the blackbox face
recognition engine will be optimally exploited using the global distillation
process. Then an attention-based generator is presented for a highly robust
generator to synthesize realistic faces with ID preservation. We have evaluated
our method on the challenging face recognition databases, i.e. CelebA, LFW,
AgeDB, CFP-FP, and consistently achieved the state-of-the-art results. The
advancement of DAB-GAN is also proven on both image realism and ID preservation
properties.
</p></li>
</ul>

<h3>Title: Multi-modal Streaming 3D Object Detection. (arXiv:2209.04966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04966">http://arxiv.org/abs/2209.04966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04966] Multi-modal Streaming 3D Object Detection](http://arxiv.org/abs/2209.04966)</code></li>
<li>Summary: <p>Modern autonomous vehicles rely heavily on mechanical LiDARs for perception.
Current perception methods generally require 360{\deg} point clouds, collected
sequentially as the LiDAR scans the azimuth and acquires consecutive
wedge-shaped slices. The acquisition latency of a full scan (~ 100ms) may lead
to outdated perception which is detrimental to safe operation. Recent streaming
perception works proposed directly processing LiDAR slices and compensating for
the narrow field of view (FOV) of a slice by reusing features from preceding
slices. These works, however, are all based on a single modality and require
past information which may be outdated. Meanwhile, images from high-frequency
cameras can support streaming models as they provide a larger FoV compared to a
LiDAR slice. However, this difference in FoV complicates sensor fusion. To
address this research gap, we propose an innovative camera-LiDAR streaming 3D
object detection framework that uses camera images instead of past LiDAR slices
to provide an up-to-date, dense, and wide context for streaming perception. The
proposed method outperforms prior streaming models on the challenging NuScenes
benchmark. It also outperforms powerful full-scan detectors while being much
faster. Our method is shown to be robust to missing camera images, narrow LiDAR
slices, and small camera-LiDAR miscalibration.
</p></li>
</ul>

<h3>Title: Stability of Syntactic Dialect Classification Over Space and Time. (arXiv:2209.04958v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04958">http://arxiv.org/abs/2209.04958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04958] Stability of Syntactic Dialect Classification Over Space and Time](http://arxiv.org/abs/2209.04958)</code></li>
<li>Summary: <p>This paper analyses the degree to which dialect classifiers based on
syntactic representations remain stable over space and time. While previous
work has shown that the combination of grammar induction and geospatial text
classification produces robust dialect models, we do not know what influence
both changing grammars and changing populations have on dialect models. This
paper constructs a test set for 12 dialects of English that spans three years
at monthly intervals with a fixed spatial distribution across 1,120 cities.
Syntactic representations are formulated within the usage-based Construction
Grammar paradigm (CxG). The decay rate of classification performance for each
dialect over time allows us to identify regions undergoing syntactic change.
And the distribution of classification accuracy within dialect regions allows
us to identify the degree to which the grammar of a dialect is internally
heterogeneous. The main contribution of this paper is to show that a rigorous
evaluation of dialect classification models can be used to find both variation
over space and change over time.
</p></li>
</ul>

<h3>Title: Reconstruction of Long-Term Historical Demand Data. (arXiv:2209.04693v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04693">http://arxiv.org/abs/2209.04693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04693] Reconstruction of Long-Term Historical Demand Data](http://arxiv.org/abs/2209.04693)</code></li>
<li>Summary: <p>Long-term planning of a robust power system requires the understanding of
changing demand patterns. Electricity demand is highly weather sensitive. Thus,
the supply side variation from introducing intermittent renewable sources,
juxtaposed with variable demand, will introduce additional challenges in the
grid planning process. By understanding the spatial and temporal variability of
temperature over the US, the response of demand to natural variability and
climate change-related effects on temperature can be separated, especially
because the effects due to the former factor are not known. Through this
project, we aim to better support the technology &amp; policy development process
for power systems by developing machine and deep learning 'back-forecasting'
models to reconstruct multidecadal demand records and study the natural
variability of temperature and its influence on demand.
</p></li>
</ul>

<h3>Title: A Complex Network based Graph Embedding Method for Link Prediction. (arXiv:2209.04884v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04884">http://arxiv.org/abs/2209.04884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04884] A Complex Network based Graph Embedding Method for Link Prediction](http://arxiv.org/abs/2209.04884)</code></li>
<li>Summary: <p>Graph embedding methods aim at finding useful graph representations by
mapping nodes to a low-dimensional vector space. It is a task with important
downstream applications, such as link prediction, graph reconstruction, data
visualization, node classification, and language modeling. In recent years, the
field of graph embedding has witnessed a shift from linear algebraic approaches
towards local, gradient-based optimization methods combined with random walks
and deep neural networks to tackle the problem of embedding large graphs.
However, despite this improvement in the optimization tools, graph embedding
methods are still generically designed in a way that is oblivious to the
particularities of real-life networks. Indeed, there has been significant
progress in understanding and modeling complex real-life networks in recent
years. However, the obtained results have had a minor influence on the
development of graph embedding algorithms. This paper aims to remedy this by
designing a graph embedding method that takes advantage of recent valuable
insights from the field of network science. More precisely, we present a novel
graph embedding approach based on the popularity-similarity and local
attraction paradigms. We evaluate the performance of the proposed approach on
the link prediction task on a large number of real-life networks. We show,
using extensive experimental analysis, that the proposed method outperforms
state-of-the-art graph embedding algorithms. We also demonstrate its robustness
to data scarcity and the choice of embedding dimensionality.
</p></li>
</ul>

<h3>Title: CARE: Certifiably Robust Learning with Reasoning via Variational Inference. (arXiv:2209.05055v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05055">http://arxiv.org/abs/2209.05055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05055] CARE: Certifiably Robust Learning with Reasoning via Variational Inference](http://arxiv.org/abs/2209.05055)</code></li>
<li>Summary: <p>Despite great recent advances achieved by deep neural networks (DNNs), they
are often vulnerable to adversarial attacks. Intensive research efforts have
been made to improve the robustness of DNNs; however, most empirical defenses
can be adaptively attacked again, and the theoretically certified robustness is
limited, especially on large-scale datasets. One potential root cause of such
vulnerabilities for DNNs is that although they have demonstrated powerful
expressiveness, they lack the reasoning ability to make robust and reliable
predictions. In this paper, we aim to integrate domain knowledge to enable
robust learning with the reasoning paradigm. In particular, we propose a
certifiably robust learning with reasoning pipeline (CARE), which consists of a
learning component and a reasoning component. Concretely, we use a set of
standard DNNs to serve as the learning component to make semantic predictions,
and we leverage the probabilistic graphical models, such as Markov logic
networks (MLN), to serve as the reasoning component to enable knowledge/logic
reasoning. However, it is known that the exact inference of MLN (reasoning) is</li>
</ul>

<h1>P-complete, which limits the scalability of the pipeline. To this end, we</h1>
<p>propose to approximate the MLN inference via variational inference based on an
efficient expectation maximization algorithm. In particular, we leverage graph
convolutional networks (GCNs) to encode the posterior distribution during
variational inference and update the parameters of GCNs (E-step) and the
weights of knowledge rules in MLN (M-step) iteratively. We conduct extensive
experiments on different datasets and show that CARE achieves significantly
higher certified robustness compared with the state-of-the-art baselines. We
additionally conducted different ablation studies to demonstrate the empirical
robustness of CARE and the effectiveness of different knowledge integration.
</p></p>
<h3>Title: Bias Challenges in Counterfactual Data Augmentation. (arXiv:2209.05104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05104">http://arxiv.org/abs/2209.05104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05104] Bias Challenges in Counterfactual Data Augmentation](http://arxiv.org/abs/2209.05104)</code></li>
<li>Summary: <p>Deep learning models tend not to be out-of-distribution robust primarily due
to their reliance on spurious features to solve the task. Counterfactual data
augmentations provide a general way of (approximately) achieving
representations that are counterfactual-invariant to spurious features, a
requirement for out-of-distribution (OOD) robustness. In this work, we show
that counterfactual data augmentations may not achieve the desired
counterfactual-invariance if the augmentation is performed by a {\em
context-guessing machine}, an abstract machine that guesses the most-likely
context of a given input. We theoretically analyze the invariance imposed by
such counterfactual data augmentations and describe an exemplar NLP task where
counterfactual data augmentation by a context-guessing machine does not lead to
robust OOD classifiers.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Improving Keyphrase Extraction with Data Augmentation and Information Filtering. (arXiv:2209.04951v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04951">http://arxiv.org/abs/2209.04951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04951] Improving Keyphrase Extraction with Data Augmentation and Information Filtering](http://arxiv.org/abs/2209.04951)</code></li>
<li>Summary: <p>Keyphrase extraction is one of the essential tasks for document understanding
in NLP. While the majority of the prior works are dedicated to the formal
setting, e.g., books, news or web-blogs, informal texts such as video
transcripts are less explored. To address this limitation, in this work we
present a novel corpus and method for keyphrase extraction from the transcripts
of the videos streamed on the Behance platform. More specifically, in this
work, a novel data augmentation is proposed to enrich the model with the
background knowledge about the keyphrase extraction task from other domains.
Extensive experiments on the proposed dataset dataset show the effectiveness of
the introduced method.
</p></li>
</ul>

<h3>Title: SmartKex: Machine Learning Assisted SSH Keys Extraction From The Heap Dump. (arXiv:2209.05243v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05243">http://arxiv.org/abs/2209.05243</a></li>
<li>Code URL: <a href="https://github.com/smartvmi/Smart-and-Naive-SSH-Key-Extraction">https://github.com/smartvmi/Smart-and-Naive-SSH-Key-Extraction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05243] SmartKex: Machine Learning Assisted SSH Keys Extraction From The Heap Dump](http://arxiv.org/abs/2209.05243)</code></li>
<li>Summary: <p>Digital forensics is the process of extracting, preserving, and documenting
evidence in digital devices. A commonly used method in digital forensics is to
extract data from the main memory of a digital device. However, the main
challenge is identifying the important data to be extracted. Several pieces of
crucial information reside in the main memory, like usernames, passwords, and
cryptographic keys such as SSH session keys. In this paper, we propose
SmartKex, a machine-learning assisted method to extract session keys from heap
memory snapshots of an OpenSSH process. In addition, we release an openly
available dataset and the corresponding toolchain for creating additional data.
Finally, we compare SmartKex with naive brute-force methods and empirically
show that SmartKex can extract the session keys with high accuracy and high
throughput. With the provided resources, we intend to strengthen the research
on the intersection between digital forensics, cybersecurity, and machine
learning.
</p></li>
</ul>

<h3>Title: Extended Feature Space-Based Automatic Melanoma Detection System. (arXiv:2209.04588v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04588">http://arxiv.org/abs/2209.04588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04588] Extended Feature Space-Based Automatic Melanoma Detection System](http://arxiv.org/abs/2209.04588)</code></li>
<li>Summary: <p>Melanoma is the deadliest form of skin cancer. Uncontrollable growth of
melanocytes leads to melanoma. Melanoma has been growing wildly in the last few
decades. In recent years, the detection of melanoma using image processing
techniques has become a dominant research field. The Automatic Melanoma
Detection System (AMDS) helps to detect melanoma based on image processing
techniques by accepting infected skin area images as input. A single lesion
image is a source of multiple features. Therefore, It is crucial to select the
appropriate features from the image of the lesion in order to increase the
accuracy of AMDS. For melanoma detection, all extracted features are not
important. Some of the extracted features are complex and require more
computation tasks, which impacts the classification accuracy of AMDS. The
feature extraction phase of AMDS exhibits more variability, therefore it is
important to study the behaviour of AMDS using individual and extended feature
extraction approaches. A novel algorithm ExtFvAMDS is proposed for the
calculation of Extended Feature Vector Space. The six models proposed in the
comparative study revealed that the HSV feature vector space for automatic
detection of melanoma using Ensemble Bagged Tree classifier on Med-Node Dataset
provided 99% AUC, 95.30% accuracy, 94.23% sensitivity, and 96.96% specificity.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Personalized Federated Learning with Communication Compression. (arXiv:2209.05148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05148">http://arxiv.org/abs/2209.05148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05148] Personalized Federated Learning with Communication Compression](http://arxiv.org/abs/2209.05148)</code></li>
<li>Summary: <p>In contrast to training traditional machine learning (ML) models in data
centers, federated learning (FL) trains ML models over local datasets contained
on resource-constrained heterogeneous edge devices. Existing FL algorithms aim
to learn a single global model for all participating devices, which may not be
helpful to all devices participating in the training due to the heterogeneity
of the data across the devices. Recently, Hanzely and Richt\'{a}rik (2020)
proposed a new formulation for training personalized FL models aimed at
balancing the trade-off between the traditional global model and the local
models that could be trained by individual devices using their private data
only. They derived a new algorithm, called Loopless Gradient Descent (L2GD), to
solve it and showed that this algorithms leads to improved communication
complexity guarantees in regimes when more personalization is required. In this
paper, we equip their L2GD algorithm with a bidirectional compression mechanism
to further reduce the communication bottleneck between the local devices and
the server. Unlike other compression-based algorithms used in the FL-setting,
our compressed L2GD algorithm operates on a probabilistic communication
protocol, where communication does not happen on a fixed schedule. Moreover,
our compressed L2GD algorithm maintains a similar convergence rate as vanilla
SGD without compression. To empirically validate the efficiency of our
algorithm, we perform diverse numerical experiments on both convex and
non-convex problems and using various compression techniques.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning. (arXiv:2209.04851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04851">http://arxiv.org/abs/2209.04851</a></li>
<li>Code URL: <a href="https://github.com/Westlake-AI/openmixup">https://github.com/Westlake-AI/openmixup</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04851] OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning](http://arxiv.org/abs/2209.04851)</code></li>
<li>Summary: <p>With the remarkable progress of deep neural networks in computer vision, data
mixing augmentation techniques are widely studied to alleviate problems of
degraded generalization when the amount of training data is limited. However,
mixup strategies have not been well assembled in current vision toolboxes. In
this paper, we propose \texttt{OpenMixup}, an open-source all-in-one toolbox
for supervised, semi-, and self-supervised visual representation learning with
mixup. It offers an integrated model design and training platform, comprising a
rich set of prevailing network architectures and modules, a collection of data
mixing augmentation methods as well as practical model analysis tools. In
addition, we also provide standard mixup image classification benchmarks on
various datasets, which expedites practitioners to make fair comparisons among
state-of-the-art methods under the same settings. The source code and user
documents are available at \url{https://github.com/Westlake-AI/openmixup}.
</p></li>
</ul>

<h3>Title: A Comparative Study on Unsupervised Anomaly Detection for Time Series: Experiments and Analysis. (arXiv:2209.04635v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04635">http://arxiv.org/abs/2209.04635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04635] A Comparative Study on Unsupervised Anomaly Detection for Time Series: Experiments and Analysis](http://arxiv.org/abs/2209.04635)</code></li>
<li>Summary: <p>The continued digitization of societal processes translates into a
proliferation of time series data that cover applications such as fraud
detection, intrusion detection, and energy management, where anomaly detection
is often essential to enable reliability and safety. Many recent studies target
anomaly detection for time series data. Indeed, area of time series anomaly
detection is characterized by diverse data, methods, and evaluation strategies,
and comparisons in existing studies consider only part of this diversity, which
makes it difficult to select the best method for a particular problem setting.
To address this shortcoming, we introduce taxonomies for data, methods, and
evaluation strategies, provide a comprehensive overview of unsupervised time
series anomaly detection using the taxonomies, and systematically evaluate and
compare state-of-the-art traditional as well as deep learning techniques. In
the empirical study using nine publicly available datasets, we apply the most
commonly-used performance evaluation metrics to typical methods under a fair
implementation standard. Based on the structuring offered by the taxonomies, we
report on empirical studies and provide guidelines, in the form of comparative
tables, for choosing the methods most suitable for particular application
settings. Finally, we propose research directions for this dynamic field.
</p></li>
</ul>

<h3>Title: Application of Machine Learning for Online Reputation Systems. (arXiv:2209.04650v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04650">http://arxiv.org/abs/2209.04650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04650] Application of Machine Learning for Online Reputation Systems](http://arxiv.org/abs/2209.04650)</code></li>
<li>Summary: <p>Users on the internet usually require venues to provide better purchasing
recommendations. This can be provided by a reputation system that processes
ratings to provide recommendations. The rating aggregation process is a main
part of reputation system to produce global opinion about the product quality.
Naive methods that are frequently used do not consider consumer profiles in its
calculation and cannot discover unfair ratings and trends emerging in new
ratings. Other sophisticated rating aggregation methods that use weighted
average technique focus on one or a few aspects of consumers profile data. This
paper proposes a new reputation system using machine learning to predict
reliability of consumers from consumer profile. In particular, we construct a
new consumer profile dataset by extracting a set of factors that have great
impact on consumer reliability, which serve as an input to machine learning
algorithms. The predicted weight is then integrated with a weighted average
method to compute product reputation score. The proposed model has been
evaluated over three MovieLens benchmarking datasets, using 10-Folds cross
validation. Furthermore, the performance of the proposed model has been
compared to previous published rating aggregation models. The obtained results
were promising which suggest that the proposed approach could be a potential
solution for reputation systems. The results of comparison demonstrated the
accuracy of our models. Finally, the proposed approach can be integrated with
online recommendation systems to provide better purchasing recommendations and
facilitate user experience on online shopping markets.
</p></li>
</ul>

<h3>Title: Fairness in Forecasting of Observations of Linear Dynamical Systems. (arXiv:2209.05274v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.05274">http://arxiv.org/abs/2209.05274</a></li>
<li>Code URL: <a href="https://github.com/Quan-Zhou/Fairness-in-Learning-of-LDS">https://github.com/Quan-Zhou/Fairness-in-Learning-of-LDS</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.05274] Fairness in Forecasting of Observations of Linear Dynamical Systems](http://arxiv.org/abs/2209.05274)</code></li>
<li>Summary: <p>In machine learning, training data often capture the behaviour of multiple
subgroups of some underlying human population. When the nature of training data
for subgroups are not controlled carefully, under-representation bias arises.
To counter this effect we introduce two natural notions of subgroup fairness
and instantaneous fairness to address such under-representation bias in
time-series forecasting problems. Here we show globally convergent methods for
the fairness-constrained learning problems using hierarchies of
convexifications of non-commutative polynomial optimisation problems. Our
empirical results on a biased data set motivated by insurance applications and
the well-known COMPAS data set demonstrate the efficacy of our methods. We also
show that by exploiting sparsity in the convexifications, we can reduce the run
time of our methods considerably.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification. (arXiv:2209.04493v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04493">http://arxiv.org/abs/2209.04493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04493] Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification](http://arxiv.org/abs/2209.04493)</code></li>
<li>Summary: <p>Machine learning methods must be trusted to make appropriate decisions in
real-world environments, even when faced with out-of-distribution (OOD)
samples. Many current approaches simply aim to detect OOD examples and alert
the user when an unrecognized input is given. However, when the OOD sample
significantly overlaps with the training data, a binary anomaly detection is
not interpretable or explainable, and provides little information to the user.
We propose a new model for OOD detection that makes predictions at varying
levels of granularity as the inputs become more ambiguous, the model
predictions become coarser and more conservative. Consider an animal classifier
that encounters an unknown bird species and a car. Both cases are OOD, but the
user gains more information if the classifier recognizes that its uncertainty
over the particular species is too large and predicts bird instead of detecting
it as OOD. Furthermore, we diagnose the classifiers performance at each level
of the hierarchy improving the explainability and interpretability of the
models predictions. We demonstrate the effectiveness of hierarchical
classifiers for both fine- and coarse-grained OOD tasks.
</p></li>
</ul>

<h3>Title: Deep Baseline Network for Time Series Modeling and Anomaly Detection. (arXiv:2209.04561v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04561">http://arxiv.org/abs/2209.04561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04561] Deep Baseline Network for Time Series Modeling and Anomaly Detection](http://arxiv.org/abs/2209.04561)</code></li>
<li>Summary: <p>Deep learning has seen increasing applications in time series in recent
years. For time series anomaly detection scenarios, such as in finance,
Internet of Things, data center operations, etc., time series usually show very
flexible baselines depending on various external factors. Anomalies unveil
themselves by lying far away from the baseline. However, the detection is not
always easy due to some challenges including baseline shifting, lacking of
labels, noise interference, real time detection in streaming data, result
interpretability, etc. In this paper, we develop a novel deep architecture to
properly extract the baseline from time series, namely Deep Baseline Network
(DBLN). By using this deep network, we can easily locate the baseline position
and then provide reliable and interpretable anomaly detection result. Empirical
evaluation on both synthetic and public real-world datasets shows that our
purely unsupervised algorithm achieves superior performance compared with
state-of-art methods and has good practical applications.
</p></li>
</ul>

<h3>Title: Temporal Pattern Mining for Analysis of Longitudinal Clinical Data: Identifying Risk Factors for Alzheimer's Disease. (arXiv:2209.04793v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.04793">http://arxiv.org/abs/2209.04793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.04793] Temporal Pattern Mining for Analysis of Longitudinal Clinical Data: Identifying Risk Factors for Alzheimer's Disease](http://arxiv.org/abs/2209.04793)</code></li>
<li>Summary: <p>A novel framework is proposed for handling the complex task of modelling and
analysis of longitudinal, multivariate, heterogeneous clinical data. This
method uses temporal abstraction to convert the data into a more appropriate
form for modelling, temporal pattern mining, to discover patterns in the
complex, longitudinal data and machine learning models of survival analysis to
select the discovered patterns. The method is applied to a real-world study of
Alzheimer's disease (AD), a progressive neurodegenerative disease that has no
cure. The patterns discovered were predictive of AD in survival analysis models
with a Concordance index of up to 0.8. This is the first work that performs
survival analysis of AD data using temporal data collections for AD. A
visualisation module also provides a clear picture of the discovered patterns
for ease of interpretability.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
