<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-22</h1>
<h3>Title: Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs</h3>
<ul>
<li><strong>Authors: </strong>Miguel Lopez-Duran, Julian Fierrez, Aythami Morales, Ruben Tolosana, Oscar Delgado-Mohatar, Alvaro Ortigosa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14699">https://arxiv.org/abs/2505.14699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14699">https://arxiv.org/pdf/2505.14699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14699]] Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs(https://arxiv.org/abs/2505.14699)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The automatic analysis of document layouts in digital-born PDF documents remains a challenging problem due to the heterogeneous arrangement of textual and nontextual elements and the imprecision of the textual metadata in the Portable Document Format. In this work, we benchmark Graph Neural Network (GNN) architectures for the task of fine-grained layout classification of text blocks from digital native documents. We introduce two graph construction structures: a k-closest-neighbor graph and a fully connected graph, and generate node features via pre-trained text and vision models, thus avoiding manual feature engineering. Three experimental frameworks are evaluated: single-modality (text or visual), concatenated multimodal, and dual-branch multimodal. We evaluated four foundational GNN models and compared them with the baseline. Our experiments are specifically conducted on a rich dataset of public affairs documents that includes more than 20 sources (e.g., regional and national-level official gazettes), 37K PDF documents, with 441K pages in total. Our results demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a dual-branch configuration achieves the highest per-class and overall accuracy, outperforming the baseline in some sources. These findings confirm the importance of local layout relationships and multimodal fusion exploited through GNNs for the analysis of native digital document layouts.</li>
</ul>

<h3>Title: DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance</h3>
<ul>
<li><strong>Authors: </strong>Xuan Shen, Chenxia Han, Yufa Zhou, Yanyue Xie, Yifan Gong, Quanyi Wang, Yiwei Wang, Yanzhi Wang, Pu Zhao, Jiuxiang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14708">https://arxiv.org/abs/2505.14708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14708">https://arxiv.org/pdf/2505.14708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14708]] DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance(https://arxiv.org/abs/2505.14708)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion transformer-based video generation models (DiTs) have recently attracted widespread attention for their excellent generation quality. However, their computational cost remains a major bottleneck-attention alone accounts for over 80% of total latency, and generating just 8 seconds of 720p video takes tens of minutes-posing serious challenges to practical application and scalability. To address this, we propose the DraftAttention, a training-free framework for the acceleration of video diffusion transformers with dynamic sparse attention on GPUs. We apply down-sampling to each feature map across frames in the compressed latent space, enabling a higher-level receptive field over the latent composed of hundreds of thousands of tokens. The low-resolution draft attention map, derived from draft query and key, exposes redundancy both spatially within each feature map and temporally across frames. We reorder the query, key, and value based on the draft attention map to guide the sparse attention computation in full resolution, and subsequently restore their original order after the attention computation. This reordering enables structured sparsity that aligns with hardware-optimized execution. Our theoretical analysis demonstrates that the low-resolution draft attention closely approximates the full attention, providing reliable guidance for constructing accurate sparse attention. Experimental results show that our method outperforms existing sparse attention approaches in video generation quality and achieves up to 1.75x end-to-end speedup on GPUs. Code: this https URL</li>
</ul>

<h3>Title: KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Tuan-Vinh La, Minh-Hieu Nguyen, Minh-Son Dao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14714">https://arxiv.org/abs/2505.14714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14714">https://arxiv.org/pdf/2505.14714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14714]] KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection(https://arxiv.org/abs/2505.14714)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fake news detection remains a challenging problem due to the complex interplay between textual misinformation, manipulated images, and external knowledge reasoning. While existing approaches have achieved notable results in verifying veracity and cross-modal consistency, two key challenges persist: (1) Existing methods often consider only the global image context while neglecting local object-level details, and (2) they fail to incorporate external knowledge and entity relationships for deeper semantic understanding. To address these challenges, we propose a novel multi-modal fake news detection framework that integrates visual, textual, and knowledge-based representations. Our approach leverages bottom-up attention to capture fine-grained object details, CLIP for global image semantics, and RoBERTa for context-aware text encoding. We further enhance knowledge utilization by retrieving and adaptively selecting relevant entities from a knowledge graph. The fused multi-modal features are processed through a Transformer-based classifier to predict news veracity. Experimental results demonstrate that our model outperforms recent approaches, showcasing the effectiveness of neighbor selection mechanism and multi-modal fusion for fake news detection. Our proposal introduces a new paradigm: knowledge-grounded multimodal reasoning. By integrating explicit entity-level selection and NLI-guided filtering, we shift fake news detection from feature fusion to semantically grounded verification. For reproducibility and further research, the source code is publicly at \href{this https URL}{this http URL}.</li>
</ul>

<h3>Title: Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection</h3>
<ul>
<li><strong>Authors: </strong>Guoxuan Mao, Ting Cao, Ziyang Li, Yuan Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14718">https://arxiv.org/abs/2505.14718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14718">https://arxiv.org/pdf/2505.14718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14718]] Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection(https://arxiv.org/abs/2505.14718)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation stands as a pivotal research focus in computer vision. In the context of industrial image inspection, conventional semantic segmentation models fail to maintain the segmentation consistency of fixed components across varying contextual environments due to a lack of perception of object contours. Given the real-time constraints and limited computing capability of industrial image detection machines, it is also necessary to create efficient models to reduce computational complexity. In this work, a Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes of objects to achieve excellent segmentation consistency by separately supervising the extraction of boundary and body information from images. In SPENet, a novel method is introduced for describing fuzzy boundaries to better adapt to real-world scenarios named Variable Boundary Domain (VBD). Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to measure segmentation consistency for fixed components. Our approach attains the best segmentation accuracy and competitive speed on our dataset, showcasing significant advantages in CMSE among numerous state-of-the-art real-time segmentation networks, achieving a reduction of over 50% compared to the previously top-performing models.</li>
</ul>

<h3>Title: MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion</h3>
<ul>
<li><strong>Authors: </strong>Wei Hua, Chenlin Zhou, Jibin Wu, Yansong Chua, Yangyang Shu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14719">https://arxiv.org/abs/2505.14719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14719">https://arxiv.org/pdf/2505.14719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14719]] MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion(https://arxiv.org/abs/2505.14719)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The combination of Spiking Neural Networks(SNNs) with Vision Transformer architectures has attracted significant attention due to the great potential for energy-efficient and high-performance computing paradigms. However, a substantial performance gap still exists between SNN-based and ANN-based transformer architectures. While existing methods propose spiking self-attention mechanisms that are successfully combined with SNNs, the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting features from different image scales. In this paper, we address this issue and propose MSVIT, a novel spike-driven Transformer architecture, which firstly uses multi-scale spiking attention (MSSA) to enrich the capability of spiking attention blocks. We validate our approach across various main data sets. The experimental results show that MSVIT outperforms existing SNN-based models, positioning itself as a state-of-the-art solution among SNN-transformer architectures. The codes are available at this https URL.</li>
</ul>

<h3>Title: The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Rubyet Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14727">https://arxiv.org/abs/2505.14727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14727">https://arxiv.org/pdf/2505.14727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14727]] The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents(https://arxiv.org/abs/2505.14727)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.</li>
</ul>

<h3>Title: The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute</h3>
<ul>
<li><strong>Authors: </strong>Yunho Jin, Gu-Yeon Wei, David Brooks</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14733">https://arxiv.org/abs/2505.14733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14733">https://arxiv.org/pdf/2505.14733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14733]] The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute(https://arxiv.org/abs/2505.14733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling large language models (LLMs) has driven significant advancements, yet it faces diminishing returns and escalating energy demands. This work introduces test-time compute (TTC)-allocating additional computational resources during inference-as a compelling complement to conventional scaling strategies. Specifically, we investigate whether employing TTC can achieve superior accuracy-energy trade-offs compared to simply increasing model size. Our empirical analysis reveals that TTC surpasses traditional model scaling in accuracy/energy efficiency, with notable gains in tasks demanding complex reasoning rather than mere factual recall. Further, we identify a critical interaction between TTC performance and output sequence length, demonstrating that strategically adjusting compute resources at inference time according to query complexity can substantially enhance efficiency. Our findings advocate for TTC as a promising direction, enabling more sustainable, accurate, and adaptable deployment of future language models without incurring additional pretraining costs.</li>
</ul>

<h3>Title: Leveraging Multivariate Long-Term History Representation for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Huiliang Zhang, Di Wu, Arnaud Zinflou, Stephane Dellacherie, Mouhamadou Makhtar Dione, Benoit Boulet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14737">https://arxiv.org/abs/2505.14737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14737">https://arxiv.org/pdf/2505.14737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14737]] Leveraging Multivariate Long-Term History Representation for Time Series Forecasting(https://arxiv.org/abs/2505.14737)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multivariate Time Series (MTS) forecasting has a wide range of applications in both industry and academia. Recent advances in Spatial-Temporal Graph Neural Network (STGNN) have achieved great progress in modelling spatial-temporal correlations. Limited by computational complexity, most STGNNs for MTS forecasting focus primarily on short-term and local spatial-temporal dependencies. Although some recent methods attempt to incorporate univariate history into modeling, they still overlook crucial long-term spatial-temporal similarities and correlations across MTS, which are essential for accurate forecasting. To fill this gap, we propose a framework called the Long-term Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting. Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively encode the long-term history into segment-level contextual representations and reduce point-level noise. A non-parametric Hierarchical Representation Retriever (HRetriever) is designed to include the spatial information in the long-term spatial-temporal dependency modelling and pick out the most valuable representations with no additional training. A Transformer-based Aggregator (TAggregator) selectively fuses the sparsely retrieved contextual representations based on the ranking positional embedding efficiently. Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72% on the average prediction horizons and state-of-the-art methods by 4.12% on several real-world datasets. Additionally, it consistently improves prediction accuracy by 9.8% on the top 10% of rapidly changing patterns across the datasets.</li>
</ul>

<h3>Title: Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs</h3>
<ul>
<li><strong>Authors: </strong>Heiko Oppel, Andreas Spilz, Michael Munz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14739">https://arxiv.org/abs/2505.14739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14739">https://arxiv.org/pdf/2505.14739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14739]] Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs(https://arxiv.org/abs/2505.14739)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Denoising diffusion probabilistic models are able to generate synthetic sensor signals. The training process of such a model is controlled by a loss function which measures the difference between the noise that was added in the forward process and the noise that was predicted by the diffusion model. This enables the generation of realistic data. However, the randomness within the process and the loss function itself makes it difficult to estimate the quality of the data. Therefore, we examine multiple similarity metrics and adapt an existing metric to overcome this issue by monitoring the training and synthetisation process using those metrics. The adapted metric can even be fine-tuned on the input data to comply with the requirements of an underlying classification task. We were able to significantly reduce the amount of training epochs without a performance reduction in the classification task. An optimized training process not only saves resources, but also reduces the time for training generative models.</li>
</ul>

<h3>Title: Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Kunyun Wang, Bohan Li, Kai Yu, Minyi Guo, Jieru Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14741">https://arxiv.org/abs/2505.14741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14741">https://arxiv.org/pdf/2505.14741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14741]] Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism(https://arxiv.org/abs/2505.14741)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful class of generative models across various modalities, including image, video, and audio synthesis. However, their deployment is often limited by significant inference latency, primarily due to the inherently sequential nature of the denoising process. While existing parallelization strategies attempt to accelerate inference by distributing computation across multiple devices, they typically incur high communication overhead, hindering deployment on commercial hardware. To address this challenge, we propose \textbf{ParaStep}, a novel parallelization method based on a reuse-then-predict mechanism that parallelizes diffusion inference by exploiting similarity between adjacent denoising steps. Unlike prior approaches that rely on layer-wise or stage-wise communication, ParaStep employs lightweight, step-wise communication, substantially reducing overhead. ParaStep achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD, \textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on AudioLDM2-large, while maintaining generation quality. These results highlight ParaStep as a scalable and communication-efficient solution for accelerating diffusion inference, particularly in bandwidth-constrained environments.</li>
</ul>

<h3>Title: Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis</h3>
<ul>
<li><strong>Authors: </strong>Hong Huang, Dapeng Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14742">https://arxiv.org/abs/2505.14742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14742">https://arxiv.org/pdf/2505.14742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14742]] Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis(https://arxiv.org/abs/2505.14742)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made exciting achievements across various domains, yet their deployment on resource-constrained personal devices remains hindered by the prohibitive computational and memory demands of task-specific fine-tuning. While quantization offers a pathway to efficiency, existing methods struggle to balance performance and overhead, either incurring high computational/memory costs or failing to address activation outliers, a critical bottleneck in quantized fine-tuning. To address these challenges, we propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning, certain activation outlier channels retain stable spatial positions across training iterations. Building on OSSH, we propose Quaff, a Quantized parameter-efficient fine-tuning framework for LLMs, optimizing low-precision activation representations through targeted momentum scaling. Quaff dynamically suppresses outliers exclusively in invariant channels using lightweight operations, eliminating full-precision weight storage and global rescaling while reducing quantization errors. Extensive experiments across ten benchmarks validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory savings over full-precision fine-tuning while improving accuracy by 0.6% on the Phi-3 model, reconciling the triple trade-off between efficiency, performance, and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080 Super) without sacrificing model utility, Quaff democratizes personalized LLM deployment. The code is available at this https URL.</li>
</ul>

<h3>Title: Cooperative Causal GraphSAGE</h3>
<ul>
<li><strong>Authors: </strong>Zaifa Xue, Tao Zhang, Tuo Xu, Huaixin Liang, Le Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14748">https://arxiv.org/abs/2505.14748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14748">https://arxiv.org/pdf/2505.14748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14748]] Cooperative Causal GraphSAGE(https://arxiv.org/abs/2505.14748)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>GraphSAGE is a widely used graph neural network. The introduction of causal inference has improved its robust performance and named as Causal GraphSAGE. However, Causal GraphSAGE focuses on measuring causal weighting among individual nodes, but neglecting the cooperative relationships among sampling nodes as a whole. To address this issue, this paper proposes Cooperative Causal GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal GraphSAGE. Initially, a cooperative causal structure model is constructed in the case of cooperation based on the graph structure. Subsequently, Cooperative Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley values to calculate the cooperative contribution based on causal weights of the nodes sets. CoCa-sampling guides the selection of nodes with significant cooperative causal effects during the neighborhood sampling process, thus integrating the selected neighborhood features under cooperative relationships, which takes the sampled nodes as a whole and generates more stable target node embeddings. Experiments on publicly available datasets show that the proposed method has comparable classification performance to the compared methods and outperforms under perturbations, demonstrating the robustness improvement by CoCa-sampling.</li>
</ul>

<h3>Title: Large Language Models for Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yihong Tang, Menglin Kong, Lijun Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14752">https://arxiv.org/abs/2505.14752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14752">https://arxiv.org/pdf/2505.14752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14752]] Large Language Models for Data Synthesis(https://arxiv.org/abs/2505.14752)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.</li>
</ul>

<h3>Title: $\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chih-Yu Chang, Milad Azvar, Chinedum Okwudire, Raed Al Kontar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14756">https://arxiv.org/abs/2505.14756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14756">https://arxiv.org/pdf/2505.14756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14756]] $\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization(https://arxiv.org/abs/2505.14756)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) is a sequential decision-making tool widely used for optimizing expensive black-box functions. Recently, Large Language Models (LLMs) have shown remarkable adaptability in low-data regimes, making them promising tools for black-box optimization by leveraging contextual knowledge to propose high-quality query points. However, relying solely on LLMs as optimization agents introduces risks due to their lack of explicit surrogate modeling and calibrated uncertainty, as well as their inherently opaque internal mechanisms. This structural opacity makes it difficult to characterize or control the exploration-exploitation trade-off, ultimately undermining theoretical tractability and reliability. To address this, we propose LLINBO: LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with statistical surrogate experts (e.g., Gaussian Processes (GP)). The core philosophy is to leverage contextual reasoning strengths of LLMs for early exploration, while relying on principled statistical models to guide efficient exploitation. Specifically, we introduce three mechanisms that enable this collaboration and establish their theoretical guarantees. We end the paper with a real-life proof-of-concept in the context of 3D printing. The code to reproduce the results can be found at this https URL.</li>
</ul>

<h3>Title: KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches</h3>
<ul>
<li><strong>Authors: </strong>Mingquan Feng, Yixin Huang, Yifan Fu, Shaobo Wang, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14777">https://arxiv.org/abs/2505.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14777">https://arxiv.org/pdf/2505.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14777]] KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches(https://arxiv.org/abs/2505.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The design of optimization algorithms for neural networks remains a critical challenge, with most existing methods relying on heuristic adaptations of gradient-based approaches. This paper introduces KO (Kinetics-inspired Optimizer), a novel neural optimizer inspired by kinetic theory and partial differential equation (PDE) simulations. We reimagine the training dynamics of network parameters as the evolution of a particle system governed by kinetic principles, where parameter updates are simulated via a numerical scheme for the Boltzmann transport equation (BTE) that models stochastic particle collisions. This physics-driven approach inherently promotes parameter diversity during optimization, mitigating the phenomenon of parameter condensation, i.e. collapse of network parameters into low-dimensional subspaces, through mechanisms analogous to thermal diffusion in physical systems. We analyze this property, establishing both a mathematical proof and a physical interpretation. Extensive experiments on image classification (CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam, SGD), achieving accuracy improvements while computation cost remains comparable.</li>
</ul>

<h3>Title: Efficient Privacy-Preserving Cross-Silo Federated Learning with Multi-Key Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Al Omar, Xin Yang, Euijin Choo, Omid Ardakanian</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14797">https://arxiv.org/abs/2505.14797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14797">https://arxiv.org/pdf/2505.14797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14797]] Efficient Privacy-Preserving Cross-Silo Federated Learning with Multi-Key Homomorphic Encryption(https://arxiv.org/abs/2505.14797)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is susceptible to privacy attacks, such as data reconstruction attacks, in which a semi-honest server or a malicious client infers information about other clients' datasets from their model updates or gradients. To enhance the privacy of FL, recent studies combined Multi-Key Homomorphic Encryption (MKHE) and FL, making it possible to aggregate the encrypted model updates using different keys without having to decrypt them. Despite the privacy guarantees of MKHE, existing approaches are not well-suited for real-world deployment due to their high computation and communication overhead. We propose MASER, an efficient MKHE-based Privacy-Preserving FL framework that combines consensus-based model pruning and slicing techniques to reduce this overhead. Our experimental results show that MASER is 3.03 to 8.29 times more efficient than existing MKHE-based FL approaches in terms of computation and communication overhead while maintaining comparable classification accuracy to standard FL algorithms. Compared to a vanilla FL algorithm, the overhead of MASER is only 1.48 to 5 times higher, striking a good balance between privacy, accuracy, and efficiency in both IID and non-IID settings.</li>
</ul>

<h3>Title: Text embedding models can be great data engineers</h3>
<ul>
<li><strong>Authors: </strong>Iman Kazemian, Paritosh Ramanan, Murat Yildirim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14802">https://arxiv.org/abs/2505.14802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14802">https://arxiv.org/pdf/2505.14802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14802]] Text embedding models can be great data engineers(https://arxiv.org/abs/2505.14802)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Data engineering pipelines are essential - albeit costly - components of predictive analytics frameworks requiring significant engineering time and domain expertise for carrying out tasks such as data ingestion, preprocessing, feature extraction, and feature engineering. In this paper, we propose ADEPT, an automated data engineering pipeline via text embeddings. At the core of the ADEPT framework is a simple yet powerful idea that the entropy of embeddings corresponding to textually dense raw format representation of time series can be intuitively viewed as equivalent (or in many cases superior) to that of numerically dense vector representations obtained by data engineering pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text embeddings to represent the diverse data sources, and (ii) constructs a variational information bottleneck criteria to mitigate entropy variance in text embeddings of time series data. ADEPT provides an end-to-end automated implementation of predictive models that offers superior predictive performance despite issues such as missing data, ill-formed records, improper or corrupted data formats and irregular timestamps. Through exhaustive experiments, we show that the ADEPT outperforms the best existing benchmarks in a diverse set of datasets from large-scale applications across healthcare, finance, science and industrial internet of things. Our results show that ADEPT can potentially leapfrog many conventional data pipeline steps thereby paving the way for efficient and scalable automation pathways for diverse data science applications.</li>
</ul>

<h3>Title: SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yu Liu, Weiyao Tao, Tong Xia, Simon Knight, Tingting Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14803">https://arxiv.org/abs/2505.14803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14803">https://arxiv.org/pdf/2505.14803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14803]] SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis(https://arxiv.org/abs/2505.14803)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Survival analysis, which estimates the probability of event occurrence over time from censored data, is fundamental in numerous real-world applications, particularly in high-stakes domains such as healthcare and risk assessment. Despite advances in numerous survival models, quantifying the uncertainty of predictions from these models remains underexplored and challenging. The lack of reliable uncertainty quantification limits the interpretability and trustworthiness of survival models, hindering their adoption in clinical decision-making and other sensitive applications. To bridge this gap, in this work, we introduce SurvUnc, a novel meta-model based framework for post-hoc uncertainty quantification for survival models. SurvUnc introduces an anchor-based learning strategy that integrates concordance knowledge into meta-model optimization, leveraging pairwise ranking performance to estimate uncertainty effectively. Notably, our framework is model-agnostic, ensuring compatibility with any survival model without requiring modifications to its architecture or access to its internal parameters. Especially, we design a comprehensive evaluation pipeline tailored to this critical yet overlooked problem. Through extensive experiments on four publicly available benchmarking datasets and five representative survival models, we demonstrate the superiority of SurvUnc across multiple evaluation scenarios, including selective prediction, misprediction detection, and out-of-domain detection. Our results highlight the effectiveness of SurvUnc in enhancing model interpretability and reliability, paving the way for more trustworthy survival predictions in real-world applications.</li>
</ul>

<h3>Title: Automated Journalistic Questions: A New Method for Extracting 5W1H in French</h3>
<ul>
<li><strong>Authors: </strong>Richard Khoury, Maxence Verhaverbeke, Julie A. Gramaccia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14804">https://arxiv.org/abs/2505.14804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14804">https://arxiv.org/pdf/2505.14804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14804]] Automated Journalistic Questions: A New Method for Extracting 5W1H in French(https://arxiv.org/abs/2505.14804)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The 5W1H questions - who, what, when, where, why and how - are commonly used in journalism to ensure that an article describes events clearly and systematically. An- swering them is a crucial prerequisites for tasks such as summarization, clustering, and news aggregation. In this paper, we design the first automated extraction pipeline to get 5W1H information from French news articles. To evaluate the performance of our algo- rithm, we also create a corpus of 250 Quebec news articles with 5W1H answers marked by four human annotators. Our results demonstrate that our pipeline performs as well in this task as the large language model GPT-4o.</li>
</ul>

<h3>Title: Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14810">https://arxiv.org/abs/2505.14810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14810">https://arxiv.org/pdf/2505.14810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14810]] Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models(https://arxiv.org/abs/2505.14810)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at this https URL.</li>
</ul>

<h3>Title: WebNovelBench: Placing LLM Novelists on the Web Novel Distribution</h3>
<ul>
<li><strong>Authors: </strong>Leon Lin, Jun Zheng, Haidong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14818">https://arxiv.org/abs/2505.14818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14818">https://arxiv.org/pdf/2505.14818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14818]] WebNovelBench: Placing LLM Novelists on the Web Novel Distribution(https://arxiv.org/abs/2505.14818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.</li>
</ul>

<h3>Title: Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Runze Zhao, Yue Yu, Adams Yiyue Zhu, Chen Yang, Dongruo Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14821">https://arxiv.org/abs/2505.14821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14821">https://arxiv.org/pdf/2505.14821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14821]] Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation(https://arxiv.org/abs/2505.14821)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Continuous-time reinforcement learning (CTRL) provides a principled framework for sequential decision-making in environments where interactions evolve continuously over time. Despite its empirical success, the theoretical understanding of CTRL remains limited, especially in settings with general function approximation. In this work, we propose a model-based CTRL algorithm that achieves both sample and computational efficiency. Our approach leverages optimism-based confidence sets to establish the first sample complexity guarantee for CTRL with general function approximation, showing that a near-optimal policy can be learned with a suboptimality gap of $\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$ measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the distributional Eluder dimensions of the reward and dynamic functions, respectively, capturing the complexity of general function approximation in reinforcement learning. Moreover, we introduce structured policy updates and an alternative measurement strategy that significantly reduce the number of policy updates and rollouts while maintaining competitive sample efficiency. We implemented experiments to backup our proposed algorithms on continuous control tasks and diffusion model fine-tuning, demonstrating comparable performance with significantly fewer policy updates and rollouts.</li>
</ul>

<h3>Title: Tracing Multilingual Factual Knowledge Acquisition in Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14824">https://arxiv.org/abs/2505.14824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14824">https://arxiv.org/pdf/2505.14824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14824]] Tracing Multilingual Factual Knowledge Acquisition in Pretraining(https://arxiv.org/abs/2505.14824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are capable of recalling multilingual factual knowledge present in their pretraining data. However, most studies evaluate only the final model, leaving the development of factual recall and crosslingual consistency throughout pretraining largely unexplored. In this work, we trace how factual recall and crosslingual consistency evolve during pretraining, focusing on OLMo-7B as a case study. We find that both accuracy and consistency improve over time for most languages. We show that this improvement is primarily driven by the fact frequency in the pretraining corpus: more frequent facts are more likely to be recalled correctly, regardless of language. Yet, some low-frequency facts in non-English languages can still be correctly recalled. Our analysis reveals that these instances largely benefit from crosslingual transfer of their English counterparts -- an effect that emerges predominantly in the early stages of pretraining. We pinpoint two distinct pathways through which multilingual factual knowledge acquisition occurs: (1) frequency-driven learning, which is dominant and language-agnostic, and (2) crosslingual transfer, which is limited in scale and typically constrained to relation types involving named entities. We release our code and data to facilitate further research at this https URL.</li>
</ul>

<h3>Title: FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain</h3>
<ul>
<li><strong>Authors: </strong>Rohan Deb, Kiran Thekumparampil, Kousha Kalantari, Gaurush Hiranandani, Shoham Sabach, Branislav Kveton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14826">https://arxiv.org/abs/2505.14826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14826">https://arxiv.org/pdf/2505.14826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14826]] FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain(https://arxiv.org/abs/2505.14826)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a standard approach to adapting large language models (LLMs) to new domains. In this work, we improve the statistical efficiency of SFT by selecting an informative subset of training examples. Specifically, for a fixed budget of training examples, which determines the computational cost of fine-tuning, we determine the most informative ones. The key idea in our method is to select examples that maximize information gain, measured by the Hessian of the log-likelihood of the LLM. We approximate it efficiently by linearizing the LLM at the last layer using multinomial logistic regression models. Our approach is computationally efficient, analyzable, and performs well empirically. We demonstrate this on several problems, and back our claims with both quantitative results and an LLM evaluation.</li>
</ul>

<h3>Title: SEPS: A Separability Measure for Robust Unlearning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wonje Jeung, Sangyeon Yoon, Albert No</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14832">https://arxiv.org/abs/2505.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14832">https://arxiv.org/pdf/2505.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14832]] SEPS: A Separability Measure for Robust Unlearning in LLMs(https://arxiv.org/abs/2505.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.</li>
</ul>

<h3>Title: Robust and Efficient AI-Based Attack Recovery in Autonomous Drones</h3>
<ul>
<li><strong>Authors: </strong>Diego Ortiz Barbosa, Luis Burbano, Siwei Yang, Zijun Wang, Alvaro A. Cardenas, Cihang Xie, Yinzhi Cao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14835">https://arxiv.org/abs/2505.14835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14835">https://arxiv.org/pdf/2505.14835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14835]] Robust and Efficient AI-Based Attack Recovery in Autonomous Drones(https://arxiv.org/abs/2505.14835)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust</a></li>
<li><strong>Abstract: </strong>We introduce an autonomous attack recovery architecture to add common sense reasoning to plan a recovery action after an attack is detected. We outline use-cases of our architecture using drones, and then discuss how to implement this architecture efficiently and securely in edge devices.</li>
</ul>

<h3>Title: Subquadratic Algorithms and Hardness for Attention with Any Temperature</h3>
<ul>
<li><strong>Authors: </strong>Shreya Gupta, Boyang Huang, Barna Saha, Yinzhan Xu, Christopher Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14840">https://arxiv.org/abs/2505.14840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14840">https://arxiv.org/pdf/2505.14840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14840]] Subquadratic Algorithms and Hardness for Attention with Any Temperature(https://arxiv.org/abs/2505.14840)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite the popularity of the Transformer architecture, the standard algorithm for computing Attention suffers from quadratic time complexity in context length $n$. Alman and Song [NeurIPS 2023] showed that when the head dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$). Equivalently, subquadratic Attention is possible if and only if the softmax is applied with high temperature for $d=\Theta(\log n)$. Running times of these algorithms depend exponentially on $B$ and thus they do not lead to even a polynomial-time algorithm outside the specific range of $B$. This naturally leads to the question: when can Attention be computed efficiently without strong assumptions on temperature? Are there fast attention algorithms that scale polylogarithmically with entry size $B$? In this work, we resolve this question and characterize when fast Attention for arbitrary temperatures is possible. First, for all constant $d = O(1)$, we give the first subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm for Attention with large $B$. Our result holds even for matrices with large head dimension if they have low rank. In this regime, we also give a similar running time for Attention gradient computation, and therefore for the full LLM training process. Furthermore, we show that any substantial improvement on our algorithm is unlikely. In particular, we show that even when $d = 2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under $\mathsf{SETH}$. Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the standard algorithm is optimal under popular fine-grained complexity assumptions.</li>
</ul>

<h3>Title: Leveraging Generative AI Models to Explore Human Identity</h3>
<ul>
<li><strong>Authors: </strong>Yunha Yeo, Daeho Um</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14843">https://arxiv.org/abs/2505.14843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14843">https://arxiv.org/pdf/2505.14843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14843]] Leveraging Generative AI Models to Explore Human Identity(https://arxiv.org/abs/2505.14843)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper attempts to explore human identity by utilizing neural networks in an indirect manner. For this exploration, we adopt diffusion models, state-of-the-art AI generative models trained to create human face images. By relating the generated human face to human identity, we establish a correspondence between the face image generation process of the diffusion model and the process of human identity formation. Through experiments with the diffusion model, we observe that changes in its external input result in significant changes in the generated face image. Based on the correspondence, we indirectly confirm the dependence of human identity on external factors in the process of human identity formation. Furthermore, we introduce \textit{Fluidity of Human Identity}, a video artwork that expresses the fluid nature of human identity affected by varying external factors. The video is available at this https URL.</li>
</ul>

<h3>Title: A Comparative Study of Large Language Models and Human Personality Traits</h3>
<ul>
<li><strong>Authors: </strong>Wang Jiaqi, Wang bo, Guo fa, Cheng cheng, Yang li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14845">https://arxiv.org/abs/2505.14845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14845">https://arxiv.org/pdf/2505.14845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14845]] A Comparative Study of Large Language Models and Human Personality Traits(https://arxiv.org/abs/2505.14845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.</li>
</ul>

<h3>Title: MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Jiaqian Hu, Safinah Ali</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14848">https://arxiv.org/abs/2505.14848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14848">https://arxiv.org/pdf/2505.14848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14848]] MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation(https://arxiv.org/abs/2505.14848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction. Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity.</li>
</ul>

<h3>Title: Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14871">https://arxiv.org/abs/2505.14871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14871">https://arxiv.org/pdf/2505.14871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14871]] Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models(https://arxiv.org/abs/2505.14871)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: A self-regulated convolutional neural network for classifying variable stars</h3>
<ul>
<li><strong>Authors: </strong>Francisco Pérez-Galarce, Jorge Martínez-Palomera, Karim Pichara, Pablo Huijse, Márcio Catelan</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14877">https://arxiv.org/abs/2505.14877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14877">https://arxiv.org/pdf/2505.14877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14877]] A self-regulated convolutional neural network for classifying variable stars(https://arxiv.org/abs/2505.14877)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Over the last two decades, machine learning models have been widely applied and have proven effective in classifying variable stars, particularly with the adoption of deep learning architectures such as convolutional neural networks, recurrent neural networks, and transformer models. While these models have achieved high accuracy, they require high-quality, representative data and a large number of labelled samples for each star type to generalise well, which can be challenging in time-domain surveys. This challenge often leads to models learning and reinforcing biases inherent in the training data, an issue that is not easily detectable when validation is performed on subsamples from the same catalogue. The problem of biases in variable star data has been largely overlooked, and a definitive solution has yet to be established. In this paper, we propose a new approach to improve the reliability of classifiers in variable star classification by introducing a self-regulated training process. This process utilises synthetic samples generated by a physics-enhanced latent space variational autoencoder, incorporating six physical parameters from Gaia Data Release 3. Our method features a dynamic interaction between a classifier and a generative model, where the generative model produces ad-hoc synthetic light curves to reduce confusion during classifier training and populate underrepresented regions in the physical parameter space. Experiments conducted under various scenarios demonstrate that our self-regulated training approach outperforms traditional training methods for classifying variable stars on biased datasets, showing statistically significant improvements.</li>
</ul>

<h3>Title: Incorporating Token Usage into Prompting Strategy Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Chris Sypherd, Sergei Petrov, Sonny George, Vaishak Belle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14880">https://arxiv.org/abs/2505.14880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14880">https://arxiv.org/pdf/2505.14880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14880]] Incorporating Token Usage into Prompting Strategy Evaluation(https://arxiv.org/abs/2505.14880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models have demonstrated remarkable performance across diverse tasks. However, their task effectiveness is heavily dependent on the prompting strategy used to elicit output, which can vary widely in both performance and token usage. While task performance is often used to determine prompting strategy success, we argue that efficiency--balancing performance and token usage--can be a more practical metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a theoretical framework for describing the token usage growth of prompting strategies, and analyze Token Cost, an empirical measure of tokens per performance. We apply these to several common prompting strategies and find that increased token usage leads to drastically diminishing performance returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need for efficiency-aware evaluations.</li>
</ul>

<h3>Title: An active learning framework for multi-group mean estimation</h3>
<ul>
<li><strong>Authors: </strong>Abdellah Aznag, Rachel Cummings, Adam N. Elmachtoub</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14882">https://arxiv.org/abs/2505.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14882">https://arxiv.org/pdf/2505.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14882]] An active learning framework for multi-group mean estimation(https://arxiv.org/abs/2505.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We study a fundamental learning problem over multiple groups with unknown data distributions, where an analyst would like to learn the mean of each group. Moreover, we want to ensure that this data is collected in a relatively fair manner such that the noise of the estimate of each group is reasonable. In particular, we focus on settings where data are collected dynamically, which is important in adaptive experimentation for online platforms or adaptive clinical trials for healthcare. In our model, we employ an active learning framework to sequentially collect samples with bandit feedback, observing a sample in each period from the chosen group. After observing a sample, the analyst updates their estimate of the mean and variance of that group and chooses the next group accordingly. The analyst's objective is to dynamically collect samples to minimize the collective noise of the estimators, measured by the norm of the vector of variances of the mean estimators. We propose an algorithm, Variance-UCB, that sequentially selects groups according to an upper confidence bound on the variance estimate. We provide a general theoretical framework for providing efficient bounds on learning from any underlying distribution where the variances can be estimated reasonably. This framework yields upper bounds on regret that improve significantly upon all existing bounds, as well as a collection of new results for different objectives and distributions than those previously studied.</li>
</ul>

<h3>Title: Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14884">https://arxiv.org/abs/2505.14884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14884">https://arxiv.org/pdf/2505.14884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14884]] Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity(https://arxiv.org/abs/2505.14884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop hardware-efficient, sparsity-aware GPU kernels for selective MLP and Attention computations, delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2 \& 3, across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters</h3>
<ul>
<li><strong>Authors: </strong>Danqing Wang, Zhuorui Ye, Xinran Zhao, Fei Fang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14886">https://arxiv.org/abs/2505.14886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14886">https://arxiv.org/pdf/2505.14886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14886]] Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters(https://arxiv.org/abs/2505.14886)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Winning competitive debates requires sophisticated reasoning and argument skills. There are unique challenges in the competitive debate: (1) The time constraints force debaters to make strategic choices about which points to pursue rather than covering all possible arguments; (2) The persuasiveness of the debate relies on the back-and-forth interaction between arguments, which a single final game status cannot evaluate. To address these challenges, we propose TreeDebater, a novel debate framework that excels in competitive debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the strength of the claim, while the Debate Flow Tree tracks the debate status to identify the active actions. TreeDebater allocates its time budget among candidate actions and uses the speech time controller and feedback from the simulated audience to revise its statement. The human evaluation on both the stage-level and the debate-level comparison shows that our TreeDebater outperforms the state-of-the-art multi-agent debate system. Further investigation shows that TreeDebater shows better strategies in limiting time to important debate actions, aligning with the strategies of human debate experts.</li>
</ul>

<h3>Title: In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties</h3>
<ul>
<li><strong>Authors: </strong>Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14887">https://arxiv.org/abs/2505.14887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14887">https://arxiv.org/pdf/2505.14887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14887]] In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties(https://arxiv.org/abs/2505.14887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human listeners readily adjust to unfamiliar speakers and language varieties through exposure, but do these adaptation benefits extend to state-of-the-art spoken language models? We introduce a scalable framework that allows for in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts and audio-text pairs, and find that as few as 12 example utterances (~50 seconds) at inference time reduce word error rates by a relative 19.7% (1.2 pp.) on average across diverse English corpora. These improvements are most pronounced in low-resource varieties, when the context and target speaker match, and when more examples are provided--though scaling our procedure yields diminishing marginal returns to context length. Overall, we find that our novel ICL adaptation scheme (1) reveals a similar performance profile to human listeners, and (2) demonstrates consistent improvements to automatic speech recognition (ASR) robustness across diverse speakers and language backgrounds. While adaptation succeeds broadly, significant gaps remain for certain varieties, revealing where current models still fall short of human flexibility. We release our prompts and code on GitHub.</li>
</ul>

<h3>Title: On the (in)security of Proofs-of-Space based Longest-Chain Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Mirza Ahad Baig, Krzysztof Pietrzak</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14891">https://arxiv.org/abs/2505.14891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14891">https://arxiv.org/pdf/2505.14891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14891]] On the (in)security of Proofs-of-Space based Longest-Chain Blockchains(https://arxiv.org/abs/2505.14891)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The Nakamoto consensus protocol underlying the Bitcoin blockchain uses proof of work as a voting mechanism. Honest miners who contribute hashing power towards securing the chain try to extend the longest chain they are aware of. Despite its simplicity, Nakamoto consensus achieves meaningful security guarantees assuming that at any point in time, a majority of the hashing power is controlled by honest parties. This also holds under ``resource variability'', i.e., if the total hashing power varies greatly over time. Proofs of space (PoSpace) have been suggested as a more sustainable replacement for proofs of work. Unfortunately, no construction of a ``longest-chain'' blockchain based on PoSpace, that is secure under dynamic availability, is known. In this work, we prove that without additional assumptions no such protocol exists. We exactly quantify this impossibility result by proving a bound on the length of the fork required for double spending as a function of the adversarial capabilities. This bound holds for any chain selection rule, and we also show a chain selection rule (albeit a very strange one) that almost matches this bound. Concretely, we consider a security game in which the honest parties at any point control $\phi>1$ times more space than the adversary. The adversary can change the honest space by a factor $1\pm \varepsilon$ with every block (dynamic availability), and ``replotting'' the space takes as much time as $\rho$ blocks. We prove that no matter what chain selection rule is used, in this game the adversary can create a fork of length $\phi^2\cdot \rho / \varepsilon$ that will be picked as the winner by the chain selection rule. We also provide an upper bound that matches the lower bound up to a factor $\phi$. There exists a chain selection rule which in the above game requires forks of length at least $\phi\cdot \rho / \varepsilon$.</li>
</ul>

<h3>Title: Scaling Laws for State Dynamics in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14892">https://arxiv.org/abs/2505.14892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14892">https://arxiv.org/pdf/2505.14892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14892]] Scaling Laws for State Dynamics in Large Language Models(https://arxiv.org/abs/2505.14892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in tasks requiring internal state tracking, yet their ability to model state transition dynamics remains poorly understood. We evaluate how well LLMs capture deterministic state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, each formalizable as a finite-state system. Across tasks, we find that next-state prediction accuracy degrades with increasing state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in low-complexity settings but drops below 30% when the number of boxes or states exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50% accuracy when the number of states is > 10 and transitions are < 30. Through activation patching, we identify attention heads responsible for propagating state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10, 11, 12, and 14. While these heads successfully move relevant state features, action information is not reliably routed to the final token, indicating weak joint state-action reasoning. Our results suggest that state tracking in LLMs emerges from distributed interactions of next-token heads rather than explicit symbolic computation.</li>
</ul>

<h3>Title: Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Hootan Mahmoodiyan, Maryam Ahang, Mostafa Abbasi, Homayoun Najjaran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14896">https://arxiv.org/abs/2505.14896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14896">https://arxiv.org/pdf/2505.14896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14896]] Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis(https://arxiv.org/abs/2505.14896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Ensuring the reliable operation of power transformers is critical to grid stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but traditional methods rely on heuristic rules, which may lead to inconsistent results. Machine learning (ML)-based approaches have improved diagnostic accuracy; however, power transformers operate under varying conditions, and differences in transformer type, environmental factors, and operational settings create distribution shifts in diagnostic data. Consequently, direct model transfer between transformers often fails, making techniques for domain adaptation a necessity. To tackle this issue, this work proposes a feature-weighted domain adaptation technique that combines Maximum Mean Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign adaptable weights, prioritizing features with larger distributional discrepancies and thereby improving source and target domain alignment. Experimental evaluations on datasets for power transformers demonstrate the effectiveness of the proposed method, which achieves a 7.9% improvement over Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it outperforms both techniques across various training sample sizes, confirming its robustness for domain adaptation.</li>
</ul>

<h3>Title: Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ali Mohajerzarrinkelk, Maryam Ahang, Mehran Zoravar, Mostafa Abbasi, Homayoun Najjaran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14897">https://arxiv.org/abs/2505.14897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14897">https://arxiv.org/pdf/2505.14897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14897]] Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction(https://arxiv.org/abs/2505.14897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is an important consideration to avoid unexpected failures, reduce downtime, and promote safety and efficiency in industrial systems. Complications in degradation trends, noise presence, and the necessity to detect faults in advance make estimation of RUL a challenging task. This paper introduces a novel framework that combines wavelet-based denoising method, Wavelet Packet Decomposition (WPD), and a customized multi-channel Swin Transformer model (MCSFormer) to address these problems. With attention mechanisms incorporated for feature fusion, the model is designed to learn global and local degradation patterns utilizing hierarchical representations for enhancing predictive performance. Additionally, a customized loss function is developed as a key distinction of this work to differentiate between early and late predictions, prioritizing accurate early detection and minimizing the high operation risks of late predictions. The proposed model was evaluated with the PRONOSTIA dataset using three experiments. Intra-condition experiments demonstrated that MCSFormer outperformed state-of-the-art models, including the Adaptive Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on average across different operating conditions, respectively. In terms of cross-condition testing, it achieved superior generalization under varying operating conditions compared to the adapted ViT and Swin Transformer. Lastly, the custom loss function effectively reduced late predictions, as evidenced in a 6.3% improvement in the scoring metric while maintaining competitive overall performance. The model's robust noise resistance, generalization capability, and focus on safety make MCSFormer a trustworthy and effective predictive maintenance tool in industrial applications.</li>
</ul>

<h3>Title: Topology-aware Detection and Localization of Distributed Denial-of-Service Attacks in Network-on-Chips</h3>
<ul>
<li><strong>Authors: </strong>Hansika Weerasena, Xiaoguo Jia, Prabhat Mishra</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14898">https://arxiv.org/abs/2505.14898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14898">https://arxiv.org/pdf/2505.14898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14898]] Topology-aware Detection and Localization of Distributed Denial-of-Service Attacks in Network-on-Chips(https://arxiv.org/abs/2505.14898)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Network-on-Chip (NoC) enables on-chip communication between diverse cores in modern System-on-Chip (SoC) designs. With its shared communication fabric, NoC has become a focal point for various security threats, especially in heterogeneous and high-performance computing platforms. Among these attacks, Distributed Denial of Service (DDoS) attacks occur when multiple malicious entities collaborate to overwhelm and disrupt access to critical system components, potentially causing severe performance degradation or complete disruption of services. These attacks are particularly challenging to detect due to their distributed nature and dynamic traffic patterns in NoC, which often evade static detection rules or simple profiling. This paper presents a framework to conduct topology-aware detection and localization of DDoS attacks using Graph Neural Networks (GNNs) by analyzing NoC traffic patterns. Specifically, by modeling the NoC as a graph, our method utilizes spatiotemporal traffic features to effectively identify and localize DDoS attacks. Unlike prior works that rely on handcrafted features or threshold-based detection, our GNN-based approach operates directly on raw inter-flit delay data, learning complex traffic dependencies without manual intervention. Experimental results demonstrate that our approach can detect and localize DDoS attacks with high accuracy (up to 99\%) while maintaining consistent performance under diverse attack strategies. Furthermore, the proposed method exhibits strong robustness across varying numbers and placements of malicious IPs, different packet injection rates, application workloads, and architectural configurations, including both 2D mesh and 3D TSV-based NoCs. Our work provides a scalable, flexible, and architecture-agnostic defense mechanism, significantly improving the availability and trustworthiness of on-chip communication in future SoC designs.</li>
</ul>

<h3>Title: Concept Incongruence: An Exploration of Time and Death in Role Playing</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Bai, Ike Peng, Aditya Singh, Chenhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14905">https://arxiv.org/abs/2505.14905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14905">https://arxiv.org/pdf/2505.14905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14905]] Concept Incongruence: An Exploration of Time and Death in Role Playing(https://arxiv.org/abs/2505.14905)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Consider this prompt "Draw a unicorn with two horns". Should large language models (LLMs) recognize that a unicorn has only one horn by definition and ask users for clarifications, or proceed to generate something anyway? We introduce concept incongruence to capture such phenomena where concept boundaries clash with each other, either in user prompts or in model representations, often leading to under-specified or mis-specified behaviors. In this work, we take the first step towards defining and analyzing model behavior under concept incongruence. Focusing on temporal boundaries in the Role-Play setting, we propose three behavioral metrics--abstention rate, conditional accuracy, and answer rate--to quantify model behavior under incongruence due to the role's death. We show that models fail to abstain after death and suffer from an accuracy drop compared to the Non-Role-Play setting. Through probing experiments, we identify two main causes: (i) unreliable encoding of the "death" state across different years, leading to unsatisfactory abstention behavior, and (ii) role playing causes shifts in the model's temporal representations, resulting in accuracy drops. We leverage these insights to improve consistency in the model's abstention and answer behaviors. Our findings suggest that concept incongruence leads to unexpected model behaviors and point to future directions on improving model behavior under concept incongruence.</li>
</ul>

<h3>Title: Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain</h3>
<ul>
<li><strong>Authors: </strong>Ye Yuan, Haolun Wu, Hao Zhou, Xue Liu, Hao Chen, Yan Xin, Jianzhong (Charlie)Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14906">https://arxiv.org/abs/2505.14906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14906">https://arxiv.org/pdf/2505.14906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14906]] Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain(https://arxiv.org/abs/2505.14906)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Knowledge understanding is a foundational part of envisioned 6G networks to advance network intelligence and AI-native network architectures. In this paradigm, information extraction plays a pivotal role in transforming fragmented telecom knowledge into well-structured formats, empowering diverse AI models to better understand network terminologies. This work proposes a novel language model-based information extraction technique, aiming to extract structured entities from the telecom context. The proposed telecom structured entity extraction (TeleSEE) technique applies a token-efficient representation method to predict entity types and attribute keys, aiming to save the number of output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a hierarchical parallel decoding method, improving the standard encoder-decoder architecture by integrating additional prompting and decoding strategies into entity extraction tasks. In addition, to better evaluate the performance of the proposed technique in the telecom domain, we further designed a dataset named 6GTech, including 2390 sentences and 23747 words from more than 100 6G-related technical publications. Finally, the experiment shows that the proposed TeleSEE method achieves higher accuracy than other baseline techniques, and also presents 5 to 9 times higher sample processing speed.</li>
</ul>

<h3>Title: ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Liu, Paul Thompson, Jiaqi Rong, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14917">https://arxiv.org/abs/2505.14917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14917">https://arxiv.org/pdf/2505.14917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14917]] ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories(https://arxiv.org/abs/2505.14917)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite the many benefits of large language models (LLMs), they can also cause harm, e.g., through automatic generation of misinformation, including conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories by altering characteristic textual features, e.g., by transforming their typically strong negative emotions into a more positive tone. Although several studies have proposed automated conspiracy theory detection methods, they are usually trained using human-authored text, whose features can vary from LLM-generated text. Furthermore, several conspiracy detection models, including the previously proposed ConspEmoLLM, rely heavily on the typical emotional features of human-authored conspiracy content. As such, intentionally disguised content may evade detection. To combat such issues, we firstly developed an augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which supplements human-authored conspiracy tweets with versions rewritten by an LLM to reduce the negativity of their original sentiment. The quality of the rewritten tweets was verified by combining human and LLM-based assessment. We subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or exceeds the performance of ConspEmoLLM on the original human-authored content in ConDID, and considerably outperforms both ConspEmoLLM and several other baselines when applied to sentiment-transformed tweets in ConDID-v2. The project will be available at this https URL.</li>
</ul>

<h3>Title: Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications</h3>
<ul>
<li><strong>Authors: </strong>Fadel M. Megahed, Ying-Ju Chen, L. Allision Jones-Farmer, Younghwa Lee, Jiawei Brooke Wang, Inez M. Zwetsloot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14918">https://arxiv.org/abs/2505.14918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14918">https://arxiv.org/pdf/2505.14918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14918]] Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications(https://arxiv.org/abs/2505.14918)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.</li>
</ul>

<h3>Title: Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels</h3>
<ul>
<li><strong>Authors: </strong>Sil Hamilton, Rebecca M. M. Hicke, Matthew Wilkens, David Mimno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14925">https://arxiv.org/abs/2505.14925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14925">https://arxiv.org/pdf/2505.14925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14925]] Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels(https://arxiv.org/abs/2505.14925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond "lost in the middle" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.</li>
</ul>

<h3>Title: Colors Matter: AI-Driven Exploration of Human Feature Colors</h3>
<ul>
<li><strong>Authors: </strong>Rama Alyoubi, Taif Alharbi, Albatul Alghamdi, Yara Alshehri, Elham Alghamdi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14931">https://arxiv.org/abs/2505.14931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14931">https://arxiv.org/pdf/2505.14931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14931]] Colors Matter: AI-Driven Exploration of Human Feature Colors(https://arxiv.org/abs/2505.14931)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This study presents a robust framework that leverages advanced imaging techniques and machine learning for feature extraction and classification of key human attributes-namely skin tone, hair color, iris color, and vein-based undertones. The system employs a multi-stage pipeline involving face detection, region segmentation, and dominant color extraction to isolate and analyze these features. Techniques such as X-means clustering, alongside perceptually uniform distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV color spaces to enhance the accuracy of color differentiation. For classification, the dominant tones of the skin, hair, and iris are extracted and matched to a custom tone scale, while vein analysis from wrist images enables undertone classification into "Warm" or "Cool" based on LAB differences. Each module uses targeted segmentation and color space transformations to ensure perceptual precision. The system achieves up to 80% accuracy in tone classification using the Delta E-HSV method with Gaussian blur, demonstrating reliable performance across varied lighting and image conditions. This work highlights the potential of AI-powered color analysis and feature extraction for delivering inclusive, precise, and nuanced classification, supporting applications in beauty technology, digital personalization, and visual analytics.</li>
</ul>

<h3>Title: Foundations of Unknown-aware Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuefeng Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14933">https://arxiv.org/abs/2505.14933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14933">https://arxiv.org/pdf/2505.14933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14933]] Foundations of Unknown-aware Machine Learning(https://arxiv.org/abs/2505.14933)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to address key reliability issues arising from distributional uncertainty and unknown classes, from standard neural networks to modern foundation models like large language models (LLMs). Traditional learning paradigms, such as empirical risk minimization (ERM), assume no distribution shift between training and inference, often leading to overconfident predictions on out-of-distribution (OOD) inputs. This thesis introduces novel frameworks that jointly optimize for in-distribution accuracy and reliability to unseen data. A core contribution is the development of an unknown-aware learning framework that enables models to recognize and handle novel inputs without labeled OOD data. We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to generate informative unknowns during training. Building on this, we present SAL, a theoretical and algorithmic framework that leverages unlabeled in-the-wild data to enhance OOD detection under realistic deployment conditions. These methods demonstrate that abundant unlabeled data can be harnessed to recognize and adapt to unforeseen inputs, providing formal reliability guarantees. The thesis also extends reliable learning to foundation models. We develop HaloScope for hallucination detection in LLMs, MLLMGuard for defending against malicious prompts in multimodal models, and data cleaning methods to denoise human feedback used for better alignment. These tools target failure modes that threaten the safety of large-scale models in deployment. Overall, these contributions promote unknown-aware learning as a new paradigm, and we hope it can advance the reliability of AI systems with minimal human efforts.</li>
</ul>

<h3>Title: Unlearning Algorithmic Biases over Graphs</h3>
<ul>
<li><strong>Authors: </strong>O. Deniz Kose, Gonzalo Mateos, Yanning Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14945">https://arxiv.org/abs/2505.14945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14945">https://arxiv.org/pdf/2505.14945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14945]] Unlearning Algorithmic Biases over Graphs(https://arxiv.org/abs/2505.14945)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The growing enforcement of the right to be forgotten regulations has propelled recent advances in certified (graph) unlearning strategies to comply with data removal requests from deployed machine learning (ML) models. Motivated by the well-documented bias amplification predicament inherent to graph data, here we take a fresh look at graph unlearning and leverage it as a bias mitigation tool. Given a pre-trained graph ML model, we develop a training-free unlearning procedure that offers certifiable bias mitigation via a single-step Newton update on the model weights. This way, we contribute a computationally lightweight alternative to the prevalent training- and optimization-based fairness enhancement approaches, with quantifiable performance guarantees. We first develop a novel fairness-aware nodal feature unlearning strategy along with refined certified unlearning bounds for this setting, whose impact extends beyond the realm of graph unlearning. We then design structural unlearning methods endowed with principled selection mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning these judiciously selected elements can mitigate algorithmic biases with minimal impact on downstream utility (e.g., node classification accuracy). Experimental results over real networks corroborate the bias mitigation efficacy of our unlearning strategies, and delineate markedly favorable utility-complexity trade-offs relative to retraining from scratch using augmented graph data obtained via removals.</li>
</ul>

<h3>Title: Programmatic Video Prediction Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Tang, Kevin Ellis, Suhas Lohit, Michael J. Jones, Moitreya Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14948">https://arxiv.org/abs/2505.14948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14948">https://arxiv.org/pdf/2505.14948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14948]] Programmatic Video Prediction Using Large Language Models(https://arxiv.org/abs/2505.14948)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of the video using a set of neuro-symbolic, human-interpretable set of states (one per frame) by leveraging the inductive biases of Large (Vision) Language Models (LLM/VLM). In particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate the states of the video, given the visual context (i.e. the frames); (ii) to predict the states corresponding to future time steps by estimating the transition dynamics; (iii) to render the predicted states as visual RGB-frames. Empirical evaluations reveal that our proposed method outperforms competing techniques at the task of video frame prediction in two challenging environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits counter-factual reasoning and interpretable video generation attesting to its effectiveness and generalizability for video generation tasks.</li>
</ul>

<h3>Title: MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jose Sosa, Danila Rukhovich, Anis Kacem, Djamila Aouada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14951">https://arxiv.org/abs/2505.14951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14951">https://arxiv.org/pdf/2505.14951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14951]] MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation Tasks(https://arxiv.org/abs/2505.14951)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-modal data in Earth Observation (EO) presents a huge opportunity for improving transfer learning capabilities when pre-training deep learning models. Unlike prior work that often overlooks multi-modal EO data, recent methods have started to include it, resulting in more effective pre-training strategies. However, existing approaches commonly face challenges in effectively transferring learning to downstream tasks where the structure of available data differs from that used during pre-training. This paper addresses this limitation by exploring a more flexible multi-modal, multi-task pre-training strategy for EO data. Specifically, we adopt a Multi-modal Multi-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing diverse input modalities, including spectral, elevation, and segmentation data. The pre-trained model demonstrates robust transfer learning capabilities, outperforming state-of-the-art methods on various EO datasets for classification and segmentation tasks. Our approach exhibits significant flexibility, handling diverse input configurations without requiring modality-specific pre-trained models. Code will be available at: this https URL.</li>
</ul>

<h3>Title: Privacy Preserving Conversion Modeling in Data Clean Room</h3>
<ul>
<li><strong>Authors: </strong>Kungang Li, Xiangyi Chen, Ling Leng, Jiajing Xu, Jiankai Sun, Behnam Rezaei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14959">https://arxiv.org/abs/2505.14959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14959">https://arxiv.org/pdf/2505.14959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14959]] Privacy Preserving Conversion Modeling in Data Clean Room(https://arxiv.org/abs/2505.14959)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>In the realm of online advertising, accurately predicting the conversion rate (CVR) is crucial for enhancing advertising efficiency and user satisfaction. This paper addresses the challenge of CVR prediction while adhering to user privacy preferences and advertiser requirements. Traditional methods face obstacles such as the reluctance of advertisers to share sensitive conversion data and the limitations of model training in secure environments like data clean rooms. We propose a novel model training framework that enables collaborative model training without sharing sample-level gradients with the advertising platform. Our approach introduces several innovative components: (1) utilizing batch-level aggregated gradients instead of sample-level gradients to minimize privacy risks; (2) applying adapter-based parameter-efficient fine-tuning and gradient compression to reduce communication costs; and (3) employing de-biasing techniques to train the model under label differential privacy, thereby maintaining accuracy despite privacy-enhanced label perturbations. Our experimental results, conducted on industrial datasets, demonstrate that our method achieves competitive ROCAUC performance while significantly decreasing communication overhead and complying with both advertiser privacy requirements and user privacy choices. This framework establishes a new standard for privacy-preserving, high-performance CVR prediction in the digital advertising landscape.</li>
</ul>

<h3>Title: MedBrowseComp: Benchmarking Medical Deep Research and Computer Use</h3>
<ul>
<li><strong>Authors: </strong>Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, Danielle S. Bitterman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14963">https://arxiv.org/abs/2505.14963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14963">https://arxiv.org/pdf/2505.14963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14963]] MedBrowseComp: Benchmarking Medical Deep Research and Computer Use(https://arxiv.org/abs/2505.14963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly envisioned as decision-support tools in clinical practice, yet safe clinical reasoning demands integrating heterogeneous knowledge bases -- trials, primary studies, regulatory documents, and cost data -- under strict accuracy constraints. Existing evaluations often rely on synthetic prompts, reduce the task to single-hop factoid queries, or conflate reasoning with open-ended generation, leaving their real-world utility unclear. To close this gap, we present MedBrowseComp, the first benchmark that systematically tests an agent's ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp contains more than 1,000 human-curated questions that mirror clinical scenarios where practitioners must reconcile fragmented or conflicting information to reach an up-to-date conclusion. Applying MedBrowseComp to frontier agentic systems reveals performance shortfalls as low as ten percent, exposing a critical gap between current LLM capabilities and the rigor demanded in clinical settings. MedBrowseComp therefore offers a clear testbed for reliable medical information seeking and sets concrete goals for future model and toolchain upgrades. You can visit our project page at: this https URL</li>
</ul>

<h3>Title: The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models</h3>
<ul>
<li><strong>Authors: </strong>Dave Cook, Tim Klawa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14964">https://arxiv.org/abs/2505.14964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14964">https://arxiv.org/pdf/2505.14964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14964]] The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models(https://arxiv.org/abs/2505.14964)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>AI systems in high-consequence domains such as defense, intelligence, and disaster response must detect rare, high-impact events while operating under tight resource constraints. Traditional annotation strategies that prioritize label volume over informational value introduce redundancy and noise, limiting model generalization. This paper introduces smart-sizing, a training data strategy that emphasizes label diversity, model-guided selection, and marginal utility-based stopping. We implement this through Adaptive Label Optimization (ALO), combining pre-labeling triage, annotator disagreement analysis, and iterative feedback to prioritize labels that meaningfully improve model performance. Experiments show that models trained on 20 to 40 percent of curated data can match or exceed full-data baselines, particularly in rare-class recall and edge-case generalization. We also demonstrate how latent labeling errors embedded in training and validation sets can distort evaluation, underscoring the need for embedded audit tools and performance-aware governance. Smart-sizing reframes annotation as a feedback-driven process aligned with mission outcomes, enabling more robust models with fewer labels and supporting efficient AI development pipelines for frontier models and operational systems.</li>
</ul>

<h3>Title: STree: Speculative Tree Decoding for Hybrid State-Space Models</h3>
<ul>
<li><strong>Authors: </strong>Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14969">https://arxiv.org/abs/2505.14969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14969">https://arxiv.org/pdf/2505.14969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14969]] STree: Speculative Tree Decoding for Hybrid State-Space Models(https://arxiv.org/abs/2505.14969)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a technique to leverage hardware concurrency to improve the efficiency of large-scale autoregressive (AR) Transformer models by enabling multiple steps of token generation in a single forward pass. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead to current SSM state update implementations. With the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code will be released upon paper acceptance.</li>
</ul>

<h3>Title: DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis</h3>
<ul>
<li><strong>Authors: </strong>Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chizor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14971">https://arxiv.org/abs/2505.14971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14971">https://arxiv.org/pdf/2505.14971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14971]] DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis(https://arxiv.org/abs/2505.14971)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have revolutionized natural language processing (NLP) and expanded their applications across diverse domains. However, despite their impressive capabilities, LLMs have been shown to reflect and perpetuate harmful societal biases, including those based on ethnicity, gender, and religion. A critical and underexplored issue is the reinforcement of caste-based biases, particularly towards India's marginalized caste groups such as Dalits and Shudras. In this paper, we address this gap by proposing DECASTE, a novel, multi-dimensional framework designed to detect and assess both implicit and explicit caste biases in LLMs. Our approach evaluates caste fairness across four dimensions: socio-cultural, economic, educational, and political, using a range of customized prompting strategies. By benchmarking several state-of-the-art LLMs, we reveal that these models systematically reinforce caste biases, with significant disparities observed in the treatment of oppressed versus dominant caste groups. For example, bias scores are notably elevated when comparing Dalits and Shudras with dominant caste groups, reflecting societal prejudices that persist in model outputs. These results expose the subtle yet pervasive caste biases in LLMs and emphasize the need for more comprehensive and inclusive bias evaluation methodologies that assess the potential risks of deploying such models in real-world contexts.</li>
</ul>

<h3>Title: Flattening Hierarchies with Policy Bootstrapping</h3>
<ul>
<li><strong>Authors: </strong>John L. Zhou, Jonathan C. Kao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14975">https://arxiv.org/abs/2505.14975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14975">https://arxiv.org/pdf/2505.14975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14975]] Flattening Hierarchies with Policy Bootstrapping(https://arxiv.org/abs/2505.14975)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail.</li>
</ul>

<h3>Title: CRAFT: Training-Free Cascaded Retrieval for Tabular QA</h3>
<ul>
<li><strong>Authors: </strong>Adarsh Singh, Kushal Raj Bhandari, Jianxi Gao, Soham Dan, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14984">https://arxiv.org/abs/2505.14984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14984">https://arxiv.org/pdf/2505.14984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14984]] CRAFT: Training-Free Cascaded Retrieval for Tabular QA(https://arxiv.org/abs/2505.14984)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Table Question Answering (TQA) involves retrieving relevant tables from a large corpus to answer natural language queries. Traditional dense retrieval models, such as DTR and ColBERT, not only incur high computational costs for large-scale retrieval tasks but also require retraining or fine-tuning on new datasets, limiting their adaptability to evolving domains and knowledge. In this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that first uses a sparse retrieval model to filter a subset of candidate tables before applying more computationally expensive dense models and neural re-rankers. Our approach achieves better retrieval performance than state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further enhance table representations by generating table descriptions and titles using Gemini Flash 1.5. End-to-end TQA results using various Large Language Models (LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate $\textbf{CRAFT}$ effectiveness.</li>
</ul>

<h3>Title: Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14992">https://arxiv.org/abs/2505.14992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14992">https://arxiv.org/pdf/2505.14992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14992]] Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models(https://arxiv.org/abs/2505.14992)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Information extraction (IE) plays a crucial role in natural language processing (NLP) by converting unstructured text into structured knowledge. Deploying computationally intensive large language models (LLMs) on resource-constrained devices for information extraction is challenging, particularly due to issues like hallucinations, limited context length, and high latency-especially when handling diverse extraction schemas. To address these challenges, we propose a two-stage information extraction approach adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching (DLISC), which enhances both schema identification and schema-aware extraction in terms of effectiveness and efficiency. In particular, DLISC adopts an Identification LoRA module for retrieving the most relevant schemas to a given query, and an Extraction LoRA module for performing information extraction based on the previously selected schemas. To accelerate extraction inference, Incremental Schema Caching is incorporated to reduce redundant computation, substantially improving efficiency. Extensive experiments across multiple information extraction datasets demonstrate notable improvements in both effectiveness and efficiency.</li>
</ul>

<h3>Title: Meta-Design Matters: A Self-Design Multi-Agent System</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14996">https://arxiv.org/abs/2505.14996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14996">https://arxiv.org/pdf/2505.14996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14996]] Meta-Design Matters: A Self-Design Multi-Agent System(https://arxiv.org/abs/2505.14996)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. We introduce SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS.</li>
</ul>

<h3>Title: Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision</h3>
<ul>
<li><strong>Authors: </strong>Eric Hanchen Jiang, Haozheng Luo, Shengyuan Pang, Xiaomin Li, Zhenting Qi, Hengli Li, Cheng-Fu Yang, Zongyu Lin, Xinfeng Li, Hao Xu, Kai-Wei Chang, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.14999">https://arxiv.org/abs/2505.14999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.14999">https://arxiv.org/pdf/2505.14999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.14999]] Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision(https://arxiv.org/abs/2505.14999)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.</li>
</ul>

<h3>Title: Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems</h3>
<ul>
<li><strong>Authors: </strong>Chengwei Wei, Bin Wang, Jung-jae Kim, Nancy F. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15000">https://arxiv.org/abs/2505.15000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15000">https://arxiv.org/pdf/2505.15000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15000]] Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems(https://arxiv.org/abs/2505.15000)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs) have led to strong reasoning ability across a wide range of tasks. However, their ability to perform mathematical reasoning from spoken input remains underexplored. Prior studies on speech modality have mostly focused on factual speech understanding or simple audio reasoning tasks, providing limited insight into logical step-by-step reasoning, such as that required for mathematical problem solving. To address this gap, we introduce Spoken Math Question Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical reasoning capabilities of speech-based models, including both cascade models (ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of math problems, including pure arithmetic, single-step and multi-step contextual reasoning, and knowledge-oriented reasoning problems, all presented in unambiguous natural spoken language. Through extensive experiments, we find that: (1) while some speech LLMs perform competitively on contextual reasoning tasks involving basic arithmetic, they still struggle with direct arithmetic problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical expressions written in LaTex and have difficulty interpreting verbalized mathematical expressions; and (3) mathematical knowledge reasoning abilities are significantly degraded in current speech LLMs.</li>
</ul>

<h3>Title: Know When to Abstain: Optimal Selective Classification with Likelihood Ratios</h3>
<ul>
<li><strong>Authors: </strong>Alvin Heng, Harold Soh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15008">https://arxiv.org/abs/2505.15008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15008">https://arxiv.org/pdf/2505.15008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15008]] Know When to Abstain: Optimal Selective Classification with Likelihood Ratios(https://arxiv.org/abs/2505.15008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions. In this work, we revisit the design of optimal selection functions through the lens of the Neyman--Pearson lemma, a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test. We show that this perspective not only unifies the behavior of several post-hoc selection baselines, but also motivates new approaches to selective classification which we propose here. A central focus of our work is the setting of covariate shift, where the input distribution at test time differs from that at training. This realistic and challenging scenario remains relatively underexplored in the context of selective classification. We evaluate our proposed methods across a range of vision and language tasks, including both supervised learning and vision-language models. Our experiments demonstrate that our Neyman--Pearson-informed methods consistently outperform existing baselines, indicating that likelihood ratio-based selection offers a robust mechanism for improving selective classification under covariate shifts. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</h3>
<ul>
<li><strong>Authors: </strong>Quan Nguyen, Thanh Nguyen-Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15009">https://arxiv.org/abs/2505.15009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15009">https://arxiv.org/pdf/2505.15009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15009]] One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks(https://arxiv.org/abs/2505.15009)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the approximation capabilities and on-convergence behaviors of one-layer transformers on the noiseless and noisy in-context reasoning of next-token prediction. Existing theoretical results focus on understanding the in-context reasoning behaviors for either the first gradient step or when the number of samples is infinite. Furthermore, no convergence rates nor generalization abilities were known. Our work addresses these gaps by showing that there exists a class of one-layer transformers that are provably Bayes-optimal with both linear and ReLU attention. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss of these transformers converges at linear rate to the Bayes risk. Moreover, we prove that the trained models generalize to unseen samples as well as exhibit learning behaviors that were empirically observed in previous works. Our theoretical findings are further supported by extensive empirical validations.</li>
</ul>

<h3>Title: PsyScam: A Benchmark for Psychological Techniques in Real-World Scams</h3>
<ul>
<li><strong>Authors: </strong>Shang Ma, Tianyi Ma, Jiahao Liu, Wei Song, Zhenkai Liang, Xusheng Xiao, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15017">https://arxiv.org/abs/2505.15017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15017">https://arxiv.org/pdf/2505.15017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15017]] PsyScam: A Benchmark for Psychological Techniques in Real-World Scams(https://arxiv.org/abs/2505.15017)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Online scams have become increasingly prevalent, with scammers using psychological techniques (PTs) to manipulate victims. While existing research has developed benchmarks to study scammer behaviors, these benchmarks do not adequately reflect the PTs observed in real-world scams. To fill this gap, we introduce PsyScam, a benchmark designed to systematically capture and evaluate PTs embedded in real-world scam reports. In particular, PsyScam bridges psychology and real-world cyber security analysis through collecting a wide range of scam reports from six public platforms and grounding its annotations in well-established cognitive and psychological theories. We further demonstrate PsyScam's utility through three downstream tasks: PT classification, scam completion, and scam augmentation. Experimental results show that PsyScam presents significant challenges to existing models in both detecting and generating scam content based on the PTs used by real-world scammers. Our code and dataset are available at: this https URL.</li>
</ul>

<h3>Title: Diagnosing our datasets: How does my language model learn clinical information?</h3>
<ul>
<li><strong>Authors: </strong>Furong Jia, David Sontag, Monica Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15024">https://arxiv.org/abs/2505.15024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15024">https://arxiv.org/pdf/2505.15024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15024]] Diagnosing our datasets: How does my language model learn clinical information?(https://arxiv.org/abs/2505.15024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have performed well across various clinical natural language processing tasks, despite not being directly trained on electronic health record (EHR) data. In this work, we examine how popular open-source LLMs learn clinical information from large mined corpora through two crucial but understudied lenses: (1) their interpretation of clinical jargon, a foundational ability for understanding real-world clinical notes, and (2) their responses to unsupported medical claims. For both use cases, we investigate the frequency of relevant clinical information in their corresponding pretraining corpora, the relationship between pretraining data composition and model outputs, and the sources underlying this data. To isolate clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo. Unsurprisingly, we find that the frequency of clinical jargon mentions across major pretraining corpora correlates with model performance. However, jargon frequently appearing in clinical notes often rarely appears in pretraining corpora, revealing a mismatch between available data and real-world usage. Similarly, we find that a non-negligible portion of documents support disputed claims that can then be parroted by models. Finally, we classified and analyzed the types of online sources in which clinical jargon and unsupported medical claims appear, with implications for future dataset composition.</li>
</ul>

<h3>Title: Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15030">https://arxiv.org/abs/2505.15030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15030">https://arxiv.org/pdf/2505.15030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15030]] Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC(https://arxiv.org/abs/2505.15030)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The increasing deployment of Large Language Models (LLMs) on edge devices, driven by model advancements and hardware improvements, offers significant privacy benefits. However, these on-device LLMs inherently face performance limitations due to reduced model capacity and necessary compression techniques. To address this, we introduce a systematic methodology -- encompassing model capability, development efficiency, and system resources -- for evaluating on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to 14B parameters and seven post-training quantization (PTQ) methods on commodity laptops, yields several critical insights: 1) System-level metrics exhibit near-linear scaling with effective bits-per-weight (BPW). 2) A practical threshold exists around $\sim$3.5 effective BPW, larger models subjected to low-bit quantization consistently outperform smaller models utilizing higher bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but significant memory savings. 4) Determined by low-level implementation specifics power consumption on CPU, where computation-intensive operations spend more power than memory-intensive ones. These findings offer crucial insights and practical guidelines for the efficient deployment and optimized configuration of LLMs on resource-constrained edge devices. Our codebase is available at this https URL.</li>
</ul>

<h3>Title: Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI</h3>
<ul>
<li><strong>Authors: </strong>Wenqing Wu, Haixu Xi, Chengzhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15031">https://arxiv.org/abs/2505.15031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15031">https://arxiv.org/pdf/2505.15031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15031]] Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI(https://arxiv.org/abs/2505.15031)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.</li>
</ul>

<h3>Title: RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S. Boning, Dina Katabi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15034">https://arxiv.org/abs/2505.15034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15034">https://arxiv.org/pdf/2505.15034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15034]] RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning(https://arxiv.org/abs/2505.15034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has recently emerged as a compelling approach for enhancing the reasoning capabilities of large language models (LLMs), where an LLM generator serves as a policy guided by a verifier (reward model). However, current RL post-training methods for LLMs typically use verifiers that are fixed (rule-based or frozen pretrained) or trained discriminatively via supervised fine-tuning (SFT). Such designs are susceptible to reward hacking and generalize poorly beyond their training distributions. To overcome these limitations, we propose Tango, a novel framework that uses RL to concurrently train both an LLM generator and a verifier in an interleaved manner. A central innovation of Tango is its generative, process-level LLM verifier, which is trained via RL and co-evolves with the generator. Importantly, the verifier is trained solely based on outcome-level verification correctness rewards without requiring explicit process-level annotations. This generative RL-trained verifier exhibits improved robustness and superior generalization compared to deterministic or SFT-trained verifiers, fostering effective mutual reinforcement with the generator. Extensive experiments demonstrate that both components of Tango achieve state-of-the-art results among 7B/8B-scale models: the generator attains best-in-class performance across five competition-level math benchmarks and four challenging out-of-domain reasoning tasks, while the verifier leads on the ProcessBench dataset. Remarkably, both components exhibit particularly substantial improvements on the most difficult mathematical reasoning problems. Code is at: this https URL.</li>
</ul>

<h3>Title: Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering</h3>
<ul>
<li><strong>Authors: </strong>Haiyan Zhao, Xuansheng Wu, Fan Yang, Bo Shen, Ninghao Liu, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15038">https://arxiv.org/abs/2505.15038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15038">https://arxiv.org/pdf/2505.15038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15038]] Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering(https://arxiv.org/abs/2505.15038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Linear Concept Vectors have proven effective for steering large language models (LLMs). While existing approaches like linear probing and difference-in-means derive these vectors from LLM hidden representations, diverse data introduces noises (i.e., irrelevant features) that challenge steering robustness. To address this, we propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy features from hidden representations. When applied to linear probing and difference-in-means, our method improves their steering success rates. We validate our noise hypothesis through counterfactual experiments and feature visualizations.</li>
</ul>

<h3>Title: RLBenchNet: The Right Network for the Right Reinforcement Learning Task</h3>
<ul>
<li><strong>Authors: </strong>Ivan Smirnov, Shangding Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15040">https://arxiv.org/abs/2505.15040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15040">https://arxiv.org/pdf/2505.15040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15040]] RLBenchNet: The Right Network for the Right Reinforcement Learning Task(https://arxiv.org/abs/2505.15040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has seen significant advancements through the application of various neural network architectures. In this study, we systematically investigate the performance of several neural networks in RL tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP), Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit (GRU). Through comprehensive evaluation across continuous control, discrete decision-making, and memory-based environments, we identify architecture-specific strengths and limitations. Our results reveal that: (1) MLPs excel in fully observable continuous control tasks, providing an optimal balance of performance and efficiency; (2) recurrent architectures like LSTM and GRU offer robust performance in partially observable environments with moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2 successfully solve the most challenging memory-intensive tasks, with Mamba-2 requiring 8x less memory than Transformer-XL. These findings provide insights for researchers and practitioners, enabling more informed architecture selection based on specific task characteristics and computational constraints. Code is available at: this https URL</li>
</ul>

<h3>Title: Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective</h3>
<ul>
<li><strong>Authors: </strong>Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, Chen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15045">https://arxiv.org/abs/2505.15045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15045">https://arxiv.org/pdf/2505.15045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15045]] Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective(https://arxiv.org/abs/2505.15045)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.</li>
</ul>

<h3>Title: ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wu, Lutao Yan, Leixian Shen, Yinan Mei, Jiannan Wang, Yuyu Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15046">https://arxiv.org/abs/2505.15046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15046">https://arxiv.org/pdf/2505.15046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15046]] ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding(https://arxiv.org/abs/2505.15046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.</li>
</ul>

<h3>Title: PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yingming Pu, Tao Lin, Hongyu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15047">https://arxiv.org/abs/2505.15047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15047">https://arxiv.org/pdf/2505.15047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15047]] PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration(https://arxiv.org/abs/2505.15047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate remarkable potential for scientific discovery. Existing approaches, however, often automate scientific discovery using predefined workflows that lack rationality constraints. This often leads to aimless hypothesizing and a failure to consistently link hypotheses with evidence, thereby hindering systematic uncertainty reduction. Overcoming these limitations fundamentally requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an information-theoretical framework, treating automated scientific discovery as a structured uncertainty reduction problem guided by principles (e.g., scientific laws). In evaluations across three distinct scientific domains -- discovering nanomaterial structures, bio-molecules, and superconductor candidates with targeted properties -- our method significantly improves discovery efficiency, reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property values versus exploration steps, and enhances solution quality by 94.06\% compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a Plug-and-Play method, establishing a novel paradigm shift in highly efficient automated scientific discovery, paving the way for more robust and accelerated AI-driven research. Code is publicly available at our \href{this https URL}{GitHub}.</li>
</ul>

<h3>Title: Improving the fact-checking performance of language models by relying on their entailment ability</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Kumar, Debajyoti Mazumder, Ayush Garg, Jasabanta Patro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15050">https://arxiv.org/abs/2505.15050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15050">https://arxiv.org/pdf/2505.15050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15050]] Improving the fact-checking performance of language models by relying on their entailment ability(https://arxiv.org/abs/2505.15050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automated fact-checking is a crucial task in this digital age. To verify a claim, current approaches majorly follow one of two strategies i.e. (i) relying on embedded knowledge of language models, and (ii) fine-tuning them with evidence pieces. While the former can make systems to hallucinate, the later have not been very successful till date. The primary reason behind this is that fact verification is a complex process. Language models have to parse through multiple pieces of evidence before making a prediction. Further, the evidence pieces often contradict each other. This makes the reasoning process even more complex. We proposed a simple yet effective approach where we relied on entailment and the generative ability of language models to produce ''supporting'' and ''refuting'' justifications (for the truthfulness of a claim). We trained language models based on these justifications and achieved superior results. Apart from that, we did a systematic comparison of different prompting and fine-tuning strategies, as it is currently lacking in the literature. Some of our observations are: (i) training language models with raw evidence sentences registered an improvement up to 8.20% in macro-F1, over the best performing baseline for the RAW-FC dataset, (ii) similarly, training language models with prompted claim-evidence understanding (TBE-2) registered an improvement (with a margin up to 16.39%) over the baselines for the same dataset, (iii) training language models with entailed justifications (TBE-3) outperformed the baselines by a huge margin (up to 28.57% and 44.26% for LIAR-RAW and RAW-FC, respectively). We have shared our code repository to reproduce the results.</li>
</ul>

<h3>Title: An Empirical Analysis of EOS Blockchain: Architecture, Contract, and Security</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Liu, Yingjie Mao, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15051">https://arxiv.org/abs/2505.15051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15051">https://arxiv.org/pdf/2505.15051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15051]] An Empirical Analysis of EOS Blockchain: Architecture, Contract, and Security(https://arxiv.org/abs/2505.15051)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the rapid development of blockchain technology, various blockchain systems are exhibiting vitality and potential. As a representative of Blockchain 3.0, the EOS blockchain has been regarded as a strong competitor to Ethereum. Nevertheless, compared with Bitcoin and Ethereum, academic research and in-depth analyses of EOS remain scarce. To address this gap, this study conducts a comprehensive investigation of the EOS blockchain from five key dimensions: system architecture, decentralization, performance, smart contracts, and behavioral security. The architectural analysis focuses on six core components of the EOS system, detailing their functionalities and operational workflows. The decentralization and performance evaluations, based on data from the XBlock data-sharing platform, reveal several critical issues: low account activity, limited participation in the supernode election process, minimal variation in the set of block producers, and a substantial gap between actual throughput and the claimed million-level performance. Five types of contract vulnerabilities are identified in the smart contract dimension, and four mainstream vulnerability detection platforms are introduced and comparatively analyzed. In terms of behavioral security, four real-world attacks targeting the structural characteristics of EOS are summarized. This study contributes to the ongoing development of the EOS blockchain and provides valuable insights for enhancing the security and regulatory mechanisms of blockchain ecosystems.</li>
</ul>

<h3>Title: Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory</h3>
<ul>
<li><strong>Authors: </strong>Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15055">https://arxiv.org/abs/2505.15055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15055">https://arxiv.org/pdf/2505.15055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15055]] Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory(https://arxiv.org/abs/2505.15055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining main-stream prominent LLM benchmarks using results from diverse models. We first propose a new framework for accurate and reliable estimations of item characteristics and model abilities. Specifically, we propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive analysis which reveals significant and varied shortcomings in the measurement quality of current benchmarks. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.</li>
</ul>

<h3>Title: Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15062">https://arxiv.org/abs/2505.15062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15062">https://arxiv.org/pdf/2505.15062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15062]] Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning(https://arxiv.org/abs/2505.15062)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate "hormones helping mental disorders" with "melatonin being a hormone and insomnia a mental disorder" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and $\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.</li>
</ul>

<h3>Title: UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15063">https://arxiv.org/abs/2505.15063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15063">https://arxiv.org/pdf/2505.15063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15063]] UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking(https://arxiv.org/abs/2505.15063)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid use of large language models (LLMs) has raised critical concerns regarding the factual reliability of their outputs, especially in low-resource languages such as Urdu. Existing automated fact-checking solutions overwhelmingly focus on English, leaving a significant gap for the 200+ million Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first comprehensive, modular fact-checking framework specifically tailored for Urdu. Our system features a dynamic, multi-strategy evidence retrieval pipeline that combines monolingual and translation-based approaches to address the scarcity of high-quality Urdu evidence. We curate and release two new hand-annotated benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating LLM factuality. Extensive experiments demonstrate that UrduFactCheck, particularly its translation-augmented variants, consistently outperforms baselines and open-source alternatives on multiple metrics. We further benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in Urdu, highlighting persistent gaps between proprietary and open-source models. UrduFactCheck's code and datasets are open-sourced and publicly available at this https URL.</li>
</ul>

<h3>Title: Generalization Through Growth: Hidden Dynamics Controls Depth Dependence</h3>
<ul>
<li><strong>Authors: </strong>Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15064">https://arxiv.org/abs/2505.15064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15064">https://arxiv.org/pdf/2505.15064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15064]] Generalization Through Growth: Hidden Dynamics Controls Depth Dependence(https://arxiv.org/abs/2505.15064)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent theory has reduced the depth dependence of generalization bounds from exponential to polynomial and even depth-independent rates, yet these results remain tied to specific architectures and Euclidean inputs. We present a unified framework for arbitrary \blue{pseudo-metric} spaces in which a depth-\(k\) network is the composition of continuous hidden maps \(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to \mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$ isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of the semigroup generated by the hidden layers. By Gromov's theorem polynomial (resp. exponential) growth corresponds to virtually nilpotent (resp. expanding) dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$ (sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further provide covering-number estimates showing that expanding dynamics yield an exponential parameter saving via compositional expressivity. Our results decouple specification from implementation, offering architecture-agnostic and dynamical-systems-aware guarantees applicable to modern deep-learning paradigms such as test-time inference and diffusion models.</li>
</ul>

<h3>Title: In-Domain African Languages Translation Using LLMs and Multi-armed Bandits</h3>
<ul>
<li><strong>Authors: </strong>Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15069">https://arxiv.org/abs/2505.15069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15069">https://arxiv.org/pdf/2505.15069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15069]] In-Domain African Languages Translation Using LLMs and Multi-armed Bandits(https://arxiv.org/abs/2505.15069)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Machine Translation (NMT) systems face significant challenges when working with low-resource languages, particularly in domain adaptation tasks. These difficulties arise due to limited training data and suboptimal model generalization, As a result, selecting an optimal model for translation is crucial for achieving strong performance on in-domain data, particularly in scenarios where fine-tuning is not feasible or practical. In this paper, we investigate strategies for selecting the most suitable NMT model for a given domain using bandit-based algorithms, including Upper Confidence Bound, Linear UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively addresses the resource constraints by facilitating optimal model selection with high confidence. We evaluate the approach across three African languages and domains, demonstrating its robustness and effectiveness in both scenarios where target data is available and where it is absent.</li>
</ul>

<h3>Title: Can Large Language Models Understand Internet Buzzwords Through User-Generated Content</h3>
<ul>
<li><strong>Authors: </strong>Chen Huang, Junkai Luo, Xinzuo Wang, Wenqiang Lei, Jiancheng Lv</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15071">https://arxiv.org/abs/2505.15071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15071">https://arxiv.org/pdf/2505.15071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15071]] Can Large Language Models Understand Internet Buzzwords Through User-Generated Content(https://arxiv.org/abs/2505.15071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The massive user-generated content (UGC) available in Chinese social media is giving rise to the possibility of studying internet buzzwords. In this paper, we study if large language models (LLMs) can generate accurate definitions for these buzzwords based on UGC as examples. Our work serves a threefold contribution. First, we introduce CHEER, the first dataset of Chinese internet buzzwords, each annotated with a definition and relevant UGC. Second, we propose a novel method, called RESS, to effectively steer the comprehending process of LLMs to produce more accurate buzzword definitions, mirroring the skills of human language learning. Third, with CHEER, we benchmark the strengths and weaknesses of various off-the-shelf definition generation methods and our RESS. Our benchmark demonstrates the effectiveness of RESS while revealing crucial shared challenges: over-reliance on prior exposure, underdeveloped inferential abilities, and difficulty identifying high-quality UGC to facilitate comprehension. We believe our work lays the groundwork for future advancements in LLM-based definition generation. Our dataset and code are available at this https URL.</li>
</ul>

<h3>Title: DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15074">https://arxiv.org/abs/2505.15074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15074">https://arxiv.org/pdf/2505.15074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15074]] DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data(https://arxiv.org/abs/2505.15074)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks.</li>
</ul>

<h3>Title: Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15075">https://arxiv.org/abs/2505.15075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15075">https://arxiv.org/pdf/2505.15075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15075]] Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs(https://arxiv.org/abs/2505.15075)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.</li>
</ul>

<h3>Title: Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Alessandro dos Santos Ferreira, Ana Paula Marques Ramos, José Marcato Junior, Wesley Nunes Gonçalves</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15077">https://arxiv.org/abs/2505.15077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15077">https://arxiv.org/pdf/2505.15077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15077]] Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation(https://arxiv.org/abs/2505.15077)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Urban forests play a key role in enhancing environmental quality and supporting biodiversity in cities. Mapping and monitoring these green spaces are crucial for urban planning and conservation, yet accurately detecting trees is challenging due to complex landscapes and the variability in image resolution caused by different satellite sensors or UAV flight altitudes. While deep learning architectures have shown promise in addressing these challenges, their effectiveness remains strongly dependent on the availability of large and manually labeled datasets, which are often expensive and difficult to obtain in sufficient quantity. In this work, we propose a novel pipeline that integrates domain adaptation with GANs and Diffusion models to enhance the quality of low-resolution aerial images. Our proposed pipeline enhances low-resolution imagery while preserving semantic content, enabling effective tree segmentation without requiring large volumes of manually annotated data. Leveraging models such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we generate realistic and structurally consistent synthetic samples that expand the training dataset and unify scale across domains. This approach not only improves the robustness of segmentation models across different acquisition conditions but also provides a scalable and replicable solution for remote sensing scenarios with scarce annotation resources. Experimental results demonstrated an improvement of over 50% in IoU for low-resolution images, highlighting the effectiveness of our method compared to traditional pipelines.</li>
</ul>

<h3>Title: SUS backprop: linear backpropagation algorithm for long inputs in transformers</h3>
<ul>
<li><strong>Authors: </strong>Sergey Pankov, Georges Harik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15080">https://arxiv.org/abs/2505.15080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15080">https://arxiv.org/pdf/2505.15080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15080]] SUS backprop: linear backpropagation algorithm for long inputs in transformers(https://arxiv.org/abs/2505.15080)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>It is straightforward to design an unbiased gradient estimator that stochastically cuts the backpropagation flow through any part of a computational graph. By cutting the parts that have little effect on the computation, one can potentially save a significant amount of back-propagation computation in exchange for a minimal increase in the stochastic gradient variance, in some situations. Such a situation occurs in the attention mechanism of the transformer architecture. For long sequences, attention becomes the limiting factor, as its compute requirements increase quadratically with sequence length $n$. At the same time, most attention weights become very small, as most attention heads tend to connect a given token with only a small fraction of other tokens in the sequence. These weights become promising targets for cutting backpropagation. We propose a simple probabilistic rule controlled by a single parameter $c$ that cuts backpropagation through most attention weights, leaving at most $c$ interactions per token per attention head. This brings a factor of $c/n$ reduction in the compute required for the attention backpropagation, turning it from quadratic $O(n^2)$ to linear complexity $O(nc)$. We have empirically verified that, for a typical transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing $c \sim 20-30$) results in relative gradient variance increase of only about $1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable to efficient sparse matrix implementation, thus being promising for making the cost of a backward pass negligible relative to the cost of a forward pass when training a transformer model on long sequences.</li>
</ul>

<h3>Title: Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15083">https://arxiv.org/abs/2505.15083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15083">https://arxiv.org/pdf/2505.15083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15083]] Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features(https://arxiv.org/abs/2505.15083)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a crucial role in various applications, particularly in healthcare, where accurate predictions of future health trajectories can significantly impact clinical decision-making. Ensuring transparency and explainability of the models responsible for these tasks is essential for their adoption in critical settings. Recent work has explored a top-down approach to bi-level transparency, focusing on understanding trends and properties of predicted time series using static features. In this work, we extend this framework by incorporating exogenous time series features alongside static features in a structured manner, while maintaining cohesive interpretation. Our approach leverages the insights of trajectory comprehension to introduce an encoding mechanism for exogenous time series, where they are decomposed into meaningful trends and properties, enabling the extraction of interpretable patterns. Through experiments on several synthetic datasets, we demonstrate that our approach remains predictive while preserving interpretability and robustness. This work represents a step towards developing robust, and generalized time series forecasting models. The code is available at this https URL</li>
</ul>

<h3>Title: DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer</h3>
<ul>
<li><strong>Authors: </strong>Sona Elza Simon, Preethi Jyothi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15090">https://arxiv.org/abs/2505.15090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15090">https://arxiv.org/pdf/2505.15090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15090]] DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer(https://arxiv.org/abs/2505.15090)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Effective cross-lingual transfer remains a critical challenge in scaling the benefits of large language models from high-resource to low-resource languages. Towards this goal, prior studies have explored many approaches to combine task knowledge from task-specific data in a (high-resource) source language and language knowledge from unlabeled text in a (low-resource) target language. One notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual transfer that learns task-specific and language-specific sparse masks to select a subset of the pretrained model's parameters that are further fine-tuned. These sparse fine-tuned vectors (SFTs) are subsequently composed with the pretrained model to facilitate zero-shot cross-lingual transfer to a task in a target language, using only task-specific data from a source language. These sparse masks for SFTs were identified using a simple magnitude-based pruning. In our work, we introduce DeFT-X, a novel composable SFT approach that denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs. We evaluate DeFT-X on a diverse set of extremely low-resource languages for sentiment classification (NusaX) and natural language inference (AmericasNLI) and demonstrate that it performs at par or outperforms SFT and other prominent cross-lingual transfer baselines.</li>
</ul>

<h3>Title: SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jing Yu, Yuqi Tang, Kehua Feng, Mingyang Rao, Lei Liang, Zhiqiang Zhang, Mengshu Sun, Wen Zhang, Qiang Zhang, Keyan Ding, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15094">https://arxiv.org/abs/2505.15094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15094">https://arxiv.org/pdf/2505.15094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15094]] SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models(https://arxiv.org/abs/2505.15094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning. However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs. It comprises ten domain-specific sub-datasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts. SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multi-source information integration, and Context-aware inference, through a variety of question formats. We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.</li>
</ul>

<h3>Title: Cost-aware LLM-based Online Dataset Annotation</h3>
<ul>
<li><strong>Authors: </strong>Eray Can Elumar, Cem Tekin, Osman Yagan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15101">https://arxiv.org/abs/2505.15101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15101">https://arxiv.org/pdf/2505.15101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15101]] Cost-aware LLM-based Online Dataset Annotation(https://arxiv.org/abs/2505.15101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.</li>
</ul>

<h3>Title: Mechanistic evaluation of Transformers and state space models</h3>
<ul>
<li><strong>Authors: </strong>Aryaman Arora, Neil Rathi, Nikil Roashan Selvam, Róbert Csórdas, Dan Jurafsky, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15105">https://arxiv.org/abs/2505.15105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15105">https://arxiv.org/pdf/2505.15105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15105]] Mechanistic evaluation of Transformers and state space models(https://arxiv.org/abs/2505.15105)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State space models (SSMs) for language modelling promise an efficient and performant alternative to quadratic-attention Transformers, yet show variable performance on recalling basic information from the context. While performance on synthetic tasks like Associative Recall (AR) can point to this deficiency, behavioural metrics provide little information as to why--on a mechanistic level--certain architectures fail and others succeed. To address this, we conduct experiments on AR and find that only Transformers and Based SSM models fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3, Hyena) fail. We then use causal interventions to explain why. We find that Transformers and Based learn to store key-value associations in-context using induction heads. By contrast, the SSMs compute these associations only at the last state, with only Mamba succeeding because of its short convolution component. To extend and deepen these findings, we introduce Associative Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR introduces language-like hierarchical structure into the AR setting. We find that all architectures learn the same mechanism as they did for AR, and the same three models succeed at the task. These results reveal that architectures with similar accuracy may still have substantive differences, motivating the adoption of mechanistic evaluations.</li>
</ul>

<h3>Title: StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15107">https://arxiv.org/abs/2505.15107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15107">https://arxiv.org/pdf/2505.15107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15107]] StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization(https://arxiv.org/abs/2505.15107)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our implementation is publicly available at this https URL.</li>
</ul>

<h3>Title: A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents</h3>
<ul>
<li><strong>Authors: </strong>Ian Steenstra, Timothy W. Bickmore</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15108">https://arxiv.org/abs/2505.15108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15108">https://arxiv.org/pdf/2505.15108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15108]] A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents(https://arxiv.org/abs/2505.15108)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.</li>
</ul>

<h3>Title: RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals</h3>
<ul>
<li><strong>Authors: </strong>Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15110">https://arxiv.org/abs/2505.15110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15110">https://arxiv.org/pdf/2505.15110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15110]] RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals(https://arxiv.org/abs/2505.15110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The table reasoning task, crucial for efficient data acquisition, aims to answer questions based on the given table. Recently, reasoning large language models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance reasoning capabilities, leading to brilliant performance on table reasoning. However, Long CoT suffers from high cost for training and exhibits low reliability due to table content hallucinations. Therefore, we propose Row-of-Thought (RoT), which performs iteratively row-wise table traversal, allowing for reasoning extension and reflection-based refinement at each traversal. Scaling reasoning length by row-wise traversal and leveraging reflection capabilities of LLMs, RoT is training-free. The sequential traversal encourages greater attention to the table, thus reducing hallucinations. Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions and TableBench with comparable models, proving its effectiveness. Also, RoT outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.</li>
</ul>

<h3>Title: iPad: Iterative Proposal-centric End-to-End Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Ke Guo, Haochen Liu, Xiaojun Wu, Jia Pan, Chen Lv</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15111">https://arxiv.org/abs/2505.15111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15111">https://arxiv.org/pdf/2505.15111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15111]] iPad: Iterative Proposal-centric End-to-End Autonomous Driving(https://arxiv.org/abs/2505.15111)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>End-to-end (E2E) autonomous driving systems offer a promising alternative to traditional modular pipelines by reducing information loss and error accumulation, with significant potential to enhance both mobility and safety. However, most existing E2E approaches directly generate plans based on dense bird's-eye view (BEV) grid features, leading to inefficiency and limited planning awareness. To address these limitations, we propose iterative Proposal-centric autonomous driving (iPad), a novel framework that places proposals - a set of candidate future plans - at the center of feature extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder that iteratively refines proposals and their associated features through proposal-anchored attention, effectively fusing multi-view image data. Additionally, we introduce two lightweight, proposal-centric auxiliary tasks - mapping and prediction - that improve planning quality with minimal computational overhead. Extensive experiments on the NAVSIM and CARLA Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art performance while being significantly more efficient than prior leading methods.</li>
</ul>

<h3>Title: An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15117">https://arxiv.org/abs/2505.15117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15117">https://arxiv.org/pdf/2505.15117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15117]] An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents(https://arxiv.org/abs/2505.15117)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at this https URL.</li>
</ul>

<h3>Title: Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Ta Duc Huy, Duy Anh Huynh, Yutong Xie, Yuankai Qi, Qi Chen, Phi Le Nguyen, Sen Kim Tran, Son Lam Phung, Anton van den Hengel, Zhibin Liao, Minh-Son To, Johan W. Verjans, Vu Minh Hieu Phan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15123">https://arxiv.org/abs/2505.15123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15123">https://arxiv.org/pdf/2505.15123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15123]] Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding(https://arxiv.org/abs/2505.15123)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Visual grounding (VG) is the capability to identify the specific regions in an image associated with a particular text description. In medical imaging, VG enhances interpretability by highlighting relevant pathological features corresponding to textual descriptions, improving model transparency and trustworthiness for wider adoption of deep learning models in clinical practice. Current models struggle to associate textual descriptions with disease regions due to inefficient attention mechanisms and a lack of fine-grained token representations. In this paper, we empirically demonstrate two key observations. First, current VLMs assign high norms to background tokens, diverting the model's attention from regions of disease. Second, the global tokens used for cross-modal learning are not representative of local disease tokens. This hampers identifying correlations between the text and disease tokens. To address this, we introduce simple, yet effective Disease-Aware Prompting (DAP) process, which uses the explainability map of a VLM to identify the appropriate image features. This simple strategy amplifies disease-relevant regions while suppressing background interference. Without any additional pixel-level annotations, DAP improves visual grounding accuracy by 20.74% compared to state-of-the-art methods across three major chest X-ray datasets.</li>
</ul>

<h3>Title: A Survey On Secure Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Taobo Liao, Taoran Li, Prathamesh Nadkarni</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15124">https://arxiv.org/abs/2505.15124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15124">https://arxiv.org/pdf/2505.15124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15124]] A Survey On Secure Machine Learning(https://arxiv.org/abs/2505.15124)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>In this survey, we will explore the interaction between secure multiparty computation and the area of machine learning. Recent advances in secure multiparty computation (MPC) have significantly improved its applicability in the realm of machine learning (ML), offering robust solutions for privacy-preserving collaborative learning. This review explores key contributions that leverage MPC to enable multiple parties to engage in ML tasks without compromising the privacy of their data. The integration of MPC with ML frameworks facilitates the training and evaluation of models on combined datasets from various sources, ensuring that sensitive information remains encrypted throughout the process. Innovations such as specialized software frameworks and domain-specific languages streamline the adoption of MPC in ML, optimizing performance and broadening its usage. These frameworks address both semi-honest and malicious threat models, incorporating features such as automated optimizations and cryptographic auditing to ensure compliance and data integrity. The collective insights from these studies highlight MPC's potential in fostering collaborative yet confidential data analysis, marking a significant stride towards the realization of secure and efficient computational solutions in privacy-sensitive industries. This paper investigates a spectrum of SecureML libraries that includes cryptographic protocols, federated learning frameworks, and privacy-preserving algorithms. By surveying the existing literature, this paper aims to examine the efficacy of these libraries in preserving data privacy, ensuring model confidentiality, and fortifying ML systems against adversarial attacks. Additionally, the study explores an innovative application domain for SecureML techniques: the integration of these methodologies in gaming environments utilizing ML.</li>
</ul>

<h3>Title: Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, Ramtin Pedarsani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15130">https://arxiv.org/abs/2505.15130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15130">https://arxiv.org/pdf/2505.15130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15130]] Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models(https://arxiv.org/abs/2505.15130)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT. In this work, we propose AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates adversarial fine-tuning as a minimax optimization problem and provides theoretical guarantees for convergence under smoothness and nonconvex-strong-concavity assumptions. Empirical results across eight datasets using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly improves robustness against common adversarial attacks (e.g., FGSM, PGD), without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical and theoretically grounded approach for robust adaptation of VLMs in resource-constrained settings.</li>
</ul>

<h3>Title: The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15134">https://arxiv.org/abs/2505.15134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15134">https://arxiv.org/pdf/2505.15134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15134]] The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning(https://arxiv.org/abs/2505.15134)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates.</li>
</ul>

<h3>Title: Multispectral Detection Transformer with Infrared-Centric Sensor Fusion</h3>
<ul>
<li><strong>Authors: </strong>Seongmin Hwang, Daeyoung Han, Moongu Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15137">https://arxiv.org/abs/2505.15137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15137">https://arxiv.org/pdf/2505.15137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15137]] Multispectral Detection Transformer with Infrared-Centric Sensor Fusion(https://arxiv.org/abs/2505.15137)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Multispectral object detection aims to leverage complementary information from visible (RGB) and infrared (IR) modalities to enable robust performance under diverse environmental conditions. In this letter, we propose IC-Fusion, a multispectral object detector that effectively fuses visible and infrared features through a lightweight and modalityaware design. Motivated by wavelet analysis and empirical observations, we find that IR images contain structurally rich high-frequency information critical for object localization, while RGB images provide complementary semantic context. To exploit this, we adopt a compact RGB backbone and design a novel fusion module comprising a Multi-Scale Feature Distillation (MSFD) block to enhance RGB features and a three-stage fusion block with Cross-Modal Channel Shuffle Gate (CCSG) and Cross-Modal Large Kernel Gate (CLKG) to facilitate effective cross-modal interaction. Experiments on the FLIR and LLVIP benchmarks demonstrate the effectiveness and efficiency of our IR-centric fusion strategy. Our code is available at this https URL.</li>
</ul>

<h3>Title: Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Badhan Mazumder, Lei Wu, Vince D. Calhoun, Dong Hye Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15139">https://arxiv.org/abs/2505.15139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15139">https://arxiv.org/pdf/2505.15139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15139]] Unified Cross-Modal Attention-Mixer Based Structural-Functional Connectomics Fusion for Neuropsychiatric Disorder Diagnosis(https://arxiv.org/abs/2505.15139)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gaining insights into the structural and functional mechanisms of the brain has been a longstanding focus in neuroscience research, particularly in the context of understanding and treating neuropsychiatric disorders such as Schizophrenia (SZ). Nevertheless, most of the traditional multimodal deep learning approaches fail to fully leverage the complementary characteristics of structural and functional connectomics data to enhance diagnostic performance. To address this issue, we proposed ConneX, a multimodal fusion method that integrates cross-attention mechanism and multilayer perceptron (MLP)-Mixer for refined feature fusion. Modality-specific backbone graph neural networks (GNNs) were firstly employed to obtain feature representation for each modality. A unified cross-modal attention network was then introduced to fuse these embeddings by capturing intra- and inter-modal interactions, while MLP-Mixer layers refined global and local features, leveraging higher-order dependencies for end-to-end classification with a multi-head joint loss. Extensive evaluations demonstrated improved performance on two distinct clinical datasets, highlighting the robustness of our proposed framework.</li>
</ul>

<h3>Title: EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression</h3>
<ul>
<li><strong>Authors: </strong>Tong Cheng, Fu Jie, Xinpeng Ling, Huifa Li, Zhili Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15140">https://arxiv.org/abs/2505.15140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15140">https://arxiv.org/pdf/2505.15140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15140]] EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression(https://arxiv.org/abs/2505.15140)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have been widely used for graph analysis. Federated Graph Learning (FGL) is an emerging learning framework to collaboratively train graph data from various clients. However, since clients are required to upload model parameters to the server in each round, this provides the server with an opportunity to infer each client's data privacy. In this paper, we focus on label distribution attacks(LDAs) that aim to infer the label distributions of the clients' local data. We take the first step to attack client's label distributions in FGL. Firstly, we observe that the effectiveness of LDA is closely related to the variance of node embeddings in GNNs. Next, we analyze the relation between them and we propose a new attack named EC-LDA, which significantly improves the attack effectiveness by compressing node embeddings. Thirdly, extensive experiments on node classification and link prediction tasks across six widely used graph datasets show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal values under both Cos-sim and JS-div evaluation metrics in the CoraFull and LastFM datasets. Finally, we explore the robustness of EC-LDA under differential privacy protection.</li>
</ul>

<h3>Title: BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent Y. F. Tan, Zhuoran Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15141">https://arxiv.org/abs/2505.15141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15141">https://arxiv.org/pdf/2505.15141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15141]] BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms(https://arxiv.org/abs/2505.15141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts.</li>
</ul>

<h3>Title: Filtering Learning Histories Enhances In-Context Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weiqin Chen, Xinjie Zhang, Dharmashankar Subramanian, Santiago Paternain</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15143">https://arxiv.org/abs/2505.15143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15143">https://arxiv.org/pdf/2505.15143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15143]] Filtering Learning Histories Enhances In-Context Reinforcement Learning(https://arxiv.org/abs/2505.15143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformer models (TMs) have exhibited remarkable in-context reinforcement learning (ICRL) capabilities, allowing them to generalize to and improve in previously unseen environments without re-training or fine-tuning. This is typically accomplished by imitating the complete learning histories of a source RL algorithm over a substantial amount of pretraining environments, which, however, may transfer suboptimal behaviors inherited from the source algorithm/dataset. Therefore, in this work, we address the issue of inheriting suboptimality from the perspective of dataset preprocessing. Motivated by the success of the weighted empirical risk minimization, we propose a simple yet effective approach, learning history filtering (LHF), to enhance ICRL by reweighting and filtering the learning histories based on their improvement and stability characteristics. To the best of our knowledge, LHF is the first approach to avoid source suboptimality by dataset preprocessing, and can be combined with the current state-of-the-art (SOTA) ICRL algorithms. We substantiate the effectiveness of LHF through a series of experiments conducted on the well-known ICRL benchmarks, encompassing both discrete environments and continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD, DPT, DICP) as the backbones. LHF exhibits robust performance across a variety of suboptimal scenarios, as well as under varying hyperparameters and sampling strategies. Notably, the superior performance of LHF becomes more pronounced in the presence of noisy data, indicating the significance of filtering learning histories.</li>
</ul>

<h3>Title: CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinran Wang, Songyu Xu, Xiangxuan Shan, Yuxuan Zhang, Muxi Diao, Xueyan Duan, Yanhua Huang, Kongming Liang, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15145">https://arxiv.org/abs/2505.15145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15145">https://arxiv.org/pdf/2505.15145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15145]] CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation(https://arxiv.org/abs/2505.15145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and reproduce cinematographic techniques remains largely uncharted, hindered by the scarcity of expert-annotated data. To bridge this gap, we present CineTechBench, a pioneering benchmark founded on precise, manual annotation by seasoned cinematography experts across key cinematography dimensions. Our benchmark covers seven essential aspects-shot scale, shot angle, composition, camera movement, lighting, color, and focal length-and includes over 600 annotated movie images and 120 movie clips with clear cinematographic techniques. For the understanding task, we design question answer pairs and annotated descriptions to assess MLLMs' ability to interpret and explain cinematographic techniques. For the generation task, we assess advanced video generation models on their capacity to reconstruct cinema-quality camera movements given conditions such as textual prompts or keyframes. We conduct a large-scale evaluation on 15+ MLLMs and 5+ video generation models. Our results offer insights into the limitations of current models and future directions for cinematography understanding and generation in automatically film production and appreciation. The code and benchmark can be accessed at this https URL.</li>
</ul>

<h3>Title: From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Quanwei Liu, Tao Huang, Yanni Dong, Jiaqi Yang, Wei Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15147">https://arxiv.org/abs/2505.15147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15147">https://arxiv.org/pdf/2505.15147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15147]] From Pixels to Images: Deep Learning Advances in Remote Sensing Image Semantic Segmentation(https://arxiv.org/abs/2505.15147)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Remote sensing images (RSIs) capture both natural and human-induced changes on the Earth's surface, serving as essential data for environmental monitoring, urban planning, and resource management. Semantic segmentation (SS) of RSIs enables the fine-grained interpretation of surface features, making it a critical task in remote sensing analysis. With the increasing diversity and volume of RSIs collected by sensors on various platforms, traditional processing methods struggle to maintain efficiency and accuracy. In response, deep learning (DL) has emerged as a transformative approach, enabling substantial advances in remote sensing image semantic segmentation (RSISS) by automating feature extraction and improving segmentation accuracy across diverse modalities. This paper revisits the evolution of DL-based RSISS by categorizing existing approaches into four stages: the early pixel-based methods, the prevailing patch-based and tile-based techniques, and the emerging image-based strategies enabled by foundation models. We analyze these developments from the perspective of feature extraction and learning strategies, revealing the field's progression from pixel-level to tile-level and from unimodal to multimodal segmentation. Furthermore, we conduct a comprehensive evaluation of nearly 40 advanced techniques on a unified dataset to quantitatively characterize their performance and applicability. This review offers a holistic view of DL-based SS for RS, highlighting key advancements, comparative insights, and open challenges to guide future research.</li>
</ul>

<h3>Title: Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Xiaohou Shi, Ke Li, Aobo Liang, Yan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15151">https://arxiv.org/abs/2505.15151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15151">https://arxiv.org/pdf/2505.15151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15151]] Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines(https://arxiv.org/abs/2505.15151)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the past few years, time series foundation models have achieved superior predicting accuracy. However, real-world time series often exhibit significant diversity in their temporal patterns across different time spans and domains, making it challenging for a single model architecture to fit all complex scenarios. In addition, time series data may have multiple variables exhibiting complex correlations between each other. Recent mainstream works have focused on modeling times series in a channel-independent manner in both pretraining and finetuning stages, overlooking the valuable inter-series dependencies. To this end, we propose \textbf{Time Tracker} for better predictions on multivariate time series data. Firstly, we leverage sparse mixture of experts (MoE) within Transformers to handle the modeling of diverse time series patterns, thereby alleviating the learning difficulties of a single model while improving its generalization. Besides, we propose Any-variate Attention, enabling a unified model structure to seamlessly handle both univariate and multivariate time series, thereby supporting channel-independent modeling during pretraining and channel-mixed modeling for finetuning. Furthermore, we design a graph learning module that constructs relations among sequences from frequency-domain features, providing more precise guidance to capture inter-series dependencies in channel-mixed modeling. Based on these advancements, Time Tracker achieves state-of-the-art performance in predicting accuracy, model generalization and adaptability.</li>
</ul>

<h3>Title: Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation</h3>
<ul>
<li><strong>Authors: </strong>Nanxu Gong, Zijun Li, Sixun Dong, Haoyue Bai, Wangyang Ying, Xinyuan Wang, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15152">https://arxiv.org/abs/2505.15152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15152">https://arxiv.org/pdf/2505.15152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15152]] Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation(https://arxiv.org/abs/2505.15152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Feature Transformation (FT) crafts new features from original ones via mathematical operations to enhance dataset expressiveness for downstream models. However, existing FT methods exhibit critical limitations: discrete search struggles with enormous combinatorial spaces, impeding practical use; and continuous search, being highly sensitive to initialization and step sizes, often becomes trapped in local optima, restricting global exploration. To overcome these limitations, DIFFT redefines FT as a reward-guided generative task. It first learns a compact and expressive latent space for feature sets using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then navigates this space to generate high-quality feature embeddings, its trajectory guided by a performance evaluator towards task-specific optima. This synthesis of global distribution learning (from LDM) and targeted optimization (reward guidance) produces potent embeddings, which a novel semi-autoregressive decoder efficiently converts into structured, discrete features, preserving intra-feature dependencies while allowing parallel inter-feature generation. Extensive experiments on 14 benchmark datasets show DIFFT consistently outperforms state-of-the-art baselines in predictive accuracy and robustness, with significantly lower training and inference times.</li>
</ul>

<h3>Title: Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, Han Wang, Can Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15154">https://arxiv.org/abs/2505.15154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15154">https://arxiv.org/pdf/2505.15154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15154]] Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning(https://arxiv.org/abs/2505.15154)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reasoning have significantly enhanced the capabilities of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) across diverse tasks. However, excessive reliance on chain-of-thought (CoT) reasoning can impair model performance and brings unnecessarily lengthened outputs, reducing efficiency. Our work reveals that prolonged reasoning does not universally improve accuracy and even degrade performance on simpler tasks. To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel framework that dynamically switches between short answers and long-form reasoning based on the model perplexity. CAR first generates a short answer and evaluates its perplexity, triggering reasoning only when the model exhibits low confidence (i.e., high perplexity). Experiments across diverse multimodal VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both short-answer and long-form reasoning approaches, striking an optimal balance between accuracy and efficiency.</li>
</ul>

<h3>Title: Privacy-Preserving Socialized Recommendation based on Multi-View Clustering in a Cloud Environment</h3>
<ul>
<li><strong>Authors: </strong>Cheng Guo, Jing Jia, Peng Wang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15156">https://arxiv.org/abs/2505.15156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15156">https://arxiv.org/pdf/2505.15156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15156]] Privacy-Preserving Socialized Recommendation based on Multi-View Clustering in a Cloud Environment(https://arxiv.org/abs/2505.15156)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Recommendation as a service has improved the quality of our lives and plays a significant role in variant aspects. However, the preference of users may reveal some sensitive information, so that the protection of privacy is required. In this paper, we propose a privacy-preserving, socialized, recommendation protocol that introduces information collected from online social networks to enhance the quality of the recommendation. The proposed scheme can calculate the similarity between users to determine their potential relationships and interests, and it also can protect the users' privacy from leaking to an untrusted third party. The security analysis and experimental results showed that our proposed scheme provides excellent performance and is feasible for real-world applications.</li>
</ul>

<h3>Title: ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yunsheng Ma, Burhaneddin Yaman, Xin Ye, Mahmut Yurt, Jingru Luo, Abhirup Mallik, Ziran Wang, Liu Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15158">https://arxiv.org/abs/2505.15158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15158">https://arxiv.org/pdf/2505.15158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15158]] ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving(https://arxiv.org/abs/2505.15158)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances have explored integrating large language models (LLMs) into end-to-end autonomous driving systems to enhance generalization and interpretability. However, most existing approaches are limited to either driving performance or vision-language reasoning, making it difficult to achieve both simultaneously. In this paper, we propose ALN-P3, a unified co-distillation framework that introduces cross-modal alignment between "fast" vision-based autonomous driving systems and "slow" language-driven reasoning modules. ALN-P3 incorporates three novel alignment mechanisms: Perception Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A), which explicitly align visual tokens with corresponding linguistic outputs across the full perception, prediction, and planning stack. All alignment modules are applied only during training and incur no additional costs during inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X, TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both driving decisions and language reasoning, achieving state-of-the-art results.</li>
</ul>

<h3>Title: Lossless Token Merging Even Without Fine-Tuning in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Lee, Dong-Wan Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15160">https://arxiv.org/abs/2505.15160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15160">https://arxiv.org/pdf/2505.15160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15160]] Lossless Token Merging Even Without Fine-Tuning in Vision Transformers(https://arxiv.org/abs/2505.15160)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Although Vision Transformers (ViTs) have become the standard architecture in computer vision, their massive sizes lead to significant computational overhead. Token compression techniques have attracted considerable attention to address this issue, but they often suffer from severe information loss, requiring extensive additional training to achieve practical performance. In this paper, we propose Adaptive Token Merging (ATM), a novel method that ensures lossless token merging, eliminating the need for fine-tuning while maintaining competitive performance. ATM adaptively reduces tokens across layers and batches by carefully adjusting layer-specific similarity thresholds, thereby preventing the undesirable merging of less similar tokens with respect to each layer. Furthermore, ATM introduces a novel token matching technique that considers not only similarity but also merging sizes, particularly for the final layers, to minimize the information loss incurred from each merging operation. We empirically validate our method across a wide range of pretrained models, demonstrating that ATM not only outperforms all existing training-free methods but also surpasses most training-intensive approaches, even without additional training. Remarkably, training-free ATM achieves over a 30% reduction in FLOPs for the DeiT-T and DeiT-S models without any drop in their original accuracy.</li>
</ul>

<h3>Title: AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15173">https://arxiv.org/abs/2505.15173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15173">https://arxiv.org/pdf/2505.15173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15173]] AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection(https://arxiv.org/abs/2505.15173)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, particularly in video generation, has led to unprecedented creative capabilities but also increased threats to information integrity, identity security, and public trust. Existing detection methods, while effective in general scenarios, lack robust solutions for human-centric videos, which pose greater risks due to their realism and potential for legal and ethical misuse. Moreover, current detection approaches often suffer from poor generalization, limited scalability, and reliance on labor-intensive supervised fine-tuning. To address these challenges, we propose AvatarShield, the first interpretable MLLM-based framework for detecting human-centric fake videos, enhanced via Group Relative Policy Optimization (GRPO). Through our carefully designed accuracy detection reward and temporal compensation reward, it effectively avoids the use of high-cost text annotation data, enabling precise temporal modeling and forgery detection. Meanwhile, we design a dual-encoder architecture, combining high-level semantic reasoning and low-level artifact amplification to guide MLLMs in effective forgery detection. We further collect FakeHumanVid, a large-scale human-centric video benchmark that includes synthesis methods guided by pose, audio, and text inputs, enabling rigorous evaluation of detection methods in real-world scenes. Extensive experiments show that AvatarShield significantly outperforms existing approaches in both in-domain and cross-domain detection, setting a new standard for human-centric video forensics.</li>
</ul>

<h3>Title: Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss</h3>
<ul>
<li><strong>Authors: </strong>Bo-Han Lai, Pin-Han Huang, Bo-Han Kung, Shang-Tse Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15174">https://arxiv.org/abs/2505.15174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15174">https://arxiv.org/pdf/2505.15174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15174]] Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss(https://arxiv.org/abs/2505.15174)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel, efficient Block Reflector Orthogonal (BRO) layer that enhances the capability of orthogonal layers on constructing more expressive Lipschitz neural architectures. In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to increase margin for most data points. This enables Lipschitz models to provide better certified robustness. By employing our BRO layer and loss function, we design BRONet - a simple yet effective Lipschitz neural network that achieves state-of-the-art certified robustness. Extensive experiments and empirical analysis on CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms existing baselines. The implementation is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets</h3>
<ul>
<li><strong>Authors: </strong>Qian Zhou, Xianda Guo, Jilong Wang, Chuanfu Shen, Zhongyuan Wang, Hua Zou, Qin Zou, Chao Liang, Chen Long, Gang Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15176">https://arxiv.org/abs/2505.15176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15176">https://arxiv.org/pdf/2505.15176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15176]] Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets(https://arxiv.org/abs/2505.15176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalized gait recognition, which aims to achieve robust performance across diverse domains, remains a challenging problem due to severe domain shifts in viewpoints, appearances, and environments. While mixed-dataset training is widely used to enhance generalization, it introduces new obstacles including inter-dataset optimization conflicts and redundant or noisy samples, both of which hinder effective representation learning. To address these challenges, we propose a unified framework that systematically improves cross-domain gait recognition. First, we design a disentangled triplet loss that isolates supervision signals across datasets, mitigating gradient conflicts during optimization. Second, we introduce a targeted dataset distillation strategy that filters out the least informative 20\% of training samples based on feature redundancy and prediction uncertainty, enhancing data efficiency. Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that our method significantly improves cross-dataset recognition for both GaitBase and DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will be released at this https URL.</li>
</ul>

<h3>Title: NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Gu, Ziyue Qiao, Xiao Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15180">https://arxiv.org/abs/2505.15180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15180">https://arxiv.org/pdf/2505.15180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15180]] NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration(https://arxiv.org/abs/2505.15180)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown remarkable performance across various domains, yet they often struggle with model bias, particularly in the presence of class imbalance. This bias can lead to suboptimal performance and unfair predictions, especially for underrepresented classes. We introduce NeuBM (Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs through neutral input calibration. NeuBM leverages a dynamically updated neutral graph to estimate and correct the inherent biases of the model. By subtracting the logits obtained from the neutral graph from those of the input graph, NeuBM effectively recalibrates the model's predictions, reducing bias across different classes. Our method integrates seamlessly into existing GNN architectures and training procedures, requiring minimal computational overhead. Extensive experiments on multiple benchmark datasets demonstrate that NeuBM significantly improves the balanced accuracy and recall of minority classes, while maintaining strong overall performance. The effectiveness of NeuBM is particularly pronounced in scenarios with severe class imbalance and limited labeled data, where traditional methods often struggle. We provide theoretical insights into how NeuBM achieves bias mitigation, relating it to the concept of representation balancing. Our analysis reveals that NeuBM not only adjusts the final predictions but also influences the learning of balanced feature representations throughout the network.</li>
</ul>

<h3>Title: AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Yangting Shi, Renjie He, Le Hui, Xiang Li, Jian Yang, Ming-Ming Cheng, Yimian Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15184">https://arxiv.org/abs/2505.15184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15184">https://arxiv.org/pdf/2505.15184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15184]] AuxDet: Auxiliary Metadata Matters for Omni-Domain Infrared Small Target Detection(https://arxiv.org/abs/2505.15184)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Omni-domain infrared small target detection (IRSTD) poses formidable challenges, as a single model must seamlessly adapt to diverse imaging systems, varying resolutions, and multiple spectral bands simultaneously. Current approaches predominantly rely on visual-only modeling paradigms that not only struggle with complex background interference and inherently scarce target features, but also exhibit limited generalization capabilities across complex omni-scene environments where significant domain shifts and appearance variations occur. In this work, we reveal a critical oversight in existing paradigms: the neglect of readily available auxiliary metadata describing imaging parameters and acquisition conditions, such as spectral bands, sensor platforms, resolution, and observation perspectives. To address this limitation, we propose the Auxiliary Metadata Driven Infrared Small Target Detector (AuxDet), a novel multi-modal framework that fundamentally reimagines the IRSTD paradigm by incorporating textual metadata for scene-aware optimization. Through a high-dimensional fusion module based on multi-layer perceptrons (MLPs), AuxDet dynamically integrates metadata semantics with visual features, guiding adaptive representation learning for each individual sample. Additionally, we design a lightweight prior-initialized enhancement module using 1D convolutional blocks to further refine fused features and recover fine-grained target cues. Extensive experiments on the challenging WideIRSTD-Full benchmark demonstrate that AuxDet consistently outperforms state-of-the-art methods, validating the critical role of auxiliary information in improving robustness and accuracy in omni-domain IRSTD tasks. Code is available at this https URL.</li>
</ul>

<h3>Title: MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15185">https://arxiv.org/abs/2505.15185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15185">https://arxiv.org/pdf/2505.15185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15185]] MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models(https://arxiv.org/abs/2505.15185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at this https URL.</li>
</ul>

<h3>Title: Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Hana Satou, Alan Mitkiy, F Monkey</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15191">https://arxiv.org/abs/2505.15191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15191">https://arxiv.org/pdf/2505.15191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15191]] Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation(https://arxiv.org/abs/2505.15191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transfer learning under domain shift remains a fundamental challenge due to the divergence between source and target data manifolds. In this paper, we propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework that decomposes adversarial perturbations into on-manifold and off-manifold components to simultaneously capture semantic variation and model brittleness. We theoretically demonstrate that enforcing on-manifold consistency reduces hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions. Moreover, we introduce a geometry-aware alignment loss that minimizes geodesic discrepancy between source and target manifolds. Experiments on DomainNet, VisDA, and Office-Home show that MAADA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, demonstrating superior structural robustness and cross-domain generalization.</li>
</ul>

<h3>Title: Leveraging Foundation Models for Multimodal Graph-Based Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ziaeetabar, Florentin Wörgötter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15192">https://arxiv.org/abs/2505.15192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15192">https://arxiv.org/pdf/2505.15192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15192]] Leveraging Foundation Models for Multimodal Graph-Based Action Recognition(https://arxiv.org/abs/2505.15192)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Foundation models have ushered in a new era for multimodal video understanding by enabling the extraction of rich spatiotemporal and semantic representations. In this work, we introduce a novel graph-based framework that integrates a vision-language foundation, leveraging VideoMAE for dynamic visual encoding and BERT for contextual textual embedding, to address the challenge of recognizing fine-grained bimanual manipulation actions. Departing from conventional static graph architectures, our approach constructs an adaptive multimodal graph where nodes represent frames, objects, and textual annotations, and edges encode spatial, temporal, and semantic relationships. These graph structures evolve dynamically based on learned interactions, allowing for flexible and context-aware reasoning. A task-specific attention mechanism within a Graph Attention Network further enhances this reasoning by modulating edge importance based on action semantics. Through extensive evaluations on diverse benchmark datasets, we demonstrate that our method consistently outperforms state-of-the-art baselines, underscoring the strength of combining foundation models with dynamic graph-based reasoning for robust and generalizable action recognition.</li>
</ul>

<h3>Title: GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hana Satou, F Monkey</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15194">https://arxiv.org/abs/2505.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15194">https://arxiv.org/pdf/2505.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15194]] GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation(https://arxiv.org/abs/2505.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Domain adaptation remains a challenge when there is significant manifold discrepancy between source and target domains. Although recent methods leverage manifold-aware adversarial perturbations to perform data augmentation, they often neglect precise manifold alignment and systematic exploration of structured perturbations. To address this, we propose GAMA (Geometry-Aware Manifold Alignment), a structured framework that achieves explicit manifold alignment via adversarial perturbation guided by geometric information. GAMA systematically employs tangent space exploration and manifold-constrained adversarial optimization, simultaneously enhancing semantic consistency, robustness to off-manifold deviations, and cross-domain alignment. Theoretical analysis shows that GAMA tightens the generalization bound via structured regularization and explicit alignment. Empirical results on DomainNet, VisDA, and Office-Home demonstrate that GAMA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, exhibiting superior robustness, generalization, and manifold alignment capability.</li>
</ul>

<h3>Title: Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</h3>
<ul>
<li><strong>Authors: </strong>Christian Walder, Deep Karkhanis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15201">https://arxiv.org/abs/2505.15201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15201">https://arxiv.org/pdf/2505.15201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15201]] Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems(https://arxiv.org/abs/2505.15201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function. While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains. We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.</li>
</ul>

<h3>Title: DUSK: Do Not Unlearn Shared Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15209">https://arxiv.org/abs/2505.15209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15209">https://arxiv.org/pdf/2505.15209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15209]] DUSK: Do Not Unlearn Shared Knowledge(https://arxiv.org/abs/2505.15209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about the unauthorized use of copyrighted or sensitive data. Machine unlearning aims to remove such 'forget' data while preserving utility and information from the 'retain' set. However, existing evaluations typically assume that forget and retain sets are fully disjoint, overlooking realistic scenarios where they share overlapping content. For instance, a news article may need to be unlearned, even though the same event, such as an earthquake in Japan, is also described factually on Wikipedia. Effective unlearning should remove the specific phrasing of the news article while preserving publicly supported facts. In this paper, we introduce DUSK, a benchmark designed to evaluate unlearning methods under realistic data overlap. DUSK constructs document sets that describe the same factual content in different styles, with some shared information appearing across all sets and other content remaining unique to each. When one set is designated for unlearning, an ideal method should remove its unique content while preserving shared facts. We define seven evaluation metrics to assess whether unlearning methods can achieve this selective removal. Our evaluation of nine recent unlearning methods reveals a key limitation: while most can remove surface-level text, they often fail to erase deeper, context-specific knowledge without damaging shared content. We release DUSK as a public benchmark to support the development of more precise and reliable unlearning techniques for real-world applications.</li>
</ul>

<h3>Title: Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jie Ma, Ning Qu, Zhitao Gao, Rui Xing, Jun Liu, Hongbin Pei, Jiang Xie, Linyun Song, Pinghui Wang, Jing Tao, Zhou Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15210">https://arxiv.org/abs/2505.15210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15210">https://arxiv.org/pdf/2505.15210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15210]] Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs(https://arxiv.org/abs/2505.15210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at this https URL.</li>
</ul>

<h3>Title: Group Distributionally Robust Optimization with Flexible Sample Queries</h3>
<ul>
<li><strong>Authors: </strong>Haomin Bai, Dingzhi Yu, Shuai Li, Haipeng Luo, Lijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15212">https://arxiv.org/abs/2505.15212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15212">https://arxiv.org/pdf/2505.15212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15212]] Group Distributionally Robust Optimization with Flexible Sample Queries(https://arxiv.org/abs/2505.15212)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Group distributionally robust optimization (GDRO) aims to develop models that perform well across $m$ distributions simultaneously. Existing GDRO algorithms can only process a fixed number of samples per iteration, either 1 or $m$, and therefore can not support scenarios where the sample size varies dynamically. To address this limitation, we investigate GDRO with flexible sample queries and cast it as a two-player game: one player solves an online convex optimization problem, while the other tackles a prediction with limited advice (PLA) problem. Within such a game, we propose a novel PLA algorithm, constructing appropriate loss estimators for cases where the sample size is either 1 or not, and updating the decision using follow-the-regularized-leader. Then, we establish the first high-probability regret bound for non-oblivious PLA. Building upon the above approach, we develop a GDRO algorithm that allows an arbitrary and varying sample size per round, achieving a high-probability optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t \frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$. This result demonstrates that the optimization error decreases as the number of samples increases and implies a consistent sample complexity of $O(m\log (m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary and real-world multi-class datasets.</li>
</ul>

<h3>Title: KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Sampanna Yashwant Kahu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15213">https://arxiv.org/abs/2505.15213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15213">https://arxiv.org/pdf/2505.15213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15213]] KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning(https://arxiv.org/abs/2505.15213)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Efficient task scheduling is paramount in the Linux kernel, where the Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance high utilization with interactive responsiveness. This research pioneers the use of deep learning techniques to predict the sequence of tasks selected by CFS, aiming to evaluate the feasibility of a more generalized and potentially more adaptive task scheduler for diverse workloads. Our core contributions are twofold: first, the systematic generation and curation of a novel scheduling dataset from a running Linux kernel, capturing real-world CFS behavior; and second, the development, training, and evaluation of a Long Short-Term Memory (LSTM) network designed to accurately forecast the next task to be scheduled. This paper further discusses the practical pathways and implications of integrating such a predictive model into the kernel's scheduling framework. The findings and methodologies presented herein open avenues for data-driven advancements in kernel scheduling, with the full source code provided for reproducibility and further exploration.</li>
</ul>

<h3>Title: BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</h3>
<ul>
<li><strong>Authors: </strong>Andy K. Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Y. Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, Sara Hong, Nardos Demilew, Shivatmica Murgai, Jason Tran, Nishka Kacheria, Ethan Ho, Denis Liu, Lauren McLane, Olivia Bruvik, Dai-Rong Han, Seungwoo Kim, Akhil Vyas, Cuiyuanxiu Chen, Ryan Li, Weiran Xu, Jonathan Z. Ye, Prerit Choudhary, Siddharth M. Bhatia, Vikram Sivashankar, Yuxuan Bao, Dawn Song, Dan Boneh, Daniel E. Ho, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15216">https://arxiv.org/abs/2505.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15216">https://arxiv.org/pdf/2505.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15216]] BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems(https://arxiv.org/abs/2505.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>AI agents have the potential to significantly alter the cybersecurity landscape. To help us understand this change, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a specific vulnerability), and Patch (patching a specific vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code, OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch, mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit scores of 32.5% and 57.5% respectively; in contrast, the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 40-67.5% and Patch scores of 45-60%.</li>
</ul>

<h3>Title: Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Haotian Qin, Dongliang Chang, Yueying Gao, Bingyao Yu, Lei Chen, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15217">https://arxiv.org/abs/2505.15217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15217">https://arxiv.org/pdf/2505.15217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15217]] Multimodal Conditional Information Bottleneck for Generalizable AI-Generated Image Detection(https://arxiv.org/abs/2505.15217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although existing CLIP-based methods for detecting AI-generated images have achieved promising results, they are still limited by severe feature redundancy, which hinders their generalization ability. To address this issue, incorporating an information bottleneck network into the task presents a straightforward solution. However, relying solely on image-corresponding prompts results in suboptimal performance due to the inherent diversity of prompts. In this paper, we propose a multimodal conditional bottleneck network to reduce feature redundancy while enhancing the discriminative power of features extracted by CLIP, thereby improving the model's generalization ability. We begin with a semantic analysis experiment, where we observe that arbitrary text features exhibit lower cosine similarity with real image features than with fake image features in the CLIP feature space, a phenomenon we refer to as "bias". Therefore, we introduce InfoFD, a text-guided AI-generated image detection framework. InfoFD consists of two key components: the Text-Guided Conditional Information Bottleneck (TGCIB) and Dynamic Text Orthogonalization (DTO). TGCIB improves the generalizability of learned representations by conditioning on both text and class modalities. DTO dynamically updates weighted text features, preserving semantic information while leveraging the global "bias". Our model achieves exceptional generalization performance on the GenImage dataset and latest generative models. Our code is available at this https URL.</li>
</ul>

<h3>Title: Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Mathew Vanherreweghe, Lirandë Pira, Patrick Rebentrost</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15228">https://arxiv.org/abs/2505.15228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15228">https://arxiv.org/pdf/2505.15228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15228]] Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks(https://arxiv.org/abs/2505.15228)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture combining Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO). Our primary contribution involves reformulating the degree selection problem as a QUBO task, reducing the complexity from $O(D^N)$ to a single optimization step per layer. This approach enables efficient degree selection across neurons while maintaining computational tractability. The architecture performs well in regression tasks with limited data, showing good robustness to input scales and natural regularization properties from its polynomial basis. Additionally, theoretical analysis establishes connections between CP-KAN's performance and properties of financial time series. Our empirical validation across multiple domains demonstrates competitive performance compared to several traditional architectures tested, especially in scenarios where data efficiency and numerical stability are important. Our implementation, including strategies for managing computational overhead in larger networks is available in Ref.~\citep{cpkan_implementation}.</li>
</ul>

<h3>Title: Multilingual Prompting for Improving LLM Generation Diversity</h3>
<ul>
<li><strong>Authors: </strong>Qihan Wang, Shidong Pan, Tal Linzen, Emily Black</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15229">https://arxiv.org/abs/2505.15229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15229">https://arxiv.org/pdf/2505.15229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15229]] Multilingual Prompting for Improving LLM Generation Diversity(https://arxiv.org/abs/2505.15229)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.</li>
</ul>

<h3>Title: Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers</h3>
<ul>
<li><strong>Authors: </strong>Peter Súkeník, Christoph H. Lampert, Marco Mondelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15239">https://arxiv.org/abs/2505.15239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15239">https://arxiv.org/pdf/2505.15239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15239]] Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers(https://arxiv.org/abs/2505.15239)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.</li>
</ul>

<h3>Title: GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer</h3>
<ul>
<li><strong>Authors: </strong>Kim Yun, Hana Satou, F Monkey</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15241">https://arxiv.org/abs/2505.15241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15241">https://arxiv.org/pdf/2505.15241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15241]] GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer(https://arxiv.org/abs/2505.15241)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite progress in geometry-aware domain adaptation, current methods such as GAMA still suffer from two unresolved issues: (1) insufficient disentanglement of task-relevant and task-irrelevant manifold dimensions, and (2) rigid perturbation schemes that ignore per-class alignment asymmetries. To address this, we propose GAMA++, a novel framework that introduces (i) latent space disentanglement to isolate label-consistent manifold directions from nuisance factors, and (ii) an adaptive contrastive perturbation strategy that tailors both on- and off-manifold exploration to class-specific manifold curvature and alignment discrepancy. We further propose a cross-domain contrastive consistency loss that encourages local semantic clusters to align while preserving intra-domain diversity. Our method achieves state-of-the-art results on DomainNet, Office-Home, and VisDA benchmarks under both standard and few-shot settings, with notable improvements in class-level alignment fidelity and boundary robustness. GAMA++ sets a new standard for semantic geometry alignment in transfer learning.</li>
</ul>

<h3>Title: Adaptive Plan-Execute Framework for Smart Contract Security Auditing</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wei, Jing Sun, Zijian Zhang, Zhe Hou, Zixiao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15242">https://arxiv.org/abs/2505.15242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15242">https://arxiv.org/pdf/2505.15242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15242]] Adaptive Plan-Execute Framework for Smart Contract Security Auditing(https://arxiv.org/abs/2505.15242)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown great promise in code analysis and auditing; however, they still struggle with hallucinations and limited context-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute framework that enhances smart contract security analysis through dynamic audit planning and structured execution. Unlike conventional LLM-based auditing approaches that follow fixed workflows and predefined steps, SmartAuditFlow dynamically generates and refines audit plans based on the unique characteristics of each smart contract. It continuously adjusts its auditing strategy in response to intermediate LLM outputs and newly detected vulnerabilities, ensuring a more adaptive and precise security assessment. The framework then executes these plans step by step, applying a structured reasoning process to enhance vulnerability detection accuracy while minimizing hallucinations and false positives. To further improve audit precision, SmartAuditFlow integrates iterative prompt optimization and external knowledge sources, such as static analysis tools and Retrieval-Augmented Generation (RAG). This ensures audit decisions are contextually informed and backed by real-world security knowledge, producing comprehensive security reports. Extensive evaluations across multiple benchmarks demonstrate that SmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on common and critical vulnerabilities, 41.2 percent accuracy for comprehensive coverage of known smart contract weaknesses in real-world projects, and successfully identifying all 13 tested CVEs. These results highlight SmartAuditFlow's scalability, cost-effectiveness, and superior adaptability over traditional static analysis tools and contemporary LLM-based approaches, establishing it as a robust solution for automated smart contract auditing.</li>
</ul>

<h3>Title: Reliable Vertical Federated Learning in 5G Core Network Architecture</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Mestoukirdi, Mourad Khanfouci</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15244">https://arxiv.org/abs/2505.15244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15244">https://arxiv.org/pdf/2505.15244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15244]] Reliable Vertical Federated Learning in 5G Core Network Architecture(https://arxiv.org/abs/2505.15244)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This work proposes a new algorithm to mitigate model generalization loss in Vertical Federated Learning (VFL) operating under client reliability constraints within 5G Core Networks (CNs). Recently studied and endorsed by 3GPP, VFL enables collaborative and load-balanced model training and inference across the CN. However, the performance of VFL significantly degrades when the Network Data Analytics Functions (NWDAFs) - which serve as primary clients for VFL model training and inference - experience reliability issues stemming from resource constraints and operational overhead. Unlike edge environments, CN environments adopt fundamentally different data management strategies, characterized by more centralized data orchestration capabilities. This presents opportunities to implement better distributed solutions that take full advantage of the CN data handling flexibility. Leveraging this flexibility, we propose a method that optimizes the vertical feature split among clients while centrally defining their local models based on reliability metrics. Our empirical evaluation demonstrates the effectiveness of our proposed algorithm, showing improved performance over traditional baseline methods.</li>
</ul>

<h3>Title: Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework</h3>
<ul>
<li><strong>Authors: </strong>Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15245">https://arxiv.org/abs/2505.15245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15245">https://arxiv.org/pdf/2505.15245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15245]] Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework(https://arxiv.org/abs/2505.15245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Mitigating Spurious Correlations with Causal Logit Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoling Zhou, Wei Ye, Rui Xie, Shikun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15246">https://arxiv.org/abs/2505.15246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15246">https://arxiv.org/pdf/2505.15246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15246]] Mitigating Spurious Correlations with Causal Logit Perturbation(https://arxiv.org/abs/2505.15246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has seen widespread success in various domains such as science, industry, and society. However, it is acknowledged that certain approaches suffer from non-robustness, relying on spurious correlations for predictions. Addressing these limitations is of paramount importance, necessitating the development of methods that can disentangle spurious correlations. {This study attempts to implement causal models via logit perturbations and introduces a novel Causal Logit Perturbation (CLP) framework to train classifiers with generated causal logit perturbations for individual samples, thereby mitigating the spurious associations between non-causal attributes (i.e., image backgrounds) and classes.} {Our framework employs a} perturbation network to generate sample-wise logit perturbations using a series of training characteristics of samples as inputs. The whole framework is optimized by an online meta-learning-based learning algorithm and leverages human causal knowledge by augmenting metadata in both counterfactual and factual manners. Empirical evaluations on four typical biased learning scenarios, including long-tail learning, noisy label learning, generalized long-tail learning, and subpopulation shift learning, demonstrate that CLP consistently achieves state-of-the-art performance. Moreover, visualization results support the effectiveness of the generated causal perturbations in redirecting model attention towards causal image attributes and dismantling spurious associations.</li>
</ul>

<h3>Title: Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yerin Hwang, Dongryeol Lee, Kyungmin Min, Taegwan Kang, Yong-il Kim, Kyomin Jung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15249">https://arxiv.org/abs/2505.15249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15249">https://arxiv.org/pdf/2505.15249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15249]] Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation(https://arxiv.org/abs/2505.15249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Recently, large vision-language models (LVLMs) have emerged as the preferred tools for judging text-image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist under prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges.</li>
</ul>

<h3>Title: Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Idriss Malek, Abhijit Sharma, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15251">https://arxiv.org/abs/2505.15251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15251">https://arxiv.org/pdf/2505.15251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15251]] Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets(https://arxiv.org/abs/2505.15251)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although Generative Flow Networks (GFlowNets) are designed to capture multiple modes of a reward function, they often suffer from mode collapse in practice, getting trapped in early discovered modes and requiring prolonged training to find diverse solutions. Existing exploration techniques may rely on heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel approach where an auxiliary GFlowNet's exploration is directly driven by the main GFlowNet's training loss. By prioritizing trajectories where the main model exhibits high loss, LGGFN focuses sampling on poorly understood regions of the state space. This targeted exploration significantly accelerates the discovery of diverse, high-reward samples. Empirically, across various benchmarks including grid environments, structured sequence generation, and Bayesian structure learning, LGGFN consistently enhances exploration efficiency and sample diversity compared to baselines. For instance, on a challenging sequence generation task, it discovered over 40 times more unique valid modes while simultaneously reducing the exploration error metric by approximately 99\%.</li>
</ul>

<h3>Title: An Efficient Private GPT Never Autoregressively Decodes</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Li, Yue Guan, Kang Yang, Yu Feng, Ning Liu, Yu Yu, Jingwen Leng, Minyi Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15252">https://arxiv.org/abs/2505.15252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15252">https://arxiv.org/pdf/2505.15252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15252]] An Efficient Private GPT Never Autoregressively Decodes(https://arxiv.org/abs/2505.15252)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, transformer, generative</a></li>
<li><strong>Abstract: </strong>The wide deployment of the generative pre-trained transformer (GPT) has raised privacy concerns for both clients and servers. While cryptographic primitives can be employed for secure GPT inference to protect the privacy of both parties, they introduce considerable performance this http URL accelerate secure inference, this study proposes a public decoding and secure verification approach that utilizes public GPT models, motivated by the observation that securely decoding one and multiple tokens takes a similar latency. The client uses the public model to generate a set of tokens, which are then securely verified by the private model for acceptance. The efficiency of our approach depends on the acceptance ratio of tokens proposed by the public model, which we improve from two aspects: (1) a private sampling protocol optimized for cryptographic primitives and (2) model alignment using knowledge distillation. Our approach improves the efficiency of secure decoding while maintaining the same level of privacy and generation quality as standard secure decoding. Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to standard decoding across three pairs of public-private models and different network conditions.</li>
</ul>

<h3>Title: MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15255">https://arxiv.org/abs/2505.15255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15255">https://arxiv.org/pdf/2505.15255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15255]] MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation(https://arxiv.org/abs/2505.15255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Its covert nature and the complexity of manipulation strategies make it challenging to detect, even for state-of-the-art large language models (LLMs). This concealment also hinders the manual collection of large-scale, high-quality annotations essential for training effective models. Although recent efforts have sought to improve LLM's performance on this task, progress remains limited due to the scarcity of real-world annotated datasets. To address these challenges, we propose MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs' ability to detect mental manipulation in multi-turn dialogue. Our approach includes: (i) EvoSA, an unsupervised data expansion method based on evolutionary operations and speech act theory; (ii) teacher-model-generated multi-task supervision; and (iii) progressive knowledge distillation from complex to simpler tasks. We then constructed the ReaMent dataset with 5,000 real-world dialogue samples, using a MentalMAC-distilled model to assist human annotation. Vast experiments demonstrate that our method significantly narrows the gap between student and teacher models and outperforms competitive LLMs across key evaluation metrics. All code, datasets, and checkpoints will be released upon paper acceptance. Warning: This paper contains content that may be offensive to readers.</li>
</ul>

<h3>Title: Zero-Shot Gaze-based Volumetric Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15256">https://arxiv.org/abs/2505.15256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15256">https://arxiv.org/pdf/2505.15256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15256]] Zero-Shot Gaze-based Volumetric Medical Image Segmentation(https://arxiv.org/abs/2505.15256)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation.</li>
</ul>

<h3>Title: When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Jiahe Guo, Yang Deng, Tongtong Wu, Wenxuan Zhang, Yulin Hu, Xingyu Sui, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15257">https://arxiv.org/abs/2505.15257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15257">https://arxiv.org/pdf/2505.15257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15257]] When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners(https://arxiv.org/abs/2505.15257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-source LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively decoupled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training such as supervised fine-tuning or reinforcement learning, our training-free ablation achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization.</li>
</ul>

<h3>Title: ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search</h3>
<ul>
<li><strong>Authors: </strong>Hyunseok Lee, Jeonghoon Kim, Beomjun Kim, Jihoon Tack, Chansong Jo, Jaehong Lee, Cheonbok Park, Sookyo In, Jinwoo Shin, Kang Min Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15259">https://arxiv.org/abs/2505.15259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15259">https://arxiv.org/pdf/2505.15259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15259]] ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search(https://arxiv.org/abs/2505.15259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines).</li>
</ul>

<h3>Title: AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection</h3>
<ul>
<li><strong>Authors: </strong>Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15261">https://arxiv.org/abs/2505.15261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15261">https://arxiv.org/pdf/2505.15261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15261]] AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection(https://arxiv.org/abs/2505.15261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Existing AI-generated text detection methods heavily depend on large annotated datasets and external threshold tuning, restricting interpretability, adaptability, and zero-shot effectiveness. To address these limitations, we propose AGENT-X, a zero-shot multi-agent framework informed by classical rhetoric and systemic functional linguistics. Specifically, we organize detection guidelines into semantic, stylistic, and structural dimensions, each independently evaluated by specialized linguistic agents that provide explicit reasoning and robust calibrated confidence via semantic steering. A meta agent integrates these assessments through confidence-aware aggregation, enabling threshold-free, interpretable classification. Additionally, an adaptive Mixture-of-Agent router dynamically selects guidelines based on inferred textual characteristics. Experiments on diverse datasets demonstrate that AGENT-X substantially surpasses state-of-the-art supervised and zero-shot approaches in accuracy, interpretability, and generalization.</li>
</ul>

<h3>Title: gen2seg: Generative Models Enable Generalizable Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Om Khangaonkar, Hamed Pirsiavash</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15263">https://arxiv.org/abs/2505.15263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15263">https://arxiv.org/pdf/2505.15263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15263]] gen2seg: Generative Models Enable Generalizable Instance Segmentation(https://arxiv.org/abs/2505.15263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.</li>
</ul>

<h3>Title: Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Zihao Pan, Yu Tong, Weibin Wu, Jingyi Wang, Lifeng Chen, Zhe Zhao, Jiajia Wei, Yitong Qiao, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15265">https://arxiv.org/abs/2505.15265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15265">https://arxiv.org/pdf/2505.15265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15265]] Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs(https://arxiv.org/abs/2505.15265)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.</li>
</ul>

<h3>Title: LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, Jieru Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15269">https://arxiv.org/abs/2505.15269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15269">https://arxiv.org/pdf/2505.15269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15269]] LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval(https://arxiv.org/abs/2505.15269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent developments in Video Large Language Models (Video LLMs) have enabled models to process long video sequences and demonstrate remarkable performance. Nonetheless, studies predominantly focus on offline video question answering, neglecting memory usage and response speed that are essential in various real-world applications, such as Deepseek services, autonomous driving, and robotics. To mitigate these challenges, we propose $\textbf{LiveVLM}$, a training-free framework specifically designed for streaming, online video understanding and real-time interaction. Unlike existing works that process videos only after one question is posed, LiveVLM constructs an innovative streaming-oriented KV cache to process video streams in real-time, retain long-term video details and eliminate redundant KVs, ensuring prompt responses to user queries. For continuous video streams, LiveVLM generates and compresses video key-value tensors (video KVs) to reserve visual information while improving memory efficiency. Furthermore, when a new question is proposed, LiveVLM incorporates an online question-answering process that efficiently fetches both short-term and long-term visual information, while minimizing interference from redundant context. Extensive experiments demonstrate that LiveVLM enables the foundation LLaVA-OneVision model to process 44$\times$ number of frames on the same device, and achieves up to 5$\times$ speedup in response speed compared with SoTA online methods at an input of 256 frames, while maintaining the same or better model performance.</li>
</ul>

<h3>Title: Scaling Diffusion Transformers Efficiently via $μ$P</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zheng, Xinyu Zhang, Rongzhen Wang, Wei Huang, Zhi Tian, Weilin Huang, Jun Zhu, Chongxuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15270">https://arxiv.org/abs/2505.15270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15270">https://arxiv.org/pdf/2505.15270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15270]] Scaling Diffusion Transformers Efficiently via $μ$P(https://arxiv.org/abs/2505.15270)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\mu$P on text-to-image generation by scaling PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\mu$P as a principled and efficient framework for scaling diffusion Transformers.</li>
</ul>

<h3>Title: DiffProb: Data Pruning for Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Eduarda Caldeira, Jan Niklas Kolf, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15272">https://arxiv.org/abs/2505.15272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15272">https://arxiv.org/pdf/2505.15272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15272]] DiffProb: Data Pruning for Face Recognition(https://arxiv.org/abs/2505.15272)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Face recognition models have made substantial progress due to advances in deep learning and the availability of large-scale datasets. However, reliance on massive annotated datasets introduces challenges related to training computational cost and data storage, as well as potential privacy concerns regarding managing large face datasets. This paper presents DiffProb, the first data pruning approach for the application of face recognition. DiffProb assesses the prediction probabilities of training samples within each identity and prunes the ones with identical or close prediction probability values, as they are likely reinforcing the same decision boundaries, and thus contribute minimally with new information. We further enhance this process with an auxiliary cleaning mechanism to eliminate mislabeled and label-flipped samples, boosting data quality with minimal loss. Extensive experiments on CASIA-WebFace with different pruning ratios and multiple benchmarks, including LFW, CFP-FP, and IJB-C, demonstrate that DiffProb can prune up to 50% of the dataset while maintaining or even, in some settings, improving the verification accuracies. Additionally, we demonstrate DiffProb's robustness across different architectures and loss functions. Our method significantly reduces training cost and data volume, enabling efficient face recognition training and reducing the reliance on massive datasets and their demanding management.</li>
</ul>

<h3>Title: Web-Shepherd: Advancing PRMs for Reinforcing Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong-woo Kwak, Dongjin Kang, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15277">https://arxiv.org/abs/2505.15277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15277">https://arxiv.org/pdf/2505.15277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15277]] Web-Shepherd: Advancing PRMs for Reinforcing Web Agents(https://arxiv.org/abs/2505.15277)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.</li>
</ul>

<h3>Title: Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeongjeong Kim, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15291">https://arxiv.org/abs/2505.15291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15291">https://arxiv.org/pdf/2505.15291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15291]] Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization(https://arxiv.org/abs/2505.15291)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced text generation capabilities, including tasks like summarization, often producing coherent and fluent outputs. However, faithfulness to source material remains a significant challenge due to the generation of hallucinations. While extensive research focuses on detecting and reducing these inaccuracies, less attention has been paid to the positional distribution of hallucination within generated text, particularly in long outputs. In this work, we investigate where hallucinations occur in LLM-based long response generation, using long document summarization as a key case study. Focusing on the challenging setting of long context-aware long response generation, we find a consistent and concerning phenomenon: hallucinations tend to concentrate disproportionately in the latter parts of the generated long response. To understand this bias, we explore potential contributing factors related to the dynamics of attention and decoding over long sequences. Furthermore, we investigate methods to mitigate this positional hallucination, aiming to improve faithfulness specifically in the concluding segments of long outputs.</li>
</ul>

<h3>Title: LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qianyue Hao, Yiwen Song, Qingmin Liao, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15293">https://arxiv.org/abs/2505.15293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15293">https://arxiv.org/pdf/2505.15293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15293]] LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models(https://arxiv.org/abs/2505.15293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy exploration is critical in reinforcement learning (RL), where existing approaches include greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design LLM-Explorer to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at this https URL for reproducibility.</li>
</ul>

<h3>Title: R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections</h3>
<ul>
<li><strong>Authors: </strong>Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15294">https://arxiv.org/abs/2505.15294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15294">https://arxiv.org/pdf/2505.15294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15294]] R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections(https://arxiv.org/abs/2505.15294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose R3GS, a robust reconstruction and relocalization framework tailored for unconstrained datasets. Our method uses a hybrid representation during training. Each anchor combines a global feature from a convolutional neural network (CNN) with a local feature encoded by the multiresolution hash grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict the attributes of each Gaussians, including color, opacity, and covariance. To mitigate the adverse effects of transient objects on the reconstruction process, we ffne-tune a lightweight human detection network. Once ffne-tuned, this network generates a visibility map that efffciently generalizes to other transient objects (such as posters, banners, and cars) with minimal need for further adaptation. Additionally, to address the challenges posed by sky regions in outdoor scenes, we propose an effective sky-handling technique that incorporates a depth prior as a constraint. This allows the inffnitely distant sky to be represented on the surface of a large-radius sky sphere, signiffcantly reducing ffoaters caused by errors in sky reconstruction. Furthermore, we introduce a novel relocalization method that remains robust to changes in lighting conditions while estimating the camera pose of a given image within the reconstructed 3DGS scene. As a result, R3GS significantly enhances rendering ffdelity, improves both training and rendering efffciency, and reduces storage requirements. Our method achieves state-of-the-art performance compared to baseline methods on in-the-wild datasets. The code will be made open-source following the acceptance of the paper.</li>
</ul>

<h3>Title: Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Yixiao Liu, Jingheng Pan, Liang Ding, Longyue Wang, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15297">https://arxiv.org/abs/2505.15297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15297">https://arxiv.org/pdf/2505.15297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15297]] Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites(https://arxiv.org/abs/2505.15297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detoxifying offensive language while preserving the speaker's original intent is a challenging yet critical goal for improving the quality of online interactions. Although large language models (LLMs) show promise in rewriting toxic content, they often default to overly polite rewrites, distorting the emotional tone and communicative intent. This problem is especially acute in Chinese, where toxicity often arises implicitly through emojis, homophones, or discourse context. We present ToxiRewriteCN, the first Chinese detoxification dataset explicitly designed to preserve sentiment polarity. The dataset comprises 1,556 carefully annotated triplets, each containing a toxic sentence, a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five real-world scenarios: standard expressions, emoji-induced and homophonic toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs, including commercial and open-source models with variant architectures, across four dimensions: detoxification accuracy, fluency, content preservation, and sentiment polarity. Results show that while commercial and MoE models perform best overall, all models struggle to balance safety with emotional fidelity in more subtle or context-heavy settings such as emoji, homophone, and dialogue-based inputs. We release ToxiRewriteCN to support future research on controllable, sentiment-aware detoxification for Chinese.</li>
</ul>

<h3>Title: Multi-Hop Question Generation via Dual-Perspective Keyword Guidance</h3>
<ul>
<li><strong>Authors: </strong>Maodong Li, Longyin Zhang, Fang Kong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15299">https://arxiv.org/abs/2505.15299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15299">https://arxiv.org/pdf/2505.15299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15299]] Multi-Hop Question Generation via Dual-Perspective Keyword Guidance(https://arxiv.org/abs/2505.15299)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords (i.e., question and document keywords) and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner's intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task.</li>
</ul>

<h3>Title: Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Song, Qianyue Hao, Qingmin Liao, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15306">https://arxiv.org/abs/2505.15306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15306">https://arxiv.org/pdf/2505.15306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15306]] Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One(https://arxiv.org/abs/2505.15306)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model ensemble is a useful approach in reinforcement learning (RL) for training effective agents. Despite wide success of RL, training effective agents remains difficult due to the multitude of factors requiring careful tuning, such as algorithm selection, hyperparameter settings, and even random seed choices, all of which can significantly influence an agent's performance. Model ensemble helps overcome this challenge by combining multiple weak agents into a single, more powerful one, enhancing overall performance. However, existing ensemble methods, such as majority voting and Boltzmann addition, are designed as fixed strategies and lack a semantic understanding of specific tasks, limiting their adaptability and effectiveness. To address this, we propose LLM-Ens, a novel approach that enhances RL model ensemble with task-specific semantic understandings driven by large language models (LLMs). Given a task, we first design an LLM to categorize states in this task into distinct 'situations', incorporating high-level descriptions of the task conditions. Then, we statistically analyze the strengths and weaknesses of each individual agent to be used in the ensemble in each situation. During the inference time, LLM-Ens dynamically identifies the changing task situation and switches to the agent that performs best in the current situation, ensuring dynamic model selection in the evolving task condition. Our approach is designed to be compatible with agents trained with different random seeds, hyperparameter settings, and various RL algorithms. Extensive experiments on the Atari benchmark show that LLM-Ens significantly improves the RL model ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility, our code is open-source at this https URL.</li>
</ul>

<h3>Title: BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ji Guo, Xiaolei Wen, Wenbo Jiang, Cheng Huang, Jinjin Li, Hongwei Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15308">https://arxiv.org/abs/2505.15308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15308">https://arxiv.org/pdf/2505.15308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15308]] BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution(https://arxiv.org/abs/2505.15308)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>With the widespread application of super-resolution (SR) in various fields, researchers have begun to investigate its security. Previous studies have demonstrated that SR models can also be subjected to backdoor attacks through data poisoning, affecting downstream tasks. A backdoor SR model generates an attacker-predefined target image when given a triggered image while producing a normal high-resolution (HR) output for clean images. However, prior backdoor attacks on SR models have primarily focused on the stealthiness of poisoned low-resolution (LR) images while ignoring the stealthiness of poisoned HR images, making it easy for users to detect anomalous data. To address this problem, we propose BadSR, which improves the stealthiness of poisoned HR images. The key idea of BadSR is to approximate the clean HR image and the pre-defined target image in the feature space while ensuring that modifications to the clean HR image remain within a constrained range. The poisoned HR images generated by BadSR can be integrated with existing triggers. To further improve the effectiveness of BadSR, we design an adversarially optimized trigger and a backdoor gradient-driven poisoned sample selection method based on a genetic algorithm. The experimental results show that BadSR achieves a high attack success rate in various models and data sets, significantly affecting downstream tasks.</li>
</ul>

<h3>Title: Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yurun Yuan, Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15311">https://arxiv.org/abs/2505.15311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15311">https://arxiv.org/pdf/2505.15311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15311]] Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning(https://arxiv.org/abs/2505.15311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.</li>
</ul>

<h3>Title: Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Shu, Vasileios Lampos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15312">https://arxiv.org/abs/2505.15312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15312">https://arxiv.org/pdf/2505.15312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15312]] Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting(https://arxiv.org/abs/2505.15312)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multivariable time series forecasting methods can integrate information from exogenous variables, leading to significant prediction accuracy gains. Transformer architecture has been widely applied in various time series forecasting models due to its ability to capture long-range sequential dependencies. However, a naïve application of transformers often struggles to effectively model complex relationships among variables over time. To mitigate against this, we propose a novel architecture, namely the Spectral Operator Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to the input and incorporates spectral analysis using the Koopman operator. Its predictive skill relies on the Multivariable Coherence Attention (MVCA), an operation that leverages spectral coherence to model variable dependencies. Our empirical analysis shows that Sonnet yields the best performance on $34$ out of $47$ forecasting tasks with an average mean absolute error (MAE) reduction of $1.1\%$ against the most competitive baseline (different per task). We further show that MVCA -- when put in place of the naïve attention used in various deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$ on average in the most challenging forecasting tasks.</li>
</ul>

<h3>Title: FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion</h3>
<ul>
<li><strong>Authors: </strong>Kazuaki Mishima, Antoni Bigata Casademunt, Stavros Petridis, Maja Pantic, Kenji Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15313">https://arxiv.org/abs/2505.15313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15313">https://arxiv.org/pdf/2505.15313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15313]] FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion(https://arxiv.org/abs/2505.15313)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human facial images encode a rich spectrum of information, encompassing both stable identity-related traits and mutable attributes such as pose, expression, and emo- tion. While recent advances in image generation have enabled high-quality identity- conditional face synthesis, precise control over non-identity attributes remains challeng- ing, and disentangling identity from these mutable factors is particularly difficult. To address these limitations, we propose a novel identity-conditional diffusion model that introduces two lightweight control modules designed to independently manipulate facial pose, expression, and emotion without compromising identity preservation. These mod- ules are embedded within the cross-attention layers of the base diffusion model, enabling precise attribute control with minimal parameter overhead. Furthermore, our tailored training strategy, which leverages cross-attention between the identity feature and each non-identity control feature, encourages identity features to remain orthogonal to control signals, enhancing controllability and diversity. Quantitative and qualitative evaluations, along with perceptual user studies, demonstrate that our method surpasses existing ap- proaches in terms of control accuracy over pose, expression, and emotion, while also improving generative diversity under identity-only conditioning.</li>
</ul>

<h3>Title: Emotional Supporters often Use Multiple Strategies in a Single Turn</h3>
<ul>
<li><strong>Authors: </strong>Xin Bai, Guanyi Chen, Tingting He, Chenlian Zhou, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15316">https://arxiv.org/abs/2505.15316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15316">https://arxiv.org/pdf/2505.15316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15316]] Emotional Supporters often Use Multiple Strategies in a Single Turn(https://arxiv.org/abs/2505.15316)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotional Support Conversations (ESC) are crucial for providing empathy, validation, and actionable guidance to individuals in distress. However, existing definitions of the ESC task oversimplify the structure of supportive responses, typically modelling them as single strategy-utterance pairs. Through a detailed corpus analysis of the ESConv dataset, we identify a common yet previously overlooked phenomenon: emotional supporters often employ multiple strategies consecutively within a single turn. We formally redefine the ESC task to account for this, proposing a revised formulation that requires generating the full sequence of strategy-utterance pairs given a dialogue history. To facilitate this refined task, we introduce several modelling approaches, including supervised deep learning models and large language models. Our experiments show that, under this redefined task, state-of-the-art LLMs outperform both supervised models and human supporters. Notably, contrary to some earlier findings, we observe that LLMs frequently ask questions and provide suggestions, demonstrating more holistic support capabilities.</li>
</ul>

<h3>Title: CEBSNet: Change-Excited and Background-Suppressed Network with Temporal Dependency Modeling for Bitemporal Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Qi'ao Xu, Yan Xing, Jiali Hu, Yunan Jia, Rui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15322">https://arxiv.org/abs/2505.15322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15322">https://arxiv.org/pdf/2505.15322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15322]] CEBSNet: Change-Excited and Background-Suppressed Network with Temporal Dependency Modeling for Bitemporal Change Detection(https://arxiv.org/abs/2505.15322)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Change detection, a critical task in remote sensing and computer vision, aims to identify pixel-level differences between image pairs captured at the same geographic area but different times. It faces numerous challenges such as illumination variation, seasonal changes, background interference, and shooting angles, especially with a large time gap between images. While current methods have advanced, they often overlook temporal dependencies and overemphasize prominent changes while ignoring subtle but equally important changes. To address these limitations, we introduce \textbf{CEBSNet}, a novel change-excited and background-suppressed network with temporal dependency modeling for change detection. During the feature extraction, we utilize a simple Channel Swap Module (CSM) to model temporal dependency, reducing differences and noise. The Feature Excitation and Suppression Module (FESM) is developed to capture both obvious and subtle changes, maintaining the integrity of change regions. Additionally, we design a Pyramid-Aware Spatial-Channel Attention module (PASCA) to enhance the ability to detect change regions at different sizes and focus on critical regions. We conduct extensive experiments on three common street view datasets and two remote sensing datasets, and our method achieves the state-of-the-art performance.</li>
</ul>

<h3>Title: Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack</h3>
<ul>
<li><strong>Authors: </strong>Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15323">https://arxiv.org/abs/2505.15323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15323">https://arxiv.org/pdf/2505.15323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15323]] Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack(https://arxiv.org/abs/2505.15323)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using *first-token probability* (FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP can be fragile: models may assign high probability to unrelated tokens (*misalignment*) or use a valid token merely as part of a generic preamble rather than as a clear answer choice (*misinterpretation*), undermining the reliability of symbolic evaluation. We propose a simple solution: the *prefilling attack*, a structured natural-language prefix (e.g., "*The correct option is:*") prepended to the model output. Originally explored in AI safety, we repurpose prefilling to steer the model to respond with a clean, valid option, without modifying its parameters. Empirically, the FTP with prefilling strategy substantially improves accuracy, calibration, and output consistency across a broad set of LLMs and MCQA benchmarks. It outperforms standard FTP and often matches the performance of open-ended generation approaches that require full decoding and external classifiers, while being significantly more efficient. Our findings suggest that prefilling is a simple, robust, and low-cost method to enhance the reliability of FTP-based evaluation in multiple-choice settings.</li>
</ul>

<h3>Title: Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows</h3>
<ul>
<li><strong>Authors: </strong>Anqiao Ouyang, Hongyi Ke, Qi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15329">https://arxiv.org/abs/2505.15329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15329">https://arxiv.org/pdf/2505.15329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15329]] Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows(https://arxiv.org/abs/2505.15329)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Invertible neural architectures have recently attracted attention for their compactness, interpretability, and information-preserving properties. In this work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines invertible monotonic activation functions with reversible filter structures, and could be extended using Invertible ResNets. This architecture is examined in learning low-dimensional representations of one-dimensional nonlinear wave interactions and exact circular translation symmetry. Dimensionality is preserved across layers, except for a Fourier truncation step in the latent space, which enables dimensionality reduction while maintaining shift equivariance and interpretability. Our results demonstrate that FINE significantly outperforms classical linear methods such as Discrete Fourier Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves reconstruction accuracy better than conventional deep autoencoders with convolutional layers (CNN) - while using substantially smaller models and offering superior physical interpretability. These findings suggest that invertible single-neuron networks, when combined with spectral truncation, offer a promising framework for learning compact and interpretable representations of physics datasets, and symmetry-aware representation learning in physics-informed machine learning.</li>
</ul>

<h3>Title: Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ria Shekhawat, Hailin Li, Raghavendra Ramachandra, Sushma Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15332">https://arxiv.org/abs/2505.15332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15332">https://arxiv.org/pdf/2505.15332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15332]] Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models(https://arxiv.org/abs/2505.15332)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric, interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Leveraging the power of multimodal large language models (LLMs) offers a promising approach to enhancing the accuracy and interpretability of morphing attack detection (MAD), especially in real-world biometric applications. This work introduces the use of LLMs for differential morphing attack detection (D-MAD). To the best of our knowledge, this is the first study to employ multimodal LLMs to D-MAD using real biometric data. To effectively utilize these models, we design Chain-of-Thought (CoT)-based prompts to reduce failure-to-answer rates and enhance the reasoning behind decisions. Our contributions include: (1) the first application of multimodal LLMs for D-MAD using real data subjects, (2) CoT-based prompt engineering to improve response reliability and explainability, (3) comprehensive qualitative and quantitative benchmarking of LLM performance using data from 54 individuals captured in passport enrollment scenarios, and (4) comparative analysis of two multimodal LLMs: ChatGPT-4o and Gemini providing insights into their morphing attack detection accuracy and decision transparency. Experimental results show that ChatGPT-4o outperforms Gemini in detection accuracy, especially against GAN-based morphs, though both models struggle under challenging conditions. While Gemini offers more consistent explanations, ChatGPT-4o is more resilient but prone to a higher failure-to-answer rate.</li>
</ul>

<h3>Title: My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping</h3>
<ul>
<li><strong>Authors: </strong>Hon Ming Yam, Zhongliang Guo, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15336">https://arxiv.org/abs/2505.15336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15336">https://arxiv.org/pdf/2505.15336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15336]] My Face Is Mine, Not Yours: Facial Protection Against Diffusion Model Face Swapping(https://arxiv.org/abs/2505.15336)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of diffusion-based deepfake technologies poses significant risks for unauthorized and unethical facial image manipulation. While traditional countermeasures have primarily focused on passive detection methods, this paper introduces a novel proactive defense strategy through adversarial attacks that preemptively protect facial images from being exploited by diffusion-based deepfake systems. Existing adversarial protection methods predominantly target conventional generative architectures (GANs, AEs, VAEs) and fail to address the unique challenges presented by diffusion models, which have become the predominant framework for high-quality facial deepfakes. Current diffusion-specific adversarial approaches are limited by their reliance on specific model architectures and weights, rendering them ineffective against the diverse landscape of diffusion-based deepfake implementations. Additionally, they typically employ global perturbation strategies that inadequately address the region-specific nature of facial manipulation in deepfakes.</li>
</ul>

<h3>Title: Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15337">https://arxiv.org/abs/2505.15337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15337">https://arxiv.org/pdf/2505.15337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15337]] Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors(https://arxiv.org/abs/2505.15337)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.</li>
</ul>

<h3>Title: SSR: Speculative Parallel Scaling Reasoning in Test-time</h3>
<ul>
<li><strong>Authors: </strong>Yuanlin Chu, Bo Wang, Xiang Liu, Hong Chen, Aiwei Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15340">https://arxiv.org/abs/2505.15340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15340">https://arxiv.org/pdf/2505.15340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15340]] SSR: Speculative Parallel Scaling Reasoning in Test-time(https://arxiv.org/abs/2505.15340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive results on multi-step mathematical reasoning, yet at the cost of high computational overhead. This challenge is particularly acute for test-time scaling methods such as parallel decoding, which increase answer diversity but scale poorly in efficiency. To address this efficiency-accuracy trade-off, we propose SSR (Speculative Parallel Scaling Reasoning), a training-free framework that leverages a key insight: by introducing speculative decoding at the step level, we can accelerate reasoning without sacrificing correctness. SSR integrates two components: a Selective Parallel Module (SPM) that identifies a small set of promising reasoning strategies via model-internal scoring, and Step-level Speculative Decoding (SSD), which enables efficient draft-target collaboration for fine-grained reasoning acceleration. Experiments on three mathematical benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR achieves strong gains over baselines. For instance, on LiveMathBench, SSR improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in accuracy.</li>
</ul>

<h3>Title: FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management</h3>
<ul>
<li><strong>Authors: </strong>Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15347">https://arxiv.org/abs/2505.15347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15347">https://arxiv.org/pdf/2505.15347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15347]] FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management(https://arxiv.org/abs/2505.15347)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed in multi-turn conversational applications, where the management of the Key-Value (KV) Cache presents a significant bottleneck. The linear growth of the KV Cache with dialogue history imposes substantial computational costs, and existing eviction strategies often degrade performance by repeatedly compressing early conversational context, leading to information loss and context forgetting. This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism} for KV Cache management, which can be applied to any KV Cache compression method without training. FlowKV's core innovation is a multi-turn isolation mechanism that preserves the accumulated compressed KV cache from past turns. Compression is then strategically applied only to the newly generated KV pairs of the latest completed turn, effectively preventing the re-compression of older context and thereby mitigating catastrophic forgetting. Our results demonstrate that FlowKV consistently and significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention from 10.90\% to 75.40\%, particularly in later conversational turns.</li>
</ul>

<h3>Title: Revealing Language Model Trajectories via Kullback-Leibler Divergence</h3>
<ul>
<li><strong>Authors: </strong>Ryo Kishino, Yusuke Takase, Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15353">https://arxiv.org/abs/2505.15353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15353">https://arxiv.org/pdf/2505.15353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15353]] Revealing Language Model Trajectories via Kullback-Leibler Divergence(https://arxiv.org/abs/2505.15353)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A recently proposed method enables efficient estimation of the KL divergence between language models, including models with different architectures, by assigning coordinates based on log-likelihood vectors. To better understand the behavior of this metric, we systematically evaluate KL divergence across a wide range of conditions using publicly available language models. Our analysis covers comparisons between pretraining checkpoints, fine-tuned and base models, and layers via the logit lens. We find that trajectories of language models, as measured by KL divergence, exhibit a spiral structure during pretraining and thread-like progressions across layers. Furthermore, we show that, in terms of diffusion exponents, model trajectories in the log-likelihood space are more constrained than those in weight space.</li>
</ul>

<h3>Title: NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging</h3>
<ul>
<li><strong>Authors: </strong>Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weinan Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15356">https://arxiv.org/abs/2505.15356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15356">https://arxiv.org/pdf/2505.15356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15356]] NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging(https://arxiv.org/abs/2505.15356)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges.</li>
</ul>

<h3>Title: Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model</h3>
<ul>
<li><strong>Authors: </strong>Angelique Mangubat, Shane Gilroy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15358">https://arxiv.org/abs/2505.15358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15358">https://arxiv.org/pdf/2505.15358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15358]] Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model(https://arxiv.org/abs/2505.15358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Road safety is a critical challenge, particularly for cyclists, who are among the most vulnerable road users. This study aims to enhance road safety by proposing a novel benchmark for bicycle occlusion level classification using advanced computer vision techniques. Utilizing a parts-based detection model, images are annotated and processed through a custom image detection pipeline. A novel method of bicycle occlusion level is proposed to objectively quantify the visibility and occlusion level of bicycle semantic parts. The findings indicate that the model robustly quantifies the visibility and occlusion level of bicycles, a significant improvement over the subjective methods used by the current state of the art. Widespread use of the proposed methodology will facilitate the accurate performance reporting of cyclist detection algorithms for occluded cyclists, informing the development of more robust vulnerable road user detection methods for autonomous vehicles.</li>
</ul>

<h3>Title: Distributionally Robust Federated Learning with Client Drift Minimization</h3>
<ul>
<li><strong>Authors: </strong>Mounssif Krouka, Chaouki Ben Issaid, Mehdi Bennis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15371">https://arxiv.org/abs/2505.15371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15371">https://arxiv.org/pdf/2505.15371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15371]] Distributionally Robust Federated Learning with Client Drift Minimization(https://arxiv.org/abs/2505.15371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) faces critical challenges, particularly in heterogeneous environments where non-independent and identically distributed data across clients can lead to unfair and inefficient model performance. In this work, we introduce \textit{DRDM}, a novel algorithm that addresses these issues by combining a distributionally robust optimization (DRO) framework with dynamic regularization to mitigate client drift. \textit{DRDM} frames the training as a min-max optimization problem aimed at maximizing performance for the worst-case client, thereby promoting robustness and fairness. This robust objective is optimized through an algorithm leveraging dynamic regularization and efficient local updates, which significantly reduces the required number of communication rounds. Moreover, we provide a theoretical convergence analysis for convex smooth objectives under partial participation. Extensive experiments on three benchmark datasets, covering various model architectures and data heterogeneity levels, demonstrate that \textit{DRDM} significantly improves worst-case test accuracy while requiring fewer communication rounds than existing state-of-the-art baselines. Furthermore, we analyze the impact of signal-to-noise ratio (SNR) and bandwidth on the energy consumption of participating clients, demonstrating that the number of local update steps can be adaptively selected to achieve a target worst-case test accuracy with minimal total energy cost across diverse communication environments.</li>
</ul>

<h3>Title: X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Ruihan Tao, Qiguang Chen, Mengkang Hu, Libo Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15372">https://arxiv.org/abs/2505.15372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15372">https://arxiv.org/pdf/2505.15372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15372]] X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System(https://arxiv.org/abs/2505.15372)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.</li>
</ul>

<h3>Title: RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Naman Patel, Prashanth Krishnamurthy, Farshad Khorrami</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15373">https://arxiv.org/abs/2505.15373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15373">https://arxiv.org/pdf/2505.15373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15373]] RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation(https://arxiv.org/abs/2505.15373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Mapping and understanding complex 3D environments is fundamental to how autonomous systems perceive and interact with the physical world, requiring both precise geometric reconstruction and rich semantic comprehension. While existing 3D semantic mapping systems excel at reconstructing and identifying predefined object instances, they lack the flexibility to efficiently build semantic maps with open-vocabulary during online operation. Although recent vision-language models have enabled open-vocabulary object recognition in 2D images, they haven't yet bridged the gap to 3D spatial understanding. The critical challenge lies in developing a training-free unified system that can simultaneously construct accurate 3D maps while maintaining semantic consistency and supporting natural language interactions in real time. In this paper, we develop a zero-shot framework that seamlessly integrates GPU-accelerated geometric reconstruction with open-vocabulary vision-language models through online instance-level semantic embedding fusion, guided by hierarchical object association with spatial indexing. Our training-free system achieves superior performance through incremental processing and unified geometric-semantic updates, while robustly handling 2D segmentation inconsistencies. The proposed general-purpose 3D scene understanding framework can be used for various tasks including zero-shot 3D instance retrieval, segmentation, and object detection to reason about previously unseen objects and interpret natural language queries. The project page is available at this https URL.</li>
</ul>

<h3>Title: Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving Intrusion Detection in Industrial IoT</h3>
<ul>
<li><strong>Authors: </strong>Anas Ali, Mubashar Husain, Peter Hans</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15376">https://arxiv.org/abs/2505.15376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15376">https://arxiv.org/pdf/2505.15376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15376]] Federated Learning-Enhanced Blockchain Framework for Privacy-Preserving Intrusion Detection in Industrial IoT(https://arxiv.org/abs/2505.15376)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Industrial Internet of Things (IIoT) systems have become integral to smart manufacturing, yet their growing connectivity has also exposed them to significant cybersecurity threats. Traditional intrusion detection systems (IDS) often rely on centralized architectures that raise concerns over data privacy, latency, and single points of failure. In this work, we propose a novel Federated Learning-Enhanced Blockchain Framework (FL-BCID) for privacy-preserving intrusion detection tailored for IIoT environments. Our architecture combines federated learning (FL) to ensure decentralized model training with blockchain technology to guarantee data integrity, trust, and tamper resistance across IIoT nodes. We design a lightweight intrusion detection model collaboratively trained using FL across edge devices without exposing sensitive data. A smart contract-enabled blockchain system records model updates and anomaly scores to establish accountability. Experimental evaluations using the ToN-IoT and N-BaIoT datasets demonstrate the superior performance of our framework, achieving 97.3% accuracy while reducing communication overhead by 41% compared to baseline centralized methods. Our approach ensures privacy, scalability, and robustness-critical for secure industrial operations. The proposed FL-BCID system provides a promising solution for enhancing trust and privacy in modern IIoT security architectures.</li>
</ul>

<h3>Title: The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization</h3>
<ul>
<li><strong>Authors: </strong>Raphael Sulzer, Liuyun Duan, Nicolas Girard, Florent Lafarge</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15379">https://arxiv.org/abs/2505.15379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15379">https://arxiv.org/pdf/2505.15379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15379]] The P$^3$ dataset: Pixels, Points and Polygons for Multimodal Building Vectorization(https://arxiv.org/abs/2505.15379)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present the P$^3$ dataset, a large-scale multimodal benchmark for building vectorization, constructed from aerial LiDAR point clouds, high-resolution aerial imagery, and vectorized 2D building outlines, collected across three continents. The dataset contains over 10 billion LiDAR points with decimeter-level accuracy and RGB images at a ground sampling distance of 25 centimeter. While many existing datasets primarily focus on the image modality, P$^3$ offers a complementary perspective by also incorporating dense 3D information. We demonstrate that LiDAR point clouds serve as a robust modality for predicting building polygons, both in hybrid and end-to-end learning frameworks. Moreover, fusing aerial LiDAR and imagery further improves accuracy and geometric quality of predicted polygons. The P$^3$ dataset is publicly available, along with code and pretrained weights of three state-of-the-art models for building polygon prediction at this https URL .</li>
</ul>

<h3>Title: Real-Time Detection of Insider Threats Using Behavioral Analytics and Deep Evidential Clustering</h3>
<ul>
<li><strong>Authors: </strong>Anas Ali, Mubashar Husain, Peter Hans</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15383">https://arxiv.org/abs/2505.15383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15383">https://arxiv.org/pdf/2505.15383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15383]] Real-Time Detection of Insider Threats Using Behavioral Analytics and Deep Evidential Clustering(https://arxiv.org/abs/2505.15383)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Insider threats represent one of the most critical challenges in modern cybersecurity. These threats arise from individuals within an organization who misuse their legitimate access to harm the organization's assets, data, or operations. Traditional security mechanisms, primarily designed for external attackers, fall short in identifying these subtle and context-aware threats. In this paper, we propose a novel framework for real-time detection of insider threats using behavioral analytics combined with deep evidential clustering. Our system captures and analyzes user activities, applies context-rich behavioral features, and classifies potential threats using a deep evidential clustering model that estimates both cluster assignment and epistemic uncertainty. The proposed model dynamically adapts to behavioral changes and significantly reduces false positives. We evaluate our framework on benchmark insider threat datasets such as CERT and TWOS, achieving an average detection accuracy of 94.7% and a 38% reduction in false positives compared to traditional clustering methods. Our results demonstrate the effectiveness of integrating uncertainty modeling in threat detection pipelines. This research provides actionable insights for deploying intelligent, adaptive, and robust insider threat detection systems across various enterprise environments.</li>
</ul>

<h3>Title: RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R. (May)Fung, Xinlei He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15386">https://arxiv.org/abs/2505.15386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15386">https://arxiv.org/pdf/2505.15386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15386]] RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection(https://arxiv.org/abs/2505.15386)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.</li>
</ul>

<h3>Title: An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Biquan Bie, Zuqiu Na, Weilin Ruan, Songxin Lei, Yutao Yue, Xinlei He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15392">https://arxiv.org/abs/2505.15392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15392">https://arxiv.org/pdf/2505.15392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15392]] An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations(https://arxiv.org/abs/2505.15392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) like ChatGPT has advanced natural language processing, yet concerns about cognitive biases are growing. In this paper, we investigate the anchoring effect, a cognitive bias where the mind relies heavily on the first information as anchors to make affected judgments. We explore whether LLMs are affected by anchoring, the underlying mechanisms, and potential mitigation strategies. To facilitate studies at scale on the anchoring effect, we introduce a new dataset, SynAnchors. Combining refined evaluation metrics, we benchmark current widely used LLMs. Our findings show that LLMs' anchoring bias exists commonly with shallow-layer acting and is not eliminated by conventional strategies, while reasoning can offer some mitigation. This recontextualization via cognitive psychology urges that LLM evaluations focus not on standard benchmarks or over-optimized robustness tests, but on cognitive-bias-aware trustworthy evaluation.</li>
</ul>

<h3>Title: Expanding Zero-Shot Object Counting with Rich Prompts</h3>
<ul>
<li><strong>Authors: </strong>Huilin Zhu, Senyao Li, Jingling Yuan, Zhengwei Yang, Yu Guo, Wenxuan Liu, Xian Zhong, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15398">https://arxiv.org/abs/2505.15398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15398">https://arxiv.org/pdf/2505.15398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15398]] Expanding Zero-Shot Object Counting with Rich Prompts(https://arxiv.org/abs/2505.15398)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Expanding pre-trained zero-shot counting models to handle unseen categories requires more than simply adding new prompts, as this approach does not achieve the necessary alignment between text and visual features for accurate counting. We introduce RichCount, the first framework to address these limitations, employing a two-stage training strategy that enhances text encoding and strengthens the model's association with objects in images. RichCount improves zero-shot counting for unseen categories through two key objectives: (1) enriching text features with a feed-forward network and adapter trained on text-image similarity, thereby creating robust, aligned representations; and (2) applying this refined encoder to counting tasks, enabling effective generalization across diverse prompts and complex images. In this manner, RichCount goes beyond simple prompt expansion to establish meaningful feature alignment that supports accurate counting across novel categories. Extensive experiments on three benchmark datasets demonstrate the effectiveness of RichCount, achieving state-of-the-art performance in zero-shot counting and significantly enhancing generalization to unseen categories in open-world scenarios.</li>
</ul>

<h3>Title: Visual Question Answering on Multiple Remote Sensing Image Modalities</h3>
<ul>
<li><strong>Authors: </strong>Hichem Boussaid, Lucrezia Tosato, Flora Weissgerber, Camille Kurtz, Laurent Wendling, Sylvain Lobry</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15401">https://arxiv.org/abs/2505.15401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15401">https://arxiv.org/pdf/2505.15401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15401]] Visual Question Answering on Multiple Remote Sensing Image Modalities(https://arxiv.org/abs/2505.15401)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The extraction of visual features is an essential step in Visual Question Answering (VQA). Building a good visual representation of the analyzed scene is indeed one of the essential keys for the system to be able to correctly understand the latter in order to answer complex questions. In many fields such as remote sensing, the visual feature extraction step could benefit significantly from leveraging different image modalities carrying complementary spectral, spatial and contextual information. In this work, we propose to add multiple image modalities to VQA in the particular context of remote sensing, leading to a novel task for the computer vision community. To this end, we introduce a new VQA dataset, named TAMMI (Text and Multi-Modal Imagery) with diverse questions on scenes described by three different modalities (very high resolution RGB, multi-spectral imaging data and synthetic aperture radar). Thanks to an automated pipeline, this dataset can be easily extended according to experimental needs. We also propose the MM-RSVQA (Multi-modal Multi-resolution Remote Sensing Visual Question Answering) model, based on VisualBERT, a vision-language transformer, to effectively combine the multiple image modalities and text through a trainable fusion process. A preliminary experimental study shows promising results of our methodology on this challenging dataset, with an accuracy of 65.56% on the targeted VQA task. This pioneering work paves the way for the community to a new multi-modal multi-resolution VQA task that can be applied in other imaging domains (such as medical imaging) where multi-modality can enrich the visual representation of a scene. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks</h3>
<ul>
<li><strong>Authors: </strong>Uranik Berisha, Jens Mehnert, Alexandru Paul Condurache</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15414">https://arxiv.org/abs/2505.15414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15414">https://arxiv.org/pdf/2505.15414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15414]] Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks(https://arxiv.org/abs/2505.15414)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers have emerged as the state-of-the-art models in various Computer Vision tasks, but their high computational and resource demands pose significant challenges. While Mixture-of-Experts (MoE) can make these models more efficient, they often require costly retraining or even training from scratch. Recent developments aim to reduce these computational costs by leveraging pretrained networks. These have been shown to produce sparse activation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder blocks, allowing for conditional activation of only relevant subnetworks for each sample. Building on this idea, we propose a new method to construct MoE variants from pretrained models. Our approach extracts expert subnetworks from the model's MLP layers post-training in two phases. First, we cluster output activations to identify distinct activation patterns. In the second phase, we use these clusters to extract the corresponding subnetworks responsible for producing them. On ImageNet-1k recognition tasks, we demonstrate that these extracted experts can perform surprisingly well out of the box and require only minimal fine-tuning to regain 98% of the original performance, all while reducing MACs and model size, by up to 36% and 32% respectively.</li>
</ul>

<h3>Title: Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Wenjie Qu, Yanze Jiang, Zichen Liu, Yue Liu, Shengfang Zhai, Yinpeng Dong, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15420">https://arxiv.org/abs/2505.15420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15420">https://arxiv.org/pdf/2505.15420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15420]] Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries(https://arxiv.org/abs/2505.15420)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by incorporating external knowledge bases, but they are vulnerable to privacy risks from data extraction attacks. Existing extraction methods typically rely on malicious inputs such as prompt injection or jailbreaking, making them easily detectable via input- or output-level detection. In this paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts knowledge extraction on RAG systems through benign queries. IKEA first leverages anchor concepts to generate queries with the natural appearance, and then designs two mechanisms to lead to anchor concept thoroughly 'explore' the RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples anchor concepts based on past query-response patterns to ensure the queries' relevance to RAG documents; (2) Trust Region Directed Mutation, which iteratively mutates anchor concepts under similarity constraints to further exploit the embedding space. Extensive experiments demonstrate IKEA's effectiveness under various defenses, surpassing baselines by over 80% in extraction efficiency and 90% in attack success rate. Moreover, the substitute RAG system built from IKEA's extractions consistently outperforms those based on baseline methods across multiple evaluation tasks, underscoring the significant privacy risk in RAG systems.</li>
</ul>

<h3>Title: Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches</h3>
<ul>
<li><strong>Authors: </strong>Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15422">https://arxiv.org/abs/2505.15422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15422">https://arxiv.org/pdf/2505.15422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15422]] Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches(https://arxiv.org/abs/2505.15422)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Authorship analysis plays an important role in diverse domains, including forensic linguistics, academia, cybersecurity, and digital content authentication. This paper presents a systematic literature review on two key sub-tasks of authorship analysis; Author Attribution and Author Verification. The review explores SOTA methodolo- gies, ranging from traditional ML approaches to DL models and LLMs, highlighting their evolution, strengths, and limitations, based on studies conducted from 2015 to 2024. Key contributions include a comprehensive analysis of methods, techniques, their corresponding feature extraction techniques, datasets used, and emerging chal- lenges in authorship analysis. The study highlights critical research gaps, particularly in low-resource language processing, multilingual adaptation, cross-domain generaliza- tion, and AI-generated text detection. This review aims to help researchers by giving an overview of the latest trends and challenges in authorship analysis. It also points out possible areas for future study. The goal is to support the development of better, more reliable, and accurate authorship analysis system in diverse textual domain.</li>
</ul>

<h3>Title: SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding</h3>
<ul>
<li><strong>Authors: </strong>Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.EM, stat.AP, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15423">https://arxiv.org/abs/2505.15423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15423">https://arxiv.org/pdf/2505.15423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15423]] SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding(https://arxiv.org/abs/2505.15423)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Capturing nonlinear relationships without sacrificing interpretability remains a persistent challenge in regression modeling. We introduce SplitWise, a novel framework that enhances stepwise regression. It adaptively transforms numeric predictors into threshold-based binary features using shallow decision trees, but only when such transformations improve model fit, as assessed by the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). This approach preserves the transparency of linear models while flexibly capturing nonlinear effects. Implemented as a user-friendly R package, SplitWise is evaluated on both synthetic and real-world datasets. The results show that it consistently produces more parsimonious and generalizable models than traditional stepwise and penalized regression techniques.</li>
</ul>

<h3>Title: On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?</h3>
<ul>
<li><strong>Authors: </strong>Raza Imam, Rufael Marew, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15425">https://arxiv.org/abs/2505.15425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15425">https://arxiv.org/pdf/2505.15425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15425]] On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?(https://arxiv.org/abs/2505.15425)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical Vision-Language Models (MVLMs) have achieved par excellence generalization in medical image analysis, yet their performance under noisy, corrupted conditions remains largely untested. Clinical imaging is inherently susceptible to acquisition artifacts and noise; however, existing evaluations predominantly assess generally clean datasets, overlooking robustness -- i.e., the model's ability to perform under real-world distortions. To address this gap, we first introduce MediMeta-C, a corruption benchmark that systematically applies several perturbations across multiple medical imaging datasets. Combined with MedMNIST-C, this establishes a comprehensive robustness evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to enhance resilience against corruptions. Through extensive experiments, we benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that existing models exhibit severe degradation under corruption and struggle with domain-modality tradeoffs. Our findings highlight the necessity of diverse training and robust adaptation strategies, demonstrating that efficient low-rank adaptation when paired with few-shot tuning, improves robustness while preserving generalization across modalities.</li>
</ul>

<h3>Title: Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Li, Die Chen, Mingyuan Fan, Cen Chen, Yaliang Li, Yanhao Wang, Wenmeng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15427">https://arxiv.org/abs/2505.15427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15427">https://arxiv.org/pdf/2505.15427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15427]] Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions(https://arxiv.org/abs/2505.15427)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, diffusion</a></li>
<li><strong>Abstract: </strong>The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, hindering their practical use in real-world applications. In response to this challenge, prior work has focused on employing security filters to identify and exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their social responsibility. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.</li>
</ul>

<h3>Title: Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15431">https://arxiv.org/abs/2505.15431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15431">https://arxiv.org/pdf/2505.15431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15431]] Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought(https://arxiv.org/abs/2505.15431)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep "thinking" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.</li>
</ul>

<h3>Title: Set-LLM: A Permutation-Invariant LLM</h3>
<ul>
<li><strong>Authors: </strong>Beni Egressy, Jan Stühmer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15433">https://arxiv.org/abs/2505.15433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15433">https://arxiv.org/pdf/2505.15433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15433]] Set-LLM: A Permutation-Invariant LLM(https://arxiv.org/abs/2505.15433)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs as automated evaluators in AI pipelines, comparing output generated by different models. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while eliminating order sensitivity.</li>
</ul>

<h3>Title: Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Jianyuan Guo, Peike Li, Trevor Cohn</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15438">https://arxiv.org/abs/2505.15438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15438">https://arxiv.org/pdf/2505.15438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15438]] Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation(https://arxiv.org/abs/2505.15438)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sign Language Translation (SLT) aims to map sign language videos to spoken language text. A common approach relies on gloss annotations as an intermediate representation, decomposing SLT into two sub-tasks: video-to-gloss recognition and gloss-to-text translation. While effective, this paradigm depends on expert-annotated gloss labels, which are costly and rarely available in existing datasets, limiting its scalability. To address this challenge, we propose a gloss-free pseudo gloss generation framework that eliminates the need for human-annotated glosses while preserving the structured intermediate representation. Specifically, we prompt a Large Language Model (LLM) with a few example text-gloss pairs using in-context learning to produce draft sign glosses from spoken language text. To enhance the correspondence between LLM-generated pseudo glosses and the sign sequences in video, we correct the ordering in the pseudo glosses for better alignment via a weakly supervised learning process. This reordering facilitates the incorporation of auxiliary alignment objectives, and allows for the use of efficient supervision via a Connectionist Temporal Classification (CTC) loss. We train our SLT mode, which consists of a vision encoder and a translator, through a three-stage pipeline, which progressively narrows the modality gap between sign language and spoken language. Despite its simplicity, our approach outperforms previous state-of-the-art gloss-free frameworks on two SLT benchmarks and achieves competitive results compared to gloss-based methods.</li>
</ul>

<h3>Title: Stronger ViTs With Octic Equivariance</h3>
<ul>
<li><strong>Authors: </strong>David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15441">https://arxiv.org/abs/2505.15441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15441">https://arxiv.org/pdf/2505.15441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15441]] Stronger ViTs With Octic Equivariance(https://arxiv.org/abs/2505.15441)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent efforts at scaling computer vision models have established Vision Transformers (ViTs) as the leading architecture. ViTs incorporate weight sharing over image patches as an important inductive bias. In this work, we show that ViTs benefit from incorporating equivariance under the octic group, i.e., reflections and 90-degree rotations, as a further inductive bias. We develop new architectures, octic ViTs, that use octic-equivariant layers and put them to the test on both supervised and self-supervised learning. Through extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show that octic ViTs yield more computationally efficient networks while also improving performance. In particular, we achieve approximately 40% reduction in FLOPs for ViT-H while simultaneously improving both classification and segmentation results.</li>
</ul>

<h3>Title: On the Generalization vs Fidelity Paradox in Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Suhas Kamasetty Ramesh, Ayan Sengupta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15442">https://arxiv.org/abs/2505.15442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15442">https://arxiv.org/pdf/2505.15442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15442]] On the Generalization vs Fidelity Paradox in Knowledge Distillation(https://arxiv.org/abs/2505.15442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to $10\%$, with a peak task specific gain of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students' performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.</li>
</ul>

<h3>Title: ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziqiang Xu, Qi Dai, Tian Xie, Yifan Yang, Kai Qiu, DongDong Chen, Zuxuan Wu, Chong Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15447">https://arxiv.org/abs/2505.15447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15447">https://arxiv.org/pdf/2505.15447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15447]] ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning(https://arxiv.org/abs/2505.15447)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Video understanding is inherently intention-driven-humans naturally focus on relevant frames based on their goals. Recent advancements in multimodal large language models (MLLMs) have enabled flexible query-driven reasoning; however, video-based frameworks like Video Chain-of-Thought lack direct training signals to effectively identify relevant frames. Current approaches often rely on heuristic methods or pseudo-label supervised annotations, which are both costly and limited in scalability across diverse scenarios. To overcome these challenges, we introduce ViaRL, the first framework to leverage rule-based reinforcement learning (RL) for optimizing frame selection in intention-driven video understanding. An iterated amplification strategy is adopted to perform alternating cyclic training in the video CoT system, where each component undergoes iterative cycles of refinement to improve its capabilities. ViaRL utilizes the answer accuracy of a downstream model as a reward signal to train a frame selector through trial-and-error, eliminating the need for expensive annotations while closely aligning with human-like learning processes. Comprehensive experiments across multiple benchmarks, including VideoMME, LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior temporal grounding performance and robust generalization across diverse video understanding tasks, highlighting its effectiveness and scalability. Notably, ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which is required to search a specific needle within a long video and regarded as one of the most suitable benchmarks for evaluating temporal grounding.</li>
</ul>

<h3>Title: Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Die Chen, Zhiwen Li, Cen Chen, Yuexiang Xie, Xiaodan Li, Jinyan Ye, Yingda Chen, Yaliang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15450">https://arxiv.org/abs/2505.15450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15450">https://arxiv.org/pdf/2505.15450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15450]] Comprehensive Evaluation and Analysis for NSFW Concept Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2505.15450)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have gained widespread application across various domains, demonstrating remarkable creative potential. However, the strong generalization capabilities of diffusion models can inadvertently lead to the generation of not-safe-for-work (NSFW) content, posing significant risks to their safe deployment. While several concept erasure methods have been proposed to mitigate the issue associated with NSFW content, a comprehensive evaluation of their effectiveness across various scenarios remains absent. To bridge this gap, we introduce a full-pipeline toolkit specifically designed for concept erasure and conduct the first systematic study of NSFW concept erasure methods. By examining the interplay between the underlying mechanisms and empirical observations, we provide in-depth insights and practical guidance for the effective application of concept erasure methods in various real-world scenarios, with the aim of advancing the understanding of content safety in diffusion models and establishing a solid foundation for future research and development in this critical area.</li>
</ul>

<h3>Title: Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15456">https://arxiv.org/abs/2505.15456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15456">https://arxiv.org/pdf/2505.15456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15456]] Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment(https://arxiv.org/abs/2505.15456)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios and long-term personalization due to their inherently static and shallow designs. In this work, we introduce the Reinforcement Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts with a simulated user model to iteratively infer and refine user profiles through dialogue. The training process is guided by a dual-level reward structure: the Profile Reward encourages accurate construction of user representations, while the Response Reward incentivizes generation of responses consistent with the inferred profile. We instantiate RLPA by fine-tuning Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art performance in personalized dialogue. Empirical evaluations demonstrate that Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines, and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o. Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting user preferences, sustaining long-term personalization and delivering more efficient inference compared to recent reasoning-focused LLMs. These results emphasize the potential of dynamic profile inference as a more effective paradigm for building personalized dialogue systems.</li>
</ul>

<h3>Title: Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yukun Zhao, Lingyong Yan, Zhenyang Li, Shuaiqiang Wang, Zhumin Chen, Zhaochun Ren, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15467">https://arxiv.org/abs/2505.15467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15467">https://arxiv.org/pdf/2505.15467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15467]] Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning(https://arxiv.org/abs/2505.15467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have achieved remarkable success in various tasks. However, it is challenging for them to learn new tasks incrementally due to catastrophic forgetting. Existing approaches rely on experience replay, optimization constraints, or task differentiation, which encounter strict limitations in real-world scenarios. To address these issues, we propose Joint Flashback Adaptation. We first introduce flashbacks -- a limited number of prompts from old tasks -- when adapting to new tasks and constrain the deviations of the model outputs compared to the original one. We then interpolate latent tasks between flashbacks and new tasks to enable jointly learning relevant latent tasks, new tasks, and flashbacks, alleviating data sparsity in flashbacks and facilitating knowledge sharing for smooth adaptation. Our method requires only a limited number of flashbacks without access to the replay data and is task-agnostic. We conduct extensive experiments on state-of-the-art large language models across 1000+ instruction-following tasks, arithmetic reasoning tasks, and general reasoning tasks. The results demonstrate the superior performance of our method in improving generalization on new tasks and reducing forgetting in old tasks.</li>
</ul>

<h3>Title: CoLA: Collaborative Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yiyun Zhou, Chang Yao, Jingyuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15471">https://arxiv.org/abs/2505.15471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15471">https://arxiv.org/pdf/2505.15471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15471]] CoLA: Collaborative Low-Rank Adaptation(https://arxiv.org/abs/2505.15471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The scaling law of Large Language Models (LLMs) reveals a power-law relationship, showing diminishing return on performance as model scale increases. While training LLMs from scratch is resource-intensive, fine-tuning a pre-trained model for specific tasks has become a practical alternative. Full fine-tuning (FFT) achieves strong performance; however, it is computationally expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like LoRA, have been proposed to address these challenges by freezing the pre-trained model and adding lightweight task-specific modules. LoRA, in particular, has proven effective, but its application to multi-task scenarios is limited by interference between tasks. Recent approaches, such as Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these issues but still struggle with sample scarcity and noise interference due to their fixed structure. In response, we propose CoLA, a more flexible LoRA architecture with an efficient initialization scheme, and introduces three collaborative strategies to enhance performance by better utilizing the quantitative relationships between matrices $A$ and $B$. Our experiments demonstrate the effectiveness and robustness of CoLA, outperforming existing PEFT methods, especially in low-sample scenarios. Our data and code are fully publicly available at this https URL.</li>
</ul>

<h3>Title: PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Song Dai, Yibo Yan, Jiamin Su, Dongfang Zihao, Yubo Gao, Yonghua Hei, Jungang Li, Junyan Zhang, Sicheng Tao, Zhuoran Gao, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15472">https://arxiv.org/abs/2505.15472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15472">https://arxiv.org/pdf/2505.15472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15472]] PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions(https://arxiv.org/abs/2505.15472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in diverse reasoning tasks, yet their application to complex physics reasoning remains underexplored. Physics reasoning presents unique challenges, requiring grounding in physical conditions and the interpretation of multimodal information. Current physics benchmarks are limited, often focusing on text-only inputs or solely on problem-solving, thereby overlooking the critical intermediate steps of variable identification and process formulation. To address these limitations, we introduce PhysicsArena, the first multimodal physics reasoning benchmark designed to holistically evaluate MLLMs across three critical dimensions: variable identification, physical process formulation, and solution derivation. PhysicsArena aims to provide a comprehensive platform for assessing and advancing the multimodal physics reasoning abilities of MLLMs.</li>
</ul>

<h3>Title: LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhanyue Qin, Yue Ding, Deyuan Liu, Qingbin Liu, Junxian Cai, Xi Chen, Zhiying Tu, Dianhui Chu, Cuiyun Gao, Dianbo Sui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15475">https://arxiv.org/abs/2505.15475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15475">https://arxiv.org/pdf/2505.15475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15475]] LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models(https://arxiv.org/abs/2505.15475)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Nowadays, Large Language Models (LLMs) have attracted widespread attention due to their powerful performance. However, due to the unavoidable exposure to socially biased data during training, LLMs tend to exhibit social biases, particularly gender bias. To better explore and quantifying the degree of gender bias in LLMs, we propose a pair of datasets named GenBiasEval and GenHintEval, respectively. The GenBiasEval is responsible for evaluating the degree of gender bias in LLMs, accompanied by an evaluation metric named AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is used to assess whether LLMs can provide responses consistent with prompts that contain gender hints, along with the accompanying evaluation metric UB-Score (UnBias Score). Besides, in order to mitigate gender bias in LLMs more effectively, we present the LFTF (Locating First and Then Fine-Tuning) this http URL algorithm first ranks specific LLM blocks by their relevance to gender bias in descending order using a metric called BMI (Block Mitigating Importance Score). Based on this ranking, the block most strongly associated with gender bias is then fine-tuned using a carefully designed loss function. Numerous experiments have shown that our proposed LFTF algorithm can significantly mitigate gender bias in LLMs while maintaining their general capabilities.</li>
</ul>

<h3>Title: Pura: An Efficient Privacy-Preserving Solution for Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Guotao Xu, Bowen Zhao, Yang Xiao, Yantao Zhong, Liang Zhai, Qingqi Pei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15476">https://arxiv.org/abs/2505.15476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15476">https://arxiv.org/pdf/2505.15476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15476]] Pura: An Efficient Privacy-Preserving Solution for Face Recognition(https://arxiv.org/abs/2505.15476)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Face recognition is an effective technology for identifying a target person by facial images. However, sensitive facial images raises privacy concerns. Although privacy-preserving face recognition is one of potential solutions, this solution neither fully addresses the privacy concerns nor is efficient enough. To this end, we propose an efficient privacy-preserving solution for face recognition, named Pura, which sufficiently protects facial privacy and supports face recognition over encrypted data efficiently. Specifically, we propose a privacy-preserving and non-interactive architecture for face recognition through the threshold Paillier cryptosystem. Additionally, we carefully design a suite of underlying secure computing protocols to enable efficient operations of face recognition over encrypted data directly. Furthermore, we introduce a parallel computing mechanism to enhance the performance of the proposed secure computing protocols. Privacy analysis demonstrates that Pura fully safeguards personal facial privacy. Experimental evaluations demonstrate that Pura achieves recognition speeds up to 16 times faster than the state-of-the-art.</li>
</ul>

<h3>Title: KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance</h3>
<ul>
<li><strong>Authors: </strong>Qihuang Zhong, Liang Ding, Xiantao Cai, Juhua Liu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15480">https://arxiv.org/abs/2505.15480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15480">https://arxiv.org/pdf/2505.15480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15480]] KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance(https://arxiv.org/abs/2505.15480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a common approach to improve the domain-specific question-answering (QA) performance of large language models (LLMs). However, recent literature reveals that due to the conflicts between LLMs' internal knowledge and the context knowledge of training data, vanilla SFT using the full QA training set is usually suboptimal. In this paper, we first design a query diversification strategy for robust conflict detection and then conduct a series of experiments to analyze the impact of knowledge conflict. We find that 1) training samples with varied conflicts contribute differently, where SFT on the data with large conflicts leads to catastrophic performance drops; 2) compared to directly filtering out the conflict data, appropriately applying the conflict data would be more beneficial. Motivated by this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to adapt the training weight by assigning different rewards for different training samples according to conflict level. Extensive experiments show that KaFT brings consistent and significant improvements across four LLMs. More analyses prove that KaFT effectively improves the model generalization and alleviates the hallucination.</li>
</ul>

<h3>Title: Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data under Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Ye Zheng, Sumita Mishra, Yidan Hu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15483">https://arxiv.org/abs/2505.15483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15483">https://arxiv.org/pdf/2505.15483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15483]] Optimal Piecewise-based Mechanism for Collecting Bounded Numerical Data under Local Differential Privacy(https://arxiv.org/abs/2505.15483)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Numerical data with bounded domains is a common data type in personal devices, such as wearable sensors. While the collection of such data is essential for third-party platforms, it raises significant privacy concerns. Local differential privacy (LDP) has been shown as a framework providing provable individual privacy, even when the third-party platform is untrusted. For numerical data with bounded domains, existing state-of-the-art LDP mechanisms are piecewise-based mechanisms, which are not optimal, leading to reduced data utility. This paper investigates the optimal design of piecewise-based mechanisms to maximize data utility under LDP. We demonstrate that existing piecewise-based mechanisms are heuristic forms of the $3$-piecewise mechanism, which is far from enough to study optimality. We generalize the $3$-piecewise mechanism to its most general form, i.e. $m$-piecewise mechanism with no pre-defined form of each piece. Under this form, we derive the closed-form optimal mechanism by combining analytical proofs and off-the-shelf optimization solvers. Next, we extend the generalized piecewise-based mechanism to the circular domain (along with the classical domain), defined on a cyclic range where the distance between the two endpoints is zero. By incorporating this property, we design the optimal mechanism for the circular domain, achieving significantly improved data utility compared with existing mechanisms. Our proposed mechanisms guarantee optimal data utility under LDP among all generalized piecewise-based mechanisms. We show that they also achieve optimal data utility in two common applications of LDP: distribution estimation and mean estimation. Theoretical analyses and experimental evaluations prove and validate the data utility advantages of our proposed mechanisms.</li>
</ul>

<h3>Title: Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ce Zhang, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15491">https://arxiv.org/abs/2505.15491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15491">https://arxiv.org/pdf/2505.15491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15491]] Spectral-Aware Global Fusion for RGB-Thermal Semantic Segmentation(https://arxiv.org/abs/2505.15491)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation relying solely on RGB data often struggles in challenging conditions such as low illumination and obscured views, limiting its reliability in critical applications like autonomous driving. To address this, integrating additional thermal radiation data with RGB images demonstrates enhanced performance and robustness. However, how to effectively reconcile the modality discrepancies and fuse the RGB and thermal features remains a well-known challenge. In this work, we address this challenge from a novel spectral perspective. We observe that the multi-modal features can be categorized into two spectral components: low-frequency features that provide broad scene context, including color variations and smooth areas, and high-frequency features that capture modality-specific details such as edges and textures. Inspired by this, we propose the Spectral-aware Global Fusion Network (SGFNet) to effectively enhance and fuse the multi-modal features by explicitly modeling the interactions between the high-frequency, modality-specific features. Our experimental results demonstrate that SGFNet outperforms the state-of-the-art methods on the MFNet and PST900 datasets.</li>
</ul>

<h3>Title: Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Federico Ranaldi, Andrea Zugarini, Leonardo Ranaldi, Fabio Massimo Zanzotto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15501">https://arxiv.org/abs/2505.15501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15501">https://arxiv.org/pdf/2505.15501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15501]] Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs(https://arxiv.org/abs/2505.15501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce the concept of protoknowledge to formalize and measure how sequences of tokens encoding Knowledge Graphs are internalized during pretraining and utilized at inference time by Large Language Models (LLMs). Indeed, LLMs have demonstrated the ability to memorize vast amounts of token sequences during pretraining, and a central open question is how they leverage this memorization as reusable knowledge through generalization. We then categorize protoknowledge into lexical, hierarchical, and topological forms, varying on the type of knowledge that needs to be activated. We measure protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general properties such as semantic bias. We then investigate the impact of protoknowledge on Text-to-SPARQL performance by varying prompting strategies depending on input conditions. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query. This methodology provides a practical tool to explore Semantic-Level Data Contamination and serves as an effective strategy for Closed-Pretraining models.</li>
</ul>

<h3>Title: Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Debarshi Brahma, Anuska Roy, Soma Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15506">https://arxiv.org/abs/2505.15506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15506">https://arxiv.org/pdf/2505.15506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15506]] Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts(https://arxiv.org/abs/2505.15506)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recently, Vision-Language foundation models like CLIP and ALIGN, which are pre-trained on large-scale data have shown remarkable zero-shot generalization to diverse datasets with different classes and even domains. In this work, we take a step further and analyze whether these models can be adapted to target datasets having very different distributions and classes compared to what these models have been trained on, using only a few labeled examples from the target dataset. In such scenarios, finetuning large pretrained models is challenging due to problems of overfitting as well as loss of generalization, and has not been well explored in prior literature. Since, the pre-training data of such models are unavailable, it is difficult to comprehend the performance on various downstream datasets. First, we try to answer the question: Given a target dataset with a few labelled examples, can we estimate whether further fine-tuning can enhance the performance compared to zero-shot evaluation? by analyzing the common vision-language embedding space. Based on the analysis, we propose a novel prompt-tuning method, PromptMargin for adapting such large-scale VLMs directly on the few target samples. PromptMargin effectively tunes the text as well as visual prompts for this task, and has two main modules: 1) Firstly, we use a selective augmentation strategy to complement the few training samples in each task; 2) Additionally, to ensure robust training in the presence of unfamiliar class names, we increase the inter-class margin for improved class discrimination using a novel Multimodal Margin Regularizer. Extensive experiments and analysis across fifteen target benchmark datasets, with varying degrees of distribution shifts from natural images, shows the effectiveness of the proposed framework over the existing state-of-the-art approaches applied to this setting. this http URL.</li>
</ul>

<h3>Title: Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Mahesh Godavarti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15507">https://arxiv.org/abs/2505.15507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15507">https://arxiv.org/pdf/2505.15507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15507]] Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning(https://arxiv.org/abs/2505.15507)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a new algebraic structure for multi-dimensional compositional embeddings, built on directional non-commutative monoidal operators. The core contribution of this work is this novel framework, which exhibits appealing theoretical properties (associativity along each dimension and an interchange law ensuring global consistency) while remaining compatible with modern machine learning architectures. Our construction defines a distinct composition operator circ_i for each axis i, ensuring associative combination along each axis without imposing global commutativity. Importantly, all axis-specific operators commute with one another, enforcing a global interchange law that enables consistent crossaxis compositions. This is, to our knowledge, the first approach that provides a common foundation that generalizes classical sequence-modeling paradigms (e.g., structured state-space models (SSMs) and transformer self-attention) to a unified multi-dimensional framework. For example, specific one-dimensional instances of our framework can recover the familiar affine transformation algebra, vanilla self-attention, and the SSM-style recurrence. The higher-dimensional generalizations naturally support recursive, structure-aware operations in embedding spaces. We outline several potential applications unlocked by this structure-including structured positional encodings in Transformers, directional image embeddings, and symbolic modeling of sequences or grids-indicating that it could inform future deep learning model designs. We formally establish the algebraic properties of our framework and discuss efficient implementations. Finally, as our focus is theoretical, we include no experiments here and defer empirical validation to future work, which we plan to undertake.</li>
</ul>

<h3>Title: Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, Libo Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15510">https://arxiv.org/abs/2505.15510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15510">https://arxiv.org/pdf/2505.15510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15510]] Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought(https://arxiv.org/abs/2505.15510)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing performance and interpretability. Recent MCoT methods fall into two categories: (i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved image-text outputs. Despite advances in both approaches, the mechanisms driving these improvements are not fully understood. To fill this gap, we first reveal that MCoT boosts LVLMs by incorporating visual thoughts, which convey image information to the reasoning process regardless of the MCoT format, depending only on clarity and conciseness of expression. Furthermore, to explore visual thoughts systematically, we define four distinct forms of visual thought expressions and analyze them comprehensively. Our findings demonstrate that these forms differ in clarity and conciseness, yielding varying levels of MCoT improvement. Additionally, we explore the internal nature of visual thoughts, finding that visual thoughts serve as intermediaries between the input image and reasoning to deeper transformer layers, enabling more advanced visual information transmission. We hope that the visual thoughts can inspire further breakthroughs for future MCoT research.</li>
</ul>

<h3>Title: NOMAD Projection</h3>
<ul>
<li><strong>Authors: </strong>Brandon Duderstadt, Zach Nussbaum, Laurens van der Maaten</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15511">https://arxiv.org/abs/2505.15511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15511">https://arxiv.org/pdf/2505.15511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15511]] NOMAD Projection(https://arxiv.org/abs/2505.15511)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>The rapid adoption of generative AI has driven an explosion in the size of datasets consumed and produced by AI models. Traditional methods for unstructured data visualization, such as t-SNE and UMAP, have not kept up with the pace of dataset scaling. This presents a significant challenge for AI explainability, which relies on methods such as t-SNE and UMAP for exploratory data analysis. In this paper, we introduce Negative Or Mean Affinity Discrimination (NOMAD) Projection, the first method for unstructured data visualization via nonlinear dimensionality reduction that can run on multiple GPUs at train time. We provide theory that situates NOMAD Projection as an approximate upper bound on the InfoNC-t-SNE loss, and empirical results that demonstrate NOMAD Projection's superior performance and speed profile compared to existing state-of-the-art methods. We demonstrate the scalability of NOMAD Projection by computing the first complete data map of Multilingual Wikipedia.</li>
</ul>

<h3>Title: Explainable embeddings with Distance Explainer</h3>
<ul>
<li><strong>Authors: </strong>Christiaan Meijer, E. G. Patrick Bos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15516">https://arxiv.org/abs/2505.15516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15516">https://arxiv.org/pdf/2505.15516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15516]] Explainable embeddings with Distance Explainer(https://arxiv.org/abs/2505.15516)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>While eXplainable AI (XAI) has advanced significantly, few methods address interpretability in embedded vector spaces where dimensions represent complex abstractions. We introduce Distance Explainer, a novel method for generating local, post-hoc explanations of embedded spaces in machine learning models. Our approach adapts saliency-based techniques from RISE to explain the distance between two embedded data points by assigning attribution values through selective masking and distance-ranked mask filtering. We evaluate Distance Explainer on cross-modal embeddings (image-image and image-caption pairs) using established XAI metrics including Faithfulness, Sensitivity/Robustness, and Randomization. Experiments with ImageNet and CLIP models demonstrate that our method effectively identifies features contributing to similarity or dissimilarity between embedded data points while maintaining high robustness and consistency. We also explore how parameter tuning, particularly mask quantity and selection strategy, affects explanation quality. This work addresses a critical gap in XAI research and enhances transparency and trustworthiness in deep learning applications utilizing embedded spaces.</li>
</ul>

<h3>Title: Detection of Underwater Multi-Targets Based on Self-Supervised Learning and Deformable Path Aggregation Feature Pyramid Network</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15518">https://arxiv.org/abs/2505.15518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15518">https://arxiv.org/pdf/2505.15518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15518]] Detection of Underwater Multi-Targets Based on Self-Supervised Learning and Deformable Path Aggregation Feature Pyramid Network(https://arxiv.org/abs/2505.15518)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To overcome the constraints of the underwater environment and improve the accuracy and robustness of underwater target detection models, this paper develops a specialized dataset for underwater target detection and proposes an efficient algorithm for underwater multi-target detection. A self-supervised learning based on the SimSiam structure is employed for the pre-training of underwater target detection network. To address the problems of low detection accuracy caused by low contrast, mutual occlusion and dense distribution of underwater targets in underwater object detection, a detection model suitable for underwater target detection is proposed by introducing deformable convolution and dilated convolution. The proposed detection model can obtain more effective information by increasing the receptive field. In addition, the regression loss function EIoU is introduced, which improves model performance by separately calculating the width and height losses of the predicted box. Experiment results show that the accuracy of the underwater target detection has been improved by the proposed detector.</li>
</ul>

<h3>Title: Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lang Gao, Kaiyang Wan, Wei Liu, Chenxi Wang, Zirui Song, Zixiang Xu, Yanbo Wang, Veselin Stoyanov, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15524">https://arxiv.org/abs/2505.15524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15524">https://arxiv.org/pdf/2505.15524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15524]] Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs(https://arxiv.org/abs/2505.15524)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.</li>
</ul>

<h3>Title: PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15528">https://arxiv.org/abs/2505.15528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15528">https://arxiv.org/pdf/2505.15528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15528]] PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting(https://arxiv.org/abs/2505.15528)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent years have seen substantial improvements in the ability to generate synthetic 3D objects using AI. However, generating complex 3D objects, such as plants, remains a considerable challenge. Current generative 3D models struggle with plant generation compared to general objects, limiting their usability in plant analysis tools, which require fine detail and accurate geometry. We introduce PlantDreamer, a novel approach to 3D synthetic plant generation, which can achieve greater levels of realism for complex plant geometry and textures than available text-to-3D models. To achieve this, our new generation pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an adaptable Gaussian culling algorithm, which directly improve textural realism and geometric integrity of generated 3D plant models. Additionally, PlantDreamer enables both purely synthetic plant generation, by leveraging L-System-generated meshes, and the enhancement of real-world plant point clouds by converting them into 3D Gaussian Splats. We evaluate our approach by comparing its outputs with state-of-the-art text-to-3D models, demonstrating that PlantDreamer outperforms existing methods in producing high-fidelity synthetic plants. Our results indicate that our approach not only advances synthetic plant generation, but also facilitates the upgrading of legacy point cloud datasets, making it a valuable tool for 3D phenotyping applications.</li>
</ul>

<h3>Title: seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Andrew Caunes, Thierry Chateau, Vincent Fremont</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15545">https://arxiv.org/abs/2505.15545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15545">https://arxiv.org/pdf/2505.15545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15545]] seg_3D_by_PC2D: Multi-View Projection for Domain Generalization and Adaptation in 3D Semantic Segmentation(https://arxiv.org/abs/2505.15545)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D semantic segmentation plays a pivotal role in autonomous driving and road infrastructure analysis, yet state-of-the-art 3D models are prone to severe domain shift when deployed across different datasets. We propose a novel multi-view projection framework that excels in both domain generalization (DG) and unsupervised domain adaptation (UDA). Our approach first aligns Lidar scans into coherent 3D scenes and renders them from multiple virtual camera poses to create a large-scale synthetic 2D dataset (PC2D). We then use it to train a 2D segmentation model in-domain. During inference, the model processes hundreds of views per scene; the resulting logits are back-projected to 3D with an occlusion-aware voting scheme to generate final point-wise labels. Our framework is modular and enables extensive exploration of key design parameters, such as view generation optimization (VGO), visualization modality optimization (MODO), and 2D model choice. We evaluate on the nuScenes and SemanticKITTI datasets under both the DG and UDA settings. We achieve state-of-the-art results in UDA and close to state-of-the-art in DG, with particularly large gains on large, static classes. Our code and dataset generation tools will be publicly available at this https URL</li>
</ul>

<h3>Title: Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution</h3>
<ul>
<li><strong>Authors: </strong>Suvadeep Hajra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15548">https://arxiv.org/abs/2505.15548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15548">https://arxiv.org/pdf/2505.15548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15548]] Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution(https://arxiv.org/abs/2505.15548)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer language models have driven significant progress across various fields, including natural language processing and computer vision. A central component of these models is the self-attention (SA) mechanism, which learns rich vector representations of tokens by modeling their relationships with others in a sequence. However, despite extensive research, transformers continue to suffer from training instability -- often manifesting as spikes or divergence in the training loss during a run. In this work, we identify one source of this instability: SA's limited ability to capture short-range dependencies, especially in tasks like language modeling, where almost every token heavily relies on its nearby neighbors. This limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing training. To address this, we propose decomposing the SA into local (short-range) and global (long-range) attention heads. This decomposed attention, referred to as Long Short-attention (LS-attention), mitigates logit explosion and results in more stable training compared to an equivalent multi-head self-attention (MHSA). Empirical comparisons with two alternative training stabilization methods show that LS-attention reduces the validation perplexity to nearly 2/5 of that achieved by one method and reaches a similar perplexity as the other method using only 1/20 of the GPU hours. Additionally, our experiments demonstrate that LS-attention reduces inference latency by up to 36% compared to a state-of-the-art implementation of equivalent MHSA.</li>
</ul>

<h3>Title: Social Bias in Popular Question-Answering Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Angelie Kraft, Judith Simon, Sonja Schimmler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15553">https://arxiv.org/abs/2505.15553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15553">https://arxiv.org/pdf/2505.15553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15553]] Social Bias in Popular Question-Answering Benchmarks(https://arxiv.org/abs/2505.15553)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Question-answering (QA) and reading comprehension (RC) benchmarks are essential for assessing the capabilities of large language models (LLMs) in retrieving and reproducing knowledge. However, we demonstrate that popular QA and RC benchmarks are biased and do not cover questions about different demographics or regions in a representative way, potentially due to a lack of diversity of those involved in their creation. We perform a qualitative content analysis of 30 benchmark papers and a quantitative analysis of 20 respective benchmark datasets to learn (1) who is involved in the benchmark creation, (2) how social bias is addressed or prevented, and (3) whether the demographics of the creators and annotators correspond to particular biases in the content. Most analyzed benchmark papers provided insufficient information regarding the stakeholders involved in benchmark creation, particularly the annotators. Notably, just one of the benchmark papers explicitly reported measures taken to address social representation issues. Moreover, the data analysis revealed gender, religion, and geographic biases across a wide range of encyclopedic, commonsense, and scholarly benchmarks. More transparent and bias-aware QA and RC benchmark creation practices are needed to facilitate better scrutiny and incentivize the development of fairer LLMs.</li>
</ul>

<h3>Title: DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion</h3>
<ul>
<li><strong>Authors: </strong>Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kökciyan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15554">https://arxiv.org/abs/2505.15554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15554">https://arxiv.org/pdf/2505.15554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15554]] DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion(https://arxiv.org/abs/2505.15554)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Critical questions are essential resources to provoke critical thinking when encountering an argumentative text. We present our system for the Critical Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach leverages large language models (LLMs) with chain-of-thought prompting to generate critical questions guided by Walton's argumentation schemes. For each input intervention, we conversationally prompt LLMs to instantiate the corresponding argument scheme template to first obtain structured arguments, and then generate relevant critical questions. Following this, we rank all the available critical questions by prompting LLMs to select the top 3 most helpful questions based on the original intervention text. This combination of structured argumentation theory and step-by-step reasoning enables the generation of contextually relevant and diverse critical questions. Our pipeline achieves competitive performance in the final test set, showing its potential to foster critical thinking given argumentative text and detect missing or uninformed claims. Code available at \href{this https URL}{DayDreamer}.</li>
</ul>

<h3>Title: Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection</h3>
<ul>
<li><strong>Authors: </strong>Julian Oelhaf, Georg Kordowich, Changhun Kim, Paula Andrea Perez-Toro, Andreas Maier, Johann Jager, Siming Bayer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15560">https://arxiv.org/abs/2505.15560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15560">https://arxiv.org/pdf/2505.15560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15560]] Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection(https://arxiv.org/abs/2505.15560)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Germany's transition to a renewable energy-based power system is reshaping grid operations, requiring advanced monitoring and control to manage decentralized generation. Machine learning (ML) has emerged as a powerful tool for power system protection, particularly for fault detection (FD) and fault line identification (FLI) in transmission grids. However, ML model reliability depends on data quality and availability. Data sparsity resulting from sensor failures, communication disruptions, or reduced sampling rates poses a challenge to ML-based FD and FLI. Yet, its impact has not been systematically validated prior to this work. In response, we propose a framework to assess the impact of data sparsity on ML-based FD and FLI performance. We simulate realistic data sparsity scenarios, evaluate their impact, derive quantitative insights, and demonstrate the effectiveness of this evaluation strategy by applying it to an existing ML-based framework. Results show the ML model remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after a 50x data reduction. In contrast, FLI is more sensitive, with performance decreasing by 55.61% for missing voltage measurements and 9.73% due to communication failures at critical network points. These findings offer actionable insights for optimizing ML models for real-world grid protection. This enables more efficient FD and supports targeted improvements in FLI.</li>
</ul>

<h3>Title: Model Checking the Security of the Lightning Network</h3>
<ul>
<li><strong>Authors: </strong>Matthias Grundmann, Hannes Hartenstein</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15568">https://arxiv.org/abs/2505.15568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15568">https://arxiv.org/pdf/2505.15568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15568]] Model Checking the Security of the Lightning Network(https://arxiv.org/abs/2505.15568)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Payment channel networks are an approach to improve the scalability of blockchain-based cryptocurrencies. The Lightning Network is a payment channel network built for Bitcoin that is already used in practice. Because the Lightning Network is used for transfer of financial value, its security in the presence of adversarial participants should be verified. The Lightning protocol's complexity makes it hard to assess whether the protocol is secure. To enable computer-aided security verification of Lightning, we formalize the protocol in TLA+ and formally specify the security property that honest users are guaranteed to retrieve their correct balance. While model checking provides a fully automated verification of the security property, the state space of the protocol's specification is so large that model checking becomes unfeasible. We make model checking the Lightning Network possible using two refinement steps that we verify using proofs. In a first step, we prove that the model of time used in the protocol can be abstracted using ideas from the research of timed automata. In a second step, we prove that it suffices to model check the protocol for single payment channels and the protocol for multi-hop payments separately. These refinements reduce the state space sufficiently to allow for model checking Lightning with models with payments over up to four hops and two concurrent payments. These results indicate that the current specification of Lightning is secure.</li>
</ul>

<h3>Title: Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback</h3>
<ul>
<li><strong>Authors: </strong>Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Haifeng Chen, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15572">https://arxiv.org/abs/2505.15572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15572">https://arxiv.org/pdf/2505.15572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15572]] Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback(https://arxiv.org/abs/2505.15572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The data-to-equation (Data2Eqn) task aims to discover interpretable mathematical equations that map observed values to labels, offering physical insights and broad applicability across academic and industrial domains. Genetic programming and traditional deep learning-based approaches suffer from search inefficiency and poor generalization on small task-specific datasets. Foundation models showed promise in this area, but existing approaches suffer from: 1) They are pretrained on general-purpose data distributions, making them less effective for domain-specific tasks; and 2) their training objectives focus on token-level alignment, overlooking mathematical semantics, which can lead to inaccurate equations. To address these issues, we aim to enhance the domain adaptability of foundation models for Data2Eqn tasks. In this work, we propose a reinforcement learning-based finetuning framework that directly optimizes the generation policy of a pretrained model through reward signals derived from downstream numerical fitness. Our method allows the model to adapt to specific and complex data distributions and generate mathematically meaningful equations. Extensive experiments demonstrate that our approach improves both the accuracy and robustness of equation generation under complex distributions.</li>
</ul>

<h3>Title: Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Hossein Zakerinia, Jonathan Scott, Christoph H. Lampert</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15579">https://arxiv.org/abs/2505.15579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15579">https://arxiv.org/pdf/2505.15579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15579]] Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions(https://arxiv.org/abs/2505.15579)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Personalized federated learning has emerged as a popular approach to training on devices holding statistically heterogeneous data, known as clients. However, most existing approaches require a client to have labeled data for training or finetuning in order to obtain their own personalized model. In this paper we address this by proposing FLowDUP, a novel method that is able to generate a personalized model using only a forward pass with unlabeled data. The generated model parameters reside in a low-dimensional subspace, enabling efficient communication and computation. FLowDUP's learning objective is theoretically motivated by our new transductive multi-task PAC-Bayesian generalization bound, that provides performance guarantees for unlabeled clients. The objective is structured in such a way that it allows both clients with labeled data and clients with only unlabeled data to contribute to the training process. To supplement our theoretical results we carry out a thorough experimental evaluation of FLowDUP, demonstrating strong empirical performance on a range of datasets with differing sorts of statistically heterogeneous clients. Through numerous ablation studies, we test the efficacy of the individual components of the method.</li>
</ul>

<h3>Title: UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Sam Kwong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15581">https://arxiv.org/abs/2505.15581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15581">https://arxiv.org/pdf/2505.15581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15581]] UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset(https://arxiv.org/abs/2505.15581)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at this https URL.</li>
</ul>

<h3>Title: World Models as Reference Trajectories for Rapid Motor Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Carlos Stein Brito, Daniel McNamee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15589">https://arxiv.org/abs/2505.15589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15589">https://arxiv.org/pdf/2505.15589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15589]] World Models as Reference Trajectories for Rapid Motor Adaptation(https://arxiv.org/abs/2505.15589)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deploying learned control policies in real-world environments poses a fundamental challenge. When system dynamics change unexpectedly, performance degrades until models are retrained on new data. We introduce Reflexive World Models (RWM), a dual control framework that uses world model predictions as implicit reference trajectories for rapid adaptation. Our method separates the control problem into long-term reward maximization through reinforcement learning and robust motor execution through rapid latent control. This dual architecture achieves significantly faster adaptation with low online computational cost compared to model-based RL baselines, while maintaining near-optimal performance. The approach combines the benefits of flexible policy learning through reinforcement learning with rapid error correction capabilities, providing a principled approach to maintaining performance in high-dimensional continuous control tasks under varying dynamics.</li>
</ul>

<h3>Title: VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Niccolo Avogaro, Thomas Frick, Yagmur G. Cinar, Daniel Caraballo, Cezary Skura, Filip M. Janicki, Piotr Kluska, Brown Ebouky, Nicola Farronato, Florian Scheidegger, Cristiano Malossi, Konrad Schindler, Andrea Bartezzaghi, Roy Assaf, Mattia Rigotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15592">https://arxiv.org/abs/2505.15592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15592">https://arxiv.org/pdf/2505.15592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15592]] VP Lab: a PEFT-Enabled Visual Prompting Laboratory for Semantic Segmentation(https://arxiv.org/abs/2505.15592)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Large-scale pretrained vision backbones have transformed computer vision by providing powerful feature extractors that enable various downstream tasks, including training-free approaches like visual prompting for semantic segmentation. Despite their success in generic scenarios, these models often fall short when applied to specialized technical domains where the visual features differ significantly from their training distribution. To bridge this gap, we introduce VP Lab, a comprehensive iterative framework that enhances visual prompting for robust segmentation model development. At the core of VP Lab lies E-PEFT, a novel ensemble of parameter-efficient fine-tuning techniques specifically designed to adapt our visual prompting pipeline to specific domains in a manner that is both parameter- and data-efficient. Our approach not only surpasses the state-of-the-art in parameter-efficient fine-tuning for the Segment Anything Model (SAM), but also facilitates an interactive, near-real-time loop, allowing users to observe progressively improving results as they experiment within the framework. By integrating E-PEFT with visual prompting, we demonstrate a remarkable 50\% increase in semantic segmentation mIoU performance across various technical datasets using only 5 validated images, establishing a new paradigm for fast, efficient, and interactive model deployment in new, challenging domains. This work comes in the form of a demonstration.</li>
</ul>

<h3>Title: Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off</h3>
<ul>
<li><strong>Authors: </strong>Yury Belousov, Brian Pulfer, Vitaliy Kinakh, Slava Voloshynovskiy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15594">https://arxiv.org/abs/2505.15594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15594">https://arxiv.org/pdf/2505.15594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15594]] Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off(https://arxiv.org/abs/2505.15594)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>While foundation models demonstrate impressive performance across various tasks, they remain vulnerable to adversarial inputs. Current research explores various approaches to enhance model robustness, with Diffusion Denoised Smoothing emerging as a particularly promising technique. This method employs a pretrained diffusion model to preprocess inputs before model inference. Yet, its effectiveness remains largely unexplored beyond classification. We aim to address this gap by analyzing three datasets with four distinct downstream tasks under three different adversarial attack algorithms. Our findings reveal that while foundation models maintain resilience against conventional transformations, applying high-noise diffusion denoising to clean images without any distortions significantly degrades performance by as high as 57%. Low-noise diffusion settings preserve performance but fail to provide adequate protection across all attack types. Moreover, we introduce a novel attack strategy specifically targeting the diffusion process itself, capable of circumventing defenses in the low-noise regime. Our results suggest that the trade-off between adversarial robustness and performance remains a challenge to be addressed.</li>
</ul>

<h3>Title: From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15607">https://arxiv.org/abs/2505.15607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15607">https://arxiv.org/pdf/2505.15607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15607]] From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning(https://arxiv.org/abs/2505.15607)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning.</li>
</ul>

<h3>Title: LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruilin Yao, Bo Zhang, Jirui Huang, Xinwei Long, Yifang Zhang, Tianyu Zou, Yufei Wu, Shichao Su, Yifan Xu, Wenxi Zeng, Zhaoyu Yang, Guoyou Li, Shilan Zhang, Zichan Li, Yaxiong Chen, Shengwu Xiong, Peng Xu, Jiajun Zhang, Bowen Zhou, David Clifton, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15616">https://arxiv.org/abs/2505.15616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15616">https://arxiv.org/pdf/2505.15616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15616]] LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models(https://arxiv.org/abs/2505.15616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved significant advances in integrating visual and linguistic information, yet their ability to reason about complex and real-world scenarios remains limited. The existing benchmarks are usually constructed in the task-oriented manner without guarantee that different task samples come from the same data distribution, thus they often fall short in evaluating the synergistic effects of lower-level perceptual capabilities on higher-order reasoning. To lift this limitation, we contribute Lens, a multi-level benchmark with 3.4K contemporary images and 60K+ human-authored questions covering eight tasks and 12 daily scenarios, forming three progressive task tiers, i.e., perception, understanding, and reasoning. One feature is that each image is equipped with rich annotations for all tasks. Thus, this dataset intrinsically supports to evaluate MLLMs to handle image-invariable prompts, from basic perception to compositional reasoning. In addition, our images are manully collected from the social media, in which 53% were published later than Jan. 2025. We evaluate 15+ frontier MLLMs such as Qwen2.5-VL-72B, InternVL3-78B, GPT-4o and two reasoning models QVQ-72B-preview and Kimi-VL. These models are released later than Dec. 2024, and none of them achieve an accuracy greater than 60% in the reasoning tasks. Project page: this https URL. ICCV 2025 workshop page: this https URL</li>
</ul>

<h3>Title: Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15623">https://arxiv.org/abs/2505.15623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15623">https://arxiv.org/pdf/2505.15623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15623]] Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning(https://arxiv.org/abs/2505.15623)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate considerable potential in various natural language tasks but face significant challenges in mathematical reasoning, particularly in executing precise, multi-step logic. However, current evaluation frameworks judge their performance solely based on accuracy, which only accounts for the final answer. This study explores these pitfalls by employing a novel evaluation framework. We propose an evaluation metric called the MAPLE score, which holistically quantifies reasoning misalignment by integrating error rates, redundancy, and validity.</li>
</ul>

<h3>Title: Mechanistic Insights into Grokking from the Embedding Layer</h3>
<ul>
<li><strong>Authors: </strong>H.V.AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Munachiso Nwadike, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15624">https://arxiv.org/abs/2505.15624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15624">https://arxiv.org/pdf/2505.15624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15624]] Mechanistic Insights into Grokking from the Embedding Layer(https://arxiv.org/abs/2505.15624)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Grokking, a delayed generalization in neural networks after perfect training performance, has been observed in Transformers and MLPs, but the components driving it remain underexplored. We show that embeddings are central to grokking: introducing them into MLPs induces delayed generalization in modular arithmetic tasks, whereas MLPs without embeddings can generalize immediately. Our analysis identifies two key mechanisms: (1) Embedding update dynamics, where rare tokens stagnate due to sparse gradient updates and weight decay, and (2) Bilinear coupling, where the interaction between embeddings and downstream weights introduces saddle points and increases sensitivity to initialization. To confirm these mechanisms, we investigate frequency-aware sampling, which balances token updates by minimizing gradient variance, and embedding-specific learning rates, derived from the asymmetric curvature of the bilinear loss landscape. We prove that an adaptive learning rate ratio, \(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot \frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating convergence. Our methods not only improve grokking dynamics but also extend to broader challenges in Transformer optimization, where bilinear interactions hinder efficient training.</li>
</ul>

<h3>Title: Aligning Explanations with Human Communication</h3>
<ul>
<li><strong>Authors: </strong>Jacopo Teneggi, Zhenzhen Wang, Paul H. Yi, Tianmin Shu, Jeremias Sulam</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15626">https://arxiv.org/abs/2505.15626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15626">https://arxiv.org/pdf/2505.15626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15626]] Aligning Explanations with Human Communication(https://arxiv.org/abs/2505.15626)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Machine learning explainability aims to make the decision-making process of black-box models more transparent by finding the most important input features for a given prediction task. Recent works have proposed composing explanations from semantic concepts (e.g., colors, patterns, shapes) that are inherently interpretable to the user of a model. However, these methods generally ignore the communicative context of explanation-the ability of the user to understand the prediction of the model from the explanation. For example, while a medical doctor might understand an explanation in terms of clinical markers, a patient may need a more accessible explanation to make sense of the same diagnosis. In this paper, we address this gap with listener-adaptive explanations. We propose an iterative procedure grounded in principles of pragmatic reasoning and the rational speech act to generate explanations that maximize communicative utility. Our procedure only needs access to pairwise preferences between candidate explanations, relevant in real-world scenarios where a listener model may not be available. We evaluate our method in image classification tasks, demonstrating improved alignment between explanations and listener preferences across three datasets. Furthermore, we perform a user study that demonstrates our explanations increase communicative utility.</li>
</ul>

<h3>Title: Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions</h3>
<ul>
<li><strong>Authors: </strong>David Thulke, Jakob Kemmler, Christian Dugast, Hermann Ney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15633">https://arxiv.org/abs/2505.15633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15633">https://arxiv.org/pdf/2505.15633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15633]] Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions(https://arxiv.org/abs/2505.15633)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models that use retrieval augmented generation have the potential to unlock valuable knowledge for researchers, policymakers, and the public by making long and technical climate-related documents more accessible. While this approach can help alleviate factual hallucinations by relying on retrieved passages as additional context, its effectiveness depends on whether the model's output remains faithful to these passages. To address this, we explore the automatic assessment of faithfulness of different models in this setting. We then focus on ClimateGPT, a large language model specialised in climate science, to examine which factors in its instruction fine-tuning impact the model's faithfulness. By excluding unfaithful subsets of the model's training data, we develop ClimateGPT Faithful+, which achieves an improvement in faithfulness from 30% to 57% in supported atomic claims according to our automatic metric.</li>
</ul>

<h3>Title: Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15634">https://arxiv.org/abs/2505.15634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15634">https://arxiv.org/pdf/2505.15634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15634]] Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models(https://arxiv.org/abs/2505.15634)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.</li>
</ul>

<h3>Title: Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Pujun Xue, Junyi Ge, Xiaotong Jiang, Siyang Song, Zijian Wu, Yupeng Huo, Weicheng Xie, Linlin Shen, Xiaoqin Zhou, Xiaofeng Liu, Min Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15637">https://arxiv.org/abs/2505.15637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15637">https://arxiv.org/pdf/2505.15637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15637]] Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking(https://arxiv.org/abs/2505.15637)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Malocclusion is a major challenge in orthodontics, and its complex presentation and diverse clinical manifestations make accurate localization and diagnosis particularly important. Currently, one of the major shortcomings facing the field of dental image analysis is the lack of large-scale, accurately labeled datasets dedicated to malocclusion issues, which limits the development of automated diagnostics in the field of dentistry and leads to a lack of diagnostic accuracy and efficiency in clinical practice. Therefore, in this study, we propose the Oral and Maxillofacial Natural Images (OMNI) dataset, a novel and comprehensive dental image dataset aimed at advancing the study of analyzing dental images for issues of malocclusion. Specifically, the dataset contains 4166 multi-view images with 384 participants in data collection and annotated by professional dentists. In addition, we performed a comprehensive validation of the created OMNI dataset, including three CNN-based methods, two Transformer-based methods, and one GNN-based method, and conducted automated diagnostic experiments for malocclusion issues. The experimental results show that the OMNI dataset can facilitate the automated diagnosis research of malocclusion issues and provide a new benchmark for the research in this field. Our OMNI dataset and baseline code are publicly available at this https URL.</li>
</ul>

<h3>Title: FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhen Sun, Ziyi Zhang, Zeren Luo, Zeyang Sha, Tianshuo Cong, Zheng Li, Shiwen Cui, Weiqiang Wang, Jiaheng Wei, Xinlei He, Qi Li, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15644">https://arxiv.org/abs/2505.15644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15644">https://arxiv.org/pdf/2505.15644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15644]] FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models(https://arxiv.org/abs/2505.15644)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fine-grained edited image detection of localized edits in images is crucial for assessing content authenticity, especially given that modern diffusion models and image editing methods can produce highly realistic manipulations. However, this domain faces three challenges: (1) Binary classifiers yield only a global real-or-fake label without providing localization; (2) Traditional computer vision methods often rely on costly pixel-level annotations; and (3) No large-scale, high-quality dataset exists for modern image-editing detection techniques. To address these gaps, we develop an automated data-generation pipeline to create FragFake, the first dedicated benchmark dataset for edited image detection, which includes high-quality images from diverse editing models and a wide variety of edited objects. Based on FragFake, we utilize Vision Language Models (VLMs) for the first time in the task of edited image classification and edited region localization. Experimental results show that fine-tuned VLMs achieve higher average Object Precision across all datasets, significantly outperforming pretrained models. We further conduct ablation and transferability analyses to evaluate the detectors across various configurations and editing scenarios. To the best of our knowledge, this work is the first to reformulate localized image edit detection as a vision-language understanding task, establishing a new paradigm for the field. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity.</li>
</ul>

<h3>Title: Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!</h3>
<ul>
<li><strong>Authors: </strong>Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15656">https://arxiv.org/abs/2505.15656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15656">https://arxiv.org/pdf/2505.15656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15656]] Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!(https://arxiv.org/abs/2505.15656)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at this https URL.</li>
</ul>

<h3>Title: Graph Conditional Flow Matching for Relational Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Davide Scassola, Sebastiano Saccani, Luca Bortolussi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15668">https://arxiv.org/abs/2505.15668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15668">https://arxiv.org/pdf/2505.15668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15668]] Graph Conditional Flow Matching for Relational Data Generation(https://arxiv.org/abs/2505.15668)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Data synthesis is gaining momentum as a privacy-enhancing technology. While single-table tabular data generation has seen considerable progress, current methods for multi-table data often lack the flexibility and expressiveness needed to capture complex relational structures. In particular, they struggle with long-range dependencies and complex foreign-key relationships, such as tables with multiple parent tables or multiple types of links between the same pair of tables. We propose a generative model for relational data that generates the content of a relational dataset given the graph formed by the foreign-key relationships. We do this by learning a deep generative model of the content of the whole relational database by flow matching, where the neural network trained to denoise records leverages a graph neural network to obtain information from connected records. Our method is flexible, as it can support relational datasets with complex structures, and expressive, as the generation of each record can be influenced by any other record within the same connected component. We evaluate our method on several benchmark datasets and show that it achieves state-of-the-art performance in terms of synthetic data fidelity.</li>
</ul>

<h3>Title: UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Ningyu Zhang, Kun Wang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15674">https://arxiv.org/abs/2505.15674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15674">https://arxiv.org/pdf/2505.15674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15674]] UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models(https://arxiv.org/abs/2505.15674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.</li>
</ul>

<h3>Title: A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability</h3>
<ul>
<li><strong>Authors: </strong>Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, Zhiming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15683">https://arxiv.org/abs/2505.15683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15683">https://arxiv.org/pdf/2505.15683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15683]] A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability(https://arxiv.org/abs/2505.15683)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, federate, transformer</a></li>
<li><strong>Abstract: </strong>Private data is typically larger and of higher quality than public data, offering great potential to improve LLM. However, its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based split learning model has emerged, offloading most model parameters to the server while retaining only the embedding and output layers on clients to ensure privacy. However, it still faces significant challenges in security, efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks, leading to reverse engineering of private data; 2) the autoregressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a secure, efficient, and adaptive federated split framework based on LLaMA2. First, we place some input and output blocks on the local client and inject Gaussian noise into forward-pass hidden states, enabling secure end-to-end propagation. Second, we employ client-batch and server-hierarchical strategies to achieve parallel training, along with attention-mask compression and KV cache mechanisms to accelerate inference, reducing communication costs effectively. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements and hardware limitations. Experiments on NLU, summarization and conversational QA tasks show that FL-LLaMA maintains performance comparable to centralized LLaMA2, and achieves up to 2x train speedups and 8x inference speedups. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FL-LLaMA in security and adaptability.</li>
</ul>

<h3>Title: ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy</h3>
<ul>
<li><strong>Authors: </strong>Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15684">https://arxiv.org/abs/2505.15684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15684">https://arxiv.org/pdf/2505.15684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15684]] ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy(https://arxiv.org/abs/2505.15684)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), the excessive length of reasoning tokens increases latency and KV cache memory usage, and may even truncate final answers under context limits. We propose ThinkLess, an inference-efficient framework that terminates reasoning generation early and maintains output quality without modifying the model. Atttention analysis reveals that answer tokens focus minimally on earlier reasoning steps and primarily attend to the reasoning terminator token, due to information migration under causal masking. Building on this insight, ThinkLess inserts the terminator token at earlier positions to skip redundant reasoning while preserving the underlying knowledge transfer. To prevent format discruption casued by early termination, ThinkLess employs a lightweight post-regulation mechanism, relying on the model's natural instruction-following ability to produce well-structured answers. Without fine-tuning or auxiliary data, ThinkLess achieves comparable accuracy to full-length CoT decoding while greatly reducing decoding time and memory consumption.</li>
</ul>

<h3>Title: Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15692">https://arxiv.org/abs/2505.15692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15692">https://arxiv.org/pdf/2505.15692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15692]] Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities(https://arxiv.org/abs/2505.15692)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.</li>
</ul>

<h3>Title: A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Zhou, Yulian Wu, Francesco Orabona</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15694">https://arxiv.org/abs/2505.15694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15694">https://arxiv.org/pdf/2505.15694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15694]] A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO(https://arxiv.org/abs/2505.15694)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>In this paper, we theoretically investigate the effects of noisy labels in offline alignment, with a focus on the interplay between privacy and robustness against adversarial corruption. Specifically, under linear modeling assumptions, we present a unified analysis covering both reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under different privacy-corruption scenarios, such as Local differential privacy-then-Corruption (LTC), where human preference labels are privatized before being corrupted by an adversary, and Corruption-then-Local differential privacy (CTL), where labels are corrupted before privacy protection. Our analysis leverages a reduction framework that reduces the offline alignment problem under linear modeling assumptions to parameter estimation in logistic regression. This framework allows us to establish an interesting separation result between LTC and CTL, demonstrating that LTC presents a greater challenge than CTL in offline alignment, even under linear models. As important by-products, our findings also advance the state-of-the-art theoretical results in offline alignment under privacy-only or corruption-only scenarios.</li>
</ul>

<h3>Title: Can Large Language Models be Effective Online Opinion Miners?</h3>
<ul>
<li><strong>Authors: </strong>Ryang Heo, Yongsik Seo, Junseong Lee, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15695">https://arxiv.org/abs/2505.15695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15695">https://arxiv.org/pdf/2505.15695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15695]] Can Large Language Models be Effective Online Opinion Miners?(https://arxiv.org/abs/2505.15695)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The surge of user-generated online content presents a wealth of insights into customer preferences and market trends. However, the highly diverse, complex, and context-rich nature of such contents poses significant challenges to traditional opinion mining approaches. To address this, we introduce Online Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol designed to assess the ability of large language models (LLMs) to mine opinions effectively from diverse and intricate online environments. OOMB provides extensive (entity, feature, opinion) tuple annotations and a comprehensive opinion-centric summary that highlights key opinion topics within each content, thereby enabling the evaluation of both the extractive and abstractive capabilities of models. Through our proposed benchmark, we conduct a comprehensive analysis of which aspects remain challenging and where LLMs exhibit adaptability, to explore whether they can effectively serve as opinion miners in realistic online scenarios. This study lays the foundation for LLM-based opinion mining and discusses directions for future research in this field.</li>
</ul>

<h3>Title: LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15702">https://arxiv.org/abs/2505.15702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15702">https://arxiv.org/pdf/2505.15702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15702]] LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing(https://arxiv.org/abs/2505.15702)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models often contain factually incorrect or outdated knowledge, giving rise to model editing methods for precise knowledge updates. However, current mainstream locate-then-edit approaches exhibit a progressive performance decline during sequential editing, due to inadequate mechanisms for long-term knowledge preservation. To tackle this, we model the sequential editing as a constrained stochastic programming. Given the challenges posed by the cumulative preservation error constraint and the gradually revealed editing tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov optimization to decompose the long-term constrained programming into tractable stepwise subproblems for efficient solving. This is the first model editing framework with rigorous theoretical guarantees, achieving asymptotic optimal editing performance while meeting the constraints of long-term knowledge preservation. Experimental results show that our framework scales sequential editing capacity to over 10,000 edits while stabilizing general capabilities and boosting average editing efficacy by 11.89\% over SOTA baselines. Furthermore, it can be leveraged to enhance the performance of baseline methods. Our code is released on this https URL.</li>
</ul>

<h3>Title: Advancing LLM Safe Alignment with Safety Representation Ranking</h3>
<ul>
<li><strong>Authors: </strong>Tianqi Du, Zeming Wei, Quan Chen, Chenheng Zhang, Yisen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15710">https://arxiv.org/abs/2505.15710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15710">https://arxiv.org/pdf/2505.15710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15710]] Advancing LLM Safe Alignment with Safety Representation Ranking(https://arxiv.org/abs/2505.15710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has demonstrated milestone success in a variety of tasks, yet their potential for generating harmful content has raised significant safety concerns. Existing safety evaluation approaches typically operate directly on textual responses, overlooking the rich information embedded in the model's internal representations. In this paper, we propose Safety Representation Ranking (SRR), a listwise ranking framework that selects safe responses using hidden states from the LLM itself. SRR encodes both instructions and candidate completions using intermediate transformer representations and ranks candidates via a lightweight similarity-based scorer. Our approach directly leverages internal model states and supervision at the list level to capture subtle safety signals. Experiments across multiple benchmarks show that SRR significantly improves robustness to adversarial prompts. Our code will be available upon publication.</li>
</ul>

<h3>Title: TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games</h3>
<ul>
<li><strong>Authors: </strong>Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15712">https://arxiv.org/abs/2505.15712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15712">https://arxiv.org/pdf/2505.15712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15712]] TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games(https://arxiv.org/abs/2505.15712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces TurnaboutLLM, a novel framework and dataset for evaluating the deductive reasoning abilities of Large Language Models (LLMs) by leveraging the interactive gameplay of detective games Ace Attorney and Danganronpa. The framework tasks LLMs with identifying contradictions between testimonies and evidences within long narrative contexts, a challenging task due to the large answer space and diverse reasoning types presented by its questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at limitations of popular strategies for enhancing deductive reasoning such as extensive thinking and Chain-of-Thought prompting. The results also suggest varying effects of context size, the number of reasoning step and answer space size on model performance. Overall, TurnaboutLLM presents a substantial challenge for LLMs' deductive reasoning abilities in complex, narrative-rich environments.</li>
</ul>

<h3>Title: Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling</h3>
<ul>
<li><strong>Authors: </strong>He Hu, Yucheng Zhou, Juzheng Si, Qianning Wang, Hengheng Zhang, Fuji Ren, Fei Ma, Laizhong Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15715">https://arxiv.org/abs/2505.15715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15715">https://arxiv.org/pdf/2505.15715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15715]] Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling(https://arxiv.org/abs/2505.15715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) hold significant potential for mental health support, capable of generating empathetic responses and simulating therapeutic conversations. However, existing LLM-based approaches often lack the clinical grounding necessary for real-world psychological counseling, particularly in explicit diagnostic reasoning aligned with standards like the DSM/ICD and incorporating diverse therapeutic modalities beyond basic empathy or single strategies. To address these critical limitations, we propose PsyLLM, the first large language model designed to systematically integrate both diagnostic and therapeutic reasoning for mental health counseling. To develop the PsyLLM, we propose a novel automated data synthesis pipeline. This pipeline processes real-world mental health posts, generates multi-turn dialogue structures, and leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate detailed clinical reasoning processes. Rigorous multi-dimensional filtering ensures the generation of high-quality, clinically aligned dialogue data. In addition, we introduce a new benchmark and evaluation protocol, assessing counseling quality across four key dimensions: comprehensiveness, professionalism, authenticity, and safety. Our experiments demonstrate that PsyLLM significantly outperforms state-of-the-art baseline models on this benchmark.</li>
</ul>

<h3>Title: Privacy-Preserving Conformal Prediction Under Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Coby Penso, Bar Mahpud, Jacob Goldberger, Or Sheffet</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15721">https://arxiv.org/abs/2505.15721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15721">https://arxiv.org/pdf/2505.15721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15721]] Privacy-Preserving Conformal Prediction Under Local Differential Privacy(https://arxiv.org/abs/2505.15721)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.</li>
</ul>

<h3>Title: Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Luo, Yiyi Chen, Johannes Bjerva, Qiongxiu Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15722">https://arxiv.org/abs/2505.15722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15722">https://arxiv.org/pdf/2505.15722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15722]] Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities(https://arxiv.org/abs/2505.15722)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present the first comprehensive study of Memorization in Multilingual Large Language Models (MLLMs), analyzing 95 languages using models across diverse model scales, architectures, and memorization definitions. As MLLMs are increasingly deployed, understanding their memorization behavior has become critical. Yet prior work has focused primarily on monolingual models, leaving multilingual memorization underexplored, despite the inherently long-tailed nature of training corpora. We find that the prevailing assumption, that memorization is highly correlated with training data availability, fails to fully explain memorization patterns in MLLMs. We hypothesize that treating languages in isolation - ignoring their similarities - obscures the true patterns of memorization. To address this, we propose a novel graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization. Our analysis reveals that among similar languages, those with fewer training tokens tend to exhibit higher memorization, a trend that only emerges when cross-lingual relationships are explicitly modeled. These findings underscore the importance of a language-aware perspective in evaluating and mitigating memorization vulnerabilities in MLLMs. This also constitutes empirical evidence that language similarity both explains Memorization in MLLMs and underpins Cross-lingual Transferability, with broad implications for multilingual NLP.</li>
</ul>

<h3>Title: VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models</h3>
<ul>
<li><strong>Authors: </strong>Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15727">https://arxiv.org/abs/2505.15727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15727">https://arxiv.org/pdf/2505.15727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15727]] VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models(https://arxiv.org/abs/2505.15727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has accelerated the development of multi-modal models capable of vocal communication. Unlike text-based interactions, speech conveys rich and diverse information, including semantic content, acoustic variations, paralanguage cues, and environmental context. However, existing evaluations of speech interaction models predominantly focus on the quality of their textual responses, often overlooking critical aspects of vocal performance and lacking benchmarks with vocal-specific test instances. To address this gap, we propose VocalBench, a comprehensive benchmark designed to evaluate speech interaction models' capabilities in vocal communication. VocalBench comprises 9,400 carefully curated instances across four key dimensions: semantic quality, acoustic performance, conversational abilities, and robustness. It covers 16 fundamental skills essential for effective vocal interaction. Experimental results reveal significant variability in current model capabilities, each exhibiting distinct strengths and weaknesses, and provide valuable insights to guide future research in speech-based interaction systems. Code and evaluation instances are available at this https URL.</li>
</ul>

<h3>Title: DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15734">https://arxiv.org/abs/2505.15734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15734">https://arxiv.org/pdf/2505.15734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15734]] DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning(https://arxiv.org/abs/2505.15734)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.</li>
</ul>

<h3>Title: RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15737">https://arxiv.org/abs/2505.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15737">https://arxiv.org/pdf/2505.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15737]] RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction(https://arxiv.org/abs/2505.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics.</li>
</ul>

<h3>Title: Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15738">https://arxiv.org/abs/2505.15738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15738">https://arxiv.org/pdf/2505.15738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15738]] Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses(https://arxiv.org/abs/2505.15738)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are rapidly deployed in real-world applications ranging from chatbots to agentic systems. Alignment is one of the main approaches used to defend against attacks such as prompt injection and jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even against Greedy Coordinate Gradient (GCG), a white-box attack that generates adversarial suffixes to induce attacker-desired outputs. However, this search space over discrete tokens is extremely large, making the task of finding successful attacks difficult. GCG has, for instance, been shown to converge to local minima, making it sensitive to initialization choices. In this paper, we assess the future-proof robustness of these defenses using a more informed threat model: attackers who have access to some information about the alignment process. Specifically, we propose an informed white-box attack leveraging the intermediate model checkpoints to initialize GCG, with each checkpoint acting as a stepping stone for the next one. We show this approach to be highly effective across state-of-the-art (SOTA) defenses and models. We further show our informed initialization to outperform other initialization methods and show a gradient-informed checkpoint selection strategy to greatly improve attack performance and efficiency. Importantly, we also show our method to successfully find universal adversarial suffixes -- single suffixes effective across diverse inputs. Our results show that, contrary to previous beliefs, effective adversarial suffixes do exist against SOTA alignment-based defenses, that these can be found by existing attack methods when adversaries exploit alignment knowledge, and that even universal suffixes exist. Taken together, our results highlight the brittleness of current alignment-based methods and the need to consider stronger threat models when testing the safety of LLMs.</li>
</ul>

<h3>Title: Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Kanan Kiguchi, Yunhao Tu, Katsuhiro Ajito, Fady Alnajjar, Kazuyuki Murase</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15747">https://arxiv.org/abs/2505.15747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15747">https://arxiv.org/pdf/2505.15747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15747]] Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs(https://arxiv.org/abs/2505.15747)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.</li>
</ul>

<h3>Title: Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Taiye Chen, Zeming Wei, Ang Li, Yisen Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15753">https://arxiv.org/abs/2505.15753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15753">https://arxiv.org/pdf/2505.15753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15753]] Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval(https://arxiv.org/abs/2505.15753)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to be vulnerable to jailbreaking attacks, wherein adversaries exploit carefully engineered prompts to induce harmful or unethical responses. Such threats have raised critical concerns about the safety and reliability of LLMs in real-world deployment. While existing defense mechanisms partially mitigate such risks, subsequent advancements in adversarial techniques have enabled novel jailbreaking methods to circumvent these protections, exposing the limitations of static defense frameworks. In this work, we explore defending against evolving jailbreaking threats through the lens of context retrieval. First, we conduct a preliminary study demonstrating that even a minimal set of safety-aligned examples against a particular jailbreak can significantly enhance robustness against this attack pattern. Building on this insight, we further leverage the retrieval-augmented generation (RAG) techniques and propose Safety Context Retrieval (SCR), a scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our comprehensive experiments demonstrate how SCR achieves superior defensive performance against both established and emerging jailbreaking tactics, contributing a new paradigm to LLM safety. Our code will be available upon publication.</li>
</ul>

<h3>Title: Exploring The Visual Feature Space for Multimodal Neural Decoding</h3>
<ul>
<li><strong>Authors: </strong>Weihao Xia, Cengiz Oztireli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15755">https://arxiv.org/abs/2505.15755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15755">https://arxiv.org/pdf/2505.15755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15755]] Exploring The Visual Feature Space for Multimodal Neural Decoding(https://arxiv.org/abs/2505.15755)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The intrication of brain signals drives research that leverages multimodal AI to align brain modalities with visual and textual data for explainable descriptions. However, most existing studies are limited to coarse interpretations, lacking essential details on object descriptions, locations, attributes, and their relationships. This leads to imprecise and ambiguous reconstructions when using such cues for visual decoding. To address this, we analyze different choices of vision feature spaces from pre-trained visual components within Multimodal Large Language Models (MLLMs) and introduce a zero-shot multimodal brain decoding method that interacts with these models to decode across multiple levels of granularities. % To assess a model's ability to decode fine details from brain signals, we propose the Multi-Granularity Brain Detail Understanding Benchmark (MG-BrainDub). This benchmark includes two key tasks: detailed descriptions and salient question-answering, with metrics highlighting key visual elements like objects, attributes, and relationships. Our approach enhances neural decoding precision and supports more accurate neuro-decoding applications. Code will be available at this https URL.</li>
</ul>

<h3>Title: Constructing a 3D Town from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Kaizhi Zheng, Ruijian Zhang, Jing Gu, Jie Yang, Xin Eric Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15765">https://arxiv.org/abs/2505.15765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15765">https://arxiv.org/pdf/2505.15765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15765]] Constructing a 3D Town from a Single Image(https://arxiv.org/abs/2505.15765)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Acquiring detailed 3D scenes typically demands costly equipment, multi-view data, or labor-intensive modeling. Therefore, a lightweight alternative, generating complex 3D scenes from a single top-down image, plays an essential role in real-world applications. While recent 3D generative models have achieved remarkable results at the object level, their extension to full-scene generation often leads to inconsistent geometry, layout hallucinations, and low-quality meshes. In this work, we introduce 3DTown, a training-free framework designed to synthesize realistic and coherent 3D scenes from a single top-down view. Our method is grounded in two principles: region-based generation to improve image-to-3D alignment and resolution, and spatial-aware 3D inpainting to ensure global scene coherence and high-quality geometry generation. Specifically, we decompose the input image into overlapping regions and generate each using a pretrained 3D object generator, followed by a masked rectified flow inpainting process that fills in missing geometry while maintaining structural continuity. This modular design allows us to overcome resolution bottlenecks and preserve spatial structure without requiring 3D supervision or fine-tuning. Extensive experiments across diverse scenes show that 3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity. Our results demonstrate that high-quality 3D town generation is achievable from a single image using a principled, training-free approach.</li>
</ul>

<h3>Title: Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention</h3>
<ul>
<li><strong>Authors: </strong>Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15774">https://arxiv.org/abs/2505.15774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15774">https://arxiv.org/pdf/2505.15774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15774]] Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention(https://arxiv.org/abs/2505.15774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encounter significant challenges in long-sequence inference due to computational inefficiency and redundant processing, driving interest in context compression techniques. Existing methods often rely on token importance to perform hard local compression or encode context into latent representations for soft global compression. However, the uneven distribution of textual content relevance and the diversity of demands for user instructions mean these approaches frequently lead to the loss of potentially valuable information. To address this, we propose $\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for LLMs, which integrates both global and local perspectives to guide context compression while retaining both the essential semantics and critical details for task completion. Specifically, we employ a hybrid adapter to refine global semantics with the global view, based on the observation that different adapters excel at different tasks. Then we incorporate a classification layer that assigns a retention probability to each context token based on the local view, determining whether it should be retained or discarded. To foster a balanced integration of global and local compression, we introduce auxiliary paraphrasing and completion pretraining before instruction tuning. This promotes a synergistic integration that emphasizes instruction-relevant information while preserving essential local details, ultimately balancing local and global information retention in context compression. Experiments show that our HyCo$_2$ method significantly enhances long-text reasoning while reducing token usage. It improves the performance of various LLM series by an average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover, HyCo$_2$ matches the performance of uncompressed methods while reducing token consumption by 88.8\%.</li>
</ul>

<h3>Title: ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15776">https://arxiv.org/abs/2505.15776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15776">https://arxiv.org/pdf/2505.15776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15776]] ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning(https://arxiv.org/abs/2505.15776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision.</li>
</ul>

<h3>Title: dKV-Cache: The Cache for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyin Ma, Runpeng Yu, Gongfan Fang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15781">https://arxiv.org/abs/2505.15781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15781">https://arxiv.org/pdf/2505.15781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15781]] dKV-Cache: The Cache for Diffusion Language Models(https://arxiv.org/abs/2505.15781)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.</li>
</ul>

<h3>Title: Large Language Models as Computable Approximations to Solomonoff Induction</h3>
<ul>
<li><strong>Authors: </strong>Jun Wan, Lingrui Mei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15784">https://arxiv.org/abs/2505.15784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15784">https://arxiv.org/pdf/2505.15784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15784]] Large Language Models as Computable Approximations to Solomonoff Induction(https://arxiv.org/abs/2505.15784)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) calls for a rigorous theoretical framework to explain their empirical success. While significant progress has been made in understanding LLM behaviors, existing theoretical frameworks remain fragmented in explaining emergent phenomena through a unified mathematical lens. We establish the first formal connection between LLM architectures and Algorithmic Information Theory (AIT) by proving two fundamental results: (1) the training process computationally approximates Solomonoff prior through loss minimization interpreted as program length optimization, and (2) next-token prediction implements approximate Solomonoff induction. We leverage AIT to provide a unified theoretical explanation for in-context learning, few-shot learning, and scaling laws. Furthermore, our theoretical insights lead to a principled method for few-shot example selection that prioritizes samples where models exhibit lower predictive confidence. We demonstrate through experiments on diverse text classification benchmarks that this strategy yields significant performance improvements, particularly for smaller model architectures, when compared to selecting high-confidence examples. Our framework bridges the gap between theoretical foundations and practical LLM behaviors, providing both explanatory power and actionable insights for future model development.</li>
</ul>

<h3>Title: Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates</h3>
<ul>
<li><strong>Authors: </strong>Zahra Khatti, Daniel P. Robinson, Frank E. Curtis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15788">https://arxiv.org/abs/2505.15788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15788">https://arxiv.org/pdf/2505.15788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15788]] Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates(https://arxiv.org/abs/2505.15788)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>A new strategy for fair supervised machine learning is proposed. The main advantages of the proposed strategy as compared to others in the literature are as follows. (a) We introduce a new smooth nonconvex surrogate to approximate the Heaviside functions involved in discontinuous unfairness measures. The surrogate is based on smoothing methods from the optimization literature, and is new for the fair supervised learning literature. The surrogate is a tight approximation which ensures the trained prediction models are fair, as opposed to other (e.g., convex) surrogates that can fail to lead to a fair prediction model in practice. (b) Rather than rely on regularizers (that lead to optimization problems that are difficult to solve) and corresponding regularization parameters (that can be expensive to tune), we propose a strategy that employs hard constraints so that specific tolerances for unfairness can be enforced without the complications associated with the use of regularization. (c)~Our proposed strategy readily allows for constraints on multiple (potentially conflicting) unfairness measures at the same time. Multiple measures can be considered with a regularization approach, but at the cost of having even more difficult optimization problems to solve and further expense for tuning. By contrast, through hard constraints, our strategy leads to optimization models that can be solved tractably with minimal tuning.</li>
</ul>

<h3>Title: VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL</h3>
<ul>
<li><strong>Authors: </strong>Fengyuan Dai, Zifeng Zhuang, Yufei Huang, Siteng Huang, Bangyan Liao, Donglin Wang, Fajie Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15791">https://arxiv.org/abs/2505.15791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15791">https://arxiv.org/pdf/2505.15791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15791]] VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL(https://arxiv.org/abs/2505.15791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions.</li>
</ul>

<h3>Title: Long-Form Information Alignment Evaluation Beyond Atomic Facts</h3>
<ul>
<li><strong>Authors: </strong>Danna Zheng, Mirella Lapata, Jeff Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15792">https://arxiv.org/abs/2505.15792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15792">https://arxiv.org/pdf/2505.15792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15792]] Long-Form Information Alignment Evaluation Beyond Atomic Facts(https://arxiv.org/abs/2505.15792)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by "montaging" truthful statements without introducing explicit hallucinations. We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%. To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Reverse Engineering Human Preferences with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15795">https://arxiv.org/abs/2505.15795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15795">https://arxiv.org/pdf/2505.15795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15795]] Reverse Engineering Human Preferences with Reinforcement Learning(https://arxiv.org/abs/2505.15795)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.</li>
</ul>

<h3>Title: VoteMate: A Decentralized Application for Scalable Electronic Voting on EVM-Based Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Ivan Homoliak, Tomáš Švondr</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15797">https://arxiv.org/abs/2505.15797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15797">https://arxiv.org/pdf/2505.15797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15797]] VoteMate: A Decentralized Application for Scalable Electronic Voting on EVM-Based Blockchain(https://arxiv.org/abs/2505.15797)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Voting is a cornerstone of democracy, allowing citizens to express their will and make collective decisions. With advancing technology, online voting is gaining popularity as it enables voting from anywhere with Internet access, eliminating the need for printed ballots or polling stations. However, despite its benefits, online voting carries significant risks. A single vulnerability could be exploited to manipulate elections on a large scale. Centralized systems can be secure but may lack transparency and confidentiality, especially if those in power manipulate them. Blockchain-based voting offers a transparent, tamper-resistant alternative with end-to-end verifiability and strong security. Adding cryptographic layers can also ensure voter confidentiality.</li>
</ul>

<h3>Title: Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Taehoon Kim, Henry Gouk, Minyoung Kim, Timothy Hospedales</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15798">https://arxiv.org/abs/2505.15798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15798">https://arxiv.org/pdf/2505.15798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15798]] Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning(https://arxiv.org/abs/2505.15798)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Certifying the IID generalisation ability of deep networks is the first of many requirements for trusting AI in high-stakes applications from medicine to security. However, when instantiating generalisation bounds for deep networks it remains challenging to obtain non-vacuous guarantees, especially when applying contempo- rary large models on the small scale data prevalent in such high-stakes fields. In this paper, we draw a novel connection between a family of learning methods based on model fusion and generalisation certificates, and surprisingly show that with minor adjustment several existing learning strategies already provide non-trivial generali- sation guarantees. Essentially, by focusing on data-driven learning of downstream tasks by fusion rather than fine-tuning, the certified generalisation gap becomes tiny and independent of the base network size, facilitating its certification. Our results show for the first time non-trivial generalisation guarantees for learning with as low as 100 examples, while using vision models such as VIT-B and language models such as mistral-7B. This observation is significant as it has immediate implications for facilitating the certification of existing systems as trustworthy, and opens up new directions for research at the intersection of practice and theory.</li>
</ul>

<h3>Title: Interspatial Attention for Efficient 4D Human Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruizhi Shao, Yinghao Xu, Yujun Shen, Ceyuan Yang, Yang Zheng, Changan Chen, Yebin Liu, Gordon Wetzstein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15800">https://arxiv.org/abs/2505.15800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15800">https://arxiv.org/pdf/2505.15800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15800]] Interspatial Attention for Efficient 4D Human Video Generation(https://arxiv.org/abs/2505.15800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating photorealistic videos of digital humans in a controllable manner is crucial for a plethora of applications. Existing approaches either build on methods that employ template-based 3D representations or emerging video generation models but suffer from poor quality or limited consistency and identity preservation when generating individual or multiple digital humans. In this paper, we introduce a new interspatial attention (ISA) mechanism as a scalable building block for modern diffusion transformer (DiT)--based video generation models. ISA is a new type of cross attention that uses relative positional encodings tailored for the generation of human videos. Leveraging a custom-developed video variation autoencoder, we train a latent ISA-based diffusion model on a large corpus of video data. Our model achieves state-of-the-art performance for 4D human video synthesis, demonstrating remarkable motion consistency and identity preservation while providing precise control of the camera and body poses. Our code and model are publicly released at this https URL.</li>
</ul>

<h3>Title: VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15801">https://arxiv.org/abs/2505.15801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15801">https://arxiv.org/pdf/2505.15801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15801]] VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models(https://arxiv.org/abs/2505.15801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.</li>
</ul>

<h3>Title: STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15804">https://arxiv.org/abs/2505.15804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15804">https://arxiv.org/pdf/2505.15804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15804]] STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs(https://arxiv.org/abs/2505.15804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at this https URL.</li>
</ul>

<h3>Title: Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hwan Chang, Yumin Kim, Yonghyun Jun, Hwanhee Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15805">https://arxiv.org/abs/2505.15805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15805">https://arxiv.org/pdf/2505.15805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15805]] Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering(https://arxiv.org/abs/2505.15805)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security.</li>
</ul>

<h3>Title: The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15807">https://arxiv.org/abs/2505.15807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15807">https://arxiv.org/pdf/2505.15807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15807]] The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation(https://arxiv.org/abs/2505.15807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are able to exploit in-context learning to access external knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain unclear. In this work, we shed light on the mechanism of in-context retrieval augmentation for question answering by viewing a prompt as a composition of informational components. We propose an attribution-based method to identify specialized attention heads, revealing in-context heads that comprehend instructions and retrieve relevant contextual information, and parametric heads that store entities' relational knowledge. To better understand their roles, we extract function vectors and modify their attention weights to show how they can influence the answer generation process. Finally, we leverage the gained insights to trace the sources of knowledge used during inference, paving the way towards more safe and transparent language models.</li>
</ul>

<h3>Title: Neural Conditional Transport Maps</h3>
<ul>
<li><strong>Authors: </strong>Carlos Rodriguez-Pardo, Leonardo Chiani, Emanuele Borgonovo, Massimo Tavoni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.PR, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15808">https://arxiv.org/abs/2505.15808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15808">https://arxiv.org/pdf/2505.15808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15808]] Neural Conditional Transport Maps(https://arxiv.org/abs/2505.15808)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>We present a neural framework for learning conditional optimal transport (OT) maps between probability distributions. Our approach introduces a conditioning mechanism capable of processing both categorical and continuous conditioning variables simultaneously. At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods. Comprehensive ablation studies demonstrate the superior performance of our method over baseline configurations. Furthermore, we showcase an application to global sensitivity analysis, offering high performance in computing OT-based sensitivity indices. This work advances the state-of-the-art in conditional optimal transport, enabling broader application of optimal transport principles to complex, high-dimensional domains such as generative modeling and black-box model explainability.</li>
</ul>

<h3>Title: MMaDA: Multimodal Large Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15809">https://arxiv.org/abs/2505.15809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15809">https://arxiv.org/pdf/2505.15809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15809]] MMaDA: Multimodal Large Diffusion Language Models(https://arxiv.org/abs/2505.15809)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: this https URL</li>
</ul>

<h3>Title: Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization</h3>
<ul>
<li><strong>Authors: </strong>Satoshi Kosugi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15812">https://arxiv.org/abs/2505.15812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15812">https://arxiv.org/pdf/2505.15812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15812]] Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization(https://arxiv.org/abs/2505.15812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Exemplar-based image colorization aims to colorize a grayscale image using a reference color image, ensuring that reference colors are applied to corresponding input regions based on their semantic similarity. To achieve accurate semantic matching between regions, we leverage the self-attention module of a pre-trained diffusion model, which is trained on a large dataset and exhibits powerful attention capabilities. To harness this power, we propose a novel, fine-tuning-free approach based on a pre-trained diffusion model, making two key contributions. First, we introduce dual attention-guided color transfer. We utilize the self-attention module to compute an attention map between the input and reference images, effectively capturing semantic correspondences. The color features from the reference image is then transferred to the semantically matching regions of the input image, guided by this attention map, and finally, the grayscale features are replaced with the corresponding color features. Notably, we utilize dual attention to calculate attention maps separately for the grayscale and color images, achieving more precise semantic alignment. Second, we propose classifier-free colorization guidance, which enhances the transferred colors by combining color-transferred and non-color-transferred outputs. This process improves the quality of colorization. Our experimental results demonstrate that our method outperforms existing techniques in terms of image quality and fidelity to the reference. Specifically, we use 335 input-reference pairs from previous research, achieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to the reference). Our source code is available at this https URL.</li>
</ul>

<h3>Title: Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex</h3>
<ul>
<li><strong>Authors: </strong>Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Andrew F. Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15813">https://arxiv.org/abs/2505.15813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15813">https://arxiv.org/pdf/2505.15813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15813]] Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex(https://arxiv.org/abs/2505.15813)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity.</li>
</ul>

<h3>Title: InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yijie Zheng, Weijie Wu, Qingyun Li, Xuehui Wang, Xu Zhou, Aiai Ren, Jun Shen, Long Zhao, Guoqing Li, Xue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.15818">https://arxiv.org/abs/2505.15818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.15818">https://arxiv.org/pdf/2505.15818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.15818]] InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition(https://arxiv.org/abs/2505.15818)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Language-Guided object recognition in remote sensing imagery is crucial for large-scale mapping and automated data annotation. However, existing open-vocabulary and visual grounding methods rely on explicit category cues, limiting their ability to handle complex or implicit queries that require advanced reasoning. To address this issue, we introduce a new suite of tasks, including Instruction-Oriented Object Counting, Detection, and Segmentation (InstructCDS), covering open-vocabulary, open-ended, and open-subclass scenarios. We further present EarthInstruct, the first InstructCDS benchmark for earth observation. It is constructed from two diverse remote sensing datasets with varying spatial resolutions and annotation rules across 20 categories, necessitating models to interpret dataset-specific instructions. Given the scarcity of semantically rich labeled data in remote sensing, we propose InstructSAM, a training-free framework for instruction-driven object recognition. InstructSAM leverages large vision-language models to interpret user instructions and estimate object counts, employs SAM2 for mask proposal, and formulates mask-label assignment as a binary integer programming problem. By integrating semantic similarity with counting constraints, InstructSAM efficiently assigns categories to predicted masks without relying on confidence thresholds. Experiments demonstrate that InstructSAM matches or surpasses specialized baselines across multiple tasks while maintaining near-constant inference time regardless of object count, reducing output tokens by 89% and overall runtime by over 32% compared to direct generation approaches. We believe the contributions of the proposed tasks, benchmark, and effective approach will advance future research in developing versatile object recognition systems.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
