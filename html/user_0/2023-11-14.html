<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization. (arXiv:2311.06361v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06361">http://arxiv.org/abs/2311.06361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06361]] CALLOC: Curriculum Adversarial Learning for Secure and Robust Indoor Localization(http://arxiv.org/abs/2311.06361)</code></li>
<li>Summary: <p>Indoor localization has become increasingly vital for many applications from
tracking assets to delivering personalized services. Yet, achieving pinpoint
accuracy remains a challenge due to variations across indoor environments and
devices used to assist with localization. Another emerging challenge is
adversarial attacks on indoor localization systems that not only threaten
service integrity but also reduce localization accuracy. To combat these
challenges, we introduce CALLOC, a novel framework designed to resist
adversarial attacks and variations across indoor environments and devices that
reduce system accuracy and reliability. CALLOC employs a novel adaptive
curriculum learning approach with a domain specific lightweight scaled-dot
product attention neural network, tailored for adversarial and variation
resilience in practical use cases with resource constrained mobile devices.
Experimental evaluations demonstrate that CALLOC can achieve improvements of up
to 6.03x in mean error and 4.6x in worst-case error against state-of-the-art
indoor localization frameworks, across diverse building floorplans, mobile
devices, and adversarial attacks scenarios.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Electronic Communication Data Link Encryption Simulation Based on Wireless Communication. (arXiv:2311.06462v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06462">http://arxiv.org/abs/2311.06462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06462]] Electronic Communication Data Link Encryption Simulation Based on Wireless Communication(http://arxiv.org/abs/2311.06462)</code></li>
<li>Summary: <p>In order to improve the simulation effect of electronic communication data
link encryption, the author proposes a solution based on wireless
communication. The main content of this technology is based on the research of
wireless communication, improve the elliptic curve cryptographic algorithm to
build a system encryption model, obtain legal and valid node private keys,
evaluate and analyze the relevant security attributes of the system, verify the
security of the keys, and realize the encryption optimization of wireless
network communication. Experimental results show that: Using the improved
elliptic curve to simulate the system data chain encryption under the
certificateless public key cryptosystem in network communication, the time is
only 2.31 milliseconds, which is lower than other algorithms. Conclusion: It is
proved that the technology research based on wireless communication can
effectively improve the encryption simulation effect of electronic
communication data link.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images. (arXiv:2311.06643v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06643">http://arxiv.org/abs/2311.06643</a></li>
<li>Code URL: https://github.com/mlsysx/medpfl</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06643]] Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images(http://arxiv.org/abs/2311.06643)</code></li>
<li>Summary: <p>Federated learning (FL) is gaining increasing popularity in the medical
domain for analyzing medical images, which is considered an effective technique
to safeguard sensitive patient data and comply with privacy regulations.
However, several recent studies have revealed that the default settings of FL
may leak private training data under privacy attacks. Thus, it is still unclear
whether and to what extent such privacy risks of FL exist in the medical
domain, and if so, ``how to mitigate such risks?''. In this paper, first, we
propose a holistic framework for Medical data Privacy risk analysis and
mitigation in Federated Learning (MedPFL) to analyze privacy risks and develop
effective mitigation strategies in FL for protecting private medical data.
Second, we demonstrate the substantial privacy risks of using FL to process
medical images, where adversaries can easily perform privacy attacks to
reconstruct private medical images accurately. Third, we show that the defense
approach of adding random noises may not always work effectively to protect
medical images against privacy attacks in FL, which poses unique and pressing
challenges associated with medical data for privacy protection.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Flatness-aware Adversarial Attack. (arXiv:2311.06423v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06423">http://arxiv.org/abs/2311.06423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06423]] Flatness-aware Adversarial Attack(http://arxiv.org/abs/2311.06423)</code></li>
<li>Summary: <p>The transferability of adversarial examples can be exploited to launch
black-box attacks. However, adversarial examples often present poor
transferability. To alleviate this issue, by observing that the diversity of
inputs can boost transferability, input regularization based methods are
proposed, which craft adversarial examples by combining several transformed
inputs. We reveal that input regularization based methods make resultant
adversarial examples biased towards flat extreme regions. Inspired by this, we
propose an attack called flatness-aware adversarial attack (FAA) which
explicitly adds a flatness-aware regularization term in the optimization target
to promote the resultant adversarial examples towards flat extreme regions. The
flatness-aware regularization term involves gradients of samples around the
resultant adversarial examples but optimizing gradients requires the evaluation
of Hessian matrix in high-dimension spaces which generally is intractable. To
address the problem, we derive an approximate solution to circumvent the
construction of Hessian matrix, thereby making FAA practical and cheap.
Extensive experiments show the transferability of adversarial examples crafted
by FAA can be considerably boosted compared with state-of-the-art baselines.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images. (arXiv:2311.06400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06400">http://arxiv.org/abs/2311.06400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06400]] EviPrompt: A Training-Free Evidential Prompt Generation Method for Segment Anything Model in Medical Images(http://arxiv.org/abs/2311.06400)</code></li>
<li>Summary: <p>Medical image segmentation has immense clinical applicability but remains a
challenge despite advancements in deep learning. The Segment Anything Model
(SAM) exhibits potential in this field, yet the requirement for expertise
intervention and the domain gap between natural and medical images poses
significant obstacles. This paper introduces a novel training-free evidential
prompt generation method named EviPrompt to overcome these issues. The proposed
method, built on the inherent similarities within medical images, requires only
a single reference image-annotation pair, making it a training-free solution
that significantly reduces the need for extensive labeling and computational
resources. First, to automatically generate prompts for SAM in medical images,
we introduce an evidential method based on uncertainty estimation without the
interaction of clinical experts. Then, we incorporate the human prior into the
prompts, which is vital for alleviating the domain gap between natural and
medical images and enhancing the applicability and usefulness of SAM in medical
scenarios. EviPrompt represents an efficient and robust approach to medical
image segmentation, with evaluations across a broad range of tasks and
modalities confirming its efficacy.
</p></li>
</ul>

<h3>Title: Aria-NeRF: Multimodal Egocentric View Synthesis. (arXiv:2311.06455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06455">http://arxiv.org/abs/2311.06455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06455]] Aria-NeRF: Multimodal Egocentric View Synthesis(http://arxiv.org/abs/2311.06455)</code></li>
<li>Summary: <p>We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.
</p></li>
</ul>

<h3>Title: CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset. (arXiv:2311.06505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06505">http://arxiv.org/abs/2311.06505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06505]] CompCodeVet: A Compiler-guided Validation and Enhancement Approach for Code Dataset(http://arxiv.org/abs/2311.06505)</code></li>
<li>Summary: <p>Large language models (LLMs) have become increasingly prominent in academia
and industry due to their remarkable performance in diverse applications. As
these models evolve with increasing parameters, they excel in tasks like
sentiment analysis and machine translation. However, even models with billions
of parameters face challenges in tasks demanding multi-step reasoning. Code
generation and comprehension, especially in C and C++, emerge as significant
challenges. While LLMs trained on code datasets demonstrate competence in many
tasks, they struggle with rectifying non-compilable C and C++ code. Our
investigation attributes this subpar performance to two primary factors: the
quality of the training dataset and the inherent complexity of the problem
which demands intricate reasoning. Existing "Chain of Thought" (CoT) prompting
techniques aim to enhance multi-step reasoning. This approach, however, retains
the limitations associated with the latent drawbacks of LLMs. In this work, we
propose CompCodeVet, a compiler-guided CoT approach to produce compilable code
from non-compilable ones. Diverging from the conventional approach of utilizing
larger LLMs, we employ compilers as a teacher to establish a more robust
zero-shot thought process. The evaluation of CompCodeVet on two open-source
code datasets shows that CompCodeVet has the ability to improve the training
dataset quality for LLMs.
</p></li>
</ul>

<h3>Title: Convolve and Conquer: Data Comparison with Wiener Filters. (arXiv:2311.06558v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06558">http://arxiv.org/abs/2311.06558</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06558]] Convolve and Conquer: Data Comparison with Wiener Filters(http://arxiv.org/abs/2311.06558)</code></li>
<li>Summary: <p>Quantitative evaluations of differences and/or similarities between data
samples define and shape optimisation problems associated with learning data
distributions. Current methods to compare data often suffer from limitations in
capturing such distributions or lack desirable mathematical properties for
optimisation (e.g. smoothness, differentiability, or convexity). In this paper,
we introduce a new method to measure (dis)similarities between paired samples
inspired by Wiener-filter theory. The convolutional nature of Wiener filters
allows us to comprehensively compare data samples in a globally correlated way.
We validate our approach in four machine learning applications: data
compression, medical imaging imputation, translated classification, and
non-parametric generative modelling. Our results demonstrate increased
resolution in reconstructed images with better perceptual quality and higher
data fidelity, as well as robustness against translations, compared to
conventional mean-squared-error analogue implementations.
</p></li>
</ul>

<h3>Title: Understanding Grokking Through A Robustness Viewpoint. (arXiv:2311.06597v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06597">http://arxiv.org/abs/2311.06597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06597]] Understanding Grokking Through A Robustness Viewpoint(http://arxiv.org/abs/2311.06597)</code></li>
<li>Summary: <p>Recently, an unusual phenomenon called grokking has gained much attention,
where sometimes a neural network generalizes long after it perfectly fits the
training data. We try to understand this seemingly strange phenomenon using the
robustness of the neural network. Using a robustness viewpoint, we show that
the popular $l_2$ weight norm (metric) of the neural network is actually a
sufficient condition for grokking. As we also empirically find that $l_2$ norm
correlates with grokking on the test data not in a timely way, we propose new
metrics based on robustness and information theory and find that our new
metrics correlate well with the grokking phenomenon. Based on the previous
observations, we propose methods to speed up the generalization process. In
addition, we examine the standard training process on modulo addition dataset
and find that it hardly learns other basic group operations before grokking,
including the commutative law. Interestingly, the speed up of generalization
when using our proposed method can be partially explained by learning the
commutative law, a necessary condition when the model groks on test dataset.
</p></li>
</ul>

<h3>Title: Streamlining Energy Transition Scenarios to Key Policy Decisions. (arXiv:2311.06625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06625">http://arxiv.org/abs/2311.06625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06625]] Streamlining Energy Transition Scenarios to Key Policy Decisions(http://arxiv.org/abs/2311.06625)</code></li>
<li>Summary: <p>Uncertainties surrounding the energy transition often lead modelers to
present large sets of scenarios that are challenging for policymakers to
interpret and act upon. An alternative approach is to define a few qualitative
storylines from stakeholder discussions, which can be affected by biases and
infeasibilities. Leveraging decision trees, a popular machine-learning
technique, we derive interpretable storylines from many quantitative scenarios
and show how the key decisions in the energy transition are interlinked.
Specifically, our results demonstrate that choosing a high deployment of
renewables and sector coupling makes global decarbonization scenarios robust
against uncertainties in climate sensitivity and demand. Also, the energy
transition to a fossil-free Europe is primarily determined by choices on the
roles of bioenergy, storage, and heat electrification. Our transferrable
approach translates vast energy model results into a small set of critical
decisions, guiding decision-makers in prioritizing the key factors that will
shape the energy transition.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach. (arXiv:2311.06364v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06364">http://arxiv.org/abs/2311.06364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06364]] Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach(http://arxiv.org/abs/2311.06364)</code></li>
<li>Summary: <p>The sparsity of labelled data is an obstacle to the development of Relation
Extraction models and the completion of databases in various biomedical areas.
While being of high interest in drug-discovery, the natural-products
literature, reporting the identification of potential bioactive compounds from
organisms, is a concrete example of such an overlooked topic. To mark the start
of this new task, we created the first curated evaluation dataset and extracted
literature items from the LOTUS database to build training sets. To this end,
we developed a new sampler inspired by diversity metrics in ecology, named
Greedy Maximum Entropy sampler, or GME-sampler
(https://github.com/idiap/gme-sampler). The strategic optimization of both
balance and diversity of the selected items in the evaluation set is important
given the resource-intensive nature of manual curation. After quantifying the
noise in the training set, in the form of discrepancies between the input
abstracts text and the expected output labels, we explored different strategies
accordingly. Framing the task as an end-to-end Relation Extraction, we
evaluated the performance of standard fine-tuning as a generative task and
few-shot learning with open Large Language Models (LLaMA 7B-65B). In addition
to their evaluation in few-shot settings, we explore the potential of open
Large Language Models (Vicuna-13B) as synthetic data generator and propose a
new workflow for this purpose. All evaluated models exhibited substantial
improvements when fine-tuned on synthetic abstracts rather than the original
noisy data. We provide our best performing (f1-score=59.0) BioGPT-Large model
for end-to-end RE of natural-products relationships along with all the
generated synthetic data and the evaluation dataset. See more details at
https://github.com/idiap/abroad-re.
</p></li>
</ul>

<h3>Title: Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction. (arXiv:2311.06555v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06555">http://arxiv.org/abs/2311.06555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06555]] Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction(http://arxiv.org/abs/2311.06555)</code></li>
<li>Summary: <p>In this study, we investigate in-context learning (ICL) in document-level
event argument extraction (EAE). The paper identifies key challenges in this
problem, including example selection, context length limitation, abundance of
event types, and the limitation of Chain-of-Thought (CoT) prompting in
non-reasoning tasks. To address these challenges, we introduce the
Heuristic-Driven Link-of-Analogy (HD-LoA) prompting method. Specifically, we
hypothesize and validate that LLMs learn task-specific heuristics from
demonstrations via ICL. Building upon this hypothesis, we introduce an explicit
heuristic-driven demonstration construction approach, which transforms the
haphazard example selection process into a methodical method that emphasizes
task heuristics. Additionally, inspired by the analogical reasoning of human,
we propose the link-of-analogy prompting, which enables LLMs to process new
situations by drawing analogies to known situations, enhancing their
adaptability. Extensive experiments show that our method outperforms the
existing prompting methods and few-shot supervised learning methods, exhibiting
F1 score improvements of 4.53% and 9.38% on the document-level EAE dataset.
Furthermore, when applied to sentiment analysis and natural language inference
tasks, the HD-LoA prompting achieves accuracy gains of 2.87% and 2.63%,
indicating its effectiveness across different tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Blockchain-Enabled Federated Learning Approach for Vehicular Networks. (arXiv:2311.06372v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06372">http://arxiv.org/abs/2311.06372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06372]] Blockchain-Enabled Federated Learning Approach for Vehicular Networks(http://arxiv.org/abs/2311.06372)</code></li>
<li>Summary: <p>Data from interconnected vehicles may contain sensitive information such as
location, driving behavior, personal identifiers, etc. Without adequate
safeguards, sharing this data jeopardizes data privacy and system security. The
current centralized data-sharing paradigm in these systems raises particular
concerns about data privacy. Recognizing these challenges, the shift towards
decentralized interactions in technology, as echoed by the principles of
Industry 5.0, becomes paramount. This work is closely aligned with these
principles, emphasizing decentralized, human-centric, and secure technological
interactions in an interconnected vehicular ecosystem. To embody this, we
propose a practical approach that merges two emerging technologies: Federated
Learning (FL) and Blockchain. The integration of these technologies enables the
creation of a decentralized vehicular network. In this setting, vehicles can
learn from each other without compromising privacy while also ensuring data
integrity and accountability. Initial experiments show that compared to
conventional decentralized federated learning techniques, our proposed approach
significantly enhances the performance and security of vehicular networks. The
system's accuracy stands at 91.92\%. While this may appear to be low in
comparison to state-of-the-art federated learning models, our work is
noteworthy because, unlike others, it was achieved in a malicious vehicle
setting. Despite the challenging environment, our method maintains high
accuracy, making it a competent solution for preserving data privacy in
vehicular networks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Adaptive Language-based Mental Health Assessment with Item-Response Theory. (arXiv:2311.06467v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06467">http://arxiv.org/abs/2311.06467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06467]] Adaptive Language-based Mental Health Assessment with Item-Response Theory(http://arxiv.org/abs/2311.06467)</code></li>
<li>Summary: <p>Mental health issues widely vary across individuals - the manifestations of
signs and symptoms can be fairly heterogeneous. Recently, language-based
depression and anxiety assessments have shown promise for capturing this
heterogeneous nature by evaluating a patient's own language, but such
approaches require a large sample of words per person to be accurate. In this
work, we introduce adaptive language-based assessment - the task of iteratively
estimating an individual's psychological score based on limited language
responses to questions that the model also decides to ask. To this end, we
explore two statistical learning-based approaches for measurement/scoring:
classical test theory (CTT) and item response theory (IRT). We find that using
adaptive testing in general can significantly reduce the number of questions
required to achieve high validity (r ~ 0.7) with standardized tests, bringing
down from 11 total questions down to 3 for depression and 5 for anxiety. Given
the combinatorial nature of the problem, we empirically evaluate multiple
strategies for both the ordering and scoring objectives, introducing two new
methods: a semi-supervised item response theory based method (ALIRT), and a
supervised actor-critic based model. While both of the models achieve
significant improvements over random and fixed orderings, we find ALIRT to be a
scalable model that achieves the highest accuracy with lower numbers of
questions (e.g. achieves Pearson r ~ 0.93 after only 3 questions versus asking
all 11 questions). Overall, ALIRT allows prompting a reduced number of
questions without compromising accuracy or overhead computational costs.
</p></li>
</ul>

<h3>Title: Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06513">http://arxiv.org/abs/2311.06513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06513]] Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems(http://arxiv.org/abs/2311.06513)</code></li>
<li>Summary: <p>Recent works have shown considerable improvements in task-oriented dialogue
(TOD) systems by utilizing pretrained large language models (LLMs) in an
end-to-end manner. However, the biased behavior of each component in a TOD
system and the error propagation issue in the end-to-end framework can lead to
seriously biased TOD responses. Existing works of fairness only focus on the
total bias of a system. In this paper, we propose a diagnosis method to
attribute bias to each component of a TOD system. With the proposed attribution
method, we can gain a deeper understanding of the sources of bias.
Additionally, researchers can mitigate biased model behavior at a more granular
level. We conduct experiments to attribute the TOD system's bias toward three
demographic axes: gender, age, and race. Experimental results show that the
bias of a TOD system usually comes from the response generation model.
</p></li>
</ul>

<h3>Title: BizBench: A Quantitative Reasoning Benchmark for Business and Finance. (arXiv:2311.06602v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06602">http://arxiv.org/abs/2311.06602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06602]] BizBench: A Quantitative Reasoning Benchmark for Business and Finance(http://arxiv.org/abs/2311.06602)</code></li>
<li>Summary: <p>As large language models (LLMs) impact a growing number of complex domains,
it is becoming increasingly important to have fair, accurate, and rigorous
evaluation benchmarks. Evaluating the reasoning skills required for business
and financial NLP stands out as a particularly difficult challenge. We
introduce BizBench, a new benchmark for evaluating models' ability to reason
about realistic financial problems. BizBench comprises 8 quantitative reasoning
tasks. Notably, BizBench targets the complex task of question-answering (QA)
for structured and unstructured financial data via program synthesis (i.e.,
code generation). We introduce three diverse financially-themed code-generation
tasks from newly collected and augmented QA data. Additionally, we isolate
distinct financial reasoning capabilities required to solve these QA tasks:
reading comprehension of financial text and tables, which is required to
extract correct intermediate values; and understanding domain knowledge (e.g.,
financial formulas) needed to calculate complex solutions. Collectively, these
tasks evaluate a model's financial background knowledge, ability to extract
numeric entities from financial documents, and capacity to solve problems with
code. We conduct an in-depth evaluation of open-source and commercial LLMs,
illustrating that BizBench is a challenging benchmark for quantitative
reasoning in the finance and business domain.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding. (arXiv:2311.06456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06456">http://arxiv.org/abs/2311.06456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06456]] Asymmetric Contrastive Multimodal Learning for Advancing Chemical Understanding(http://arxiv.org/abs/2311.06456)</code></li>
<li>Summary: <p>The versatility of multimodal deep learning holds tremendous promise for
advancing scientific research and practical applications. As this field
continues to evolve, the collective power of cross-modal analysis promises to
drive transformative innovations, leading us to new frontiers in chemical
understanding and discovery. Hence, we introduce Asymmetric Contrastive
M}ultimodal Learning (ACML) as a novel approach tailored for molecules,
showcasing its potential to advance the field of chemistry. ACML harnesses the
power of effective asymmetric contrastive learning to seamlessly transfer
information from various chemical modalities to molecular graph
representations. By combining pre-trained chemical unimodal encoders and a
shallow-designed graph encoder, ACML facilitates the assimilation of
coordinated chemical semantics from different modalities, leading to
comprehensive representation learning with efficient training. This innovative
framework enhances the interpretability of learned representations and bolsters
the expressive power of graph neural networks. Through practical tasks such as
isomer discrimination and uncovering crucial chemical properties for drug
discovery, ACML exhibits its capability to revolutionize chemical research and
applications, providing a deeper understanding of chemical semantics of
different modalities.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models. (arXiv:2311.06322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06322">http://arxiv.org/abs/2311.06322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06322]] Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models(http://arxiv.org/abs/2311.06322)</code></li>
<li>Summary: <p>Diffusion models have achieved great success due to their remarkable
generation ability. However, their high computational overhead is still a
troublesome problem. Recent studies have leveraged post-training quantization
(PTQ) to compress diffusion models. However, most of them only focus on
unconditional models, leaving the quantization of widely used large pretrained
text-to-image models, e.g., Stable Diffusion, largely unexplored. In this
paper, we propose a novel post-training quantization method PCR (Progressive
Calibration and Relaxing) for text-to-image diffusion models, which consists of
a progressive calibration strategy that considers the accumulated quantization
error across timesteps, and an activation relaxing strategy that improves the
performance with negligible cost. Additionally, we demonstrate the previous
metrics for text-to-image diffusion model quantization are not accurate due to
the distribution gap. To tackle the problem, we propose a novel QDiffBench
benchmark, which utilizes data in the same domain for more accurate evaluation.
Besides, QDiffBench also considers the generalization performance of the
quantized model outside the calibration dataset. Extensive experiments on
Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our
method and benchmark. Moreover, we are the first to achieve quantization for
Stable Diffusion XL while maintaining the performance.
</p></li>
</ul>

<h3>Title: ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints. (arXiv:2311.06315v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06315">http://arxiv.org/abs/2311.06315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06315]] ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints(http://arxiv.org/abs/2311.06315)</code></li>
<li>Summary: <p>Ship design is a years-long process that requires balancing complex design
trade-offs to create a ship that is efficient and effective. Finding new ways
to improve the ship design process can lead to significant cost savings for
ship building and operation. One promising technology is generative artificial
intelligence, which has been shown to reduce design cycle time and create
novel, high-performing designs. In literature review, generative artificial
intelligence has been shown to generate ship hulls; however, ship design is
particularly difficult as the hull of a ship requires the consideration of many
objectives. This paper presents a study on the generation of parametric ship
hull designs using a parametric diffusion model that considers multiple
objectives and constraints for the hulls. This denoising diffusion
probabilistic model (DDPM) generates the tabular parametric design vectors of a
ship hull for evaluation. In addition to a tabular DDPM, this paper details
adding guidance to improve the quality of generated ship hull designs. By
leveraging classifier guidance, the DDPM produced feasible parametric ship
hulls that maintain the coverage of the initial training dataset of ship hulls
with a 99.5% rate, a 149x improvement over random sampling of the design vector
parameters across the design space. Parametric ship hulls produced with
performance guidance saw an average of 91.4% reduction in wave drag
coefficients and an average of a 47.9x relative increase in the total displaced
volume of the hulls compared to the mean performance of the hulls in the
training dataset. The use of a DDPM to generate parametric ship hulls can
reduce design time by generating high-performing hull designs for future
analysis. These generated hulls have low drag and high volume, which can reduce
the cost of operating a ship and increase its potential to generate revenue.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Towards A Unified Neural Architecture for Visual Recognition and Reasoning. (arXiv:2311.06386v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06386">http://arxiv.org/abs/2311.06386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06386]] Towards A Unified Neural Architecture for Visual Recognition and Reasoning(http://arxiv.org/abs/2311.06386)</code></li>
<li>Summary: <p>Recognition and reasoning are two pillars of visual understanding. However,
these tasks have an imbalance in focus; whereas recent advances in neural
networks have shown strong empirical performance in visual recognition, there
has been comparably much less success in solving visual reasoning. Intuitively,
unifying these two tasks under a singular framework is desirable, as they are
mutually dependent and beneficial. Motivated by the recent success of
multi-task transformers for visual recognition and language understanding, we
propose a unified neural architecture for visual recognition and reasoning with
a generic interface (e.g., tokens) for both. Our framework enables the
principled investigation of how different visual recognition tasks, datasets,
and inductive biases can help enable spatiotemporal reasoning capabilities.
Noticeably, we find that object detection, which requires spatial localization
of individual objects, is the most beneficial recognition task for reasoning.
We further demonstrate via probing that implicit object-centric representations
emerge automatically inside our framework. Intriguingly, we discover that
certain architectural choices such as the backbone model of the visual encoder
have a significant impact on visual reasoning, but little on object detection.
Given the results of our experiments, we believe that visual reasoning should
be considered as a first-class citizen alongside visual recognition, as they
are strongly correlated but benefit from potentially different design choices.
</p></li>
</ul>

<h3>Title: CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer. (arXiv:2311.06443v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06443">http://arxiv.org/abs/2311.06443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06443]] CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer(http://arxiv.org/abs/2311.06443)</code></li>
<li>Summary: <p>Reconstructing personalized animatable head avatars has significant
implications in the fields of AR/VR. Existing methods for achieving explicit
face control of 3D Morphable Models (3DMM) typically rely on multi-view images
or videos of a single subject, making the reconstruction process complex.
Additionally, the traditional rendering pipeline is time-consuming, limiting
real-time animation possibilities. In this paper, we introduce CVTHead, a novel
approach that generates controllable neural head avatars from a single
reference image using point-based neural rendering. CVTHead considers the
sparse vertices of mesh as the point set and employs the proposed
Vertex-feature Transformer to learn local feature descriptors for each vertex.
This enables the modeling of long-range dependencies among all the vertices.
Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves
comparable performance to state-of-the-art graphics-based methods. Moreover, it
enables efficient rendering of novel human heads with various expressions, head
poses, and camera views. These attributes can be explicitly controlled using
the coefficients of 3DMMs, facilitating versatile and realistic animation in
real-time scenarios.
</p></li>
</ul>

<h3>Title: DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding. (arXiv:2311.06497v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06497">http://arxiv.org/abs/2311.06497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06497]] DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding(http://arxiv.org/abs/2311.06497)</code></li>
<li>Summary: <p>Traffic accidents frequently lead to fatal injuries, contributing to over 50
million deaths until 2023. To mitigate driving hazards and ensure personal
safety, it is crucial to assist vehicles in anticipating important objects
during travel. Previous research on important object detection primarily
assessed the importance of individual participants, treating them as
independent entities and frequently overlooking the connections between these
participants. Unfortunately, this approach has proven less effective in
detecting important objects in complex scenarios. In response, we introduce
Driving scene Relationship self-Understanding transformer (DRUformer), designed
to enhance the important object detection task. The DRUformer is a
transformer-based multi-modal important object detection model that takes into
account the relationships between all the participants in the driving scenario.
Recognizing that driving intention also significantly affects the detection of
important objects during driving, we have incorporated a module for embedding
driving intention. To assess the performance of our approach, we conducted a
comparative experiment on the DRAMA dataset, pitting our model against other
state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\%
improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA
methods. Furthermore, we conducted a qualitative analysis of our model's
ability to detect important objects across different road scenarios and
classes, highlighting its effectiveness in diverse contexts. Finally, we
conducted various ablation studies to assess the efficiency of the proposed
modules in our DRUformer model.
</p></li>
</ul>

<h3>Title: VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems. (arXiv:2311.06623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06623">http://arxiv.org/abs/2311.06623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06623]] VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems(http://arxiv.org/abs/2311.06623)</code></li>
<li>Summary: <p>Enhancing roadway safety and traffic management has become an essential focus
area for a broad range of modern cyber-physical systems and intelligent
transportation systems. Vehicle Trajectory Prediction is a pivotal element
within numerous applications for highway and road safety. These applications
encompass a wide range of use cases, spanning from traffic management and
accident prevention to enhancing work-zone safety and optimizing energy
conservation. The ability to implement intelligent management in this context
has been greatly advanced by the developments in the field of Artificial
Intelligence (AI), alongside the increasing deployment of surveillance cameras
across road networks. In this paper, we introduce a novel transformer-based
approach for vehicle trajectory prediction for highway safety and surveillance,
denoted as VT-Former. In addition to utilizing transformers to capture
long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module
has been proposed to capture intricate social interactions among vehicles.
Combining these two core components culminates in a precise approach for
vehicle trajectory prediction. Our study on three benchmark datasets with three
different viewpoints demonstrates the State-of-The-Art (SoTA) performance of
VT-Former in vehicle trajectory prediction and its generalizability and
robustness. We also evaluate VT-Former's efficiency on embedded boards and
explore its potential for vehicle anomaly detection as a sample application,
showcasing its broad applicability.
</p></li>
</ul>

<h3>Title: Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs. (arXiv:2311.06401v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06401">http://arxiv.org/abs/2311.06401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06401]] Autoregressive Language Models For Estimating the Entropy of Epic EHR Audit Logs(http://arxiv.org/abs/2311.06401)</code></li>
<li>Summary: <p>EHR audit logs are a highly granular stream of events that capture clinician
activities, and is a significant area of interest for research in
characterizing clinician workflow on the electronic health record (EHR).
Existing techniques to measure the complexity of workflow through EHR audit
logs (audit logs) involve time- or frequency-based cross-sectional aggregations
that are unable to capture the full complexity of a EHR session. We briefly
evaluate the usage of transformer-based tabular language model (tabular LM) in
measuring the entropy or disorderedness of action sequences within workflow and
release the evaluated models publicly.
</p></li>
</ul>

<h2>generative</h2>
<h2>large language model</h2>
<h3>Title: LayoutPrompter: Awaken the Design Ability of Large Language Models. (arXiv:2311.06495v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06495">http://arxiv.org/abs/2311.06495</a></li>
<li>Code URL: https://github.com/microsoft/layoutgeneration</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06495]] LayoutPrompter: Awaken the Design Ability of Large Language Models(http://arxiv.org/abs/2311.06495)</code></li>
<li>Summary: <p>Conditional graphic layout generation, which automatically maps user
constraints to high-quality layouts, has attracted widespread attention today.
Although recent works have achieved promising performance, the lack of
versatility and data efficiency hinders their practical applications. In this
work, we propose LayoutPrompter, which leverages large language models (LLMs)
to address the above problems through in-context learning. LayoutPrompter is
made up of three key components, namely input-output serialization, dynamic
exemplar selection and layout ranking. Specifically, the input-output
serialization component meticulously designs the input and output formats for
each layout generation task. Dynamic exemplar selection is responsible for
selecting the most helpful prompting exemplars for a given input. And a layout
ranker is used to pick the highest quality layout from multiple outputs of
LLMs. We conduct experiments on all existing layout generation tasks using four
public datasets. Despite the simplicity of our approach, experimental results
show that LayoutPrompter can compete with or even outperform state-of-the-art
approaches on these tasks without any model training or fine-tuning. This
demonstrates the effectiveness of this versatile and training-free approach. In
addition, the ablation studies show that LayoutPrompter is significantly
superior to the training-based baseline in a low-data regime, further
indicating the data efficiency of LayoutPrompter. Our project is available at
https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter.
</p></li>
</ul>

<h3>Title: PerceptionGPT: Effectively Fusing Visual Perception into LLM. (arXiv:2311.06612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06612">http://arxiv.org/abs/2311.06612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06612]] PerceptionGPT: Effectively Fusing Visual Perception into LLM(http://arxiv.org/abs/2311.06612)</code></li>
<li>Summary: <p>The integration of visual inputs with large language models (LLMs) has led to
remarkable advancements in multi-modal capabilities, giving rise to visual
large language models (VLLMs). However, effectively harnessing VLLMs for
intricate visual perception tasks remains a challenge. In this paper, we
present a novel end-to-end framework named PerceptionGPT, which efficiently and
effectively equips the VLLMs with visual perception abilities by leveraging the
representation power of LLMs' token embedding. Our proposed method treats the
token embedding of the LLM as the carrier of spatial information, then leverage
lightweight visual task encoders and decoders to perform visual perception
tasks (e.g., detection, segmentation). Our approach significantly alleviates
the training difficulty suffered by previous approaches that formulate the
visual outputs as discrete tokens, and enables achieving superior performance
with fewer trainable parameters, less training data and shorted training time.
Moreover, as only one token embedding is required to decode the visual outputs,
the resulting sequence length during inference is significantly reduced.
Consequently, our approach enables accurate and flexible representations,
seamless integration of visual perception tasks, and efficient handling of a
multiple of visual outputs. We validate the effectiveness and efficiency of our
approach through extensive experiments. The results demonstrate significant
improvements over previous methods with much fewer trainable parameters and GPU
hours, which facilitates future research in enabling LLMs with visual
perception abilities.
</p></li>
</ul>

<h3>Title: Word Definitions from Large Language Models. (arXiv:2311.06362v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06362">http://arxiv.org/abs/2311.06362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06362]] Word Definitions from Large Language Models(http://arxiv.org/abs/2311.06362)</code></li>
<li>Summary: <p>Dictionary definitions are historically the arbitrator of what words mean,
but this primacy has come under threat by recent progress in NLP, including
word embeddings and generative models like ChatGPT. We present an exploratory
study of the degree of alignment between word definitions from classical
dictionaries and these newer computational artifacts. Specifically, we compare
definitions from three published dictionaries to those generated from variants
of ChatGPT. We show that (i) definitions from different traditional
dictionaries exhibit more surface form similarity than do model-generated
definitions, (ii) that the ChatGPT definitions are highly accurate, comparable
to traditional dictionaries, and (iii) ChatGPT-based embedding definitions
retain their accuracy even on low frequency words, much better than GloVE and
FastText word embeddings.
</p></li>
</ul>

<h3>Title: Heaps' Law in GPT-Neo Large Language Model Emulated Corpora. (arXiv:2311.06377v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06377">http://arxiv.org/abs/2311.06377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06377]] Heaps' Law in GPT-Neo Large Language Model Emulated Corpora(http://arxiv.org/abs/2311.06377)</code></li>
<li>Summary: <p>Heaps' law is an empirical relation in text analysis that predicts vocabulary
growth as a function of corpus size. While this law has been validated in
diverse human-authored text corpora, its applicability to large language model
generated text remains unexplored. This study addresses this gap, focusing on
the emulation of corpora using the suite of GPT-Neo large language models. To
conduct our investigation, we emulated corpora of PubMed abstracts using three
different parameter sizes of the GPT-Neo model. Our emulation strategy involved
using the initial five words of each PubMed abstract as a prompt and
instructing the model to expand the content up to the original abstract's
length. Our findings indicate that the generated corpora adhere to Heaps' law.
Interestingly, as the GPT-Neo model size grows, its generated vocabulary
increasingly adheres to Heaps' law as as observed in human-authored text. To
further improve the richness and authenticity of GPT-Neo outputs, future
iterations could emphasize enhancing model size or refining the model
architecture to curtail vocabulary repetition.
</p></li>
</ul>

<h3>Title: Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks. (arXiv:2311.06383v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06383">http://arxiv.org/abs/2311.06383</a></li>
<li>Code URL: https://github.com/megagonlabs/rjdb</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06383]] Distilling Large Language Models using Skill-Occupation Graph Context for HR-Related Tasks(http://arxiv.org/abs/2311.06383)</code></li>
<li>Summary: <p>Numerous HR applications are centered around resumes and job descriptions.
While they can benefit from advancements in NLP, particularly large language
models, their real-world adoption faces challenges due to absence of
comprehensive benchmarks for various HR tasks, and lack of smaller models with
competitive capabilities. In this paper, we aim to bridge this gap by
introducing the Resume-Job Description Benchmark (RJDB). We meticulously craft
this benchmark to cater to a wide array of HR tasks, including matching and
explaining resumes to job descriptions, extracting skills and experiences from
resumes, and editing resumes. To create this benchmark, we propose to distill
domain-specific knowledge from a large language model (LLM). We rely on a
curated skill-occupation graph to ensure diversity and provide context for LLMs
generation. Our benchmark includes over 50 thousand triples of job
descriptions, matched resumes and unmatched resumes. Using RJDB, we train
multiple smaller student models. Our experiments reveal that the student models
achieve near/better performance than the teacher model (GPT-4), affirming the
effectiveness of the benchmark. Additionally, we explore the utility of RJDB on
out-of-distribution data for skill extraction and resume-job description
matching, in zero-shot and weak supervision manner. We release our datasets and
code to foster further research and industry applications.
</p></li>
</ul>

<h3>Title: THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech. (arXiv:2311.06446v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06446">http://arxiv.org/abs/2311.06446</a></li>
<li>Code URL: https://github.com/mohaimeed/thos</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06446]] THOS: A Benchmark Dataset for Targeted Hate and Offensive Speech(http://arxiv.org/abs/2311.06446)</code></li>
<li>Summary: <p>Detecting harmful content on social media, such as Twitter, is made difficult
by the fact that the seemingly simple yes/no classification conceals a
significant amount of complexity. Unfortunately, while several datasets have
been collected for training classifiers in hate and offensive speech, there is
a scarcity of datasets labeled with a finer granularity of target classes and
specific targets. In this paper, we introduce THOS, a dataset of 8.3k tweets
manually labeled with fine-grained annotations about the target of the message.
We demonstrate that this dataset makes it feasible to train classifiers, based
on Large Language Models, to perform classification at this level of
granularity.
</p></li>
</ul>

<h3>Title: Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering. (arXiv:2311.06503v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06503">http://arxiv.org/abs/2311.06503</a></li>
<li>Code URL: https://github.com/zjukg/knowpat</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06503]] Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering(http://arxiv.org/abs/2311.06503)</code></li>
<li>Summary: <p>Recently, the development of large language models (LLMs) has attracted wide
attention in academia and industry. Deploying LLMs to real scenarios is one of
the key directions in the current Internet industry. In this paper, we present
a novel pipeline to apply LLMs for domain-specific question answering (QA) that
incorporates domain knowledge graphs (KGs), addressing an important direction
of LLM application. As a real-world application, the content generated by LLMs
should be user-friendly to serve the customers. Additionally, the model needs
to utilize domain knowledge properly to generate reliable answers. These two
issues are the two major difficulties in the LLM application as vanilla
fine-tuning can not adequately address them. We think both requirements can be
unified as the model preference problem that needs to align with humans to
achieve practical application. Thus, we introduce Knowledgeable Preference
AlignmenT (KnowPAT), which constructs two kinds of preference set called style
preference set and knowledge preference set respectively to tackle the two
issues. Besides, we design a new alignment objective to align the LLM
preference with human preference, aiming to train a better LLM for
real-scenario domain-specific QA to generate reliable and user-friendly
answers. Adequate experiments and comprehensive with 15 baseline methods
demonstrate that our KnowPAT is an outperforming pipeline for real-scenario
domain-specific QA with LLMs. Our code is open-source at
https://github.com/zjukg/KnowPAT.
</p></li>
</ul>

<h3>Title: Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study. (arXiv:2311.06549v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06549">http://arxiv.org/abs/2311.06549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06549]] Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study(http://arxiv.org/abs/2311.06549)</code></li>
<li>Summary: <p>The brittleness of finetuned language model performance on
out-of-distribution (OOD) test samples in unseen domains has been well-studied
for English, yet is unexplored for multi-lingual models. Therefore, we study
generalization to OOD test data specifically in zero-shot cross-lingual
transfer settings, analyzing performance impacts of both language and domain
shifts between train and test data. We further assess the effectiveness of
counterfactually augmented data (CAD) in improving OOD generalization for the
cross-lingual setting, since CAD has been shown to benefit in a monolingual
English setting. Finally, we propose two new approaches for OOD generalization
that avoid the costly annotation process associated with CAD, by exploiting the
power of recent large language models (LLMs). We experiment with 3 multilingual
models, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and
evaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and
Restaurant reviews. Results echo the OOD performance decline observed in the
monolingual English setting. Further, (i) counterfactuals from the original
high-resource language do improve OOD generalization in the low-resource
language, and (ii) our newly proposed cost-effective approaches reach similar
or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.
</p></li>
</ul>

<h3>Title: From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. (arXiv:2311.06595v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06595">http://arxiv.org/abs/2311.06595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06595]] From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL(http://arxiv.org/abs/2311.06595)</code></li>
<li>Summary: <p>The remarkable ability of Large Language Models (LLMs) to understand and
follow instructions has sometimes been limited by their in-context learning
(ICL) performance in low-resource languages. To address this, we introduce a
novel approach that leverages cross-lingual retrieval-augmented in-context
learning (CREA-ICL). By extracting semantically similar prompts from
high-resource languages, we aim to improve the zero-shot performance of
multilingual pre-trained language models (MPLMs) across diverse tasks. Though
our approach yields steady improvements in classification tasks, it faces
challenges in generation tasks. Our evaluation offers insights into the
performance dynamics of retrieval-augmented in-context learning across both
classification and generation domains.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Self-supervised Context Learning for Visual Inspection of Industrial Defects. (arXiv:2311.06504v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06504">http://arxiv.org/abs/2311.06504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06504]] Self-supervised Context Learning for Visual Inspection of Industrial Defects(http://arxiv.org/abs/2311.06504)</code></li>
<li>Summary: <p>The unsupervised visual inspection of defects in industrial products poses a
significant challenge due to substantial variations in product surfaces.
Current unsupervised models struggle to strike a balance between detecting
texture and object defects, lacking the capacity to discern latent
representations and intricate features. In this paper, we present a novel
self-supervised learning algorithm designed to derive an optimal encoder by
tackling the renowned jigsaw puzzle. Our approach involves dividing the target
image into nine patches, tasking the encoder with predicting the relative
position relationships between any two patches to extract rich semantics.
Subsequently, we introduce an affinity-augmentation method to accentuate
differences between normal and abnormal latent representations. Leveraging the
classic support vector data description algorithm yields final detection
results. Experimental outcomes demonstrate that our proposed method achieves
outstanding detection and segmentation performance on the widely used MVTec AD
dataset, with rates of 95.8% and 96.8%, respectively, establishing a
state-of-the-art benchmark for both texture and object defects. Comprehensive
experimentation underscores the effectiveness of our approach in diverse
industrial applications.
</p></li>
</ul>

<h3>Title: CrashCar101: Procedural Generation for Damage Assessment. (arXiv:2311.06536v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06536">http://arxiv.org/abs/2311.06536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06536]] CrashCar101: Procedural Generation for Damage Assessment(http://arxiv.org/abs/2311.06536)</code></li>
<li>Summary: <p>In this paper, we are interested in addressing the problem of damage
assessment for vehicles, such as cars. This task requires not only detecting
the location and the extent of the damage but also identifying the damaged
part. To train a computer vision system for the semantic part and damage
segmentation in images, we need to manually annotate images with costly pixel
annotations for both part categories and damage types. To overcome this need,
we propose to use synthetic data to train these models. Synthetic data can
provide samples with high variability, pixel-accurate annotations, and
arbitrarily large training sets without any human intervention. We propose a
procedural generation pipeline that damages 3D car models and we obtain
synthetic 2D images of damaged cars paired with pixel-accurate annotations for
part and damage categories. To validate our idea, we execute our pipeline and
render our CrashCar101 dataset. We run experiments on three real datasets for
the tasks of part and damage segmentation. For part segmentation, we show that
the segmentation models trained on a combination of real data and our synthetic
data outperform all models trained only on real data. For damage segmentation,
we show the sim2real transfer ability of CrashCar101.
</p></li>
</ul>

<h3>Title: FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image. (arXiv:2311.06551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06551">http://arxiv.org/abs/2311.06551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06551]] FDNet: Feature Decoupled Segmentation Network for Tooth CBCT Image(http://arxiv.org/abs/2311.06551)</code></li>
<li>Summary: <p>Precise Tooth Cone Beam Computed Tomography (CBCT) image segmentation is
crucial for orthodontic treatment planning. In this paper, we propose FDNet, a
Feature Decoupled Segmentation Network, to excel in the face of the variable
dental conditions encountered in CBCT scans, such as complex artifacts and
indistinct tooth boundaries. The Low-Frequency Wavelet Transform (LF-Wavelet)
is employed to enrich the semantic content by emphasizing the global structural
integrity of the teeth, while the SAM encoder is leveraged to refine the
boundary delineation, thus improving the contrast between adjacent dental
structures. By integrating these dual aspects, FDNet adeptly addresses the
semantic gap, providing a detailed and accurate segmentation. The framework's
effectiveness is validated through rigorous benchmarks, achieving the top Dice
and IoU scores of 85.28% and 75.23%, respectively. This innovative decoupling
of semantic and boundary features capitalizes on the unique strengths of each
element to significantly elevate the quality of segmentation performance.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
