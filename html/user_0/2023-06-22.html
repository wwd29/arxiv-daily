<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A new color image secret sharing protocol. (arXiv:2306.12107v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12107">http://arxiv.org/abs/2306.12107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12107] A new color image secret sharing protocol](http://arxiv.org/abs/2306.12107) #secure</code></li>
<li>Summary: <p>Visual cryptography aims to protect images against their possible
illegitimate use. Thus, one can cipher, hash, or add watermarks for protecting
copyright, among others. In this paper we provide a new solution to the problem
of secret sharing for the case when the secret is an image. Our method combines
the Shamir scheme for secret sharing using finite fields of characteristic 2
with the CBC mode of operation of a secure symmetric cryptographic scheme like
AES, so that the security relies on that of the mentioned techniques. The
resulting shares have the same resolution as that of the original image. The
idea of the method could be generalized to other multimedia formats like audio
or video, adapting the method to the corresponding encoded information.
</p></li>
</ul>

<h3>Title: Tailstorm: A Secure and Fair Blockchain for Cash Transactions. (arXiv:2306.12206v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12206">http://arxiv.org/abs/2306.12206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12206] Tailstorm: A Secure and Fair Blockchain for Cash Transactions](http://arxiv.org/abs/2306.12206) #secure</code></li>
<li>Summary: <p>Proof-of-work (PoW) cryptocurrencies rely on a balance of security and
fairness in order to maintain a sustainable ecosystem of miners and users.
Users demand fast and consistent transaction confirmation, and in exchange
drive the adoption and valuation of the cryptocurrency. Miners provide the
confirmations, however, they primarily seek rewards. In unfair systems, miners
can amplify their rewards by consolidating mining power. Centralization
however, undermines the security guarantees of the system and might discourage
users.
</p></li>
</ul>

<p>In this paper we present Tailstorm, a cryptocurrency that strikes this
balance. Tailstorm merges multiple recent protocol improvements addressing
security, confirmation latency, and throughput with a novel incentive mechanism
improving fairness. We implement a parallel proof-of-work consensus mechanism
with $k$ PoWs per block to obtain state-of-the-art consistency guarantees.
Inspired by Bobtail and Storm, we structure the individual PoWs in a tree
which, by including a list of transactions with each PoW, reduces confirmation
latency and improves throughput. Our proposed incentive mechanism discounts
rewards based on the depth of this tree. Thereby, it effectively punishes
information withholding, the core attack strategy used to reap an unfair share
of rewards.
</p>
<p>We back our claims with a comprehensive analysis. We present a generic system
model which allows us to specify Bitcoin, $B_k$, and Tailstorm from a joint set
of assumptions. We provide an analytical bound for the fairness of Tailstorm
and Bitcoin in honest networks and we confirm the results through simulation.
We evaluate the effectiveness of dishonest behaviour through reinforcement
learning. Our attack search reproduces known optimal strategies against
Bitcoin, uncovers new ones against $B_k$, and confirms that Tailstorm's reward
discounting makes it more resilient to incentive layer attacks.
</p>

<h3>Title: Do you still need a manual smart contract audit?. (arXiv:2306.12338v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12338">http://arxiv.org/abs/2306.12338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12338] Do you still need a manual smart contract audit?](http://arxiv.org/abs/2306.12338) #secure</code></li>
<li>Summary: <p>We investigate the feasibility of employing large language models (LLMs) for
conducting the security audit of smart contracts, a traditionally
time-consuming and costly process. Our research focuses on the optimization of
prompt engineering for enhanced security analysis, and we evaluate the
performance and accuracy of LLMs using a benchmark dataset comprising 52
Decentralized Finance (DeFi) smart contracts that have previously been
compromised.
</p></li>
</ul>

<p>Our findings reveal that, when applied to vulnerable contracts, both GPT-4
and Claude models correctly identify the vulnerability type in 40% of the
cases. However, these models also demonstrate a high false positive rate,
necessitating continued involvement from manual auditors. The LLMs tested
outperform a random model by 20% in terms of F1-score.
</p>
<p>To ensure the integrity of our study, we conduct mutation testing on five
newly developed and ostensibly secure smart contracts, into which we manually
insert two and 15 vulnerabilities each. This testing yielded a remarkable
best-case 78.7% true positive rate for the GPT-4-32k model. We tested both,
asking the models to perform a binary classification on whether a contract is
vulnerable, and a non-binary prompt. We also examined the influence of model
temperature variations and context length on the LLM's performance.
</p>
<p>Despite the potential for many further enhancements, this work lays the
groundwork for a more efficient and economical approach to smart contract
security audits.
</p>

<h2>security</h2>
<h3>Title: Generalizable Metric Network for Cross-domain Person Re-identification. (arXiv:2306.11991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11991">http://arxiv.org/abs/2306.11991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11991] Generalizable Metric Network for Cross-domain Person Re-identification](http://arxiv.org/abs/2306.11991) #security</code></li>
<li>Summary: <p>Person Re-identification (Re-ID) is a crucial technique for public security
and has made significant progress in supervised settings. However, the
cross-domain (i.e., domain generalization) scene presents a challenge in Re-ID
tasks due to unseen test domains and domain-shift between the training and test
sets. To tackle this challenge, most existing methods aim to learn
domain-invariant or robust features for all domains. In this paper, we observe
that the data-distribution gap between the training and test sets is smaller in
the sample-pair space than in the sample-instance space. Based on this
observation, we propose a Generalizable Metric Network (GMN) to further explore
sample similarity in the sample-pair space. Specifically, we add a Metric
Network (M-Net) after the main network and train it on positive and negative
sample-pair features, which is then employed during the test stage.
Additionally, we introduce the Dropout-based Perturbation (DP) module to
enhance the generalization capability of the metric network by enriching the
sample-pair diversity. Moreover, we develop a Pair-Identity Center (PIC) loss
to enhance the model's discrimination by ensuring that sample-pair features
with the same pair-identity are consistent. We validate the effectiveness of
our proposed method through a lot of experiments on multiple benchmark datasets
and confirm the value of each module in our GMN.
</p></li>
</ul>

<h3>Title: Decisions &amp; Disruptions 2: Decide Harder. (arXiv:2306.12168v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12168">http://arxiv.org/abs/2306.12168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12168] Decisions &amp; Disruptions 2: Decide Harder](http://arxiv.org/abs/2306.12168) #security</code></li>
<li>Summary: <p>Cyber incident response is critical to business continuity -- we describe a
new exercise that challenges professionals to play the role of Chief
Information Security Officer (CISO) for a major financial organisation. Teams
must decide how organisational team and budget resources should be deployed
across Enterprise Architecture (EA) upgrades and cyber incidents. Every choice
made has an impact -- some prevent whilst others may trigger new or continue
current attacks. We explain how the underlying platform supports these
interactions through a reactionary event mechanism that introduces events based
on the current attack surface of the organisation. We explore how our platform
manages to introduce randomness on top of triggered events to ensure that the
exercise is not deterministic and better matches incidents in the real world.
We conclude by describing next steps for the exercise and how we plan to use it
in the future to better understand risk decision making.
</p></li>
</ul>

<h3>Title: ICAR, a categorical framework to connect vulnerability, threat and asset managements. (arXiv:2306.12240v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12240">http://arxiv.org/abs/2306.12240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12240] ICAR, a categorical framework to connect vulnerability, threat and asset managements](http://arxiv.org/abs/2306.12240) #security</code></li>
<li>Summary: <p>We present ICAR, a mathematical framework derived from category theory for
representing cybersecurity NIST and MITRE's ontologies. Designed for
cybersecurity, ICAR is a category whose objects are cybersecurity knowledge
(weakness, vulnerability, impacted product, attack technique, etc.) and whose
morphisms are relations between this knowledge, that make sense for
cybersecurity. Within this rigorous and unified framework, we obtain a
knowledge graph capable of identifying the attack and weakness structures of an
IS, at the interface between description logics, database theory and
cybersecurity. We then define ten cybersecurity queries to help understand the
risks incurred by IS and organise their defence.
</p></li>
</ul>

<h3>Title: Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning. (arXiv:2306.11946v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11946">http://arxiv.org/abs/2306.11946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11946] Winter Wheat Crop Yield Prediction on Multiple Heterogeneous Datasets using Machine Learning](http://arxiv.org/abs/2306.11946) #security</code></li>
<li>Summary: <p>Winter wheat is one of the most important crops in the United Kingdom, and
crop yield prediction is essential for the nation's food security. Several
studies have employed machine learning (ML) techniques to predict crop yield on
a county or farm-based level. The main objective of this study is to predict
winter wheat crop yield using ML models on multiple heterogeneous datasets,
i.e., soil and weather on a zone-based level. Experimental results demonstrated
their impact when used alone and in combination. In addition, we employ
numerous ML algorithms to emphasize the significance of data quality in any
machine-learning strategy.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification. (arXiv:2306.11754v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11754">http://arxiv.org/abs/2306.11754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11754] Pre-Pruning and Gradient-Dropping Improve Differentially Private Image Classification](http://arxiv.org/abs/2306.11754) #privacy</code></li>
<li>Summary: <p>Scalability is a significant challenge when it comes to applying differential
privacy to training deep neural networks. The commonly used DP-SGD algorithm
struggles to maintain a high level of privacy protection while achieving high
accuracy on even moderately sized models. To tackle this challenge, we take
advantage of the fact that neural networks are overparameterized, which allows
us to improve neural network training with differential privacy. Specifically,
we introduce a new training paradigm that uses \textit{pre-pruning} and
\textit{gradient-dropping} to reduce the parameter space and improve
scalability. The process starts with pre-pruning the parameters of the original
network to obtain a smaller model that is then trained with DP-SGD. During
training, less important gradients are dropped, and only selected gradients are
updated. Our training paradigm introduces a tension between the rates of
pre-pruning and gradient-dropping, privacy loss, and classification accuracy.
Too much pre-pruning and gradient-dropping reduces the model's capacity and
worsens accuracy, while training a smaller model requires less privacy budget
for achieving good accuracy. We evaluate the interplay between these factors
and demonstrate the effectiveness of our training paradigm for both training
from scratch and fine-tuning pre-trained networks on several benchmark image
classification datasets. The tools can also be readily incorporated into
existing training paradigms.
</p></li>
</ul>

<h3>Title: Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning. (arXiv:2306.11967v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11967">http://arxiv.org/abs/2306.11967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11967] Complementary Learning Subnetworks for Parameter-Efficient Class-Incremental Learning](http://arxiv.org/abs/2306.11967) #privacy</code></li>
<li>Summary: <p>In the scenario of class-incremental learning (CIL), deep neural networks
have to adapt their model parameters to non-stationary data distributions,
e.g., the emergence of new classes over time. However, CIL models are
challenged by the well-known catastrophic forgetting phenomenon. Typical
methods such as rehearsal-based ones rely on storing exemplars of old classes
to mitigate catastrophic forgetting, which limits real-world applications
considering memory resources and privacy issues. In this paper, we propose a
novel rehearsal-free CIL approach that learns continually via the synergy
between two Complementary Learning Subnetworks. Our approach involves jointly
optimizing a plastic CNN feature extractor and an analytical feed-forward
classifier. The inaccessibility of historical data is tackled by holistically
controlling the parameters of a well-trained model, ensuring that the decision
boundary learned fits new classes while retaining recognition of previously
learned classes. Specifically, the trainable CNN feature extractor provides
task-dependent knowledge separately without interference; and the final
classifier integrates task-specific knowledge incrementally for decision-making
without forgetting. In each CIL session, it accommodates new tasks by attaching
a tiny set of declarative parameters to its backbone, in which only one matrix
per task or one vector per class is kept for knowledge retention. Extensive
experiments on a variety of task sequences show that our method achieves
competitive results against state-of-the-art methods, especially in accuracy
gain, memory cost, training efficiency, and task-order robustness. Furthermore,
to make the non-growing backbone (i.e., a model with limited network capacity)
suffice to train on more incoming tasks, a graceful forgetting implementation
on previously learned trivial tasks is empirically investigated.
</p></li>
</ul>

<h3>Title: Randomized Quantization is All You Need for Differential Privacy in Federated Learning. (arXiv:2306.11913v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11913">http://arxiv.org/abs/2306.11913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11913] Randomized Quantization is All You Need for Differential Privacy in Federated Learning](http://arxiv.org/abs/2306.11913) #privacy</code></li>
<li>Summary: <p>Federated learning (FL) is a common and practical framework for learning a
machine model in a decentralized fashion. A primary motivation behind this
decentralized approach is data privacy, ensuring that the learner never sees
the data of each local source itself. Federated learning then comes with two
majors challenges: one is handling potentially complex model updates between a
server and a large number of data sources; the other is that de-centralization
may, in fact, be insufficient for privacy, as the local updates themselves can
reveal information about the sources' data. To address these issues, we
consider an approach to federated learning that combines quantization and
differential privacy. Absent privacy, Federated Learning often relies on
quantization to reduce communication complexity. We build upon this approach
and develop a new algorithm called the \textbf{R}andomized
\textbf{Q}uantization \textbf{M}echanism (RQM), which obtains privacy through a
two-levels of randomization. More precisely, we randomly sub-sample feasible
quantization levels, then employ a randomized rounding procedure using these
sub-sampled discrete levels. We are able to establish that our results preserve
``Renyi differential privacy'' (Renyi DP). We empirically study the performance
of our algorithm and demonstrate that compared to previous work it yields
improved privacy-accuracy trade-offs for DP federated learning. To the best of
our knowledge, this is the first study that solely relies on randomized
quantization without incorporating explicit discrete noise to achieve Renyi DP
guarantees in Federated Learning systems.
</p></li>
</ul>

<h3>Title: Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition. (arXiv:2306.11924v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11924">http://arxiv.org/abs/2306.11924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11924] Deep perceptual hashing algorithms with hidden dual purpose: when client-side scanning does facial recognition](http://arxiv.org/abs/2306.11924) #privacy</code></li>
<li>Summary: <p>End-to-end encryption (E2EE) provides strong technical protections to
individuals from interferences. Governments and law enforcement agencies around
the world have however raised concerns that E2EE also allows illegal content to
be shared undetected. Client-side scanning (CSS), using perceptual hashing (PH)
to detect known illegal content before it is shared, is seen as a promising
solution to prevent the diffusion of illegal content while preserving
encryption. While these proposals raise strong privacy concerns, proponents of
the solutions have argued that the risk is limited as the technology has a
limited scope: detecting known illegal content. In this paper, we show that
modern perceptual hashing algorithms are actually fairly flexible pieces of
technology and that this flexibility could be used by an adversary to add a
secondary hidden feature to a client-side scanning system. More specifically,
we show that an adversary providing the PH algorithm can ``hide" a secondary
purpose of face recognition of a target individual alongside its primary
purpose of image copy detection. We first propose a procedure to train a
dual-purpose deep perceptual hashing model by jointly optimizing for both the
image copy detection and the targeted facial recognition task. Second, we
extensively evaluate our dual-purpose model and show it to be able to reliably
identify a target individual 67% of the time while not impacting its
performance at detecting illegal content. We also show that our model is
neither a general face detection nor a facial recognition model, allowing its
secondary purpose to be hidden. Finally, we show that the secondary purpose can
be enabled by adding a single illegal looking image to the database. Taken
together, our results raise concerns that a deep perceptual hashing-based CSS
system could turn billions of user devices into tools to locate targeted
individuals.
</p></li>
</ul>

<h3>Title: PrivSketch: A Private Sketch-based Frequency Estimation Protocol for Data Streams. (arXiv:2306.12144v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12144">http://arxiv.org/abs/2306.12144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12144] PrivSketch: A Private Sketch-based Frequency Estimation Protocol for Data Streams](http://arxiv.org/abs/2306.12144) #privacy</code></li>
<li>Summary: <p>Local differential privacy (LDP) has recently become a popular
privacy-preserving data collection technique protecting users' privacy. The
main problem of data stream collection under LDP is the poor utility due to
multi-item collection from a very large domain. This paper proposes PrivSketch,
a high-utility frequency estimation protocol taking advantage of sketches,
suitable for private data stream collection. Combining the proposed background
information and a decode-first collection-side workflow, PrivSketch improves
the utility by reducing the errors introduced by the sketching algorithm and
the privacy budget utilization when collecting multiple items. We analytically
prove the superior accuracy and privacy characteristics of PrivSketch, and also
evaluate them experimentally. Our evaluation, with several diverse synthetic
and real datasets, demonstrates that PrivSketch is 1-3 orders of magnitude
better than the competitors in terms of utility in both frequency estimation
and frequent item estimation, while being up to ~100x faster.
</p></li>
</ul>

<h3>Title: Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12194">http://arxiv.org/abs/2306.12194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12194] Split Learning in 6G Edge Networks](http://arxiv.org/abs/2306.12194) #privacy</code></li>
<li>Summary: <p>With the proliferation of distributed edge computing resources, the 6G mobile
network will evolve into a network for connected intelligence. Along this line,
the proposal to incorporate federated learning into the mobile edge has gained
considerable interest in recent years. However, the deployment of federated
learning faces substantial challenges as massive resource-limited IoT devices
can hardly support on-device model training. This leads to the emergence of
split learning (SL) which enables servers to handle the major training workload
while still enhancing data privacy. In this article, we offer a brief overview
of key advancements in SL and articulate its seamless integration with wireless
edge networks. We begin by illustrating the tailored 6G architecture to support
edge SL. Then, we examine the critical design issues for edge SL, including
innovative resource-efficient learning frameworks and resource management
strategies under a single edge server. Additionally, we expand the scope to
multi-edge scenarios, exploring multi-edge collaboration and mobility
management from a networking perspective. Finally, we discuss open problems for
edge SL, including convergence analysis, asynchronous SL and U-shaped SL.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Protecting the Decentralized Future: An Exploration of Common Blockchain Attacks and their Countermeasures. (arXiv:2306.11884v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11884">http://arxiv.org/abs/2306.11884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11884] Protecting the Decentralized Future: An Exploration of Common Blockchain Attacks and their Countermeasures](http://arxiv.org/abs/2306.11884) #protect</code></li>
<li>Summary: <p>Blockchain technology transformed the digital sphere by providing a
transparent, secure, and decentralized platform for data security across a
range of industries, including cryptocurrencies and supply chain management.
Blockchain's integrity and dependability have been jeopardized by the rising
number of security threats, which have attracted cybercriminals as a target. By
summarizing suggested fixes, this research aims to offer a thorough analysis of
mitigating blockchain attacks. The objectives of the paper include identifying
weak blockchain attacks, evaluating various solutions, and determining how
effective and effective they are at preventing these attacks. The study also
highlights how crucial it is to take into account the particular needs of every
blockchain application. This study provides beneficial perspectives and
insights for blockchain researchers and practitioners, making it essential
reading for those interested in current and future trends in blockchain
security research.
</p></li>
</ul>

<h3>Title: Cryptographic ransomware encryption detection: Survey. (arXiv:2306.12008v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12008">http://arxiv.org/abs/2306.12008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12008] Cryptographic ransomware encryption detection: Survey](http://arxiv.org/abs/2306.12008) #protect</code></li>
<li>Summary: <p>The ransomware threat has loomed over our digital life since 1989. Criminals
use this type of cyber attack to lock or encrypt victims' data, often coercing
them to pay exorbitant amounts in ransom. The damage ransomware causes ranges
from monetary losses paid for ransom at best to endangering human lives.
Cryptographic ransomware, where attackers encrypt the victim's data, stands as
the predominant ransomware variant. The primary characteristics of these
attacks have remained the same since the first ransomware attack. For this
reason, we consider this a key factor differentiating ransomware from other
cyber attacks, making it vital in tackling the threat of cryptographic
ransomware. This paper proposes a cyber kill chain that describes the modern
crypto-ransomware attack. The survey focuses on the Encryption phase as
described in our proposed cyber kill chain and its detection techniques. We
identify three main methods used in detecting encryption-related activities by
ransomware, namely API and System calls, I/O monitoring, and file system
activities monitoring. Machine learning (ML) is a tool used in all three
identified methodologies, and some of the issues within the ML domain related
to this survey are also covered as part of their respective methodologies. The
survey of selected proposals is conducted through the prism of those three
methodologies, showcasing the importance of detecting ransomware during
pre-encryption and encryption activities and the windows of opportunity to do
so. We also examine commercial crypto-ransomware protection and detection
offerings and show the gap between academic research and commercial
applications.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12043">http://arxiv.org/abs/2306.12043</a></li>
<li>Code URL: <a href="https://github.com/rainavyas/nlp_attackability">https://github.com/rainavyas/nlp_attackability</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12043] Sample Attackability in Natural Language Adversarial Attacks](http://arxiv.org/abs/2306.12043) #attack</code></li>
<li>Summary: <p>Adversarial attack research in natural language processing (NLP) has made
significant progress in designing powerful attack methods and defence
approaches. However, few efforts have sought to identify which source samples
are the most attackable or robust, i.e. can we determine for an unseen target
model, which samples are the most vulnerable to an adversarial attack. This
work formally extends the definition of sample attackability/robustness for NLP
attacks. Experiments on two popular NLP datasets, four state of the art models
and four different NLP adversarial attack methods, demonstrate that sample
uncertainty is insufficient for describing characteristics of attackable/robust
samples and hence a deep learning based detector can perform much better at
identifying the most attackable and robust samples for an unseen target model.
Nevertheless, further analysis finds that there is little agreement in which
samples are considered the most attackable/robust across different NLP attack
methods, explaining a lack of portability of attackability detection methods
across attack methods.
</p></li>
</ul>

<h3>Title: Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12161">http://arxiv.org/abs/2306.12161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12161] Adversarial Attacks Neutralization via Data Set Randomization](http://arxiv.org/abs/2306.12161) #attack</code></li>
<li>Summary: <p>Adversarial attacks on deep-learning models pose a serious threat to their
reliability and security. Existing defense mechanisms are narrow addressing a
specific type of attack or being vulnerable to sophisticated attacks. We
propose a new defense mechanism that, while being focused on image-based
classifiers, is general with respect to the cited category. It is rooted on
hyperspace projection. In particular, our solution provides a pseudo-random
projection of the original dataset into a new dataset. The proposed defense
mechanism creates a set of diverse projected datasets, where each projected
dataset is used to train a specific classifier, resulting in different trained
classifiers with different decision boundaries. During testing, it randomly
selects a classifier to test the input. Our approach does not sacrifice
accuracy over legitimate input. Other than detailing and providing a thorough
characterization of our defense mechanism, we also provide a proof of concept
of using four optimization-based adversarial attacks (PGD, FGSM, IGSM, and
C\&amp;W) and a generative adversarial attack testing them on the MNIST dataset.
Our experimental results show that our solution increases the robustness of
deep learning models against adversarial attacks and significantly reduces the
attack success rate by at least 89% for optimization attacks and 78% for
generative attacks. We also analyze the relationship between the number of used
hyperspaces and the efficacy of the defense mechanism. As expected, the two are
positively correlated, offering an easy-to-tune parameter to enforce the
desired level of security. The generality and scalability of our solution and
adaptability to different attack scenarios, combined with the excellent
achieved results, other than providing a robust defense against adversarial
attacks on deep learning networks, also lay the groundwork for future research
in the field.
</p></li>
</ul>

<h3>Title: Geometric Algorithms for $k$-NN Poisoning. (arXiv:2306.12377v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12377">http://arxiv.org/abs/2306.12377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12377] Geometric Algorithms for $k$-NN Poisoning](http://arxiv.org/abs/2306.12377) #attack</code></li>
<li>Summary: <p>We propose a label poisoning attack on geometric data sets against
$k$-nearest neighbor classification. We provide an algorithm that can compute
an $\varepsilon n$-additive approximation of the optimal poisoning in $n\cdot
2^{2^{O(d+k/\varepsilon)}}$ time for a given data set $X \in \mathbb{R}^d$,
where $|X| = n$. Our algorithm achieves its objectives through the application
of multi-scale random partitions.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Evaluating Adversarial Robustness of Convolution-based Human Motion Prediction. (arXiv:2306.11990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11990">http://arxiv.org/abs/2306.11990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11990] Evaluating Adversarial Robustness of Convolution-based Human Motion Prediction](http://arxiv.org/abs/2306.11990) #robust</code></li>
<li>Summary: <p>Human motion prediction has achieved a brilliant performance with the help of
CNNs, which facilitates human-machine cooperation. However, currently, there is
no work evaluating the potential risk in human motion prediction when facing
adversarial attacks, which may cause danger in real applications. The
adversarial attack will face two problems against human motion prediction: 1.
For naturalness, pose data is highly related to the physical dynamics of human
skeletons where Lp norm constraints cannot constrain the adversarial example
well; 2. Unlike the pixel value in images, pose data is diverse at scale
because of the different acquisition equipment and the data processing, which
makes it hard to set fixed parameters to perform attacks. To solve the problems
above, we propose a new adversarial attack method that perturbs the input human
motion sequence by maximizing the prediction error with physical constraints.
Specifically, we introduce a novel adaptable scheme that facilitates the attack
to suit the scale of the target pose and two physical constraints to enhance
the imperceptibility of the adversarial example. The evaluating experiments on
three datasets show that the prediction errors of all target models are
enlarged significantly, which means current convolution-based human motion
prediction models can be easily disturbed under the proposed attack. The
quantitative analysis shows that prior knowledge and semantic information
modeling can be the key to the adversarial robustness of human motion
predictors. The qualitative results indicate that the adversarial sample is
hard to be noticed when compared frame by frame but is relatively easy to be
detected when the sample is animated.
</p></li>
</ul>

<h3>Title: Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12070">http://arxiv.org/abs/2306.12070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12070] Task-Robust Pre-Training for Worst-Case Downstream Adaptation](http://arxiv.org/abs/2306.12070) #robust</code></li>
<li>Summary: <p>Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.
</p></li>
</ul>

<h3>Title: A Comprehensive Study on the Robustness of Image Classification and Object Detection in Remote Sensing: Surveying and Benchmarking. (arXiv:2306.12111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12111">http://arxiv.org/abs/2306.12111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12111] A Comprehensive Study on the Robustness of Image Classification and Object Detection in Remote Sensing: Surveying and Benchmarking](http://arxiv.org/abs/2306.12111) #robust</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have found widespread applications in
interpreting remote sensing (RS) imagery. However, it has been demonstrated in
previous works that DNNs are vulnerable to different types of noises,
particularly adversarial noises. Surprisingly, there has been a lack of
comprehensive studies on the robustness of RS tasks, prompting us to undertake
a thorough survey and benchmark on the robustness of image classification and
object detection in RS. To our best knowledge, this study represents the first
comprehensive examination of both natural robustness and adversarial robustness
in RS tasks. Specifically, we have curated and made publicly available datasets
that contain natural and adversarial noises. These datasets serve as valuable
resources for evaluating the robustness of DNNs-based models. To provide a
comprehensive assessment of model robustness, we conducted meticulous
experiments with numerous different classifiers and detectors, encompassing a
wide range of mainstream methods. Through rigorous evaluation, we have
uncovered insightful and intriguing findings, which shed light on the
relationship between adversarial noise crafting and model training, yielding a
deeper understanding of the susceptibility and limitations of various models,
and providing guidance for the development of more resilient and robust models
</p></li>
</ul>

<h3>Title: Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals. (arXiv:2306.12146v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12146">http://arxiv.org/abs/2306.12146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12146] Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals](http://arxiv.org/abs/2306.12146) #robust</code></li>
<li>Summary: <p>We present a human-in-the-loop dashboard tailored to diagnosing potential
spurious features that NLI models rely on for predictions. The dashboard
enables users to generate diverse and challenging examples by drawing
inspiration from GPT-3 suggestions. Additionally, users can receive feedback
from a trained NLI model on how challenging the newly created example is and
make refinements based on the feedback. Through our investigation, we discover
several categories of spurious correlations that impact the reasoning of NLI
models, which we group into three categories: Semantic Relevance, Logical
Fallacies, and Bias. Based on our findings, we identify and describe various
research opportunities, including diversifying training data and assessing NLI
models' robustness by creating adversarial test suites.
</p></li>
</ul>

<h3>Title: Topological Parallax: A Geometric Specification for Deep Perception Models. (arXiv:2306.11835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11835">http://arxiv.org/abs/2306.11835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11835] Topological Parallax: A Geometric Specification for Deep Perception Models](http://arxiv.org/abs/2306.11835) #robust</code></li>
<li>Summary: <p>For safety and robustness of AI systems, we introduce topological parallax as
a theoretical and computational tool that compares a trained model to a
reference dataset to determine whether they have similar multiscale geometric
structure. Our proofs and examples show that this geometric similarity between
dataset and model is essential to trustworthy interpolation and perturbation,
and we conjecture that this new concept will add value to the current debate
regarding the unclear relationship between overfitting and generalization in
applications of deep-learning. In typical DNN applications, an explicit
geometric description of the model is impossible, but parallax can estimate
topological features (components, cycles, voids, etc.) in the model by
examining the effect on the Rips complex of geodesic distortions using the
reference dataset. Thus, parallax indicates whether the model shares similar
multiscale geometric features with the dataset. Parallax presents theoretically
via topological data analysis [TDA] as a bi-filtered persistence module, and
the key properties of this module are stable under perturbation of the
reference dataset.
</p></li>
</ul>

<h3>Title: Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11915">http://arxiv.org/abs/2306.11915</a></li>
<li>Code URL: <a href="https://github.com/pierreosselin/structureaware">https://github.com/pierreosselin/structureaware</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11915] Structure-Aware Robustness Certificates for Graph Classification](http://arxiv.org/abs/2306.11915) #robust</code></li>
<li>Summary: <p>Certifying the robustness of a graph-based machine learning model poses a
critical challenge for safety. Current robustness certificates for graph
classifiers guarantee output invariance with respect to the total number of
node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$
ball centred on the adjacency matrix. Although theoretically attractive, this
type of isotropic structural noise can be too restrictive in practical
scenarios where some node pairs are more critical than others in determining
the classifier's output. The certificate, in this case, gives a pessimistic
depiction of the robustness of the graph model. To tackle this issue, we
develop a randomised smoothing method based on adding an anisotropic noise
distribution to the input graph structure. We show that our process generates
structural-aware certificates for our classifiers, whereby the magnitude of
robustness certificates can vary across different pre-defined structures of the
graph. We demonstrate the benefits of these certificates in both synthetic and
real-world experiments.
</p></li>
</ul>

<h3>Title: AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11971">http://arxiv.org/abs/2306.11971</a></li>
<li>Code URL: <a href="https://github.com/mikata-project/adcraft">https://github.com/mikata-project/adcraft</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11971] AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization](http://arxiv.org/abs/2306.11971) #robust</code></li>
<li>Summary: <p>We introduce \env{}, a novel benchmark environment for the Reinforcement
Learning (RL) community distinguished by its stochastic and non-stationary
properties. The environment simulates bidding and budgeting dynamics within
Search Engine Marketing (SEM), a digital marketing technique utilizing paid
advertising to enhance the visibility of websites on search engine results
pages (SERPs). The performance of SEM advertisement campaigns depends on
several factors, including keyword selection, ad design, bid management, budget
adjustments, and performance monitoring. Deep RL recently emerged as a
potential strategy to optimize campaign profitability within the complex and
dynamic landscape of SEM but it requires substantial data, which may be costly
or infeasible to acquire in practice. Our customizable environment enables
practitioners to assess and enhance the robustness of RL algorithms pertinent
to SEM bid and budget management without such costs. Through a series of
experiments within the environment, we demonstrate the challenges imposed by
sparsity and non-stationarity on agent convergence and performance. We hope
these challenges further encourage discourse and development around effective
strategies for managing real-world uncertainties.
</p></li>
</ul>

<h3>Title: Introspective Action Advising for Interpretable Transfer Learning. (arXiv:2306.12314v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12314">http://arxiv.org/abs/2306.12314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12314] Introspective Action Advising for Interpretable Transfer Learning](http://arxiv.org/abs/2306.12314) #robust</code></li>
<li>Summary: <p>Transfer learning can be applied in deep reinforcement learning to accelerate
the training of a policy in a target task by transferring knowledge from a
policy learned in a related source task. This is commonly achieved by copying
pretrained weights from the source policy to the target policy prior to
training, under the constraint that they use the same model architecture.
However, not only does this require a robust representation learned over a wide
distribution of states -- often failing to transfer between specialist models
trained over single tasks -- but it is largely uninterpretable and provides
little indication of what knowledge is transferred. In this work, we propose an
alternative approach to transfer learning between tasks based on action
advising, in which a teacher trained in a source task actively guides a
student's exploration in a target task. Through introspection, the teacher is
capable of identifying when advice is beneficial to the student and should be
given, and when it is not. Our approach allows knowledge transfer between
policies agnostic of the underlying representations, and we empirically show
that this leads to improved convergence rates in Gridworld and Atari
environments while providing insight into what knowledge is transferred.
</p></li>
</ul>

<h3>Title: PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning. (arXiv:2306.12370v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12370">http://arxiv.org/abs/2306.12370</a></li>
<li>Code URL: <a href="https://github.com/automl/mf-prior-exp">https://github.com/automl/mf-prior-exp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12370] PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning](http://arxiv.org/abs/2306.12370) #robust</code></li>
<li>Summary: <p>Hyperparameters of Deep Learning (DL) pipelines are crucial for their
downstream performance. While a large number of methods for Hyperparameter
Optimization (HPO) have been developed, their incurred costs are often
untenable for modern DL. Consequently, manual experimentation is still the most
prevalent approach to optimize hyperparameters, relying on the researcher's
intuition, domain knowledge, and cheap preliminary explorations. To resolve
this misalignment between HPO algorithms and DL researchers, we propose
PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs
and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency
across a range of DL benchmarks and show its gains under informative expert
input and robustness against poor expert beliefs
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Analyzing Font Style Usage and Contextual Factors in Real Images. (arXiv:2306.12050v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12050">http://arxiv.org/abs/2306.12050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12050] Analyzing Font Style Usage and Contextual Factors in Real Images](http://arxiv.org/abs/2306.12050) #extraction</code></li>
<li>Summary: <p>There are various font styles in the world. Different styles give different
impressions and readability. This paper analyzes the relationship between font
styles and contextual factors that might affect font style selection with
large-scale datasets. For example, we will analyze the relationship between
font style and its surrounding object (such as ``bus'') by using about 800,000
words in the Open Images dataset. We also use a book cover dataset to analyze
the relationship between font styles with book genres. Moreover, the meaning of
the word is assumed as another contextual factor. For these numeric analyses,
we utilize our own font-style feature extraction model and word2vec. As a
result of co-occurrence-based relationship analysis, we found several instances
of specific font styles being used for specific contextual factors.
</p></li>
</ul>

<h3>Title: Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension. (arXiv:2306.12069v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12069">http://arxiv.org/abs/2306.12069</a></li>
<li>Code URL: <a href="https://github.com/cather-chen/logical-reasoning-graph">https://github.com/cather-chen/logical-reasoning-graph</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12069] Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension](http://arxiv.org/abs/2306.12069) #extraction</code></li>
<li>Summary: <p>Machine reading comprehension (MRC) poses new challenges over logical
reasoning, which aims to understand the implicit logical relations entailed in
the given contexts and perform inference over them. Due to the complexity of
logic, logical relations exist at different granularity levels. However, most
existing methods of logical reasoning individually focus on either entity-aware
or discourse-based information but ignore the hierarchical relations that may
even have mutual effects. In this paper, we propose a holistic graph network
(HGN) which deals with context at both discourse level and word level, as the
basis for logical reasoning, to provide a more fine-grained relation
extraction. Specifically, node-level and type-level relations, which can be
interpreted as bridges in the reasoning process, are modeled by a hierarchical
interaction mechanism to improve the interpretation of MRC systems.
Experimental results on logical reasoning QA datasets (ReClor and LogiQA) and
natural language inference datasets (SNLI and ANLI) show the effectiveness and
generalization of our method, and in-depth analysis verifies its capability to
understand complex logical relations.
</p></li>
</ul>

<h3>Title: Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12245">http://arxiv.org/abs/2306.12245</a></li>
<li>Code URL: <a href="https://github.com/geekjuruo/beer2">https://github.com/geekjuruo/beer2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12245] Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking](http://arxiv.org/abs/2306.12245) #extraction</code></li>
<li>Summary: <p>Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Personalized Federated Learning with Feature Alignment and Classifier Collaboration. (arXiv:2306.11867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11867">http://arxiv.org/abs/2306.11867</a></li>
<li>Code URL: <a href="https://github.com/jianxu95/fedpac">https://github.com/jianxu95/fedpac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11867] Personalized Federated Learning with Feature Alignment and Classifier Collaboration](http://arxiv.org/abs/2306.11867) #federate</code></li>
<li>Summary: <p>Data heterogeneity is one of the most challenging issues in federated
learning, which motivates a variety of approaches to learn personalized models
for participating clients. One such approach in deep neural networks based
tasks is employing a shared feature representation and learning a customized
classifier head for each client. However, previous works do not utilize the
global knowledge during local representation learning and also neglect the
fine-grained collaboration between local classifier heads, which limit the
model generalization ability. In this work, we conduct explicit local-global
feature alignment by leveraging global semantic knowledge for learning a better
representation. Moreover, we quantify the benefit of classifier combination for
each client as a function of the combining weights and derive an optimization
problem for estimating optimal weights. Finally, extensive evaluation results
on benchmark datasets with various heterogeneous data scenarios demonstrate the
effectiveness of our proposed method. Code is available at
https://github.com/JianXu95/FedPAC
</p></li>
</ul>

<h3>Title: FLGo: A Fully Customizable Federated Learning Platform. (arXiv:2306.12079v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12079">http://arxiv.org/abs/2306.12079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12079] FLGo: A Fully Customizable Federated Learning Platform](http://arxiv.org/abs/2306.12079) #federate</code></li>
<li>Summary: <p>Federated learning (FL) has found numerous applications in healthcare,
finance, and IoT scenarios. Many existing FL frameworks offer a range of
benchmarks to evaluate the performance of FL under realistic conditions.
However, the process of customizing simulations to accommodate
application-specific settings, data heterogeneity, and system heterogeneity
typically remains unnecessarily complicated. This creates significant hurdles
for traditional ML researchers in exploring the usage of FL, while also
compromising the shareability of codes across FL frameworks. To address this
issue, we propose a novel lightweight FL platform called FLGo, to facilitate
cross-application FL studies with a high degree of shareability. Our platform
offers 40+ benchmarks, 20+ algorithms, and 2 system simulators as
out-of-the-box plugins. We also provide user-friendly APIs for quickly
customizing new plugins that can be readily shared and reused for improved
reproducibility. Finally, we develop a range of experimental tools, including
parallel acceleration, experiment tracker and analyzer, and parameters
auto-tuning. FLGo is maintained at \url{flgo-xmu.github.io}.
</p></li>
</ul>

<h3>Title: An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning. (arXiv:2306.12088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12088">http://arxiv.org/abs/2306.12088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12088] An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning](http://arxiv.org/abs/2306.12088) #federate</code></li>
<li>Summary: <p>Communication overhead is one of the major challenges in Federated
Learning(FL). A few classical schemes assume the server can extract the
auxiliary information about training data of the participants from the local
models to construct a central dummy dataset. The server uses the dummy dataset
to finetune aggregated global model to achieve the target test accuracy in
fewer communication rounds. In this paper, we summarize the above solutions
into a data-based communication-efficient FL framework. The key of the proposed
framework is to design an efficient extraction module(EM) which ensures the
dummy dataset has a positive effect on finetuning aggregated global model.
Different from the existing methods that use generator to design EM, our
proposed method, FedINIBoost borrows the idea of gradient match to construct
EM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in two
steps for each participant at each communication round. Then the server
aggregates all the proxy datasets to form a central dummy dataset, which is
used to finetune aggregated global model. Extensive experiments verify the
superiority of our method compared with the existing classical method, FedAVG,
FedProx, Moon and FedFTG. Moreover, FedINIBoost plays a significant role in
finetuning the performance of aggregated global model at the initial stage of
FL.
</p></li>
</ul>

<h3>Title: MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12212">http://arxiv.org/abs/2306.12212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12212] MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates](http://arxiv.org/abs/2306.12212) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a promising framework for privacy-preserving
collaborative learning. In FL, the model training tasks are distributed to
clients and only the model updates need to be collected at a central server.
However, when being deployed at the mobile edge network, clients (e.g.,
smartphones and wearables) may have unpredictable availability and randomly
drop out of any training iteration, which hinders FL from achieving the
convergence. This paper tackles such a critical challenge of FL. In particular,
we first investigate the convergence of the classical FedAvg algorithm with
arbitrary client dropouts. We find that with the common choice of a decaying
learning rate, FedAvg can only oscillate within the neighborhood of a
stationary point of the global loss function, which is caused by the divergence
between the aggregated update and the desired central update. Motivated by this
new observation, we then design a novel training algorithm named MimiC, where
the server modifies each received model update based on the previous ones. The
proposed modification of the received model updates is able to mimic the
imaginary central update irrespective of the dropout clients. The theoretical
analysis of MimiC shows that the divergence between the aggregated update and
the central update diminishes with a proper choice of the learning rates,
leading to its convergence. Simulation results further demonstrate that MimiC
maintains stable convergence performance in the presence of client dropouts and
learns better models than the baseline methods.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase. (arXiv:2306.12423v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12423">http://arxiv.org/abs/2306.12423</a></li>
<li>Code URL: <a href="https://github.com/qiuyu96/carver">https://github.com/qiuyu96/carver</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12423] Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase](http://arxiv.org/abs/2306.12423) #fair</code></li>
<li>Summary: <p>Despite the rapid advance of 3D-aware image synthesis, existing studies
usually adopt a mixture of techniques and tricks, leaving it unclear how each
part contributes to the final performance in terms of generality. Following the
most popular and effective paradigm in this field, which incorporates a neural
radiance field (NeRF) into the generator of a generative adversarial network
(GAN), we build a well-structured codebase, dubbed Carver, through modularizing
the generation process. Such a design allows researchers to develop and replace
each module independently, and hence offers an opportunity to fairly compare
various approaches and recognize their contributions from the module
perspective. The reproduction of a range of cutting-edge algorithms
demonstrates the availability of our modularized codebase. We also perform a
variety of in-depth analyses, such as the comparison across different types of
point feature, the necessity of the tailing upsampler in the generator, the
reliance on the camera pose prior, etc., which deepen our understanding of
existing methods and point out some further directions of the research work. We
release code and models at https://github.com/qiuyu96/Carver to facilitate the
development and evaluation of this field.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging. (arXiv:2306.12054v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12054">http://arxiv.org/abs/2306.12054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12054] A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging](http://arxiv.org/abs/2306.12054) #interpretability</code></li>
<li>Summary: <p>Staging of liver fibrosis is important in the diagnosis and treatment
planning of patients suffering from liver diseases. Current deep learning-based
methods using abdominal magnetic resonance imaging (MRI) usually take a
sub-region of the liver as an input, which nevertheless could miss critical
information. To explore richer representations, we formulate this task as a
multi-view learning problem and employ multiple sub-regions of the liver.
Previously, features or predictions are usually combined in an implicit manner,
and uncertainty-aware methods have been proposed. However, these methods could
be challenged to capture cross-view representations, which can be important in
the accurate prediction of staging. Therefore, we propose a reliable multi-view
learning method with interpretable combination rules, which can model global
representations to improve the accuracy of predictions. Specifically, the
proposed method estimates uncertainties based on subjective logic to improve
reliability, and an explicit combination rule is applied based on
Dempster-Shafer's evidence theory with good power of interpretability.
Moreover, a data-efficient transformer is introduced to capture representations
in the global view. Results evaluated on enhanced MRI data show that our method
delivers superior performance over existing multi-view learning methods.
</p></li>
</ul>

<h3>Title: Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions. (arXiv:2306.12244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12244">http://arxiv.org/abs/2306.12244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12244] Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions](http://arxiv.org/abs/2306.12244) #interpretability</code></li>
<li>Summary: <p>We propose a logic-informed knowledge-driven modeling framework for human
movements by analyzing their trajectories. Our approach is inspired by the fact
that human actions are usually driven by their intentions or desires, and are
influenced by environmental factors such as the spatial relationships with
surrounding objects. In this paper, we introduce a set of spatial-temporal
logic rules as knowledge to explain human actions. These rules will be
automatically discovered from observational data. To learn the model parameters
and the rule content, we design an expectation-maximization (EM) algorithm,
which treats the rule content as latent variables. The EM algorithm alternates
between the E-step and M-step: in the E-step, the posterior distribution over
the latent rule content is evaluated; in the M-step, the rule generator and
model parameters are jointly optimized by maximizing the current expected
log-likelihood. Our model may have a wide range of applications in areas such
as sports analytics, robotics, and autonomous cars, where understanding human
movements are essential. We demonstrate the model's superior interpretability
and prediction performance on pedestrian and NBA basketball player datasets,
both achieving promising results.
</p></li>
</ul>

<h3>Title: Interactive Molecular Discovery with Natural Language. (arXiv:2306.11976v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11976">http://arxiv.org/abs/2306.11976</a></li>
<li>Code URL: <a href="https://github.com/ellenzzn/chatmol">https://github.com/ellenzzn/chatmol</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11976] Interactive Molecular Discovery with Natural Language](http://arxiv.org/abs/2306.11976) #interpretability</code></li>
<li>Summary: <p>Natural language is expected to be a key medium for various human-machine
interactions in the era of large language models. When it comes to the
biochemistry field, a series of tasks around molecules (e.g., property
prediction, molecule mining, etc.) are of great significance while having a
high technical threshold. Bridging the molecule expressions in natural language
and chemical language can not only hugely improve the interpretability and
reduce the operation difficulty of these tasks, but also fuse the chemical
knowledge scattered in complementary materials for a deeper comprehension of
molecules. Based on these benefits, we propose the conversational molecular
design, a novel task adopting natural language for describing and editing
target molecules. To better accomplish this task, we design ChatMol, a
knowledgeable and versatile generative pre-trained model, enhanced by injecting
experimental property information, molecular spatial knowledge, and the
associations between natural and chemical languages into it. Several typical
solutions including large language models (e.g., ChatGPT) are evaluated,
proving the challenge of conversational molecular design and the effectiveness
of our knowledge enhancement method. Case observations and analysis are
conducted to provide directions for further exploration of natural-language
interaction in molecular discovery.
</p></li>
</ul>

<h3>Title: Feature Interactions Reveal Linguistic Structure in Language Models. (arXiv:2306.12181v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12181">http://arxiv.org/abs/2306.12181</a></li>
<li>Code URL: <a href="https://github.com/jumelet/fidam-eval">https://github.com/jumelet/fidam-eval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12181] Feature Interactions Reveal Linguistic Structure in Language Models](http://arxiv.org/abs/2306.12181) #interpretability</code></li>
<li>Summary: <p>We study feature interactions in the context of feature attribution methods
for post-hoc interpretability. In interpretability research, getting to grips
with feature interactions is increasingly recognised as an important challenge,
because interacting features are key to the success of neural networks. Feature
interactions allow a model to build up hierarchical representations for its
input, and might provide an ideal starting point for the investigation into
linguistic structure in language models. However, uncovering the exact role
that these interactions play is also difficult, and a diverse range of
interaction attribution methods has been proposed. In this paper, we focus on
the question which of these methods most faithfully reflects the inner workings
of the target models. We work out a grey box methodology, in which we train
models to perfection on a formal language classification task, using PCFGs. We
show that under specific configurations, some methods are indeed able to
uncover the grammatical rules acquired by a model. Based on these findings we
extend our evaluation to a case study on language models, providing novel
insights into the linguistic structure that these models have acquired.
</p></li>
</ul>

<h3>Title: ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data. (arXiv:2306.12330v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12330">http://arxiv.org/abs/2306.12330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12330] ProtoGate: Prototype-based Neural Networks with Local Feature Selection for Tabular Biomedical Data](http://arxiv.org/abs/2306.12330) #interpretability</code></li>
<li>Summary: <p>Tabular biomedical data poses challenges in machine learning because it is
often high-dimensional and typically low-sample-size. Previous research has
attempted to address these challenges via feature selection approaches, which
can lead to unstable performance on real-world data. This suggests that current
methods lack appropriate inductive biases that capture patterns common to
different samples. In this paper, we propose ProtoGate, a prototype-based
neural model that introduces an inductive bias by attending to both homogeneity
and heterogeneity across samples. ProtoGate selects features in a
global-to-local manner and leverages them to produce explainable predictions
via an interpretable prototype-based model. We conduct comprehensive
experiments to evaluate the performance of ProtoGate on synthetic and
real-world datasets. Our results show that exploiting the homogeneous and
heterogeneous patterns in the data can improve prediction accuracy while
prototypes imbue interpretability.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?. (arXiv:2306.11985v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11985">http://arxiv.org/abs/2306.11985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11985] Evaluation of Popular XAI Applied to Clinical Prediction Models: Can They be Trusted?](http://arxiv.org/abs/2306.11985) #explainability</code></li>
<li>Summary: <p>The absence of transparency and explainability hinders the clinical adoption
of Machine learning (ML) algorithms. Although various methods of explainable
artificial intelligence (XAI) have been suggested, there is a lack of
literature that delves into their practicality and assesses them based on
criteria that could foster trust in clinical environments. To address this gap
this study evaluates two popular XAI methods used for explaining predictive
models in the healthcare context in terms of whether they (i) generate
domain-appropriate representation, i.e. coherent with respect to the
application task, (ii) impact clinical workflow and (iii) are consistent. To
that end, explanations generated at the cohort and patient levels were
analysed. The paper reports the first benchmarking of the XAI methods applied
to risk prediction models obtained by evaluating the concordance between
generated explanations and the trigger of a future clinical deterioration
episode recorded by the data collection system. We carried out an analysis
using two Electronic Medical Records (EMR) datasets sourced from Australian
major hospitals. The findings underscore the limitations of state-of-the-art
XAI methods in the clinical context and their potential benefits. We discuss
these limitations and contribute to the theoretical development of trustworthy
XAI solutions where clinical decision support guides the choice of intervention
by suggesting the pattern or drivers for clinical deterioration in the future.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards. (arXiv:2306.11763v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11763">http://arxiv.org/abs/2306.11763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11763] Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards](http://arxiv.org/abs/2306.11763) #diffusion</code></li>
<li>Summary: <p>Deep object detection models have achieved notable successes in recent years,
but one major obstacle remains: the requirement for a large amount of training
data. Obtaining such data is a tedious process and is mainly time consuming,
leading to the exploration of new research avenues like synthetic data
generation techniques. In this study, we explore the usability of Stable
Diffusion 2.1-base for generating synthetic datasets of apple trees for object
detection and compare it to a baseline model trained on real-world data. After
creating a dataset of realistic apple trees with prompt engineering and
utilizing a previously trained Stable Diffusion model, the custom dataset was
annotated and evaluated by training a YOLOv5m object detection model to predict
apples in a real-world apple detection dataset. YOLOv5m was chosen for its
rapid inference time and minimal hardware demands. Results demonstrate that the
model trained on generated data is slightly underperforming compared to a
baseline model trained on real-world images when evaluated on a set of
real-world images. However, these findings remain highly promising, as the
average precision difference is only 0.09 and 0.06, respectively. Qualitative
results indicate that the model can accurately predict the location of apples,
except in cases of heavy shading. These findings illustrate the potential of
synthetic data generation techniques as a viable alternative to the collection
of extensive training data for object detection models.
</p></li>
</ul>

<h3>Title: Ambigram Generation by A Diffusion Model. (arXiv:2306.12049v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12049">http://arxiv.org/abs/2306.12049</a></li>
<li>Code URL: <a href="https://github.com/univ-esuty/ambifusion">https://github.com/univ-esuty/ambifusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12049] Ambigram Generation by A Diffusion Model](http://arxiv.org/abs/2306.12049) #diffusion</code></li>
<li>Summary: <p>Ambigrams are graphical letter designs that can be read not only from the
original direction but also from a rotated direction (especially with 180
degrees). Designing ambigrams is difficult even for human experts because
keeping their dual readability from both directions is often difficult. This
paper proposes an ambigram generation model. As its generation module, we use a
diffusion model, which has recently been used to generate high-quality
photographic images. By specifying a pair of letter classes, such as 'A' and
'B', the proposed model generates various ambigram images which can be read as
'A' from the original direction and 'B' from a direction rotated 180 degrees.
Quantitative and qualitative analyses of experimental results show that the
proposed model can generate high-quality and diverse ambigrams. In addition, we
define ambigramability, an objective measure of how easy it is to generate
ambigrams for each letter pair. For example, the pair of 'A' and 'V' shows a
high ambigramability (that is, it is easy to generate their ambigrams), and the
pair of 'D' and 'K' shows a lower ambigramability. The ambigramability gives
various hints of the ambigram generation not only for computers but also for
human experts. The code can be found at
(https://github.com/univ-esuty/ambifusion).
</p></li>
</ul>

<h3>Title: HSR-Diff:Hyperspectral Image Super-Resolution via Conditional Diffusion Models. (arXiv:2306.12085v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12085">http://arxiv.org/abs/2306.12085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12085] HSR-Diff:Hyperspectral Image Super-Resolution via Conditional Diffusion Models](http://arxiv.org/abs/2306.12085) #diffusion</code></li>
<li>Summary: <p>Despite the proven significance of hyperspectral images (HSIs) in performing
various computer vision tasks, its potential is adversely affected by the
low-resolution (LR) property in the spatial domain, resulting from multiple
physical factors. Inspired by recent advancements in deep generative models, we
propose an HSI Super-resolution (SR) approach with Conditional Diffusion Models
(HSR-Diff) that merges a high-resolution (HR) multispectral image (MSI) with
the corresponding LR-HSI. HSR-Diff generates an HR-HSI via repeated refinement,
in which the HR-HSI is initialized with pure Gaussian noise and iteratively
refined. At each iteration, the noise is removed with a Conditional Denoising
Transformer (CDF ormer) that is trained on denoising at different noise levels,
conditioned on the hierarchical feature maps of HR-MSI and LR-HSI. In addition,
a progressive learning strategy is employed to exploit the global information
of full-resolution images. Systematic experiments have been conducted on four
public datasets, demonstrating that HSR-Diff outperforms state-of-the-art
methods.
</p></li>
</ul>

<h3>Title: DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation. (arXiv:2306.12422v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12422">http://arxiv.org/abs/2306.12422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12422] DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation](http://arxiv.org/abs/2306.12422) #diffusion</code></li>
<li>Summary: <p>Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled text-to-3D content creation by optimizing a randomly
initialized Neural Radiance Fields (NeRF) with score distillation. However, the
resultant 3D models exhibit two limitations: (a) quality concerns such as
saturated color and the Janus problem; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
NeRF optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns NeRF optimization with the sampling process of diffusion model.
Extensive experiments show that our simple redesign significantly improves
text-to-3D content creation with higher quality and diversity.
</p></li>
</ul>

<h3>Title: Reward Shaping via Diffusion Process in Reinforcement Learning. (arXiv:2306.11885v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11885">http://arxiv.org/abs/2306.11885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11885] Reward Shaping via Diffusion Process in Reinforcement Learning](http://arxiv.org/abs/2306.11885) #diffusion</code></li>
<li>Summary: <p>Reinforcement Learning (RL) models have continually evolved to navigate the
exploration - exploitation trade-off in uncertain Markov Decision Processes
(MDPs). In this study, I leverage the principles of stochastic thermodynamics
and system dynamics to explore reward shaping via diffusion processes. This
provides an elegant framework as a way to think about exploration-exploitation
trade-off. This article sheds light on relationships between information
entropy, stochastic system dynamics, and their influences on entropy
production. This exploration allows us to construct a dual-pronged framework
that can be interpreted as either a maximum entropy program for deriving
efficient policies or a modified cost optimization program accounting for
informational costs and benefits. This work presents a novel perspective on the
physical nature of information and its implications for online learning in
MDPs, consequently providing a better understanding of information-oriented
formulations in RL.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents Challenge 2023. (arXiv:2306.11868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11868">http://arxiv.org/abs/2306.11868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11868] Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents Challenge 2023](http://arxiv.org/abs/2306.11868) #transformer</code></li>
<li>Summary: <p>This technical report presents our 1st place solution for the Waymo Open Sim
Agents Challenge (WOSAC) 2023. Our proposed MultiVerse Transformer for Agent
simulation (MVTA) effectively leverages transformer-based motion prediction
approaches, and is tailored for closed-loop simulation of agents. In order to
produce simulations with a high degree of realism, we design novel training and
sampling methods, and implement a receding horizon prediction mechanism. In
addition, we introduce a variable-length history aggregation method to mitigate
the compounding error that can arise during closed-loop autoregressive
execution. On the WOSAC, our MVTA and its enhanced version MVTE reach a realism
meta-metric of 0.5091 and 0.5168, respectively, outperforming all the other
methods on the leaderboard.
</p></li>
</ul>

<h3>Title: TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings. (arXiv:2306.11955v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11955">http://arxiv.org/abs/2306.11955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11955] TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings](http://arxiv.org/abs/2306.11955) #transformer</code></li>
<li>Summary: <p>Machine Learning (ML) models struggle with data that changes over time or
across domains due to factors such as noise, occlusion, illumination, or
frequency, unlike humans who can learn from such non independent and
identically distributed data. Consequently, a Continual Learning (CL) approach
is indispensable, particularly, Domain-Incremental Learning. In this paper, we
propose a novel pipeline for identifying tasks in domain-incremental learning
scenarios without supervision. The pipeline comprises four steps. First, we
obtain base embeddings from the raw data using an existing transformer-based
model. Second, we group the embedding densities based on their similarity to
obtain the nearest points to each cluster centroid. Third, we train an
incremental task classifier using only these few points. Finally, we leverage
the lightweight computational requirements of the pipeline to devise an
algorithm that decides in an online fashion when to learn a new task using the
task classifier and a drift detector. We conduct experiments using the SODA10M
real-world driving dataset and several CL strategies. We demonstrate that the
performance of these CL strategies with our pipeline can match the ground-truth
approach, both in classical experiments assuming task boundaries, and also in
more realistic task-agnostic scenarios that require detecting new tasks
on-the-fly
</p></li>
</ul>

<h3>Title: ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining. (arXiv:2306.12106v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12106">http://arxiv.org/abs/2306.12106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12106] ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining](http://arxiv.org/abs/2306.12106) #transformer</code></li>
<li>Summary: <p>Scene text removal (STR) aims at replacing text strokes in natural scenes
with visually coherent backgrounds. Recent STR approaches rely on iterative
refinements or explicit text masks, resulting in higher complexity and
sensitivity to the accuracy of text localization. Moreover, most existing STR
methods utilize convolutional neural networks (CNNs) for feature representation
while the potential of vision Transformers (ViTs) remains largely unexplored.
In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed
ViTEraser. Following a concise encoder-decoder framework, different types of
ViTs can be easily integrated into ViTEraser to enhance the long-range
dependencies and global reasoning. Specifically, the encoder hierarchically
maps the input image into the hidden space through ViT blocks and patch
embedding layers, while the decoder gradually upsamples the hidden features to
the text-erased image with ViT blocks and patch splitting layers. As ViTEraser
implicitly integrates text localization and inpainting, we propose a novel
end-to-end pretraining method, termed SegMIM, which focuses the encoder and
decoder on the text box segmentation and masked image modeling tasks,
respectively. To verify the effectiveness of the proposed methods, we
comprehensively explore the architecture, pretraining, and scalability of the
ViT-based encoder-decoder for STR, which provides deep insights into the
application of ViT to STR. Experimental results demonstrate that ViTEraser with
SegMIM achieves state-of-the-art performance on STR by a substantial margin.
Furthermore, the extended experiment on tampered scene text detection
demonstrates the generality of ViTEraser to other tasks. We believe this paper
can inspire more research on ViT-based STR approaches. Code will be available
at https://github.com/shannanyinxiang/ViTEraser.
</p></li>
</ul>

<h3>Title: Fast Segment Anything. (arXiv:2306.12156v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12156">http://arxiv.org/abs/2306.12156</a></li>
<li>Code URL: <a href="https://github.com/casia-iva-lab/fastsam">https://github.com/casia-iva-lab/fastsam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12156] Fast Segment Anything](http://arxiv.org/abs/2306.12156) #transformer</code></li>
<li>Summary: <p>The recently proposed segment anything model (SAM) has made a significant
influence in many computer vision tasks. It is becoming a foundation step for
many high-level tasks, like image segmentation, image caption, and image
editing. However, its huge computation costs prevent it from wider applications
in industry scenarios. The computation mainly comes from the Transformer
architecture at high-resolution inputs. In this paper, we propose a speed-up
alternative method for this fundamental task with comparable performance. By
reformulating the task as segments-generation and prompting, we find that a
regular CNN detector with an instance segmentation branch can also accomplish
this task well. Specifically, we convert this task to the well-studied instance
segmentation task and directly train the existing instance segmentation method
using only 1/50 of the SA-1B dataset published by SAM authors. With our method,
we achieve a comparable performance with the SAM method at 50 times higher
run-time speed. We give sufficient experimental results to demonstrate its
effectiveness. The codes and demos will be released at
https://github.com/CASIA-IVA-Lab/FastSAM.
</p></li>
</ul>

<h3>Title: Polygon Detection for Room Layout Estimation using Heterogeneous Graphs and Wireframes. (arXiv:2306.12203v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12203">http://arxiv.org/abs/2306.12203</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12203] Polygon Detection for Room Layout Estimation using Heterogeneous Graphs and Wireframes](http://arxiv.org/abs/2306.12203) #transformer</code></li>
<li>Summary: <p>This paper presents a neural network based semantic plane detection method
utilizing polygon representations. The method can for example be used to solve
room layout estimations tasks. The method is built on, combines and further
develops several different modules from previous research. The network takes an
RGB image and estimates a wireframe as well as a feature space using an
hourglass backbone. From these, line and junction features are sampled. The
lines and junctions are then represented as an undirected graph, from which
polygon representations of the sought planes are obtained. Two different
methods for this last step are investigated, where the most promising method is
built on a heterogeneous graph transformer. The final output is in all cases a
projection of the semantic planes in 2D. The methods are evaluated on the
Structured 3D dataset and we investigate the performance both using sampled and
estimated wireframes. The experiments show the potential of the graph-based
method by outperforming state of the art methods in Room Layout estimation in
the 2D metrics using synthetic wireframe detections.
</p></li>
</ul>

<h3>Title: Inter-Instance Similarity Modeling for Contrastive Learning. (arXiv:2306.12243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12243">http://arxiv.org/abs/2306.12243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12243] Inter-Instance Similarity Modeling for Contrastive Learning](http://arxiv.org/abs/2306.12243) #transformer</code></li>
<li>Summary: <p>The existing contrastive learning methods widely adopt one-hot instance
discrimination as pretext task for self-supervised learning, which inevitably
neglects rich inter-instance similarities among natural images, then leading to
potential representation degeneration. In this paper, we propose a novel image
mix method, PatchMix, for contrastive learning in Vision Transformer (ViT), to
model inter-instance similarities among images. Following the nature of ViT, we
randomly mix multiple images from mini-batch in patch level to construct mixed
image patch sequences for ViT. Compared to the existing sample mix methods, our
PatchMix can flexibly and efficiently mix more than two images and simulate
more complicated similarity relations among natural images. In this manner, our
contrastive framework can significantly reduce the gap between contrastive
objective and ground truth in reality. Experimental results demonstrate that
our proposed method significantly outperforms the previous state-of-the-art on
both ImageNet-1K and CIFAR datasets, e.g., 3.0% linear accuracy improvement on
ImageNet-1K and 8.7% kNN accuracy improvement on CIFAR100. Moreover, our method
achieves the leading transfer performance on downstream tasks, object detection
and instance segmentation on COCO dataset. The code is available at
https://github.com/visresearch/patchmix.
</p></li>
</ul>

<h3>Title: Wildfire Detection Via Transfer Learning: A Survey. (arXiv:2306.12276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12276">http://arxiv.org/abs/2306.12276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12276] Wildfire Detection Via Transfer Learning: A Survey](http://arxiv.org/abs/2306.12276) #transformer</code></li>
<li>Summary: <p>This paper surveys different publicly available neural network models used
for detecting wildfires using regular visible-range cameras which are placed on
hilltops or forest lookout towers. The neural network models are pre-trained on
ImageNet-1K and fine-tuned on a custom wildfire dataset. The performance of
these models is evaluated on a diverse set of wildfire images, and the survey
provides useful information for those interested in using transfer learning for
wildfire detection. Swin Transformer-tiny has the highest AUC value but
ConvNext-tiny detects all the wildfire events and has the lowest false alarm
rate in our dataset.
</p></li>
</ul>

<h3>Title: StarVQA+: Co-training Space-Time Attention for Video Quality Assessment. (arXiv:2306.12298v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12298">http://arxiv.org/abs/2306.12298</a></li>
<li>Code URL: <a href="https://github.com/gzhu-dvl/starvqaplus">https://github.com/gzhu-dvl/starvqaplus</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12298] StarVQA+: Co-training Space-Time Attention for Video Quality Assessment](http://arxiv.org/abs/2306.12298) #transformer</code></li>
<li>Summary: <p>Self-attention based Transformer has achieved great success in many computer
vision tasks. However, its application to video quality assessment (VQA) has
not been satisfactory so far. Evaluating the quality of in-the-wild videos is
challenging due to the unknown of pristine reference and shooting distortion.
This paper presents a co-trained Space-Time Attention network for the VQA
problem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately
concatenating the divided space-time attention. Then, to facilitate the
training of StarVQA+, we design a vectorized regression loss by encoding the
mean opinion score (MOS) to the probability vector and embedding a special
token as the learnable variable of MOS, leading to better fitting of human's
rating process. Finally, to solve the data hungry problem with Transformer, we
propose to co-train the spatial and temporal attention weights using both
images and videos. Various experiments are conducted on the de-facto
in-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k,
YouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate
the superiority of the proposed StarVQA+ over the state-of-the-art.
</p></li>
</ul>

<h3>Title: Retrieval-Based Transformer for Table Augmentation. (arXiv:2306.11843v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11843">http://arxiv.org/abs/2306.11843</a></li>
<li>Code URL: <a href="https://github.com/ibm/retrieval-table-augmentation">https://github.com/ibm/retrieval-table-augmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11843] Retrieval-Based Transformer for Table Augmentation](http://arxiv.org/abs/2306.11843) #transformer</code></li>
<li>Summary: <p>Data preparation, also called data wrangling, is considered one of the most
expensive and time-consuming steps when performing analytics or building
machine learning models. Preparing data typically involves collecting and
merging data from complex heterogeneous, and often large-scale data sources,
such as data lakes. In this paper, we introduce a novel approach toward
automatic data wrangling in an attempt to alleviate the effort of end-users,
e.g. data analysts, in structuring dynamic views from data lakes in the form of
tabular data. We aim to address table augmentation tasks, including row/column
population and data imputation. Given a corpus of tables, we propose a
retrieval augmented self-trained transformer model. Our self-learning strategy
consists in randomly ablating tables from the corpus and training the
retrieval-based model to reconstruct the original values or headers given the
partial tables as input. We adopt this strategy to first train the dense neural
retrieval model encoding table-parts to vectors, and then the end-to-end model
trained to perform table augmentation tasks. We test on EntiTables, the
standard benchmark for table augmentation, as well as introduce a new benchmark
to advance further research: WebTables. Our model consistently and
substantially outperforms both supervised statistical methods and the current
state-of-the-art transformer-based models.
</p></li>
</ul>

<h3>Title: Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12198">http://arxiv.org/abs/2306.12198</a></li>
<li>Code URL: <a href="https://github.com/balloutai/attention-analysis">https://github.com/balloutai/attention-analysis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12198] Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks](http://arxiv.org/abs/2306.12198) #transformer</code></li>
<li>Summary: <p>Investigating deep learning language models has always been a significant
research area due to the ``black box" nature of most advanced models. With the
recent advancements in pre-trained language models based on transformers and
their increasing integration into daily life, addressing this issue has become
more pressing. In order to achieve an explainable AI model, it is essential to
comprehend the procedural steps involved and compare them with human thought
processes. Thus, in this paper, we use simple, well-understood non-language
tasks to explore these models' inner workings. Specifically, we apply a
pre-trained language model to constrained arithmetic problems with hierarchical
structure, to analyze their attention weight scores and hidden states. The
investigation reveals promising results, with the model addressing hierarchical
problems in a moderately structured manner, similar to human problem-solving
strategies. Additionally, by inspecting the attention weights layer by layer,
we uncover an unconventional finding that layer 10, rather than the model's
final layer, is the optimal layer to unfreeze for the least parameter-intensive
approach to fine-tune the model. We support these findings with entropy
analysis and token embeddings similarity analysis. The attention analysis
allows us to hypothesize that the model can generalize to longer sequences in
ListOps dataset, a conclusion later confirmed through testing on sequences
longer than those in the training set. Lastly, by utilizing a straightforward
task in which the model predicts the winner of a Tic Tac Toe game, we identify
limitations in attention analysis, particularly its inability to capture 2D
patterns.
</p></li>
</ul>

<h3>Title: Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12205">http://arxiv.org/abs/2306.12205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12205] Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI](http://arxiv.org/abs/2306.12205) #transformer</code></li>
<li>Summary: <p>Pre-trained language models have recently emerged as a powerful tool for
fine-tuning a variety of language tasks. Ideally, when models are pre-trained
on large amount of data, they are expected to gain implicit knowledge. In this
paper, we investigate the ability of pre-trained language models to generalize
to different non-language tasks. In particular, we test them on tasks from
different domains such as computer vision, reasoning on hierarchical data, and
protein fold prediction. The four pre-trained models that we used, T5, BART,
BERT, and GPT-2 achieve outstanding results. They all have similar performance
and they outperform transformers that are trained from scratch by a large
margin. For instance, pre-trained language models perform better on the Listops
dataset, with an average accuracy of 58.7\%, compared to transformers trained
from scratch, which have an average accuracy of 29.0\%. The significant
improvement demonstrated across three types of datasets suggests that
pre-training on language helps the models to acquire general knowledge,
bringing us a step closer to general AI. We also showed that reducing the
number of parameters in pre-trained language models does not have a great
impact as the performance drops slightly when using T5-Small instead of
T5-Base. In fact, when using only 2\% of the parameters, we achieved a great
improvement compared to training from scratch. Finally, in contrast to prior
work, we find out that using pre-trained embeddings for the input layer is
necessary to achieve the desired results.
</p></li>
</ul>

<h3>Title: Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12317">http://arxiv.org/abs/2306.12317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12317] Iterated Piecewise Affine (IPA) Approximation for Language Modeling](http://arxiv.org/abs/2306.12317) #transformer</code></li>
<li>Summary: <p>In this work, we demonstrate the application of a simple first-order Taylor
expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times
m}$ and utilize it in language modeling. To enhance the basic Taylor expansion,
we introduce iteration and piecewise modeling, leading us to name the algorithm
the Iterative Piecewise Affine (IPA) approximation. The final algorithm
exhibits interesting resemblances to the Transformers decoder architecture. By
comparing parameter arrangements in IPA and Transformers, we observe a
strikingly similar performance, with IPA outperforming Transformers by 1.5\% in
the next token prediction task with cross-entropy loss for smaller sequence
lengths.
</p></li>
</ul>

<h3>Title: Training Transformers with 4-bit Integers. (arXiv:2306.11987v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11987">http://arxiv.org/abs/2306.11987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11987] Training Transformers with 4-bit Integers](http://arxiv.org/abs/2306.11987) #transformer</code></li>
<li>Summary: <p>Quantizing the activation, weight, and gradient to 4-bit is promising to
accelerate neural network training. However, existing 4-bit training methods
require custom numerical formats which are not supported by contemporary
hardware. In this work, we propose a training method for transformers with all
matrix multiplications implemented with the INT4 arithmetic. Training with an
ultra-low INT4 precision is challenging. To achieve this, we carefully analyze
the specific structures of activation and gradients in transformers to propose
dedicated quantizers for them. For forward propagation, we identify the
challenge of outliers and propose a Hadamard quantizer to suppress the
outliers. For backpropagation, we leverage the structural sparsity of gradients
by proposing bit splitting and leverage score sampling techniques to quantize
gradients accurately. Our algorithm achieves competitive accuracy on a wide
range of tasks including natural language understanding, machine translation,
and image classification. Unlike previous 4-bit training methods, our algorithm
can be implemented on the current generation of GPUs. Our prototypical linear
operator implementation is up to 2.2 times faster than the FP16 counterparts
and speeds up the training by up to 35.1%.
</p></li>
</ul>

<h3>Title: EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12059">http://arxiv.org/abs/2306.12059</a></li>
<li>Code URL: <a href="https://github.com/atomicarchitects/equiformer_v2">https://github.com/atomicarchitects/equiformer_v2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12059] EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations](http://arxiv.org/abs/2306.12059) #transformer</code></li>
<li>Summary: <p>Equivariant Transformers such as Equiformer have demonstrated the efficacy of
applying Transformers to the domain of 3D atomistic systems. However, they are
still limited to small degrees of equivariant representations due to their
computational complexity. In this paper, we investigate whether these
architectures can scale well to higher degrees. Starting from Equiformer, we
first replace $SO(3)$ convolutions with eSCN convolutions to efficiently
incorporate higher-degree tensors. Then, to better leverage the power of higher
degrees, we propose three architectural improvements -- attention
re-normalization, separable $S^2$ activation and separable layer normalization.
Putting this all together, we propose EquiformerV2, which outperforms previous
state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on
forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and
$2\times$ reduction in DFT calculations needed for computing adsorption
energies.
</p></li>
</ul>

<h3>Title: Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12077">http://arxiv.org/abs/2306.12077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12077] Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers](http://arxiv.org/abs/2306.12077) #transformer</code></li>
<li>Summary: <p>We propose a method for learning dynamical systems from high-dimensional
empirical data that combines variational autoencoders and (spatio-)temporal
attention within a framework designed to enforce certain
scientifically-motivated invariances. We focus on the setting in which data are
available from multiple different instances of a system whose underlying
dynamical model is entirely unknown at the outset. The approach rests on a
separation into an instance-specific encoding (capturing initial conditions,
constants etc.) and a latent dynamics model that is itself universal across all
instances/realizations of the system. The separation is achieved in an
automated, data-driven manner and only empirical data are required as inputs to
the model. The approach allows effective inference of system behaviour at any
continuous time but does not require an explicit neural ODE formulation, which
makes it efficient and highly scalable. We study behaviour through simple
theoretical analyses and extensive experiments on synthetic and real-world
datasets. The latter investigate learning the dynamics of complex systems based
on finite data and show that the proposed approach can outperform
state-of-the-art neural-dynamical models. We study also more general inductive
bias in the context of transfer to data obtained under entirely novel system
interventions. Overall, our results provide a promising new framework for
efficiently learning dynamical models from heterogeneous data with potential
applications in a wide range of fields including physics, medicine, biology and
engineering.
</p></li>
</ul>

<h3>Title: What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12086">http://arxiv.org/abs/2306.12086</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12086] What Constitutes Good Contrastive Learning in Time-Series Forecasting?](http://arxiv.org/abs/2306.12086) #transformer</code></li>
<li>Summary: <p>In recent years, the introduction of self-supervised contrastive learning
(SSCL) has demonstrated remarkable improvements in representation learning
across various domains, including natural language processing and computer
vision. By leveraging the inherent benefits of self-supervision, SSCL enables
the pre-training of representation models using vast amounts of unlabeled data.
Despite these advances, there remains a significant gap in understanding the
impact of different SSCL strategies on time series forecasting performance, as
well as the specific benefits that SSCL can bring. This paper aims to address
these gaps by conducting a comprehensive analysis of the effectiveness of
various training variables, including different SSCL algorithms, learning
strategies, model architectures, and their interplay. Additionally, to gain
deeper insights into the improvements brought about by SSCL in the context of
time-series forecasting, a qualitative analysis of the empirical receptive
field is performed. Through our experiments, we demonstrate that the end-to-end
training of a Transformer model using the Mean Squared Error (MSE) loss and
SSCL emerges as the most effective approach in time series forecasting.
Notably, the incorporation of the contrastive objective enables the model to
prioritize more pertinent information for forecasting, such as scale and
periodic relationships. These findings contribute to a better understanding of
the benefits of SSCL in time series forecasting and provide valuable insights
for future research in this area.
</p></li>
</ul>

<h3>Title: Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift. (arXiv:2306.12306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12306">http://arxiv.org/abs/2306.12306</a></li>
<li>Code URL: <a href="https://github.com/bdl-authors/beyond-ensembles">https://github.com/bdl-authors/beyond-ensembles</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12306] Beyond Deep Ensembles -- A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift](http://arxiv.org/abs/2306.12306) #transformer</code></li>
<li>Summary: <p>Bayesian deep learning (BDL) is a promising approach to achieve
well-calibrated predictions on distribution-shifted data. Nevertheless, there
exists no large-scale survey that evaluates recent SOTA methods on diverse,
realistic, and challenging benchmark tasks in a systematic manner. To provide a
clear picture of the current state of BDL research, we evaluate modern BDL
algorithms on real-world datasets from the WILDS collection containing
challenging classification and regression tasks, with a focus on generalization
capability and calibration under distribution shift. We compare the algorithms
on a wide range of large, convolutional and transformer-based neural network
architectures. In particular, we investigate a signed version of the expected
calibration error that reveals whether the methods are over- or
under-confident, providing further insight into the behavior of the methods.
Further, we provide the first systematic evaluation of BDL for fine-tuning
large pre-trained models, where training from scratch is prohibitively
expensive. Finally, given the recent success of Deep Ensembles, we extend
popular single-mode posterior approximations to multiple modes by the use of
ensembles. While we find that ensembling single-mode approximations generally
improves the generalization capability and calibration of the models by a
significant margin, we also identify a failure mode of ensembles when
finetuning large transformer-based language models. In this setting,
variational inference based approaches such as last-layer Bayes By Backprop
outperform other methods in terms of accuracy by a large margin, while modern
approximate inference algorithms such as SWAG achieve the best calibration.
</p></li>
</ul>

<h3>Title: Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12384">http://arxiv.org/abs/2306.12384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12384] Probing the limit of hydrologic predictability with the Transformer network](http://arxiv.org/abs/2306.12384) #transformer</code></li>
<li>Summary: <p>For a number of years since its introduction to hydrology, recurrent neural
networks like long short-term memory (LSTM) have proven remarkably difficult to
surpass in terms of daily hydrograph metrics on known, comparable benchmarks.
Outside of hydrology, Transformers have now become the model of choice for
sequential prediction tasks, making it a curious architecture to investigate.
Here, we first show that a vanilla Transformer architecture is not competitive
against LSTM on the widely benchmarked CAMELS dataset, and lagged especially
for the high-flow metrics due to short-term processes. However, a
recurrence-free variant of Transformer can obtain mixed comparisons with LSTM,
producing the same Kling-Gupta efficiency coefficient (KGE), along with other
metrics. The lack of advantages for the Transformer is linked to the Markovian
nature of the hydrologic prediction problem. Similar to LSTM, the Transformer
can also merge multiple forcing dataset to improve model performance. While the
Transformer results are not higher than current state-of-the-art, we still
learned some valuable lessons: (1) the vanilla Transformer architecture is not
suitable for hydrologic modeling; (2) the proposed recurrence-free modification
can improve Transformer performance so future work can continue to test more of
such modifications; and (3) the prediction limits on the dataset should be
close to the current state-of-the-art model. As a non-recurrent model, the
Transformer may bear scale advantages for learning from bigger datasets and
storing knowledge. This work serves as a reference point for future
modifications of the model.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Chili Pepper Disease Diagnosis via Image Reconstruction Using GrabCut and Generative Adversarial Serial Autoencoder. (arXiv:2306.12057v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12057">http://arxiv.org/abs/2306.12057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12057] Chili Pepper Disease Diagnosis via Image Reconstruction Using GrabCut and Generative Adversarial Serial Autoencoder](http://arxiv.org/abs/2306.12057) #generative</code></li>
<li>Summary: <p>With the recent development of smart farms, researchers are very interested
in such fields. In particular, the field of disease diagnosis is the most
important factor. Disease diagnosis belongs to the field of anomaly detection
and aims to distinguish whether plants or fruits are normal or abnormal. The
problem can be solved by binary or multi-classification based on CNN, but it
can also be solved by image reconstruction. However, due to the limitation of
the performance of image generation, SOTA's methods propose a score calculation
method using a latent vector error. In this paper, we propose a network that
focuses on chili peppers and proceeds with background removal through Grabcut.
It shows high performance through image-based score calculation method. Due to
the difficulty of reconstructing the input image, the difference between the
input and output images is large. However, the serial autoencoder proposed in
this paper uses the difference between the two fake images except for the
actual input as a score. We propose a method of generating meaningful images
using the GAN structure and classifying three results simultaneously by one
discriminator. The proposed method showed higher performance than previous
researches, and image-based scores showed the best performanc
</p></li>
</ul>

<h3>Title: Open-Domain Text Evaluation via Meta Distribution Modeling. (arXiv:2306.11879v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11879">http://arxiv.org/abs/2306.11879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11879] Open-Domain Text Evaluation via Meta Distribution Modeling](http://arxiv.org/abs/2306.11879) #generative</code></li>
<li>Summary: <p>Recent advances in open-domain text generation models powered by large
pre-trained language models (LLMs) have achieved remarkable performance.
However, evaluating and controlling these models for desired attributes remains
a challenge, as traditional reference-based metrics such as BLEU, ROUGE, and
METEOR are insufficient for open-ended generation tasks. Similarly, while
trainable discriminator-based evaluation metrics show promise, obtaining
high-quality training data is a non-trivial task. In this paper, we introduce a
novel approach to evaluate open-domain generation - the Meta-Distribution
Methods (MDM). Drawing on the correlation between the rising parameter counts
and the improving performance of LLMs, MDM creates a mapping from the contrast
of two probabilistic distributions -- one known to be superior to the other --
to quality measures, which can be viewed as a distribution of distributions
i.e. Meta-Distribution. We investigate MDM for open-domain text generation
evaluation under two paradigms: 1) \emph{Generative} MDM, which leverages the
Meta-Distribution Methods to generate in-domain negative samples for training
discriminator-based metrics; 2) \emph{Discriminative} MDM, which directly uses
distribution discrepancies between two language models for evaluation. Our
experiments on multi-turn dialogue and factuality in abstractive summarization
demonstrate that MDMs correlate better with human judgment than existing
automatic evaluation metrics on both tasks, highlighting the strong performance
and generalizability of such methods.
</p></li>
</ul>

<h3>Title: A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing. (arXiv:2306.12018v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12018">http://arxiv.org/abs/2306.12018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12018] A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing](http://arxiv.org/abs/2306.12018) #generative</code></li>
<li>Summary: <p>Recent years have witnessed the impressive progress in Neural Dependency
Parsing. According to the different factorization approaches to the graph joint
probabilities, existing parsers can be roughly divided into autoregressive and
non-autoregressive patterns. The former means that the graph should be
factorized into multiple sequentially dependent components, then it can be
built up component by component. And the latter assumes these components to be
independent so that they can be outputted in a one-shot manner. However, when
treating the directed edge as an explicit dependency relationship, we discover
that there is a mixture of independent and interdependent components in the
dependency graph, signifying that both aforementioned models fail to precisely
capture the explicit dependencies among nodes and edges. Based on this
property, we design a Semi-Autoregressive Dependency Parser to generate
dependency graphs via adding node groups and edge groups autoregressively while
pouring out all group elements in parallel. The model gains a trade-off between
non-autoregression and autoregression, which respectively suffer from the lack
of target inter-dependencies and the uncertainty of graph generation orders.
The experiments show the proposed parser outperforms strong baselines on
Enhanced Universal Dependencies of multiple languages, especially achieving
$4\%$ average promotion at graph-level accuracy. Also, the performances of
model variations show the importance of specific parts.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue. (arXiv:2306.12174v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12174">http://arxiv.org/abs/2306.12174</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12174] OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue](http://arxiv.org/abs/2306.12174) #large language model</code></li>
<li>Summary: <p>Large multimodal language models (LMMs) have achieved significant success in
general domains. However, due to the significant differences between medical
images and text and general web content, the performance of LMMs in medical
scenarios is limited. In ophthalmology, clinical diagnosis relies on multiple
modalities of medical images, but unfortunately, multimodal ophthalmic large
language models have not been explored to date. In this paper, we study and
construct an ophthalmic large multimodal model. Firstly, we use fundus images
as an entry point to build a disease assessment and diagnosis pipeline to
achieve common ophthalmic disease diagnosis and lesion segmentation. Then, we
establish a new ophthalmic multimodal instruction-following and dialogue
fine-tuning dataset based on disease-related knowledge data and publicly
available real-world medical dialogue. We introduce visual ability into the
large language model to complete the ophthalmic large language and vision
assistant (OphGLM). Our experimental results demonstrate that the OphGLM model
performs exceptionally well, and it has the potential to revolutionize clinical
applications in ophthalmology. The dataset, code, and models will be made
publicly available at https://github.com/ML-AILab/OphGLM.
</p></li>
</ul>

<h3>Title: Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11816">http://arxiv.org/abs/2306.11816</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11816] Learning to Generate Better Than Your LLM](http://arxiv.org/abs/2306.11816) #large language model</code></li>
<li>Summary: <p>Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning Large Language Models (LLMs) for conditional text generation. In
particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent
conversations with users by incorporating RL and feedback from humans. Inspired
by learning-to-search algorithms and capitalizing on key properties of text
generation, we seek to investigate reinforcement learning algorithms beyond
general purpose algorithms such as Proximal policy optimization (PPO). In
particular, we extend RL algorithms to allow them to interact with a dynamic
black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a
suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive
review and CommonGen text generation task from the GRUE benchmark. We show that
our RL algorithms achieve higher performance than supervised learning (SL) and
default PPO baselines, demonstrating the benefit of interaction with the guide
LLM. On CommonGen, we not only outperform our SL baselines but also improve
beyond PPO across a variety of lexical and semantic metrics beyond the one we
optimized for. Notably, on the IMDB dataset, we show that our GPT-2 based
policy outperforms the zero-shot GPT-3 oracle, indicating that our algorithms
can learn from a powerful, black-box GPT-3 oracle with a simpler, cheaper, and
publicly available GPT-2 model while gaining performance.
</p></li>
</ul>

<h3>Title: On Compositionality and Improved Training of NADO. (arXiv:2306.11825v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11825">http://arxiv.org/abs/2306.11825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11825] On Compositionality and Improved Training of NADO](http://arxiv.org/abs/2306.11825) #large language model</code></li>
<li>Summary: <p>NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable
generation with large language models. Differentiating from finetuning/prompt
tuning, it has the potential to avoid catastrophic forgetting of the large base
model and achieve guaranteed convergence to an entropy-maximized closed-form
solution without significantly limiting the model capacity. Despite its
success, several challenges arise when applying NADO to more complex scenarios.
First, the best practice of using NADO for the composition of multiple control
signals is under-explored. Second, vanilla NADO suffers from gradient vanishing
for low-probability control signals and is highly reliant on the
forward-consistency regularization. In this paper, we study the aforementioned
challenges when using NADO theoretically and empirically. We show we can
achieve guaranteed compositional generalization of NADO with a certain
practice, and propose a novel alternative parameterization of NADO to perfectly
guarantee the forward-consistency. We evaluate the improved training of NADO,
i.e. NADO++, on CommonGen. Results show that NADO++ improves the effectiveness
of the algorithm in multiple aspects.
</p></li>
</ul>

<h3>Title: Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications. (arXiv:2306.11892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11892">http://arxiv.org/abs/2306.11892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11892] Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications](http://arxiv.org/abs/2306.11892) #large language model</code></li>
<li>Summary: <p>This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications.
</p></li>
</ul>

<h3>Title: Limits for Learning with Language Models. (arXiv:2306.12213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12213">http://arxiv.org/abs/2306.12213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12213] Limits for Learning with Language Models](http://arxiv.org/abs/2306.12213) #large language model</code></li>
<li>Summary: <p>With the advent of large language models (LLMs), the trend in NLP has been to
train LLMs on vast amounts of data to solve diverse language understanding and
generation tasks. The list of LLM successes is long and varied. Nevertheless,
several recent papers provide empirical evidence that LLMs fail to capture
important aspects of linguistic meaning. Focusing on universal quantification,
we provide a theoretical foundation for these empirical findings by proving
that LLMs cannot learn certain fundamental semantic properties including
semantic entailment and consistency as they are defined in formal semantics.
More generally, we show that LLMs are unable to learn concepts beyond the first
level of the Borel Hierarchy, which imposes severe limits on the ability of
LMs, both large and small, to capture many aspects of linguistic meaning. This
means that LLMs will continue to operate without formal guarantees on tasks
that require entailments and deep linguistic understanding.
</p></li>
</ul>

<h3>Title: Solving and Generating NPR Sunday Puzzles with Large Language Models. (arXiv:2306.12255v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12255">http://arxiv.org/abs/2306.12255</a></li>
<li>Code URL: <a href="https://github.com/wellesley-easel-lab/puzzleqa">https://github.com/wellesley-easel-lab/puzzleqa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12255] Solving and Generating NPR Sunday Puzzles with Large Language Models](http://arxiv.org/abs/2306.12255) #large language model</code></li>
<li>Summary: <p>We explore the ability of large language models to solve and generate puzzles
from the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15
years of on-air puzzles. We evaluate four large language models using PUZZLEQA,
in both multiple choice and free response formats, and explore two prompt
engineering techniques to improve free response performance: chain-of-thought
reasoning and prompt summarization. We find that state-of-the-art large
language models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,
achieves 50.2% loose accuracy. However, in our few-shot puzzle generation
experiment, we find no evidence that models can generate puzzles: GPT-3.5
generates puzzles with answers that do not conform to the generated rules.
Puzzle generation remains a challenging task for future work.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation. (arXiv:2306.11734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11734">http://arxiv.org/abs/2306.11734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11734] Few-Shot Rotation-Invariant Aerial Image Semantic Segmentation](http://arxiv.org/abs/2306.11734) #segmentation</code></li>
<li>Summary: <p>Few-shot aerial image segmentation is a challenging task that involves
precisely parsing objects in query aerial images with limited annotated
support. Conventional matching methods without consideration of varying object
orientations can fail to activate same-category objects with different
orientations. Moreover, conventional algorithms can lead to false recognition
of lower-scored rotated semantic objects. In response to these challenges, the
authors propose a novel few-shot rotation-invariant aerial semantic
segmentation network (FRINet). FRINet matches each query feature
rotation-adaptively with orientation-varying yet category-consistent support
information. The segmentation predictions from different orientations are
supervised by the same label, and the backbones are pre-trained in the base
category to boost segmentation performance. Experimental results demonstrate
that FRINet achieves state-of-the-art performance in few-shot aerial semantic
segmentation benchmark.
</p></li>
</ul>

<h3>Title: MultiEarth 2023 Deforestation Challenge -- Team FOREVER. (arXiv:2306.11762v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11762">http://arxiv.org/abs/2306.11762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11762] MultiEarth 2023 Deforestation Challenge -- Team FOREVER](http://arxiv.org/abs/2306.11762) #segmentation</code></li>
<li>Summary: <p>It is important problem to accurately estimate deforestation of satellite
imagery since this approach can analyse extensive area without direct human
access. However, it is not simple problem because of difficulty in observing
the clear ground surface due to extensive cloud cover during long rainy season.
In this paper, we present a multi-view learning strategy to predict
deforestation status in the Amazon rainforest area with latest deep neural
network models. Multi-modal dataset consists of three types of different
satellites imagery, Sentinel-1, Sentinel-2 and Landsat 8 is utilized to train
and predict deforestation status. MMsegmentation framework is selected to apply
comprehensive data augmentation and diverse networks. The proposed method
effectively and accurately predicts the deforestation status of new queries.
</p></li>
</ul>

<h3>Title: Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology. (arXiv:2306.11848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11848">http://arxiv.org/abs/2306.11848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11848] Using super-resolution for enhancing visual perception and segmentation performance in veterinary cytology](http://arxiv.org/abs/2306.11848) #segmentation</code></li>
<li>Summary: <p>The primary objective of this research was to enhance the quality of semantic
segmentation in cytology images by incorporating super-resolution (SR)
architectures. An additional contribution was the development of a novel
dataset aimed at improving imaging quality in the presence of inaccurate focus.
Our experimental results demonstrate that the integration of SR techniques into
the segmentation pipeline can lead to a significant improvement of up to 25% in
the mean average precision (mAP) segmentation metric. These findings suggest
that leveraging SR architectures holds great promise for advancing the state of
the art in cytology image analysis.
</p></li>
</ul>

<h3>Title: LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching. (arXiv:2306.11925v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11925">http://arxiv.org/abs/2306.11925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11925] LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching](http://arxiv.org/abs/2306.11925) #segmentation</code></li>
<li>Summary: <p>Obtaining large pre-trained models that can be fine-tuned to new tasks with
limited annotated samples has remained an open challenge for medical imaging
data. While pre-trained deep networks on ImageNet and vision-language
foundation models trained on web-scale data are prevailing approaches, their
effectiveness on medical tasks is limited due to the significant domain shift
between natural and medical images. To bridge this gap, we introduce LVM-Med,
the first family of deep networks trained on large-scale medical datasets. We
have collected approximately 1.3 million medical images from 55 publicly
available datasets, covering a large number of organs and modalities such as
CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art
self-supervised algorithms on this dataset and propose a novel self-supervised
contrastive learning algorithm using a graph-matching formulation. The proposed
approach makes three contributions: (i) it integrates prior pair-wise image
similarity metrics based on local and global information; (ii) it captures the
structural constraints of feature embeddings through a loss function
constructed via a combinatorial graph-matching objective; and (iii) it can be
trained efficiently end-to-end using modern gradient-estimation techniques for
black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream
medical tasks ranging from segmentation and classification to object detection,
and both for the in and out-of-distribution settings. LVM-Med empirically
outperforms a number of state-of-the-art supervised, self-supervised, and
foundation models. For challenging tasks such as Brain Tumor Classification or
Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models
trained on 1 billion masks by 6-7% while using only a ResNet-50.
</p></li>
</ul>

<h3>Title: Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering. (arXiv:2306.12048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12048">http://arxiv.org/abs/2306.12048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12048] Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering](http://arxiv.org/abs/2306.12048) #segmentation</code></li>
<li>Summary: <p>Online unsupervised video object segmentation (UVOS) uses the previous frames
as its input to automatically separate the primary object(s) from a streaming
video without using any further manual annotation. A major challenge is that
the model has no access to the future and must rely solely on the history,
i.e., the segmentation mask is predicted from the current frame as soon as it
is captured. In this work, a novel contrastive motion clustering algorithm with
an optical flow as its input is proposed for the online UVOS by exploiting the
common fate principle that visual elements tend to be perceived as a group if
they possess the same motion pattern. We build a simple and effective
auto-encoder to iteratively summarize non-learnable prototypical bases for the
motion pattern, while the bases in turn help learn the representation of the
embedding network. Further, a contrastive learning strategy based on a boundary
prior is developed to improve foreground and background feature discrimination
in the representation learning stage. The proposed algorithm can be optimized
on arbitrarily-scale data i.e., frame, clip, dataset) and performed in an
online fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,
and $\textit{SegTrackV2}$ datasets show that the accuracy of our method
surpasses the previous state-of-the-art (SoTA) online UVOS method by a margin
of 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deep
subspace clustering to tackle the motion grouping, our method is able to
achieve higher accuracy at $3\times$ faster inference time compared to SoTA
online UVOS method, and making a good trade-off between effectiveness and
efficiency.
</p></li>
</ul>

<h3>Title: Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario. (arXiv:2306.12152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12152">http://arxiv.org/abs/2306.12152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12152] Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario](http://arxiv.org/abs/2306.12152) #segmentation</code></li>
<li>Summary: <p>In this paper, we tackle the problem of Egocentric Human-Object Interaction
(EHOI) detection in an industrial setting. To overcome the lack of public
datasets in this context, we propose a pipeline and a tool for generating
synthetic images of EHOIs paired with several annotations and data signals
(e.g., depth maps or instance segmentation masks). Using the proposed pipeline,
we present EgoISM-HOI a new multimodal dataset composed of synthetic EHOI
images in an industrial environment with rich annotations of hands and objects.
To demonstrate the utility and effectiveness of synthetic EHOI data produced by
the proposed tool, we designed a new method that predicts and combines
different multimodal signals to detect EHOIs in RGB images. Our study shows
that exploiting synthetic data to pre-train the proposed method significantly
improves performance when tested on real-world data. Moreover, the proposed
approach outperforms state-of-the-art class-agnostic methods. To support
research in this field, we publicly release the datasets, source code, and
pre-trained models at https://iplab.dmi.unict.it/egoism-hoi.
</p></li>
</ul>

<h3>Title: Joint Dense-Point Representation for Contour-Aware Graph Segmentation. (arXiv:2306.12155v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12155">http://arxiv.org/abs/2306.12155</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12155] Joint Dense-Point Representation for Contour-Aware Graph Segmentation](http://arxiv.org/abs/2306.12155) #segmentation</code></li>
<li>Summary: <p>We present a novel methodology that combines graph and dense segmentation
techniques by jointly learning both point and pixel contour representations,
thereby leveraging the benefits of each approach. This addresses deficiencies
in typical graph segmentation methods where misaligned objectives restrict the
network from learning discriminative vertex and contour features. Our joint
learning strategy allows for rich and diverse semantic features to be encoded,
while alleviating common contour stability issues in dense-based approaches,
where pixel-level objectives can lead to anatomically implausible topologies.
In addition, we identify scenarios where correct predictions that fall on the
contour boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accuracy against
a variety of dense- and point-based methods. Our source code is freely
available at: www.github.com/kitbransby/Joint_Graph_Segmentation
</p></li>
</ul>

<h3>Title: Multi-Task Consistency for Active Learning. (arXiv:2306.12398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.12398">http://arxiv.org/abs/2306.12398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.12398] Multi-Task Consistency for Active Learning](http://arxiv.org/abs/2306.12398) #segmentation</code></li>
<li>Summary: <p>Learning-based solutions for vision tasks require a large amount of labeled
training data to ensure their performance and reliability. In single-task
vision-based settings, inconsistency-based active learning has proven to be
effective in selecting informative samples for annotation. However, there is a
lack of research exploiting the inconsistency between multiple tasks in
multi-task networks. To address this gap, we propose a novel multi-task active
learning strategy for two coupled vision tasks: object detection and semantic
segmentation. Our approach leverages the inconsistency between them to identify
informative samples across both tasks. We propose three constraints that
specify how the tasks are coupled and introduce a method for determining the
pixels belonging to the object detected by a bounding box, to later quantify
the constraints as inconsistency scores. To evaluate the effectiveness of our
approach, we establish multiple baselines for multi-task active learning and
introduce a new metric, mean Detection Segmentation Quality (mDSQ), tailored
for the multi-task active learning comparison that addresses the performance of
both tasks. We conduct extensive experiments on the nuImages and A9 datasets,
demonstrating that our approach outperforms existing state-of-the-art methods
by up to 3.4% mDSQ on nuImages. Our approach achieves 95% of the fully-trained
performance using only 67% of the available data, corresponding to 20% fewer
labels compared to random selection and 5% fewer labels compared to
state-of-the-art selection strategy. Our code will be made publicly available
after the review process.
</p></li>
</ul>

<h3>Title: No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths. (arXiv:2306.11922v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.11922">http://arxiv.org/abs/2306.11922</a></li>
<li>Code URL: <a href="https://github.com/hiroki11x/losslandscapegeometry">https://github.com/hiroki11x/losslandscapegeometry</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.11922] No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths](http://arxiv.org/abs/2306.11922) #segmentation</code></li>
<li>Summary: <p>Understanding the optimization dynamics of neural networks is necessary for
closing the gap between theory and practice. Stochastic first-order
optimization algorithms are known to efficiently locate favorable minima in
deep neural networks. This efficiency, however, contrasts with the non-convex
and seemingly complex structure of neural loss landscapes. In this study, we
delve into the fundamental geometric properties of sampled gradients along
optimization paths. We focus on two key quantities, which appear in the
restricted secant inequality and error bound. Both hold high significance for
first-order optimization. Our analysis reveals that these quantities exhibit
predictable, consistent behavior throughout training, despite the stochasticity
induced by sampling minibatches. Our findings suggest that not only do
optimization trajectories never encounter significant obstacles, but they also
maintain stable dynamics during the majority of training. These observed
properties are sufficiently expressive to theoretically guarantee linear
convergence and prescribe learning rate schedules mirroring empirical
practices. We conduct our experiments on image classification, semantic
segmentation and language modeling across different batch sizes, network
architectures, datasets, optimizers, and initialization seeds. We discuss the
impact of each factor. Our work provides novel insights into the properties of
neural network loss functions, and opens the door to theoretical frameworks
more relevant to prevalent practice.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
