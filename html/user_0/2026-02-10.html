<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-10</h1>
<h3>Title: BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents</h3>
<ul>
<li><strong>Authors: </strong>R. James Cotton, Thomas Leonard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06975">https://arxiv.org/abs/2602.06975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06975">https://arxiv.org/pdf/2602.06975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06975]] BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents(https://arxiv.org/abs/2602.06975)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.</li>
</ul>

<h3>Title: Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Chen Shen, Wei Cheng, Jingyue Yang, Huan Zhang, Yuhan Wu, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06976">https://arxiv.org/abs/2602.06976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06976">https://arxiv.org/pdf/2602.06976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06976]] Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks(https://arxiv.org/abs/2602.06976)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.</li>
</ul>

<h3>Title: Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts</h3>
<ul>
<li><strong>Authors: </strong>Shashank</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06993">https://arxiv.org/abs/2602.06993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06993">https://arxiv.org/pdf/2602.06993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06993]] Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts(https://arxiv.org/abs/2602.06993)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights. We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface. This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.</li>
</ul>

<h3>Title: MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zhuonan Wang, Zhenxuan Fan, Siwen Tan, Yu Zhong, Yuqian Yuan, Haoyuan Li, Hao Jiang, Wenqiao Zhang, Feifei Shao, Hongwei Wang, Jun Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07011">https://arxiv.org/abs/2602.07011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07011">https://arxiv.org/pdf/2602.07011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07011]] MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation(https://arxiv.org/abs/2602.07011)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.</li>
</ul>

<h3>Title: A General Model for Retinal Segmentation and Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zhonghua Wang, Lie Ju, Sijia Li, Wei Feng, Sijin Zhou, Ming Hu, Jianhao Xiong, Xiaoying Tang, Yifan Peng, Mingquan Lin, Yaodong Ding, Yong Zeng, Wenbin Wei, Li Dong, Zongyuan Ge</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07012">https://arxiv.org/abs/2602.07012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07012">https://arxiv.org/pdf/2602.07012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07012]] A General Model for Retinal Segmentation and Quantification(https://arxiv.org/abs/2602.07012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.</li>
</ul>

<h3>Title: Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Yang, Shicheng Liu, Yuchen Yang, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07013">https://arxiv.org/abs/2602.07013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07013">https://arxiv.org/pdf/2602.07013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07013]] Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models(https://arxiv.org/abs/2602.07013)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.</li>
</ul>

<h3>Title: Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Wu, Yuxuan Han, Haijun Li, Zhao Xu, Jianshan Zhao, Xu Jin, Longyue Wang, Weihua Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07014">https://arxiv.org/abs/2602.07014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07014">https://arxiv.org/pdf/2602.07014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07014]] Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation(https://arxiv.org/abs/2602.07014)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.</li>
</ul>

<h3>Title: Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach</h3>
<ul>
<li><strong>Authors: </strong>Subreena, Mohammad Amzad Hossain, Mirza Raquib, Saydul Akbar Murad, Farida Siddiqi Prity, Muhammad Hanif, Nick Rahimi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07015">https://arxiv.org/abs/2602.07015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07015">https://arxiv.org/pdf/2602.07015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07015]] Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach(https://arxiv.org/abs/2602.07015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.</li>
</ul>

<h3>Title: Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Mostafa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07016">https://arxiv.org/abs/2602.07016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07016">https://arxiv.org/pdf/2602.07016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07016]] Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency(https://arxiv.org/abs/2602.07016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.</li>
</ul>

<h3>Title: XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Thuraya Alzubaidi, Sana Ammar, Maryam Alsharqi, Islem Rekik, Muzammil Behzad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07017">https://arxiv.org/abs/2602.07017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07017">https://arxiv.org/pdf/2602.07017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07017]] XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models(https://arxiv.org/abs/2602.07017)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.</li>
</ul>

<h3>Title: Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Yu, Yi Xin, Wenjie Zhang, Chonghan Liu, Hanzhen Zhao, Xiaoxing Hu, Xinlei Yu, Ziyue Qiao, Hao Tang, Xue Yang, Xiaobin Hu, Chengwei Qin, Hui Xiong, Yu Qiao, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07026">https://arxiv.org/abs/2602.07026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07026">https://arxiv.org/pdf/2602.07026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07026]] Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models(https://arxiv.org/abs/2602.07026)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.</li>
</ul>

<h3>Title: Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sanggeon Yun, Ryozo Masukawa, SungHeon Jeong, Wenjun Huang, Hanning Chen, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07027">https://arxiv.org/abs/2602.07027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07027">https://arxiv.org/pdf/2602.07027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07027]] Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models(https://arxiv.org/abs/2602.07027)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.</li>
</ul>

<h3>Title: A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures</h3>
<ul>
<li><strong>Authors: </strong>Kaaustaaub Shankar, Bharadwaj Dogga, Kelly Cohen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07028">https://arxiv.org/abs/2602.07028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07028">https://arxiv.org/pdf/2602.07028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07028]] A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures(https://arxiv.org/abs/2602.07028)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.</li>
</ul>

<h3>Title: Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Young Jin Ahn, Yiyang Du, Zheyuan Zhang, Haisen Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07030">https://arxiv.org/abs/2602.07030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07030">https://arxiv.org/pdf/2602.07030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07030]] Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model(https://arxiv.org/abs/2602.07030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.</li>
</ul>

<h3>Title: Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis</h3>
<ul>
<li><strong>Authors: </strong>Dong Li (1), Shuai Huang (2), Yapeng Cao (3), Yujun Cui (4), Xiaobin Wei (5), Hongtao Cao (6) ((1) Department of Civil, Environmental, and Infrastructure Engineering, George Mason University, Fairfax, VA, USA (2) National Institute of Natural Hazards, Ministry of Emergency Management, Beijing, China (3) State Key Laboratory of Cryospheric Science and Frozen Soil Engineering, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, China, Navier Laboratory, École Nationale des Ponts et Chaussées, Marne-la-Vallée Cedex 2, France (4) Navier Laboratory, École Nationale des Ponts et Chaussées, Marne-la-Vallée Cedex 2, France (5) School of Civil Engineering, Hebei University of Engineering, Handan, China (6) College of Civil Engineering, Zhejiang University of Technology, Hangzhou, China)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07031">https://arxiv.org/abs/2602.07031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07031">https://arxiv.org/pdf/2602.07031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07031]] Lagged backward-compatible physics-informed neural networks for unsaturated soil consolidation analysis(https://arxiv.org/abs/2602.07031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This study develops a Lagged Backward-Compatible Physics-Informed Neural Network (LBC-PINN) for simulating and inverting one-dimensional unsaturated soil consolidation under long-term loading. To address the challenges of coupled air and water pressure dissipation across multi-scale time domains, the framework integrates logarithmic time segmentation, lagged compatibility loss enforcement, and segment-wise transfer learning. In forward analysis, the LBC-PINN with recommended segmentation schemes accurately predicts pore air and pore water pressure evolution. Model predictions are validated against finite element method (FEM) results, with mean absolute errors below 1e-2 for time durations up to 1e10 seconds. A simplified segmentation strategy based on the characteristic air-phase dissipation time improves computational efficiency while preserving predictive accuracy. Sensitivity analyses confirm the robustness of the framework across air-to-water permeability ratios ranging from 1e-3 to 1e3.</li>
</ul>

<h3>Title: TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Md Shahriar Kabir, Sana Alamgeer, Minakshi Debnath, Anne H. H. Ngu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07033">https://arxiv.org/abs/2602.07033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07033">https://arxiv.org/pdf/2602.07033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07033]] TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare(https://arxiv.org/abs/2602.07033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.</li>
</ul>

<h3>Title: UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ji, Zhipeng Xu, Zhenghao Liu, Zulong Chen, Qian Zhang, Zhibo Yang, Junyang Lin, Yu Gu, Ge Yu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07038">https://arxiv.org/abs/2602.07038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07038">https://arxiv.org/pdf/2602.07038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07038]] UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents(https://arxiv.org/abs/2602.07038)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at this https URL.</li>
</ul>

<h3>Title: COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Magesh Rajasekaran, Md Saiful Islam Sajol, Frej Berglind, Supratik Mukhopadhyay, Kamalika Das</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07042">https://arxiv.org/abs/2602.07042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07042">https://arxiv.org/pdf/2602.07042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07042]] COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification(https://arxiv.org/abs/2602.07042)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.</li>
</ul>

<h3>Title: PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Qu, Songxiao Yang, Haolin Wang, Huadong Song, Xiaoting Guo, Wenguang Hu, Guanlin Liu, Honghe Chen, Yafei Ou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07044">https://arxiv.org/abs/2602.07044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07044">https://arxiv.org/pdf/2602.07044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07044]] PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging(https://arxiv.org/abs/2602.07044)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.</li>
</ul>

<h3>Title: VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Luo, Di Wang, Haonan Guo, Jing Zhang, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07045">https://arxiv.org/abs/2602.07045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07045">https://arxiv.org/pdf/2602.07045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07045]] VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing(https://arxiv.org/abs/2602.07045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.</li>
</ul>

<h3>Title: ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Rashid, Elvio G. Amparore, Enrico Ferrari, Damiano Verda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07047">https://arxiv.org/abs/2602.07047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07047">https://arxiv.org/pdf/2602.07047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07047]] ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees(https://arxiv.org/abs/2602.07047)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.</li>
</ul>

<h3>Title: Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead</h3>
<ul>
<li><strong>Authors: </strong>Jindong Li, Dario Zanca, Vincent Christlein, Tim Hamann, Jens Barth, Peter Kämpf, Björn Eskofier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07049">https://arxiv.org/abs/2602.07049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07049">https://arxiv.org/pdf/2602.07049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07049]] Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead(https://arxiv.org/abs/2602.07049)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.</li>
</ul>

<h3>Title: Interpreting Physics in Video World Models</h3>
<ul>
<li><strong>Authors: </strong>Sonia Joseph, Quentin Garrido, Randall Balestriero, Matthew Kowal, Thomas Fel, Shahab Bakhtiari, Blake Richards, Mike Rabbat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07050">https://arxiv.org/abs/2602.07050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07050">https://arxiv.org/pdf/2602.07050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07050]] Interpreting Physics in Video World Models(https://arxiv.org/abs/2602.07050)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers. Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.</li>
</ul>

<h3>Title: Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Karthik Sivakoti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07051">https://arxiv.org/abs/2602.07051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07051">https://arxiv.org/pdf/2602.07051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07051]] Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning(https://arxiv.org/abs/2602.07051)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.</li>
</ul>

<h3>Title: AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Chaubey, Jiacheng Pang, Maksim Siniukov, Mohammad Soleymani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07054">https://arxiv.org/abs/2602.07054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07054">https://arxiv.org/pdf/2602.07054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07054]] AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization(https://arxiv.org/abs/2602.07054)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at this https URL.</li>
</ul>

<h3>Title: RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything</h3>
<ul>
<li><strong>Authors: </strong>Di Mo, Mingyang Sun, Chengxiu Yin, Runjia Tian, Yanhong Wu, Liyan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07057">https://arxiv.org/abs/2602.07057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07057">https://arxiv.org/pdf/2602.07057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07057]] RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything(https://arxiv.org/abs/2602.07057)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.</li>
</ul>

<h3>Title: FADE: Selective Forgetting via Sparse LoRA and Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Carolina R. Kelsch, Leonardo S. B. Pereira, Natnael Mola, Luis H. Arribas, Juan C. S. M. Avedillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07058">https://arxiv.org/abs/2602.07058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07058">https://arxiv.org/pdf/2602.07058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07058]] FADE: Selective Forgetting via Sparse LoRA and Self-Distillation(https://arxiv.org/abs/2602.07058)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.</li>
</ul>

<h3>Title: TACIT: Transformation-Aware Capturing of Implicit Thought</h3>
<ul>
<li><strong>Authors: </strong>Daniel Nobrega</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07061">https://arxiv.org/abs/2602.07061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07061">https://arxiv.org/pdf/2602.07061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07061]] TACIT: Transformation-Aware Capturing of Implicit Thought(https://arxiv.org/abs/2602.07061)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present TACIT (Transformation-Aware Capturing of Implicit Thought), a diffusion-based transformer for interpretable visual reasoning. Unlike language-based reasoning systems, TACIT operates entirely in pixel space using rectified flow, enabling direct visualization of the reasoning process at each inference step. We demonstrate the approach on maze-solving, where the model learns to transform images of unsolved mazes into solutions. Key results on 1 million synthetic maze pairs include: - 192x reduction in training loss over 100 epochs - 22.7x improvement in L2 distance to ground truth - Only 10 Euler steps required (vs. 100-1000 for typical diffusion models) Quantitative analysis reveals a striking phase transition phenomenon: the solution remains invisible for 68% of the transformation (zero recall), then emerges abruptly at t=0.70 within just 2% of the process. Most remarkably, 100% of samples exhibit simultaneous emergence across all spatial regions, ruling out sequential path construction and providing evidence for holistic rather than algorithmic reasoning. This "eureka moment" pattern -- long incubation followed by sudden crystallization -- parallels insight phenomena in human cognition. The pixel-space design with noise-free flow matching provides a foundation for understanding how neural networks develop implicit reasoning strategies that operate below and before language.</li>
</ul>

<h3>Title: Video-based Music Generation</h3>
<ul>
<li><strong>Authors: </strong>Serkan Sulun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07063">https://arxiv.org/abs/2602.07063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07063">https://arxiv.org/pdf/2602.07063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07063]] Video-based Music Generation(https://arxiv.org/abs/2602.07063)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>As the volume of video content on the internet grows rapidly, finding a suitable soundtrack remains a significant challenge. This thesis presents EMSYNC (EMotion and SYNChronization), a fast, free, and automatic solution that generates music tailored to the input video, enabling content creators to enhance their productions without composing or licensing music. Our model creates music that is emotionally and rhythmically synchronized with the video. A core component of EMSYNC is a novel video emotion classifier. By leveraging pretrained deep neural networks for feature extraction and keeping them frozen while training only fusion layers, we reduce computational complexity while improving accuracy. We show the generalization abilities of our method by obtaining state-of-the-art results on Ekman-6 and MovieNet. Another key contribution is a large-scale, emotion-labeled MIDI dataset for affective music generation. We then present an emotion-based MIDI generator, the first to condition on continuous emotional values rather than discrete categories, enabling nuanced music generation aligned with complex emotional content. To enhance temporal synchronization, we introduce a novel temporal boundary conditioning method, called "boundary offset encodings," aligning musical chords with scene changes. Combining video emotion classification, emotion-based music generation, and temporal boundary conditioning, EMSYNC emerges as a fully automatic video-based music generator. User studies show that it consistently outperforms existing methods in terms of music richness, emotional alignment, temporal synchronization, and overall preference, setting a new state-of-the-art in video-based music generation.</li>
</ul>

<h3>Title: Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zihao Fan, Xin Lu, Yidi Liu, Jie Huang, Dong Li, Xueyang Fu, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07069">https://arxiv.org/abs/2602.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07069">https://arxiv.org/pdf/2602.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07069]] Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution(https://arxiv.org/abs/2602.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.</li>
</ul>

<h3>Title: Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures</h3>
<ul>
<li><strong>Authors: </strong>Vladimer Khasia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07070">https://arxiv.org/abs/2602.07070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07070">https://arxiv.org/pdf/2602.07070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07070]] Hybrid Dual-Path Linear Transformations for Efficient Transformer Architectures(https://arxiv.org/abs/2602.07070)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Standard Transformer architectures rely heavily on dense linear transformations, treating feature projection as a monolithic, full-rank operation. We argue that this formulation is inefficient and lacks the structural inductive bias necessary for distinguishing between local feature preservation and global context integration. To address this, we introduce the Hybrid Dual-Path Linear (HDPL) operator, which decomposes the affine transformation into two topologically distinct pathways: a sparse block-diagonal component for high-rank local processing, and a low-rank Variational Autoencoder (VAE) bottleneck for global context regularization. By "surgically" replacing specific projections (Query, Key, Value, Gate, Up) with HDPL operators while retaining standard dense layers for aggregation (Output, Down), we achieve a superior balance of efficiency and representational power. Experiments on the FineWeb-Edu dataset demonstrate that the HDPL architecture outperforms a standard Llama-style baseline, reducing validation loss while simultaneously reducing parameter count by 6.8%. Beyond immediate performance gains, we discuss how the explicit materialization of a probabilistic latent space within the Transformer backbone serves as a vital architectural affordance, offering new pathways for inference-time or hypernetwork induced control, continual adaptation, interpretability, and cross-model or cross-modal synchronization. The code is available at this https URL</li>
</ul>

<h3>Title: Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Nardine Basta, Firas Ben Hmida, Houssem Jmal, Muhammad Ikram, Mohamed Ali Kaafar, Andy Walker</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07073">https://arxiv.org/abs/2602.07073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07073">https://arxiv.org/pdf/2602.07073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07073]] Pro-ZD: A Transferable Graph Neural Network Approach for Proactive Zero-Day Threats Mitigation(https://arxiv.org/abs/2602.07073)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In today's enterprise network landscape, the combination of perimeter and distributed firewall rules governs connectivity. To address challenges arising from increased traffic and diverse network architectures, organizations employ automated tools for firewall rule and access policy generation. Yet, effectively managing risks arising from dynamically generated policies, especially concerning critical asset exposure, remains a major challenge. This challenge is amplified by evolving network structures due to trends like remote users, bring-your-own devices, and cloud integration. This paper introduces a novel graph neural network model for identifying weighted shortest paths. The model aids in detecting network misconfigurations and high-risk connectivity paths that threaten critical assets, potentially exploited in zero-day attacks -- cyber-attacks exploiting undisclosed vulnerabilities. The proposed Pro-ZD framework adopts a proactive approach, automatically fine-tuning firewall rules and access policies to address high-risk connections and prevent unauthorized access. Experimental results highlight the robustness and transferability of Pro-ZD, achieving over 95% average accuracy in detecting high-risk connections. \</li>
</ul>

<h3>Title: The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL</h3>
<ul>
<li><strong>Authors: </strong>Yingru Li, Jiawei Xu, Ziniu Li, Jiacai Liu, Wei Liu, Yuxuan Tong, Longtao Zheng, Zhenghai Xue, Yaxiang Zhang, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07078">https://arxiv.org/abs/2602.07078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07078">https://arxiv.org/pdf/2602.07078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07078]] The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL(https://arxiv.org/abs/2602.07078)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.</li>
</ul>

<h3>Title: Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yu-Che Tsai, Hsiang Hsiao, Kuan-Yu Chen, Shou-De Lin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07090">https://arxiv.org/abs/2602.07090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07090">https://arxiv.org/pdf/2602.07090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07090]] Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks(https://arxiv.org/abs/2602.07090)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.</li>
</ul>

<h3>Title: TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation</h3>
<ul>
<li><strong>Authors: </strong>Biao Xiong, Zhen Peng, Ping Wang, Qiegen Liu, Xian Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07100">https://arxiv.org/abs/2602.07100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07100">https://arxiv.org/pdf/2602.07100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07100]] TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation(https://arxiv.org/abs/2602.07100)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at this https URL.</li>
</ul>

<h3>Title: Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zinan Lv, Yeqian Qian, Chen Sang, Hao Liu, Danping Zou, Ming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07101">https://arxiv.org/abs/2602.07101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07101">https://arxiv.org/pdf/2602.07101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07101]] Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting(https://arxiv.org/abs/2602.07101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.</li>
</ul>

<h3>Title: Extended to Reality: Prompt Injection in 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Zhuoheng Li, Ying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07104">https://arxiv.org/abs/2602.07104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07104">https://arxiv.org/pdf/2602.07104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07104]] Extended to Reality: Prompt Injection in 3D Environments(https://arxiv.org/abs/2602.07104)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.</li>
</ul>

<h3>Title: Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Zhang, Zhipeng Li, Yiwen Guo, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07106">https://arxiv.org/abs/2602.07106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07106">https://arxiv.org/pdf/2602.07106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07106]] Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models(https://arxiv.org/abs/2602.07106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.</li>
</ul>

<h3>Title: ShallowJail: Steering Jailbreaks against Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shang Liu, Hanyu Pei, Zeyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07107">https://arxiv.org/abs/2602.07107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07107">https://arxiv.org/pdf/2602.07107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07107]] ShallowJail: Steering Jailbreaks against Large Language Models(https://arxiv.org/abs/2602.07107)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\shallow, which substantially degrades the safety of state-of-the-art LLM responses.</li>
</ul>

<h3>Title: Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jacqueline He, Jonathan Hayase, Wen-tau Yih, Sewoong Oh, Luke Zettlemoyer, Pang Wei Koh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07120">https://arxiv.org/abs/2602.07120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07120">https://arxiv.org/pdf/2602.07120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07120]] Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model(https://arxiv.org/abs/2602.07120)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.</li>
</ul>

<h3>Title: Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ward, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07126">https://arxiv.org/abs/2602.07126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07126">https://arxiv.org/pdf/2602.07126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07126]] Finding Connections: Membership Inference Attacks for the Multi-Table Synthetic Data Setting(https://arxiv.org/abs/2602.07126)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data has gained attention for enabling privacy-preserving data sharing. While substantial progress has been made in single-table synthetic generation where data are modeled at the row or item level, most real-world data exists in relational databases where a user's information spans items across multiple interconnected tables. Recent advances in synthetic relational data generation have emerged to address this complexity, yet release of these data introduce unique privacy challenges as information can be leaked not only from individual items but also through the relationships that comprise a complete user entity. To address this, we propose a novel Membership Inference Attack (MIA) setting to audit the empirical user-level privacy of synthetic relational data and show that single-table MIAs that audit at an item level underestimate user-level privacy leakage. We then propose Multi-Table Membership Inference Attack (MT-MIA), a novel adversarial attack under a No-Box threat model that targets learned representations of user entities via Heterogeneous Graph Neural Networks. By incorporating all connected items for a user, MT-MIA better targets user-level vulnerabilities induced by inter-tabular relationships than existing attacks. We evaluate MT-MIA on a range of real-world multi-table datasets and demonstrate that this vulnerability exists in state-of-the-art relational synthetic data generators, employing MT-MIA to additionally study where this leakage occurs.</li>
</ul>

<h3>Title: BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07144">https://arxiv.org/abs/2602.07144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07144">https://arxiv.org/pdf/2602.07144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07144]] BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability(https://arxiv.org/abs/2602.07144)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.</li>
</ul>

<h3>Title: Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds</h3>
<ul>
<li><strong>Authors: </strong>Rawisara Lohanimit, Yankun Wu, Amelia Katirai, Yuta Nakashima, Noa Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07149">https://arxiv.org/abs/2602.07149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07149">https://arxiv.org/pdf/2602.07149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07149]] Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds(https://arxiv.org/abs/2602.07149)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.</li>
</ul>

<h3>Title: Trojans in Artificial Intelligence (TrojAI) Final Report</h3>
<ul>
<li><strong>Authors: </strong>Kristopher W. Reese, Taylor Kulp-McDowall, Michael Majurski, Tim Blattner, Derek Juba, Peter Bajcsy, Antonio Cardone, Philippe Dessauw, Alden Dima, Anthony J. Kearsley, Melinda Kleczynski, Joel Vasanth, Walid Keyrouz, Chace Ashcraft, Neil Fendley, Ted Staley, Trevor Stout, Josh Carney, Greg Canal, Will Redman, Aurora Schmidt, Cameron Hickert, William Paul, Jared Markowitz, Nathan Drenkow, David Shriver, Marissa Connor, Keltin Grimes, Marco Christiani, Hayden Moore, Jordan Widjaja, Kasimir Gabert, Uma Balakrishnan, Satyanadh Gundimada, John Jacobellis, Sandya Lakkur, Vitus Leung, Jon Roose, Casey Battaglino, Farinaz Koushanfar, Greg Fields, Xihe Gu, Yaman Jandali, Xinqiao Zhang, Akash Vartak, Tim Oates, Ben Erichson, Michael Mahoney, Rauf Izmailov, Xiangyu Zhang, Guangyu Shen, Siyuan Cheng, Shiqing Ma, XiaoFeng Wang, Haixu Tang, Di Tang, Xiaoyi Chen, Zihao Wang, Rui Zhu, Susmit Jha, Xiao Lin, Manoj Acharya, Wenchao Li, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07152">https://arxiv.org/abs/2602.07152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07152">https://arxiv.org/pdf/2602.07152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07152]] Trojans in Artificial Intelligence (TrojAI) Final Report(https://arxiv.org/abs/2602.07152)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of "natural" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.</li>
</ul>

<h3>Title: Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity</h3>
<ul>
<li><strong>Authors: </strong>Ayush Roy, Rudrasis Chakraborty, Lav Varshney, Vishnu Suresh Lokhande</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07154">https://arxiv.org/abs/2602.07154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07154">https://arxiv.org/pdf/2602.07154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07154]] Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity(https://arxiv.org/abs/2602.07154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on this https URL.</li>
</ul>

<h3>Title: Your Language Model Secretly Contains Personality Subnetworks</h3>
<ul>
<li><strong>Authors: </strong>Ruimeng Ye, Zihan Wang, Zinan Ling, Yang Xiao, Manling Li, Xiaolong Ma, Bo Hui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07164">https://arxiv.org/abs/2602.07164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07164">https://arxiv.org/pdf/2602.07164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07164]] Your Language Model Secretly Contains Personality Subnetworks(https://arxiv.org/abs/2602.07164)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.</li>
</ul>

<h3>Title: Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control</h3>
<ul>
<li><strong>Authors: </strong>Tong Jian, Tianyu Dai, Tao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07173">https://arxiv.org/abs/2602.07173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07173">https://arxiv.org/pdf/2602.07173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07173]] Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control(https://arxiv.org/abs/2602.07173)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.</li>
</ul>

<h3>Title: DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages</h3>
<ul>
<li><strong>Authors: </strong>Yongheng Sun, Jun Shu, Jianhua Ma, Fan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07174">https://arxiv.org/abs/2602.07174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07174">https://arxiv.org/pdf/2602.07174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07174]] DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages(https://arxiv.org/abs/2602.07174)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at this https URL.</li>
</ul>

<h3>Title: Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Mohamed El Hajji, Tarek Ait Baha, Aicha Dakir, Hammou Fadili, Youssef Es-Saady</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07176">https://arxiv.org/abs/2602.07176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07176">https://arxiv.org/pdf/2602.07176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07176]] Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI(https://arxiv.org/abs/2602.07176)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.</li>
</ul>

<h3>Title: Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Zhao, Siqi Li, Yasser Shoukry, Salma Elmalaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07181">https://arxiv.org/abs/2602.07181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07181">https://arxiv.org/pdf/2602.07181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07181]] Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs(https://arxiv.org/abs/2602.07181)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.</li>
</ul>

<h3>Title: Latent Target Score Matching, with an application to Simulation-Based Inference</h3>
<ul>
<li><strong>Authors: </strong>Joohwan Ko, Tomas Geffner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07189">https://arxiv.org/abs/2602.07189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07189">https://arxiv.org/pdf/2602.07189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07189]] Latent Target Score Matching, with an application to Simulation-Based Inference(https://arxiv.org/abs/2602.07189)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Denoising score matching (DSM) for training diffusion models may suffer from high variance at low noise levels. Target Score Matching (TSM) mitigates this when clean data scores are available, providing a low-variance objective. In many applications clean scores are inaccessible due to the presence of latent variables, leaving only joint signals exposed. We propose Latent Target Score Matching (LTSM), an extension of TSM to leverage joint scores for low-variance supervision of the marginal score. While LTSM is effective at low noise levels, a mixture with DSM ensures robustness across noise scales. Across simulation-based inference tasks, LTSM consistently improves variance, score accuracy, and sample quality.</li>
</ul>

<h3>Title: Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong He, Haoyan Wei, Wei Hu, Henan Mao, C.T. Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, math.NA, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07192">https://arxiv.org/abs/2602.07192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07192">https://arxiv.org/pdf/2602.07192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07192]] Systematic Performance Assessment of Deep Material Networks for Multiscale Material Modeling(https://arxiv.org/abs/2602.07192)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep Material Networks (DMNs) are structure-preserving, mechanistic machine learning models that embed micromechanical principles into their architectures, enabling strong extrapolation capabilities and significant potential to accelerate multiscale modeling of complex microstructures. A key advantage of these models is that they can be trained exclusively on linear elastic data and then generalized to nonlinear inelastic regimes during online prediction. Despite their growing adoption, systematic evaluations of their performance across the full offline-online pipeline remain limited. This work presents a comprehensive comparative assessment of DMNs with respect to prediction accuracy, computational efficiency, and training robustness. We investigate the effects of offline training choices, including initialization, batch size, training data size, and activation regularization on online generalization performance and uncertainty. The results demonstrate that both prediction error and variance decrease with increasing training data size, while initialization and batch size can significantly influence model performance. Moreover, activation regularization is shown to play a critical role in controlling network complexity and therefore generalization performance. Compared with the original DMN, the rotation-free Interaction-based Material Network (IMN) formulation achieves a 3.4x - 4.7x speed-up in offline training, while maintaining comparable online prediction accuracy and computational efficiency. These findings clarify key trade-offs between model expressivity and efficiency in structure-preserving material networks and provide practical guidance for their deployment in multiscale material modeling.</li>
</ul>

<h3>Title: Lite-BD: A Lightweight Black-box Backdoor Defense via Reviving Multi-Stage Image Transformations</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Arafat Miah, Yu Bi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07197">https://arxiv.org/abs/2602.07197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07197">https://arxiv.org/pdf/2602.07197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07197]] Lite-BD: A Lightweight Black-box Backdoor Defense via Reviving Multi-Stage Image Transformations(https://arxiv.org/abs/2602.07197)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) are vulnerable to backdoor attacks. Due to the nature of Machine Learning as a Service (MLaaS) applications, black-box defenses are more practical than white-box methods, yet existing purification techniques suffer from key limitations: a lack of justification for specific transformations, dataset dependency, high computational overhead, and a neglect of frequency-domain transformations. This paper conducts a preliminary study on various image transformations, identifying down-upscaling as the most effective backdoor trigger disruption technique. We subsequently propose \texttt{Lite-BD}, a lightweight two-stage blackbox backdoor defense. \texttt{Lite-BD} first employs a super-resolution-based down-upscaling stage to neutralize spatial triggers. A secondary stage utilizes query-based band-by-band frequency filtering to remove triggers hidden in specific bands. Extensive experiments against state-of-the-art attacks demonstrate that \texttt{Lite-BD} provides robust and efficient protection. Codes can be found at this https URL.</li>
</ul>

<h3>Title: Condition Matters in Full-head 3D GANs</h3>
<ul>
<li><strong>Authors: </strong>Heyuan Li, Huimin Zhang, Yuda Qiu, Zhengwentai Sun, Keru Zheng, Lingteng Qiu, Peihao Li, Qi Zuo, Ce Chen, Yujian Zheng, Yuming Gu, Zilong Dong, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07198">https://arxiv.org/abs/2602.07198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07198">https://arxiv.org/pdf/2602.07198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07198]] Condition Matters in Full-head 3D GANs(https://arxiv.org/abs/2602.07198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.</li>
</ul>

<h3>Title: BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Arafat Miah, Kevin Vu, Yu Bi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07200">https://arxiv.org/abs/2602.07200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07200">https://arxiv.org/pdf/2602.07200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07200]] BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron(https://arxiv.org/abs/2602.07200)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at this https URL.</li>
</ul>

<h3>Title: DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling</h3>
<ul>
<li><strong>Authors: </strong>Bucher Sahyouni, Matthew Vowels, Liqun Chen, Simon Hadfield</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07206">https://arxiv.org/abs/2602.07206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07206">https://arxiv.org/pdf/2602.07206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07206]] DSL: Understanding and Improving Softmax Recommender Systems with Competition-Aware Scaling(https://arxiv.org/abs/2602.07206)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Softmax Loss (SL) is being increasingly adopted for recommender systems (RS) as it has demonstrated better performance, robustness and fairness. Yet in implicit-feedback, a single global temperature and equal treatment of uniformly sampled negatives can lead to brittle training, because sampled sets may contain varying degrees of relevant or informative competitors. The optimal loss sharpness for a user-item pair with a particular set of negatives, can be suboptimal or destabilising for another with different negatives. We introduce Dual-scale Softmax Loss (DSL), which infers effective sharpness from the sampled competition itself. DSL adds two complementary branches to the log-sum-exp backbone. Firstly it reweights negatives within each training instance using hardness and item--item similarity, secondly it adapts a per-example temperature from the competition intensity over a constructed competitor slate. Together, these components preserve the geometry of SL while reshaping the competition distribution across negatives and across examples. Over several representative benchmarks and backbones, DSL yields substantial gains over strong baselines, with improvements over SL exceeding $10%$ in several settings and averaging $6.22%$ across datasets, metrics, and backbones. Under out-of-distribution (OOD) popularity shift, the gains are larger, with an average of $9.31%$ improvement over SL. We further provide a theoretical, distributionally robust optimisation (DRO) analysis, which demonstrates how DSL reshapes the robust payoff and the KL deviation for ambiguous instances. This helps explain the empirically observed improvements in accuracy and robustness.</li>
</ul>

<h3>Title: Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Ju Lin, Jing Pan, Ruizhi Li, Ming Sun, Yuzong Liu, Alaa Hassan, Jing Zheng, Florian Metze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07211">https://arxiv.org/abs/2602.07211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07211">https://arxiv.org/pdf/2602.07211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07211]] Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities(https://arxiv.org/abs/2602.07211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.</li>
</ul>

<h3>Title: Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used</h3>
<ul>
<li><strong>Authors: </strong>Srijan Shakya, Anamaria-Roberta Hartl, Sepp Hochreiter, Korbinian Pöppel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07213">https://arxiv.org/abs/2602.07213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07213">https://arxiv.org/pdf/2602.07213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07213]] Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used(https://arxiv.org/abs/2602.07213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.</li>
</ul>

<h3>Title: SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yikang Yue, Yuqi Xue, Jian Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07223">https://arxiv.org/abs/2602.07223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07223">https://arxiv.org/pdf/2602.07223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07223]] SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding(https://arxiv.org/abs/2602.07223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.</li>
</ul>

<h3>Title: Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation</h3>
<ul>
<li><strong>Authors: </strong>Nethmi Jayasinghe, Diana Gontero, Spencer T. Brown, Vinod K. Sangwan, Mark C. Hersam, Amit Ranjan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07227">https://arxiv.org/abs/2602.07227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07227">https://arxiv.org/pdf/2602.07227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07227]] Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation(https://arxiv.org/abs/2602.07227)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.</li>
</ul>

<h3>Title: ArcMark: Multi-bit LLM Watermark via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Atefeh Gilani, Carol Xuan Long, Sajani Vithana, Oliver Kosut, Lalitha Sankar, Flavio P. Calmon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07235">https://arxiv.org/abs/2602.07235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07235">https://arxiv.org/pdf/2602.07235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07235]] ArcMark: Multi-bit LLM Watermark via Optimal Transport(https://arxiv.org/abs/2602.07235)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.</li>
</ul>

<h3>Title: Hydra: Robust Hardware-Assisted Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Eli Propp, Seyed Majid Zahedi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07240">https://arxiv.org/abs/2602.07240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07240">https://arxiv.org/pdf/2602.07240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07240]] Hydra: Robust Hardware-Assisted Malware Detection(https://arxiv.org/abs/2602.07240)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Malware detection using Hardware Performance Counters (HPCs) offers a promising, low-overhead approach for monitoring program behavior. However, a fundamental architectural constraint, that only a limited number of hardware events can be monitored concurrently, creates a significant bottleneck, leading to detection blind spots. Prior work has primarily focused on optimizing machine learning models for a single, statically chosen event set, or on ensembling models over the same feature set. We argue that robustness requires diversifying not only the models, but also the underlying feature sets (i.e., the monitored hardware events) in order to capture a broader spectrum of program behavior. This observation motivates the following research question: Can detection performance be improved by trading temporal granularity for broader coverage, via the strategic scheduling of different feature sets over time? To answer this question, we propose Hydra, a novel detection mechanism that partitions execution traces into time slices and learns an effective schedule of feature sets and corresponding classifiers for deployment. By cycling through complementary feature sets, Hydra mitigates the limitations of a fixed monitoring perspective. Our experimental evaluation shows that Hydra significantly outperforms state-of-the-art single-feature-set baselines, achieving a 19.32% improvement in F1 score and a 60.23% reduction in false positive rate. These results underscore the importance of feature-set diversity and establish strategic multi-feature-set scheduling as an effective principle for robust, hardware-assisted malware detection.</li>
</ul>

<h3>Title: Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit</h3>
<ul>
<li><strong>Authors: </strong>Qi Sun, Ahmed Abdo, Luis Burbano, Ziyang Li, Yaxing Yao, Alvaro Cardenas, Yinzhi Cao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07249">https://arxiv.org/abs/2602.07249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07249">https://arxiv.org/pdf/2602.07249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07249]] Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit(https://arxiv.org/abs/2602.07249)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Autonomous Vehicles (AVs), especially vision-based AVs, are rapidly being deployed without human operators. As AVs operate in safety-critical environments, understanding their robustness in an adversarial environment is an important research problem. Prior physical adversarial attacks on vision-based autonomous vehicles predominantly target immediate safety failures (e.g., a crash, a traffic-rule violation, or a transient lane departure) by inducing a short-lived perception or control error. This paper shows a qualitatively different risk: a long-horizon route integrity compromise, where an attacker gradually steers a victim AV away from its intended route and into an attacker-chosen destination while the victim continues to drive "normally." This will not pose a danger to the victim vehicle itself, but also to potential passengers sitting inside the vehicle. In this paper, we design and implement the first adversarial framework, called JackZebra, that performs route-level hijacking of a vision-based end-to-end driving stack using a physically plausible attacker vehicle with a reconfigurable display mounted on the rear. The central challenge is temporal persistence: adversarial influence must remain effective in changing viewpoints, lighting, weather, traffic, and the victim's continual replanning -- without triggering conspicuous failures. Our key insight is to treat route hijacking as a closed-loop control problem and to convert adversarial patches into steering primitives that can be selected online via an interactive adjustment loop. Our adversarial patches are also carefully designed against worst-case background and sensor variations so that the adversarial impacts on the victim. Our evaluation shows that JackZebra can successfully hijack victim vehicles to deviate from original routes and stop at adversarial destinations with a high success rate.</li>
</ul>

<h3>Title: The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models</h3>
<ul>
<li><strong>Authors: </strong>Haley Duba-Sullivan, Steven R. Young, Emma J. Reid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07251">https://arxiv.org/abs/2602.07251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07251">https://arxiv.org/pdf/2602.07251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07251]] The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models(https://arxiv.org/abs/2602.07251)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.</li>
</ul>

<h3>Title: Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing</h3>
<ul>
<li><strong>Authors: </strong>Wanru Guo, Juan Xie, Binbin Wang, Weicong Chen, Xiaoyi Lu, Vipin Chaudhary, Curtis Tatsuoka</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07258">https://arxiv.org/abs/2602.07258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07258">https://arxiv.org/pdf/2602.07258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07258]] Robust Ultra-High-Dimensional Variable Selection With Correlated Structure Using Group Testing(https://arxiv.org/abs/2602.07258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Background: High-dimensional genomic data exhibit strong group correlation structures that challenge conventional feature selection methods, which often assume feature independence or rely on pre-defined pathways and are sensitive to outliers and model misspecification. Methods: We propose the Dorfman screening framework, a multi-stage procedure that forms data-driven variable groups via hierarchical clustering, performs group and within-group hypothesis testing, and refines selection using elastic net or adaptive elastic net. Robust variants incorporate OGK-based covariance estimation, rank-based correlation, and Huber-weighted regression to handle contaminated and non-normal data. Results: In simulations, Dorfman-Sparse-Adaptive-EN performed best under normal conditions, while Robust-OGK-Dorfman-Adaptive-EN showed clear advantages under data contamination, outperforming classical Dorfman and competing methods. Applied to NSCLC gene expression data for trametinib response, robust Dorfman methods achieved the lowest prediction errors and enriched recovery of clinically relevant genes. Conclusions: The Dorfman framework provides an efficient and robust approach to genomic feature selection. Robust-OGK-Dorfman-Adaptive-EN offers strong performance under both ideal and contaminated conditions and scales to ultra-high-dimensional settings, making it well suited for modern genomic biomarker discovery.</li>
</ul>

<h3>Title: TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Junbo Jacob Lian, Feng Xiong, Yujun Sun, Kaichen Ouyang, Mingyang Yu, Shengwei Fu, Zhong Rui, Zhang Yujun, Huiling Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07262">https://arxiv.org/abs/2602.07262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07262">https://arxiv.org/pdf/2602.07262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07262]] TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition(https://arxiv.org/abs/2602.07262)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.</li>
</ul>

<h3>Title: tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Li, Dibyadeep Saha, Avni Kanodia, Fan Lai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07263">https://arxiv.org/abs/2602.07263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07263">https://arxiv.org/pdf/2602.07263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07263]] tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models(https://arxiv.org/abs/2602.07263)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.</li>
</ul>

<h3>Title: XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference</h3>
<ul>
<li><strong>Authors: </strong>Daniil Vankov, Nikita Ivkin, Kyle Ulrich, Xiang Song, Ashish Khetan, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07265">https://arxiv.org/abs/2602.07265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07265">https://arxiv.org/pdf/2602.07265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07265]] XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference(https://arxiv.org/abs/2602.07265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.</li>
</ul>

<h3>Title: VideoNeuMat: Neural Material Extraction from Generative Video Models</h3>
<ul>
<li><strong>Authors: </strong>Bowen Xue, Saeed Hadadan, Zheng Zeng, Fabrice Rousselle, Zahra Montazeri, Milos Hasan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07272">https://arxiv.org/abs/2602.07272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07272">https://arxiv.org/pdf/2602.07272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07272]] VideoNeuMat: Neural Material Extraction from Generative Video Models(https://arxiv.org/abs/2602.07272)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.</li>
</ul>

<h3>Title: VertCoHiRF: Decentralized Vertical Clustering Beyond k-means</h3>
<ul>
<li><strong>Authors: </strong>Bruno Belucci, Karim Lounici, Vladimir R. Kostic, Katia Meziani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07279">https://arxiv.org/abs/2602.07279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07279">https://arxiv.org/pdf/2602.07279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07279]] VertCoHiRF: Decentralized Vertical Clustering Beyond k-means(https://arxiv.org/abs/2602.07279)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Vertical Federated Learning (VFL) enables collaborative analysis across parties holding complementary feature views of the same samples, yet existing approaches are largely restricted to distributed variants of $k$-means, requiring centralized coordination or the exchange of feature-dependent numerical statistics, and exhibiting limited robustness under heterogeneous views or adversarial behavior. We introduce VertCoHiRF, a fully decentralized framework for vertical federated clustering based on structural consensus across heterogeneous views, allowing each agent to apply a base clustering method adapted to its local feature space in a peer-to-peer manner. Rather than exchanging feature-dependent statistics or relying on noise injection for privacy, agents cluster their local views independently and reconcile their proposals through identifier-level consensus. Consensus is achieved via decentralized ordinal ranking to select representative medoids, progressively inducing a shared hierarchical clustering across agents. Communication is limited to sample identifiers, cluster labels, and ordinal rankings, providing privacy by design while supporting overlapping feature partitions and heterogeneous local clustering methods, and yielding an interpretable shared Cluster Fusion Hierarchy (CFH) that captures cross-view agreement at multiple this http URL analyze communication complexity and robustness, and experiments demonstrate competitive clustering performance in vertical federated settings.</li>
</ul>

<h3>Title: Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency</h3>
<ul>
<li><strong>Authors: </strong>Etam Benger, Katrina Ligett</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07285">https://arxiv.org/abs/2602.07285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07285">https://arxiv.org/pdf/2602.07285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07285]] Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency(https://arxiv.org/abs/2602.07285)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.</li>
</ul>

<h3>Title: Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction</h3>
<ul>
<li><strong>Authors: </strong>Juefei Pu, Xingyu Li, Haonan Li, Zhengchuan Liang, Jonathan Cox, Yifan Wu, Kareem Shehada, Arrdya Srivastav, Zhiyun Qian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07287">https://arxiv.org/abs/2602.07287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07287">https://arxiv.org/pdf/2602.07287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07287]] Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction(https://arxiv.org/abs/2602.07287)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches. In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\% of the cases with practical time and monetary cost. Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.</li>
</ul>

<h3>Title: ACORN-IDS: Adaptive Continual Novelty Detection for Intrusion Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Sean Fuhrman, Onat Gungor, Tajana Rosing</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07291">https://arxiv.org/abs/2602.07291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07291">https://arxiv.org/pdf/2602.07291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07291]] ACORN-IDS: Adaptive Continual Novelty Detection for Intrusion Detection Systems(https://arxiv.org/abs/2602.07291)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDS) must maintain reliable detection performance under rapidly evolving benign traffic patterns and the continual emergence of cyberattacks, including zero-day threats with no labeled data available. However, most machine learning-based IDS approaches either assume static data distributions or rely on labeled attack samples, substantially limiting their applicability in real-world deployments. This setting naturally motivates continual novelty detection, which enables IDS models to incrementally adapt to non-stationary data streams without labeled attack data. In this work, we introduce ACORN-IDS, an adaptive continual novelty detection framework that learns exclusively from normal data while exploiting the inherent structure of an evolving unlabeled data stream. ACORN-IDS integrates a continual feature extractor, trained using reconstruction and metric learning objectives with clustering-based pseudo-labels, alongside a PCA-based reconstruction module for anomaly scoring. This design allows ACORN-IDS to continuously adapt to distributional shifts in both benign and malicious traffic. We conduct an extensive evaluation of ACORN-IDS on five realistic intrusion datasets under two continual learning scenarios: (i) Evolving Attacks and (ii) Evolving Normal and Attack Distributions. ACORN-IDS achieves, on average, a 62% improvement in F1-score and a 58% improvement in zero-day attack detection over the state-of-the-art unsupervised continual learning baseline. It also outperforms existing state-of-the-art novelty detection approaches while exhibiting near-zero forgetting and imposing minimal inference overhead. These results demonstrate that ACORN-IDS offers a practical, label-efficient solution for building adaptive and robust IDS in dynamic, real-world environments. We plan to release the code upon acceptance.</li>
</ul>

<h3>Title: Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Aruna Jithesh, Chinmayi Karumuri, Venkata Kiran Reddy Kotha, Meghana Doddapuneni, Taehee Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07301">https://arxiv.org/abs/2602.07301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07301">https://arxiv.org/pdf/2602.07301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07301]] Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms(https://arxiv.org/abs/2602.07301)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.</li>
</ul>

<h3>Title: Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing</h3>
<ul>
<li><strong>Authors: </strong>Kyle Williams, Andrew Seltzman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07310">https://arxiv.org/abs/2602.07310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07310">https://arxiv.org/pdf/2602.07310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07310]] Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing(https://arxiv.org/abs/2602.07310)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.</li>
</ul>

<h3>Title: LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery</h3>
<ul>
<li><strong>Authors: </strong>Difei Gu, Yunhe Gao, Gerasimos Chatzoudis, Zihan Dong, Guoning Zhang, Bangwei Guo, Yang Zhou, Mu Zhou, Dimitris Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07311">https://arxiv.org/abs/2602.07311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07311">https://arxiv.org/pdf/2602.07311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07311]] LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery(https://arxiv.org/abs/2602.07311)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.</li>
</ul>

<h3>Title: Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice</h3>
<ul>
<li><strong>Authors: </strong>Savan Doshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07319">https://arxiv.org/abs/2602.07319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07319">https://arxiv.org/pdf/2602.07319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07319]] Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice(https://arxiv.org/abs/2602.07319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.</li>
</ul>

<h3>Title: Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Philip Jacobson, Ben Feinberg, Suhas Kumar, Sapan Agarwal, T. Patrick Xiao, Christopher Bennett</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07320">https://arxiv.org/abs/2602.07320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07320">https://arxiv.org/pdf/2602.07320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07320]] Incorruptible Neural Networks: Training Models that can Generalize to Large Internal Perturbations(https://arxiv.org/abs/2602.07320)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Flat regions of the neural network loss landscape have long been hypothesized to correlate with better generalization properties. A closely related but distinct problem is training models that are robust to internal perturbations to their weights, which may be an important need for future low-power hardware platforms. In this paper, we explore the usage of two methods, sharpness-aware minimization (SAM) and random-weight perturbation (RWP), to find minima robust to a variety of random corruptions to weights. We consider the problem from two angles: generalization (how do we reduce the noise-robust generalization gap) and optimization (how do we maximize performance from optimizers when subject to strong perturbations). First, we establish, both theoretically and empirically, that an over-regularized RWP training objective is optimal for noise-robust generalization. For small-magnitude noise, we find that SAM's adversarial objective further improves performance over any RWP configuration, but performs poorly for large-magnitude noise. We link the cause of this to a vanishing-gradient effect, caused by unevenness in the loss landscape, affecting both SAM and RWP. Lastly, we demonstrate that dynamically adjusting the perturbation strength to match the evolution of the loss landscape improves optimizing for these perturbed objectives.</li>
</ul>

<h3>Title: Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation</h3>
<ul>
<li><strong>Authors: </strong>Geng Liu, Fei Zhu, Rong Feng, Changyi Ma, Shiqi Wang, Gaofeng Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07338">https://arxiv.org/abs/2602.07338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07338">https://arxiv.org/pdf/2602.07338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07338]] Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation(https://arxiv.org/abs/2602.07338)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.</li>
</ul>

<h3>Title: Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control</h3>
<ul>
<li><strong>Authors: </strong>Yonghui Yang, Wenjian Tao, Jilong Liu, Xingyu Zhu, Junfeng Fang, Weibiao Huang, Le Wu, Richang Hong, Tat-Sent Chua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07340">https://arxiv.org/abs/2602.07340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07340">https://arxiv.org/pdf/2602.07340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07340]] Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control(https://arxiv.org/abs/2602.07340)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.</li>
</ul>

<h3>Title: Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Yang, Ruijiao Li, Lifeng Wang, Shuai Zheng, Shunzheng Ma, Keyu Zhang, Tuoyu Sun, Chenyun Dai, Jie Ding, Zhuo Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07341">https://arxiv.org/abs/2602.07341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07341">https://arxiv.org/pdf/2602.07341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07341]] Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions(https://arxiv.org/abs/2602.07341)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at this https URL.</li>
</ul>

<h3>Title: Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ruturaj Reddy, Hrishav Bakul Barua, Junn Yong Loo, Thanh Thi Nguyen, Ganesh Krishnasamy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07343">https://arxiv.org/abs/2602.07343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07343">https://arxiv.org/pdf/2602.07343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07343]] Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation(https://arxiv.org/abs/2602.07343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.</li>
</ul>

<h3>Title: Optimizing Few-Step Generation with Adaptive Matching Distillation</h3>
<ul>
<li><strong>Authors: </strong>Lichen Bai, Zikai Zhou, Shitong Shao, Wenliang Zhong, Shuo Yang, Shuo Chen, Bojun Chen, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07345">https://arxiv.org/abs/2602.07345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07345">https://arxiv.org/pdf/2602.07345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07345]] Optimizing Few-Step Generation with Adaptive Matching Distillation(https://arxiv.org/abs/2602.07345)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.</li>
</ul>

<h3>Title: Controllable Value Alignment in Large Language Models through Neuron-Level Editing</h3>
<ul>
<li><strong>Authors: </strong>Yonghui Yang, Junwei Li, Jilong Liu, Yicheng He, Fengbin Zhu, Weibiao Huang, Le Wu, Richang Hong, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07356">https://arxiv.org/abs/2602.07356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07356">https://arxiv.org/pdf/2602.07356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07356]] Controllable Value Alignment in Large Language Models through Neuron-Level Editing(https://arxiv.org/abs/2602.07356)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.</li>
</ul>

<h3>Title: UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding</h3>
<ul>
<li><strong>Authors: </strong>Jiaming He, Fuming Luo, Hongwei Li, Wenbo Jiang, Wenshu Fan, Zhenbo Shi, Xudong Jiang, Yi Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07358">https://arxiv.org/abs/2602.07358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07358">https://arxiv.org/pdf/2602.07358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07358]] UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding(https://arxiv.org/abs/2602.07358)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.</li>
</ul>

<h3>Title: ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations</h3>
<ul>
<li><strong>Authors: </strong>Long S. T. Nguyen, Quan M. Bui, Tin T. Ngo, Quynh T. N. Vo, Dung N. H. Le, Tho T. Quan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07361">https://arxiv.org/abs/2602.07361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07361">https://arxiv.org/pdf/2602.07361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07361]] ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations(https://arxiv.org/abs/2602.07361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at this https URL.</li>
</ul>

<h3>Title: TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling</h3>
<ul>
<li><strong>Authors: </strong>Nisharg Nargund, Priyesh Shukla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07374">https://arxiv.org/abs/2602.07374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07374">https://arxiv.org/pdf/2602.07374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07374]] TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling(https://arxiv.org/abs/2602.07374)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at this https URL.</li>
</ul>

<h3>Title: Efficient Post-Training Pruning of Large Language Models with Statistical Correction</h3>
<ul>
<li><strong>Authors: </strong>Peiqi Yu, Jinhao Wang, Xinyi Sui, Nam Ling, Wei Wang, Wei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07375">https://arxiv.org/abs/2602.07375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07375">https://arxiv.org/pdf/2602.07375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07375]] Efficient Post-Training Pruning of Large Language Models with Statistical Correction(https://arxiv.org/abs/2602.07375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.</li>
</ul>

<h3>Title: Do Large Language Models Reflect Demographic Pluralism in Safety?</h3>
<ul>
<li><strong>Authors: </strong>Usman Naseem, Gautam Siddharth Kashyap, Sushant Kumar Ray, Rafiq Ali, Ebad Shabbir, Abdullah Mohammad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07376">https://arxiv.org/abs/2602.07376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07376">https://arxiv.org/pdf/2602.07376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07376]] Do Large Language Models Reflect Demographic Pluralism in Safety?(https://arxiv.org/abs/2602.07376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.</li>
</ul>

<h3>Title: Aegis: Towards Governance, Integrity, and Security of AI Voice Agents</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Pin-Yu Chen, Wenqi Wei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07379">https://arxiv.org/abs/2602.07379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07379">https://arxiv.org/pdf/2602.07379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07379]] Aegis: Towards Governance, Integrity, and Security of AI Voice Agents(https://arxiv.org/abs/2602.07379)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement and adoption of Audio Large Language Models (ALLMs), voice agents are now being deployed in high-stakes domains such as banking, customer service, and IT support. However, their vulnerabilities to adversarial misuse still remain unexplored. While prior work has examined aspects of trustworthiness in ALLMs, such as harmful content generation and hallucination, systematic security evaluations of voice agents are still lacking. To address this gap, we propose Aegis, a red-teaming framework for the governance, integrity, and security of voice agents. Aegis models the realistic deployment pipeline of voice agents and designs structured adversarial scenarios of critical risks, including privacy leakage, privilege escalation, resource abuse, etc. We evaluate the framework through case studies in banking call centers, IT Support, and logistics. Our evaluation shows that while access controls mitigate data-level risks, voice agents remain vulnerable to behavioral attacks that cannot be addressed through access restrictions alone, even under strict access controls. We observe systematic differences across model families, with open-weight models exhibiting higher susceptibility, underscoring the need for layered defenses that combine access control, policy enforcement, and behavioral monitoring to secure next-generation voice agents.</li>
</ul>

<h3>Title: When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified</h3>
<ul>
<li><strong>Authors: </strong>Gautam Siddharth Kashyap, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07381">https://arxiv.org/abs/2602.07381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07381">https://arxiv.org/pdf/2602.07381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07381]] When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified(https://arxiv.org/abs/2602.07381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.</li>
</ul>

<h3>Title: Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi</h3>
<ul>
<li><strong>Authors: </strong>Debtanu Datta, Rajdeep Mukherjee, Adrijit Goswami, Saptarshi Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07382">https://arxiv.org/abs/2602.07382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07382">https://arxiv.org/pdf/2602.07382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07382]] Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi(https://arxiv.org/abs/2602.07382)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.</li>
</ul>

<h3>Title: AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management</h3>
<ul>
<li><strong>Authors: </strong>Ruoyao Wen, Hao Li, Chaowei Xiao, Ning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07398">https://arxiv.org/abs/2602.07398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07398">https://arxiv.org/pdf/2602.07398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07398]] AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management(https://arxiv.org/abs/2602.07398)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack. We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length. On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Wu, Mingzhe Du, Yue Liu, Chengran Yang, Terry Yue Zhuo, Jiaheng Zhang, See-Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07422">https://arxiv.org/abs/2602.07422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07422">https://arxiv.org/pdf/2602.07422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07422]] Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model(https://arxiv.org/abs/2602.07422)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at this https URL.</li>
</ul>

<h3>Title: Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise</h3>
<ul>
<li><strong>Authors: </strong>Dingzhi Yu, Hongyi Tao, Yuanyu Wan, Luo Luo, Lijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07425">https://arxiv.org/abs/2602.07425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07425">https://arxiv.org/pdf/2602.07425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07425]] Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise(https://arxiv.org/abs/2602.07425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.</li>
</ul>

<h3>Title: Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yuanxu Sun, Yuezhou Ma, Haixu Wu, Guanyang Zeng, Muye Chen, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07429">https://arxiv.org/abs/2602.07429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07429">https://arxiv.org/pdf/2602.07429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07429]] Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers(https://arxiv.org/abs/2602.07429)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.</li>
</ul>

<h3>Title: Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis</h3>
<ul>
<li><strong>Authors: </strong>Cédric Jung, Shirin Salehi, Anke Schmeink</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07440">https://arxiv.org/abs/2602.07440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07440">https://arxiv.org/pdf/2602.07440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07440]] Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis(https://arxiv.org/abs/2602.07440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.</li>
</ul>

<h3>Title: PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization</h3>
<ul>
<li><strong>Authors: </strong>Naqcho Ali Mehdi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07446">https://arxiv.org/abs/2602.07446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07446">https://arxiv.org/pdf/2602.07446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07446]] PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization(https://arxiv.org/abs/2602.07446)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at this https URL and this https URL.</li>
</ul>

<h3>Title: SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads</h3>
<ul>
<li><strong>Authors: </strong>Tan Yu, Qian Qiao, Le Shen, Ke Zhou, Jincheng Hu, Dian Sheng, Bo Hu, Haoming Qin, Jun Gao, Changhai Zhou, Shunshun Yin, Siyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07449">https://arxiv.org/abs/2602.07449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07449">https://arxiv.org/pdf/2602.07449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07449]] SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads(https://arxiv.org/abs/2602.07449)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.</li>
</ul>

<h3>Title: DLLM Agent: See Farther, Run Faster</h3>
<ul>
<li><strong>Authors: </strong>Huiling Zhen, Weizhe Lin, Renxi Liu, Kai Han, Yiming Li, Yuchuan Tian, Hanting Chen, Xiaoguang Li, Xiaosong Li, Chen Chen, Xianzhi Yu, Mingxuan Yuan, Youliang Yan, Peifeng Qin, Jun Wang, Yu Wang, Dacheng Tao, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07451">https://arxiv.org/abs/2602.07451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07451">https://arxiv.org/pdf/2602.07451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07451]] DLLM Agent: See Farther, Run Faster(https://arxiv.org/abs/2602.07451)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.</li>
</ul>

<h3>Title: Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Namrita Varshney, Ashutosh Gupta, Arhaan Ahmad, Tanay V. Tayal, S. Akshay</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07453">https://arxiv.org/abs/2602.07453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07453">https://arxiv.org/pdf/2602.07453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07453]] Data-Aware and Scalable Sensitivity Analysis for Decision Tree Ensembles(https://arxiv.org/abs/2602.07453)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>Decision tree ensembles are widely used in critical domains, making robustness and sensitivity analysis essential to their trustworthiness. We study the feature sensitivity problem, which asks whether an ensemble is sensitive to a specified subset of features -- such as protected attributes -- whose manipulation can alter model predictions. Existing approaches often yield examples of sensitivity that lie far from the training distribution, limiting their interpretability and practical value. We propose a data-aware sensitivity framework that constrains the sensitive examples to remain close to the dataset, thereby producing realistic and interpretable evidence of model weaknesses. To this end, we develop novel techniques for data-aware search using a combination of mixed-integer linear programming (MILP) and satisfiability modulo theories (SMT) encodings. Our contributions are fourfold. First, we strengthen the NP-hardness result for sensitivity verification, showing it holds even for trees of depth 1. Second, we develop MILP-optimizations that significantly speed up sensitivity verification for single ensembles and for the first time can also handle multiclass tree ensembles. Third, we introduce a data-aware framework generating realistic examples close to the training distribution. Finally, we conduct an extensive experimental evaluation on large tree ensembles, demonstrating scalability to ensembles with up to 800 trees of depth 8, achieving substantial improvements over the state of the art. This framework provides a practical foundation for analyzing the reliability and fairness of tree-based models in high-stakes applications.</li>
</ul>

<h3>Title: SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yancheng Long, Yankai Yang, Hongyang Wei, Wei Chen, Tianke Zhang, Haonan fan, Changyi Liu, Kaiyu Jiang, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07458">https://arxiv.org/abs/2602.07458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07458">https://arxiv.org/pdf/2602.07458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07458]] SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning(https://arxiv.org/abs/2602.07458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.</li>
</ul>

<h3>Title: GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Misbah Ijaz, Saif Ur Rehman Khan, Abd Ur Rehman, Tayyaba Asif, Sebastian Vollmer, Andreas Dengel, Muhammad Nabeel Asim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07463">https://arxiv.org/abs/2602.07463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07463">https://arxiv.org/pdf/2602.07463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07463]] GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring(https://arxiv.org/abs/2602.07463)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.</li>
</ul>

<h3>Title: SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yijie Chen, Yijin Liu, Fandong Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07464">https://arxiv.org/abs/2602.07464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07464">https://arxiv.org/pdf/2602.07464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07464]] SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning(https://arxiv.org/abs/2602.07464)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at this https URL</li>
</ul>

<h3>Title: On the Importance of a Multi-Scale Calibration for Quantization</h3>
<ul>
<li><strong>Authors: </strong>Seungwoo Son, Ingyu Seong, Junhan Kim, Hyemi Jang, Yongkweon Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07465">https://arxiv.org/abs/2602.07465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07465">https://arxiv.org/pdf/2602.07465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07465]] On the Importance of a Multi-Scale Calibration for Quantization(https://arxiv.org/abs/2602.07465)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.</li>
</ul>

<h3>Title: Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data</h3>
<ul>
<li><strong>Authors: </strong>Zhuomin Liang, Liang Bai, Xian Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07475">https://arxiv.org/abs/2602.07475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07475">https://arxiv.org/pdf/2602.07475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07475]] Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data(https://arxiv.org/abs/2602.07475)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq this http URL address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.</li>
</ul>

<h3>Title: AI-Driven Predictive Modelling for Groundwater Salinization in Israel</h3>
<ul>
<li><strong>Authors: </strong>Laxmi Pandey, Ariel Meroz, Ben Cheng, Ankita Manekar, Abhijit Mukherjee, Meirav Cohen, Adway Mitra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07478">https://arxiv.org/abs/2602.07478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07478">https://arxiv.org/pdf/2602.07478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07478]] AI-Driven Predictive Modelling for Groundwater Salinization in Israel(https://arxiv.org/abs/2602.07478)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Increasing salinity and contamination of groundwater is a serious issue in many parts of the world, causing degradation of water resources. The aim of this work is to form a comprehensive understanding of groundwater salinization underlying causal factors and identify important meteorological, geological and anthropogenic drivers of salinity. We have integrated different datasets of potential covariates, to create a robust framework for machine learning based predictive models including Random Forest (RF), XGBoost, Neural network, Long Short-Term Memory (LSTM), convolution neural network (CNN) and linear regression (LR), of groundwater salinity. Additionally, Recursive Feature Elimination (RFE) followed by Global sensitivity analysis (GSA) and Explainable AI (XAI) based SHapley Additive exPlanations (SHAP) were used to estimate the importance scores and find insights into the drivers of salinization. We also did causality analysis via Double machine learning using various predictive models. From these analyses, key meteorological (Precipitation, Temperature), geological (Distance from river, Distance to saline body, TWI, Shoreline distance), and anthropogenic (Area of agriculture field, Treated Wastewater) covariates are identified to be influential drivers of groundwater salinity across Israel. XAI analysis also identified Treated Wastewater (TWW) as an essential anthropogenic driver of salinity, its significance being context-dependent but critical in vulnerable hydro-climatic environment. Our approach provides deeper insight into global salinization mechanisms at country scale, reducing AI model uncertainty and highlighting the need for tailored strategies to address salinity.</li>
</ul>

<h3>Title: Thermal odometry and dense mapping using learned ddometry and Gaussian splatting</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Zhou, Yujia Chen, Zhihao Zhan, Yuhang Ming, Jianzhu Huai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07493">https://arxiv.org/abs/2602.07493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07493">https://arxiv.org/pdf/2602.07493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07493]] Thermal odometry and dense mapping using learned ddometry and Gaussian splatting(https://arxiv.org/abs/2602.07493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.</li>
</ul>

<h3>Title: Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Shenxi Wu, Haosong Zhang, Xingjian Ma, Shirui Bian, Yichi Zhang, Xi Chen, Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07494">https://arxiv.org/abs/2602.07494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07494">https://arxiv.org/pdf/2602.07494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07494]] Hyperparameter Transfer Laws for Non-Recurrent Multi-Path Neural Networks(https://arxiv.org/abs/2602.07494)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deeper modern architectures are costly to train, making hyperparameter transfer preferable to expensive repeated tuning. Maximal Update Parametrization ($\mu$P) helps explain why many hyperparameters transfer across width. Yet depth scaling is less understood for modern architectures, whose computation graphs contain multiple parallel paths and residual aggregation. To unify various non-recurrent multi-path neural networks such as CNNs, ResNets, and Transformers, we introduce a graph-based notion of effective depth. Under stabilizing initializations and a maximal-update criterion, we show that the optimal learning rate decays with effective depth following a universal -3/2 power law. Here, the maximal-update criterion maximizes the typical one-step representation change at initialization without causing instability, and effective depth is the minimal path length from input to output, counting layers and residual additions. Experiments across diverse architectures confirm the predicted slope and enable reliable zero-shot transfer of learning rates across depths and widths, turning depth scaling into a predictable hyperparameter-transfer problem.</li>
</ul>

<h3>Title: CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Antonio Mone, Frans A. Oliehoek, Luciano Cavalcante Siebert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07496">https://arxiv.org/abs/2602.07496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07496">https://arxiv.org/pdf/2602.07496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07496]] CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning(https://arxiv.org/abs/2602.07496)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.</li>
</ul>

<h3>Title: From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection</h3>
<ul>
<li><strong>Authors: </strong>Mo Wang, Kaixuan Ren, Pratik Jalan, Ahmed Ashraf, Tuong Vy Vu, Rahul Seetharaman, Shah Nawaz, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07497">https://arxiv.org/abs/2602.07497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07497">https://arxiv.org/pdf/2602.07497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07497]] From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection(https://arxiv.org/abs/2602.07497)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.</li>
</ul>

<h3>Title: IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation</h3>
<ul>
<li><strong>Authors: </strong>Zhufeng Xu, Xuan Gao, Feng-Lin Liu, Haoxian Zhang, Zhixue Fang, Yu-Kun Lai, Xiaoqiang Liu, Pengfei Wan, Lin Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07498">https://arxiv.org/abs/2602.07498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07498">https://arxiv.org/pdf/2602.07498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07498]] IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation(https://arxiv.org/abs/2602.07498)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.</li>
</ul>

<h3>Title: Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification</h3>
<ul>
<li><strong>Authors: </strong>Jingshen Zhang, Xin Ying Qiu, Lifang Lu, Zhuhua Huang, Yutao Hu, Yuechang Wu, JunYu Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07499">https://arxiv.org/abs/2602.07499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07499">https://arxiv.org/pdf/2602.07499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07499]] Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification(https://arxiv.org/abs/2602.07499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.</li>
</ul>

<h3>Title: SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients</h3>
<ul>
<li><strong>Authors: </strong>Masato Kamba, Akiyoshi Sannai</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07513">https://arxiv.org/abs/2602.07513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07513">https://arxiv.org/pdf/2602.07513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07513]] SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients(https://arxiv.org/abs/2602.07513)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse. We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.</li>
</ul>

<h3>Title: MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Shengfang Zhai, Guanghao Jin, Yinpeng Dong, Linyi Yang, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07517">https://arxiv.org/abs/2602.07517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07517">https://arxiv.org/pdf/2602.07517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07517]] MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots(https://arxiv.org/abs/2602.07517)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.</li>
</ul>

<h3>Title: MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution</h3>
<ul>
<li><strong>Authors: </strong>Jianwen Chen, Xinyu Yang, Peng Xia, Arian Azarang, Yueh Z Lee, Gang Li, Hongtu Zhu, Yun Li, Beidi Chen, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07529">https://arxiv.org/abs/2602.07529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07529">https://arxiv.org/pdf/2602.07529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07529]] MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution(https://arxiv.org/abs/2602.07529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.</li>
</ul>

<h3>Title: Evaluating Object-Centric Models beyond Object Discovery</h3>
<ul>
<li><strong>Authors: </strong>Krishnakant Singh, Simone Schaub-Meyer, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07532">https://arxiv.org/abs/2602.07532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07532">https://arxiv.org/pdf/2602.07532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07532]] Evaluating Object-Centric Models beyond Object Discovery(https://arxiv.org/abs/2602.07532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.</li>
</ul>

<h3>Title: Fine-Grained Cat Breed Recognition with Global Context Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Mowmita Parvin Hera, Md. Shahriar Mahmud Kallol, Shohanur Rahman Nirob, Md. Badsha Bulbul, Jubayer Ahmed, M. Zhourul Islam, Hazrat Ali, Mohammmad Farhad Bulbul</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07534">https://arxiv.org/abs/2602.07534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07534">https://arxiv.org/pdf/2602.07534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07534]] Fine-Grained Cat Breed Recognition with Global Context Vision Transformer(https://arxiv.org/abs/2602.07534)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at this https URL.</li>
</ul>

<h3>Title: Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis</h3>
<ul>
<li><strong>Authors: </strong>Md Sazidur Rahman, Kjersti Engan, Kathinka Dæhli Kurz, Mahdieh Khanmohammadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07535">https://arxiv.org/abs/2602.07535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07535">https://arxiv.org/pdf/2602.07535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07535]] Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis(https://arxiv.org/abs/2602.07535)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.</li>
</ul>

<h3>Title: MUFASA: A Multi-Layer Framework for Slot Attention</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Bock, Leonie Schüßler, Krishnakant Singh, Simone Schaub-Meyer, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07544">https://arxiv.org/abs/2602.07544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07544">https://arxiv.org/pdf/2602.07544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07544]] MUFASA: A Multi-Layer Framework for Slot Attention(https://arxiv.org/abs/2602.07544)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.</li>
</ul>

<h3>Title: Improving Variable-Length Generation in Diffusion Language Models via Length Regularization</h3>
<ul>
<li><strong>Authors: </strong>Zicong Cheng, Ruixuan Jia, Jia Li, Guo-Wei Yang, Meng-Hao Guo, Shi-Min Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07546">https://arxiv.org/abs/2602.07546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07546">https://arxiv.org/pdf/2602.07546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07546]] Improving Variable-Length Generation in Diffusion Language Models via Length Regularization(https://arxiv.org/abs/2602.07546)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).</li>
</ul>

<h3>Title: Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hussni Mohd Zakir, Eric Tatt Wei Ho</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07550">https://arxiv.org/abs/2602.07550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07550">https://arxiv.org/pdf/2602.07550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07550]] Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation(https://arxiv.org/abs/2602.07550)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in this http URL code is publicly available at this https URL.</li>
</ul>

<h3>Title: FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Guandong Li, Yijun Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07554">https://arxiv.org/abs/2602.07554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07554">https://arxiv.org/pdf/2602.07554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07554]] FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation(https://arxiv.org/abs/2602.07554)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.</li>
</ul>

<h3>Title: VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation</h3>
<ul>
<li><strong>Authors: </strong>Francesco Taioli, Shiping Yang, Sonia Raychaudhuri, Marco Cristani, Unnat Jain, Angel X Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07555">https://arxiv.org/abs/2602.07555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07555">https://arxiv.org/pdf/2602.07555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07555]] VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation(https://arxiv.org/abs/2602.07555)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.</li>
</ul>

<h3>Title: Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction</h3>
<ul>
<li><strong>Authors: </strong>Antoine Gonon, Alexandre Cordonnier, Nicolas Boumal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07562">https://arxiv.org/abs/2602.07562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07562">https://arxiv.org/pdf/2602.07562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07562]] Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction(https://arxiv.org/abs/2602.07562)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.</li>
</ul>

<h3>Title: SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Zhang, Zechen Bai, Haofan Wang, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07564">https://arxiv.org/abs/2602.07564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07564">https://arxiv.org/pdf/2602.07564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07564]] SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens(https://arxiv.org/abs/2602.07564)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.</li>
</ul>

<h3>Title: Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025</h3>
<ul>
<li><strong>Authors: </strong>Jingzhe Ma, Meng Zhang, Jianlong Yu, Kun Liu, Zunxiao Xu, Xue Cheng, Junjie Zhou, Yanfei Wang, Jiahang Li, Zepeng Wang, Kazuki Osamura, Rujie Liu, Narishige Abe, Jingjie Wang, Shunli Zhang, Haojun Xie, Jiajun Wu, Weiming Wu, Wenxiong Kang, Qingshuo Gao, Jiaming Xiong, Xianye Ben, Lei Chen, Lichen Song, Junjian Cui, Haijun Xiong, Junhao Lu, Bin Feng, Mengyuan Liu, Ji Zhou, Baoquan Zhao, Ke Xu, Yongzhen Huang, Liang Wang, Manuel J Marin-Jimenez, Md Atiqur Rahman Ahad, Shiqi Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07565">https://arxiv.org/abs/2602.07565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07565">https://arxiv.org/pdf/2602.07565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07565]] Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025(https://arxiv.org/abs/2602.07565)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, fair</a></li>
<li><strong>Abstract: </strong>Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.</li>
</ul>

<h3>Title: Cross-Camera Cow Identification via Disentangled Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Runcheng Wang, Yaru Chen, Guiguo Zhang, Honghua Jiang, Yongliang Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07566">https://arxiv.org/abs/2602.07566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07566">https://arxiv.org/pdf/2602.07566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07566]] Cross-Camera Cow Identification via Disentangled Representation Learning(https://arxiv.org/abs/2602.07566)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.</li>
</ul>

<h3>Title: ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Liu, Hao Wu, Xin Qiu, Yingqi Fan, Yihan Zhang, Anhao Zhao, Yunpu Ma, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07574">https://arxiv.org/abs/2602.07574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07574">https://arxiv.org/pdf/2602.07574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07574]] ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention(https://arxiv.org/abs/2602.07574)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at this https URL.</li>
</ul>

<h3>Title: Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Yu, Wenbing Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07588">https://arxiv.org/abs/2602.07588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07588">https://arxiv.org/pdf/2602.07588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07588]] Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge(https://arxiv.org/abs/2602.07588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.</li>
</ul>

<h3>Title: Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling</h3>
<ul>
<li><strong>Authors: </strong>Jessica Ka Yi Chiu, Tom Frode Hansen, Eivind Magnus Paulsen, Ole Jakob Mengshoel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07590">https://arxiv.org/abs/2602.07590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07590">https://arxiv.org/pdf/2602.07590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07590]] Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling(https://arxiv.org/abs/2602.07590)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.</li>
</ul>

<h3>Title: Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Polina Gordienko, Christoph Jansen, Julian Rodemann, Georg Schollmeyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07593">https://arxiv.org/abs/2602.07593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07593">https://arxiv.org/pdf/2602.07593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07593]] Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking(https://arxiv.org/abs/2602.07593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.</li>
</ul>

<h3>Title: Learning to Self-Verify Makes Language Models Better Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Chen, Yu Wang, Yi Zhang, Ziang Ye, Zhengzhou Cai, Yaorui Shi, Qi Gu, Hui Su, Xunliang Cai, Xiang Wang, An Zhang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07594">https://arxiv.org/abs/2602.07594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07594">https://arxiv.org/pdf/2602.07594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07594]] Learning to Self-Verify Makes Language Models Better Reasoners(https://arxiv.org/abs/2602.07594)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.</li>
</ul>

<h3>Title: TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhi Liang, Xuan'er Wu, Yirui Liu, Yijie Fang, Yizhen Fan, Ke Hao, Rui Li, Ruiying Liu, Ziqi Ni, Peng Yu, Yanbo Wang, Haibin Huang, Qizhen Weng, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07595">https://arxiv.org/abs/2602.07595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07595">https://arxiv.org/pdf/2602.07595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07595]] TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation(https://arxiv.org/abs/2602.07595)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.</li>
</ul>

<h3>Title: Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization</h3>
<ul>
<li><strong>Authors: </strong>Xi Chen, Ming Li, Junxi Li, Changsheng Li, Peisong Wang, Lizhong Ding, Ye Yuan, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07596">https://arxiv.org/abs/2602.07596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07596">https://arxiv.org/pdf/2602.07596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07596]] Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization(https://arxiv.org/abs/2602.07596)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.</li>
</ul>

<h3>Title: Rational Transductors</h3>
<ul>
<li><strong>Authors: </strong>Mehryar Mohri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07599">https://arxiv.org/abs/2602.07599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07599">https://arxiv.org/pdf/2602.07599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07599]] Rational Transductors(https://arxiv.org/abs/2602.07599)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Standard Transformers excel at semantic modeling but struggle with rigid sequential logic and state tracking. Theoretical work establishes that self-attention is limited to $\AC^0$ (under hard attention) or $\TC^0$ (under soft attention), complexity classes that often fail to support robust length generalization on sequential problems without intermediate chain-of-thought. In this work, we introduce \emph{Rational Transductors}, a dual-stream architecture that augments the Transformer with a matrix-valued recurrence derived from Weighted Finite Automata (WFA). By injecting rational state information into the attention mechanism via a \emph{Deep Rational Injection} scheme, our framework strictly generalizes the expressive power of Transformers to capture all Regular Languages, $\NC^1$-complete problems (such as Boolean Formula Evaluation), and fundamental separations like Parity and Modular Counting, while preserving $O(L + \log T)$ parallel time complexity. We ground the architecture in a rigorous learning theory: we prove that \emph{Random Rational Features} act as a universal basis for sequential dependencies, justifying our initialization strategy, while establishing that the \emph{Differentiable Rational Feature} regime is necessary to close the representational compactness gap. Theoretical analysis and empirical results demonstrate that Rational Transductors solve the "Regular Gap," enabling robust length generalization on algorithmic tasks where standard Transformers fail, without the sequential computational bottlenecks of traditional RNNs.</li>
</ul>

<h3>Title: Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines</h3>
<ul>
<li><strong>Authors: </strong>Woojin Cho, Junghwan Park</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07603">https://arxiv.org/abs/2602.07603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07603">https://arxiv.org/pdf/2602.07603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07603]] Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines(https://arxiv.org/abs/2602.07603)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.</li>
</ul>

<h3>Title: Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hulingxiao He, Zijun Geng, Yuxin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07605">https://arxiv.org/abs/2602.07605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07605">https://arxiv.org/pdf/2602.07605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07605]] Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning(https://arxiv.org/abs/2602.07605)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at this https URL.</li>
</ul>

<h3>Title: HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology</h3>
<ul>
<li><strong>Authors: </strong>Yixin Chen, Ziyu Su, Lingbin Meng, Elshad Hasanov, Wei Chen, Anil Parwani, M. Khalid Khan Niazi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07608">https://arxiv.org/abs/2602.07608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07608">https://arxiv.org/pdf/2602.07608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07608]] HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology(https://arxiv.org/abs/2602.07608)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.</li>
</ul>

<h3>Title: SciClaimEval: Cross-modal Claim Verification in Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Xanh Ho, Yun-Ang Wu, Sunisth Kumar, Tian Cheng Xia, Florian Boudin, Andre Greiner-Petter, Akiko Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07621">https://arxiv.org/abs/2602.07621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07621">https://arxiv.org/pdf/2602.07621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07621]] SciClaimEval: Cross-modal Claim Verification in Scientific Papers(https://arxiv.org/abs/2602.07621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.</li>
</ul>

<h3>Title: Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jaewook Lee, Alexander Scarlatos, Simon Woodhead, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07639">https://arxiv.org/abs/2602.07639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07639">https://arxiv.org/pdf/2602.07639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07639]] Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization(https://arxiv.org/abs/2602.07639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.</li>
</ul>

<h3>Title: Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Feiyang Xiao, Le Xue, Wenbo Zhang, Gang Feng, Chenguang Zheng, Yuan Qi, Yuan Cheng, Zixin Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07643">https://arxiv.org/abs/2602.07643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07643">https://arxiv.org/pdf/2602.07643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07643]] Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation(https://arxiv.org/abs/2602.07643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.</li>
</ul>

<h3>Title: Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents</h3>
<ul>
<li><strong>Authors: </strong>Sai Puppala, Ismail Hossain, Md Jahangir Alam, Yoonpyo Lee, Jay Yoo, Tanzim Ahad, Syed Bahauddin Alam, Sajedul Talukder</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07652">https://arxiv.org/abs/2602.07652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07652">https://arxiv.org/pdf/2602.07652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07652]] Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents(https://arxiv.org/abs/2602.07652)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \pm 0.04$ (LangGraph) to $0.51 \pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \pm 0.08$), Authorization Confusion ($0.54 \pm 0.10$), Retrieval Poisoning ($0.47 \pm 0.09$), and Planning Manipulation ($0.44 \pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($\rho \approx 0.63$ and $\rho \approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.</li>
</ul>

<h3>Title: AirCatch: Effectively tracing advanced tag-based trackers</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kumar Mishra, Swadeep, Guevara Noubir, Mathieu Cunche</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07656">https://arxiv.org/abs/2602.07656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07656">https://arxiv.org/pdf/2602.07656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07656]] AirCatch: Effectively tracing advanced tag-based trackers(https://arxiv.org/abs/2602.07656)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Tag-based tracking ecosystems help users locate lost items, but can be leveraged for unwanted tracking and stalking. Existing protocol-driven defenses and prior academic solutions largely assume stable identifiers or predictable beaconing. However, identifier-based defenses fundamentally break down against advanced rogue trackers that aggressively rotate identifiers. We present AirCatch, a passive detection system that exploits a physical-layer constraint: while logical identifiers can change arbitrarily fast, the transmitter's analog imprint remains stable and reappears as a compact and persistently occupied region in Carrier Frequency Offset (CFO) feature space. AirCatch advances the state of the art along three axes: (i) a novel, modulation-aware CFO fingerprint that augments packet-level CFO with content-independent CFO components that amplify device distinctiveness; (ii) a new tracking detection algorithm based on high core density and persistence that is robust to contamination and evasion through per-identifier segmentation; and (iii) an ultra-low-cost receiver, an approximately 10 dollar BLE SDR named BlePhasyr, built from commodity components, that makes RF fingerprinting based detection practical in resource-constrained deployments. We evaluate AirCatch across Apple, Google, Tile, and Samsung tag families in multi-hour captures, systematically stress-test evasion using a scenario generator over a grid of transmission and rotation periods, and validate in diverse real-world mobility traces including home and office commutes, public transport, car travel, and airport journeys while sweeping background tag density. Across these stress tests, AirCatch achieves no false positives and early detection over a wide range of adversarial configurations and environments, degrading gracefully only in extreme low-rate regimes that also reduce attacker utility.</li>
</ul>

<h3>Title: Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Avinash Kumar K M, Samarth S. Raut</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07658">https://arxiv.org/abs/2602.07658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07658">https://arxiv.org/pdf/2602.07658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07658]] Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation(https://arxiv.org/abs/2602.07658)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.</li>
</ul>

<h3>Title: SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned</h3>
<ul>
<li><strong>Authors: </strong>Cen Zhang, Younggi Park, Fabian Fleischer, Yu-Fu Fu, Jiho Kim, Dongkwan Kim, Youngjoon Kim, Qingxiao Xu, Andrew Chin, Ze Sheng, Hanqing Zhao, Brian J. Lee, Joshua Wang, Michael Pelican, David J. Musliner, Jeff Huang, Jon Silliman, Mikel Mcdaniel, Jefferson Casavant, Isaac Goldthwaite, Nicholas Vidovich, Matthew Lehman, Taesoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07666">https://arxiv.org/abs/2602.07666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07666">https://arxiv.org/pdf/2602.07666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07666]] SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned(https://arxiv.org/abs/2602.07666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.</li>
</ul>

<h3>Title: Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Ross Greer, Laura Fleig, Maitrayee Keskar, Erika Maquiling, Giovanni Tapia Lopez, Angel Martinez-Sanchez, Parthib Roy, Jake Rattigan, Mira Sur, Alejandra Vidrio, Thomas Marcotte, Mohan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07668">https://arxiv.org/abs/2602.07668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07668">https://arxiv.org/pdf/2602.07668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07668]] Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making(https://arxiv.org/abs/2602.07668)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.</li>
</ul>

<h3>Title: Federated Learning with Profile Mapping under Distribution Shifts and Drifts</h3>
<ul>
<li><strong>Authors: </strong>Mohan Li, Dario Fenoglio, Martin Gjoreski, Marc Langheinrich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07671">https://arxiv.org/abs/2602.07671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07671">https://arxiv.org/pdf/2602.07671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07671]] Federated Learning with Profile Mapping under Distribution Shifts and Drifts(https://arxiv.org/abs/2602.07671)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.</li>
</ul>

<h3>Title: Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jiangnan Fang, Cheng-Tse Liu, Hanieh Deilamsalehy, Nesreen K. Ahmed, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07673">https://arxiv.org/abs/2602.07673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07673">https://arxiv.org/pdf/2602.07673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07673]] Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation(https://arxiv.org/abs/2602.07673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.</li>
</ul>

<h3>Title: ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets</h3>
<ul>
<li><strong>Authors: </strong>Bohdan Turbal, Iryna Voitsitska, Lesia Semenova</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07674">https://arxiv.org/abs/2602.07674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07674">https://arxiv.org/pdf/2602.07674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07674]] ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets(https://arxiv.org/abs/2602.07674)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.</li>
</ul>

<h3>Title: Spectral Gating Networks</h3>
<ul>
<li><strong>Authors: </strong>Jusheng Zhang, Yijia Fan, Kaitong Cai, Jing Yang, Yongsen Zheng, Kwok-Yan Lam, Liang Lin, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07679">https://arxiv.org/abs/2602.07679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07679">https://arxiv.org/pdf/2602.07679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07679]] Spectral Gating Networks(https://arxiv.org/abs/2602.07679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gating mechanisms are ubiquitous, yet a complementary question in feed-forward networks remains under-explored: how to introduce frequency-rich expressivity without sacrificing stability and scalability? This tension is exposed by spline-based Kolmogorov-Arnold Network (KAN) parameterizations, where grid refinement can induce parameter growth and brittle optimization in high dimensions. To propose a stability-preserving way to inject spectral capacity into existing MLP/FFN layers under fixed parameter and training budgets, we introduce Spectral Gating Networks (SGN), a drop-in spectral reparameterization. SGN augments a standard activation pathway with a compact spectral pathway and learnable gates that allow the model to start from a stable base behavior and progressively allocate capacity to spectral features during training. The spectral pathway is instantiated with trainable Random Fourier Features (learned frequencies and phases), replacing grid-based splines and removing resolution dependence. A hybrid GELU-Fourier formulation further improves optimization robustness while enhancing high-frequency fidelity. Across vision, NLP, audio, and PDE benchmarks, SGN consistently improves accuracy-efficiency trade-offs under comparable computational budgets, achieving 93.15% accuracy on CIFAR-10 and up to 11.7x faster inference than spline-based KAN variants. Code and trained models will be released.</li>
</ul>

<h3>Title: Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning</h3>
<ul>
<li><strong>Authors: </strong>Ross Greer, Maitrayee Keskar, Angel Martinez-Sanchez, Parthib Roy, Shashank Shriram, Mohan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07680">https://arxiv.org/abs/2602.07680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07680">https://arxiv.org/pdf/2602.07680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07680]] Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning(https://arxiv.org/abs/2602.07680)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.</li>
</ul>

<h3>Title: Process-of-Thought Reasoning for Videos</h3>
<ul>
<li><strong>Authors: </strong>Jusheng Zhang, Kaitong Cai, Jian Wang, Yongsen Zheng, Kwok-Yan Lam, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07689">https://arxiv.org/abs/2602.07689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07689">https://arxiv.org/pdf/2602.07689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07689]] Process-of-Thought Reasoning for Videos(https://arxiv.org/abs/2602.07689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.</li>
</ul>

<h3>Title: Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes</h3>
<ul>
<li><strong>Authors: </strong>Wenping Jin, Yuyang Tang, Li Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07694">https://arxiv.org/abs/2602.07694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07694">https://arxiv.org/pdf/2602.07694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07694]] Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes(https://arxiv.org/abs/2602.07694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at this https URL.</li>
</ul>

<h3>Title: A hybrid Kolmogorov-Arnold network for medical image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Deep Bhattacharyya, Ali Ayub, A. Ben Hamza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07702">https://arxiv.org/abs/2602.07702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07702">https://arxiv.org/pdf/2602.07702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07702]] A hybrid Kolmogorov-Arnold network for medical image segmentation(https://arxiv.org/abs/2602.07702)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.</li>
</ul>

<h3>Title: Dense Feature Learning via Linear Structure Preservation in Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Yuanyun Zhang, Mingxuan Zhang, Siyuan Li, Zihan Wang, Haoran Chen, Wenbo Zhou, Shi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07706">https://arxiv.org/abs/2602.07706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07706">https://arxiv.org/pdf/2602.07706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07706]] Dense Feature Learning via Linear Structure Preservation in Medical Data(https://arxiv.org/abs/2602.07706)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.</li>
</ul>

<h3>Title: Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization</h3>
<ul>
<li><strong>Authors: </strong>Ding Zhang, Siddharth Betala, Chirag Agarwal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07708">https://arxiv.org/abs/2602.07708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07708">https://arxiv.org/pdf/2602.07708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07708]] Quantifying Explanation Quality in Graph Neural Networks using Out-of-Distribution Generalization(https://arxiv.org/abs/2602.07708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of post-hoc explanations for Graph Neural Networks (GNNs) remains a significant challenge. While recent years have seen an increasing development of explainability methods, current evaluation metrics (e.g., fidelity, sparsity) often fail to assess whether an explanation identifies the true underlying causal variables. To address this, we propose the Explanation-Generalization Score (EGS), a metric that quantifies the causal relevance of GNN explanations. EGS is founded on the principle of feature invariance and posits that if an explanation captures true causal drivers, it should lead to stable predictions across distribution shifts. To quantify this, we introduce a framework that trains GNNs using explanatory subgraphs and evaluates their performance in Out-of-Distribution (OOD) settings (here, OOD generalization serves as a rigorous proxy for the explanation's causal validity). Through large-scale validation involving 11,200 model combinations across synthetic and real-world datasets, our results demonstrate that EGS provides a principled benchmark for ranking explainers based on their ability to capture causal substructures, offering a robust alternative to traditional fidelity-based metrics.</li>
</ul>

<h3>Title: Towards Robust Scaling Laws for Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Volkova, Mher Safaryan, Christoph H. Lampert, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07712">https://arxiv.org/abs/2602.07712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07712">https://arxiv.org/pdf/2602.07712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07712]] Towards Robust Scaling Laws for Optimizers(https://arxiv.org/abs/2602.07712)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The quality of Large Language Model (LLM) pretraining depends on multiple factors, including the compute budget and the choice of optimization algorithm. Empirical scaling laws are widely used to predict loss as model size and training data grow, however, almost all existing studies fix the optimizer (typically AdamW). At the same time, a new generation of optimizers (e.g., Muon, Shampoo, SOAP) promises faster and more stable convergence, but their relationship with model and data scaling is not yet well understood. In this work, we study scaling laws across different optimizers. Empirically, we show that 1) separate Chinchilla-style scaling laws for each optimizer are ill-conditioned and have highly correlated parameters. Instead, 2) we propose a more robust law with shared power-law exponents and optimizer-specific rescaling factors, which enable direct comparison between optimizers. Finally, 3) we provide a theoretical analysis of gradient-based methods for the proxy task of a convex quadratic objective, demonstrating that Chinchilla-style scaling laws emerge naturally as a result of loss decomposition into irreducible, approximation, and optimization errors.</li>
</ul>

<h3>Title: Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Roi Benita, Michael Elad, Joseph Keshet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07715">https://arxiv.org/abs/2602.07715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07715">https://arxiv.org/pdf/2602.07715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07715]] Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models(https://arxiv.org/abs/2602.07715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.</li>
</ul>

<h3>Title: All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yingjie Li, Daniel Robinson, Cunxi Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07717">https://arxiv.org/abs/2602.07717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07717">https://arxiv.org/pdf/2602.07717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07717]] All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving(https://arxiv.org/abs/2602.07717)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.</li>
</ul>

<h3>Title: ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Qi, Xinhang Chen, Huiqiang Jiang, Qitong Wang, Botao Peng, Themis Palpanas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07721">https://arxiv.org/abs/2602.07721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07721">https://arxiv.org/pdf/2602.07721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07721]] ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs(https://arxiv.org/abs/2602.07721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.</li>
</ul>

<h3>Title: IPBAC: Interaction Provenance-Based Access Control for Secure and Privacy-Aware Systems</h3>
<ul>
<li><strong>Authors: </strong>Sharif Noor Zisad, Ragib Hasan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07722">https://arxiv.org/abs/2602.07722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07722">https://arxiv.org/pdf/2602.07722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07722]] IPBAC: Interaction Provenance-Based Access Control for Secure and Privacy-Aware Systems(https://arxiv.org/abs/2602.07722)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Traditional access control systems, including RBAC, face significant limitations such as inflexible role definitions, difficulty handling dynamic scenarios, and lack of detailed accountability and traceability. To this end, we introduce the Interaction Provenance-based Access Control (IPBAC) model. In this paper, we explore the integration of interaction provenance with access control to overcome these limitations. Interaction provenance refers to the detailed recording of actions and interactions within a system, capturing comprehensive metadata such as the identity of the actor, the time of an action, and the context. IPBAC ensures stronger protection against unauthorized access, enhances traceability for auditing and compliance, and supports adaptive security policies. This provenance-based access control not only strengthens security, but also provides a robust framework for auditing and compliance.</li>
</ul>

<h3>Title: Leveraging the Power of Ensemble Learning for Secure Low Altitude Economy</h3>
<ul>
<li><strong>Authors: </strong>Yaoqi Yang, Yong Chen, Jiacheng Wang, Geng Sun, Dusit Niyato, Zhu Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07725">https://arxiv.org/abs/2602.07725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07725">https://arxiv.org/pdf/2602.07725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07725]] Leveraging the Power of Ensemble Learning for Secure Low Altitude Economy(https://arxiv.org/abs/2602.07725)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>Low Altitude Economy (LAE) holds immense promise for enhancing societal well-being and driving economic growth. However, this burgeoning field is vulnerable to security threats, particularly malicious aircraft intrusion attacks. To address the above concerns, intrusion detection systems (IDS) can be used to defend against malicious aircraft intrusions in LAE. Whereas, due to the heterogeneous data, dynamic environment, and resource-constrained devices within LAE, current IDS face challenges in detection accuracy, adaptability, and resource utilization ratio. In this regard, due to the inherent ability to combine the strengths of multiple models, ensemble learning can realize more robust and diverse anomaly detection further enhance IDS accuracy, thereby improving robustness and efficiency of the secure LAE. Unlike single-model approaches, ensemble learning can leverage the collective knowledge of its constituent models to effectively defend the malicious aircraft intrusion attacks. Specifically, this paper investigates ensemble learning for secure LAE, covering research focuses, solutions, and a case study. We first establish the rationale for ensemble learning and then review research areas and potential solutions, demonstrating the necessities and benefits of applying ensemble learning to secure LAE. Subsequently, we propose a framework of ensemble learning-enabled malicious aircrafts tracking in the secure LAE, where its feasibility and effectiveness are evaluated by the designed case study. Finally, we conclude by outlining promising future research directions for further advancing the ensemble learning-enabled secure LAE.</li>
</ul>

<h3>Title: Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sagnik Mukherjee, Lifan Yuan, Pavan Jayasinha, Dilek Hakkani-Tür, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07729">https://arxiv.org/abs/2602.07729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07729">https://arxiv.org/pdf/2602.07729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07729]] Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs(https://arxiv.org/abs/2602.07729)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.</li>
</ul>

<h3>Title: Efficient Adaptive Data Analysis over Dense Distributions</h3>
<ul>
<li><strong>Authors: </strong>Joon Suk Huh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07732">https://arxiv.org/abs/2602.07732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07732">https://arxiv.org/pdf/2602.07732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07732]] Efficient Adaptive Data Analysis over Dense Distributions(https://arxiv.org/abs/2602.07732)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.</li>
</ul>

<h3>Title: TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations</h3>
<ul>
<li><strong>Authors: </strong>Matteo Rossi, Ryan Pederson, Miles Wang-Henderson, Ben Kaufman, Edward C. Williams, Carl Underkoffler, Owen Lewis Howell, Adrian Layer, Stephan Thaler, Narbe Mardirossian, John Anthony Parkhill</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07735">https://arxiv.org/abs/2602.07735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07735">https://arxiv.org/pdf/2602.07735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07735]] TerraBind: Fast and Accurate Binding Affinity Prediction through Coarse Structural Representations(https://arxiv.org/abs/2602.07735)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present TerraBind, a foundation model for protein-ligand structure and binding affinity prediction that achieves 26-fold faster inference than state-of-the-art methods while improving affinity prediction accuracy by $\sim$20\%. Current deep learning approaches to structure-based drug design rely on expensive all-atom diffusion to generate 3D coordinates, creating inference bottlenecks that render large-scale compound screening computationally intractable. We challenge this paradigm with a critical hypothesis: full all-atom resolution is unnecessary for accurate small molecule pose and binding affinity prediction. TerraBind tests this hypothesis through a coarse pocket-level representation (protein C$_\beta$ atoms and ligand heavy atoms only) within a multimodal architecture combining COATI-3 molecular encodings and ESM-2 protein embeddings that learns rich structural representations, which are used in a diffusion-free optimization module for pose generation and a binding affinity likelihood prediction module. On structure prediction benchmarks (FoldBench, PoseBusters, Runs N' Poses), TerraBind matches diffusion-based baselines in ligand pose accuracy. Crucially, TerraBind outperforms Boltz-2 by $\sim$20\% in Pearson correlation for binding affinity prediction on both a public benchmark (CASP16) and a diverse proprietary dataset (18 biochemical/cell assays). We show that the affinity prediction module also provides well-calibrated affinity uncertainty estimates, addressing a critical gap in reliable compound prioritization for drug discovery. Furthermore, this module enables a continual learning framework and a hedged batch selection strategy that, in simulated drug discovery cycles, achieves 6$\times$ greater affinity improvement of selected molecules over greedy-based approaches.</li>
</ul>

<h3>Title: Learnable Chernoff Baselines for Inference-Time Alignment</h3>
<ul>
<li><strong>Authors: </strong>Sunil Madhow, Yuchen Liang, Ness Shroff, Yingbin Liang, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07738">https://arxiv.org/abs/2602.07738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07738">https://arxiv.org/pdf/2602.07738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07738]] Learnable Chernoff Baselines for Inference-Time Alignment(https://arxiv.org/abs/2602.07738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.</li>
</ul>

<h3>Title: Riemannian MeanFlow</h3>
<ul>
<li><strong>Authors: </strong>Dongyeop Woo, Marta Skreta, Seonghyun Park, Sungsoo Ahn, Kirill Neklyudov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07744">https://arxiv.org/abs/2602.07744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07744">https://arxiv.org/pdf/2602.07744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07744]] Riemannian MeanFlow(https://arxiv.org/abs/2602.07744)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion and flow models have become the dominant paradigm for generative modeling on Riemannian manifolds, with successful applications in protein backbone generation and DNA sequence design. However, these methods require tens to hundreds of neural network evaluations at inference time, which can become a computational bottleneck in large-scale scientific sampling workflows. We introduce Riemannian MeanFlow~(RMF), a framework for learning flow maps directly on manifolds, enabling high-quality generations with as few as one forward pass. We derive three equivalent characterizations of the manifold average velocity (Eulerian, Lagrangian, and semigroup identities), and analyze parameterizations and stabilization techniques to improve training on high-dimensional manifolds. In promoter DNA design and protein backbone generation settings, RMF achieves comparable sample quality to prior methods while requiring up to 10$\times$ fewer function evaluations. Finally, we show that few-step flow maps enable efficient reward-guided design through reward look-ahead, where terminal states can be predicted from intermediate steps at minimal additional cost.</li>
</ul>

<h3>Title: Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Haodong Li, Shaoteng Liu, Zhe Lin, Manmohan Chandraker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07775">https://arxiv.org/abs/2602.07775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07775">https://arxiv.org/pdf/2602.07775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07775]] Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion(https://arxiv.org/abs/2602.07775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: this https URL</li>
</ul>

<h3>Title: Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shenglai Zeng, Tianqi Zheng, Chuan Tian, Dante Everaert, Yau-Shian Wang, Yupin Huang, Michael J. Morais, Rohit Patki, Jinjin Tian, Xinnan Dai, Kai Guo, Monica Xiao Cheng, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07778">https://arxiv.org/abs/2602.07778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07778">https://arxiv.org/pdf/2602.07778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07778]] Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs(https://arxiv.org/abs/2602.07778)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.</li>
</ul>

<h3>Title: Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ningyu Xu, Qi Zhang, Xipeng Qiu, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07794">https://arxiv.org/abs/2602.07794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07794">https://arxiv.org/pdf/2602.07794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07794]] Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models(https://arxiv.org/abs/2602.07794)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.</li>
</ul>

<h3>Title: Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents</h3>
<ul>
<li><strong>Authors: </strong>Jiatong Li, Changdae Oh, Hyeong Kyu Choi, Jindong Wang, Sharon Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07796">https://arxiv.org/abs/2602.07796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07796">https://arxiv.org/pdf/2602.07796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07796]] Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents(https://arxiv.org/abs/2602.07796)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at this https URL.</li>
</ul>

<h3>Title: CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Ruiqi Wang, Ruikang Liu, Runyu Chen, Haoxiang Suo, Zhiyi Peng, Zhuo Tang, Changjian Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07798">https://arxiv.org/abs/2602.07798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07798">https://arxiv.org/pdf/2602.07798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07798]] CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection(https://arxiv.org/abs/2602.07798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at this https URL.</li>
</ul>

<h3>Title: Fairness Aware Reward Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ching Lam Choi, Vighnesh Subramaniam, Phillip Isola, Antonio Torralba, Stefanie Jegelka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07799">https://arxiv.org/abs/2602.07799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07799">https://arxiv.org/pdf/2602.07799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07799]] Fairness Aware Reward Optimization(https://arxiv.org/abs/2602.07799)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.</li>
</ul>

<h3>Title: Approximating Matrix Functions with Deep Neural Networks and Transformers</h3>
<ul>
<li><strong>Authors: </strong>Rahul Padmanabhan, Simone Brugiapaglia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07800">https://arxiv.org/abs/2602.07800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07800">https://arxiv.org/pdf/2602.07800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07800]] Approximating Matrix Functions with Deep Neural Networks and Transformers(https://arxiv.org/abs/2602.07800)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have revolutionized natural language processing, but their use for numerical computation has received less attention. We study the approximation of matrix functions, which map scalar functions to matrices, using neural networks including transformers. We focus on functions mapping square matrices to square matrices of the same dimension. These types of matrix functions appear throughout scientific computing, e.g., the matrix exponential in continuous-time Markov chains and the matrix sign function in stability analysis of dynamical systems. In this paper, we make two contributions. First, we prove bounds on the width and depth of ReLU networks needed to approximate the matrix exponential to an arbitrary precision. Second, we show experimentally that a transformer encoder-decoder with suitable numerical encodings can approximate certain matrix functions at a relative error of 5% with high probability. Our study reveals that the encoding scheme strongly affects performance, with different schemes working better for different functions.</li>
</ul>

<h3>Title: Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ding, Pengyu Tong, Ranjie Duan, Yunjian Zhang, Rui Sun, Yao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07804">https://arxiv.org/abs/2602.07804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07804">https://arxiv.org/pdf/2602.07804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07804]] Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models(https://arxiv.org/abs/2602.07804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.</li>
</ul>

<h3>Title: How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study</h3>
<ul>
<li><strong>Authors: </strong>Simiao Ren, Yuchen Zhou, Xingyu Shen, Kidus Zewde, Tommy Duong, George Huang, Hatsanai (Neo)Tiangratanakul, Tsang (Dennis)Ng, En Wei, Jiayu Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07814">https://arxiv.org/abs/2602.07814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07814">https://arxiv.org/pdf/2602.07814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07814]] How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study(https://arxiv.org/abs/2602.07814)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$\rho$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $\chi^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.</li>
</ul>

<h3>Title: Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures</h3>
<ul>
<li><strong>Authors: </strong>Simiao Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07815">https://arxiv.org/abs/2602.07815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07815">https://arxiv.org/pdf/2602.07815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07815]] Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures(https://arxiv.org/abs/2602.07815)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.</li>
</ul>

<h3>Title: Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chen, Yu Guan, Yajuan Huang, Chaoqi Chen, XiangJi, Qiuyun Fan, Dong Liang, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07820">https://arxiv.org/abs/2602.07820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07820">https://arxiv.org/pdf/2602.07820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07820]] Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction(https://arxiv.org/abs/2602.07820)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.</li>
</ul>

<h3>Title: Efficient Representations are Controllable Representations</h3>
<ul>
<li><strong>Authors: </strong>Charles Ye, Jasmine Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07828">https://arxiv.org/abs/2602.07828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07828">https://arxiv.org/pdf/2602.07828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07828]] Efficient Representations are Controllable Representations(https://arxiv.org/abs/2602.07828)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this. We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.</li>
</ul>

<h3>Title: SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weijiang Lv, Yaoxuan Feng, Xiaobo Xia, Jiayu Wang, Yan Jing, Wenchao Chen, Bo Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07833">https://arxiv.org/abs/2602.07833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07833">https://arxiv.org/pdf/2602.07833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07833]] SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models(https://arxiv.org/abs/2602.07833)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at this https URL.</li>
</ul>

<h3>Title: VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping</h3>
<ul>
<li><strong>Authors: </strong>Sanoojan Baliah, Yohan Abeysinghe, Rusiru Thushara, Khan Muhammad, Abhinav Dhall, Karthik Nandakumar, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07835">https://arxiv.org/abs/2602.07835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07835">https://arxiv.org/pdf/2602.07835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07835]] VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping(https://arxiv.org/abs/2602.07835)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at this https URL.</li>
</ul>

<h3>Title: Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Wang, Shiyu Ni, Zhikai Ding, Zihang Zhan, Yuanzi Li, Keping Bi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07842">https://arxiv.org/abs/2602.07842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07842">https://arxiv.org/pdf/2602.07842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07842]] Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers(https://arxiv.org/abs/2602.07842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.</li>
</ul>

<h3>Title: MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Shijie Wang, Pengfei Li, Yikun Fu, Kaifeng Liu, Fangyuan Li, Yang Liu, Xiaowei Sun, Zonglin Li, Siyao Zhao, Jian Zhao, Kai Tian, Dong Li, Junqi Gao, Yutong Zhang, Yiqun Chen, Yuqiang Li, Zoe Li, Weinan Zhang, Peng Ye, Shuyue Hu, Lei Bai, Bowen Zhou, Kaiyan Zhang, Biqing Qi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07848">https://arxiv.org/abs/2602.07848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07848">https://arxiv.org/pdf/2602.07848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07848]] MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation(https://arxiv.org/abs/2602.07848)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.</li>
</ul>

<h3>Title: Geometry-Aware Rotary Position Embedding for Consistent Video World Model</h3>
<ul>
<li><strong>Authors: </strong>Chendong Xiang, Jiajun Liu, Jintao Zhang, Xiao Yang, Zhengwei Fang, Shizun Wang, Zijun Wang, Yingtian Zou, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07854">https://arxiv.org/abs/2602.07854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07854">https://arxiv.org/pdf/2602.07854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07854]] Geometry-Aware Rotary Position Embedding for Consistent Video World Model(https://arxiv.org/abs/2602.07854)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.</li>
</ul>

<h3>Title: Dynamic Load Model for Data Centers with Pattern-Consistent Calibration</h3>
<ul>
<li><strong>Authors: </strong>Siyu Lu, Chenhan Xiao, Yang Weng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07859">https://arxiv.org/abs/2602.07859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07859">https://arxiv.org/pdf/2602.07859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07859]] Dynamic Load Model for Data Centers with Pattern-Consistent Calibration(https://arxiv.org/abs/2602.07859)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.</li>
</ul>

<h3>Title: Direct Soft-Policy Sampling via Langevin Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Donghyeon Ki, Hee-Jun Ahn, Kyungyoon Kim, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07873">https://arxiv.org/abs/2602.07873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07873">https://arxiv.org/pdf/2602.07873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07873]] Direct Soft-Policy Sampling via Langevin Dynamics(https://arxiv.org/abs/2602.07873)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.</li>
</ul>

<h3>Title: Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shankar, Yuandou Wang, Rihan Hai, Lydia Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07875">https://arxiv.org/abs/2602.07875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07875">https://arxiv.org/pdf/2602.07875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07875]] Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion(https://arxiv.org/abs/2602.07875)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: this https URL</li>
</ul>

<h3>Title: Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Wang, Huawei Fan, Yuanchao Shu, Peng Cheng, Cong Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07878">https://arxiv.org/abs/2602.07878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07878">https://arxiv.org/pdf/2602.07878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07878]] Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model(https://arxiv.org/abs/2602.07878)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. "Fill" first exhausts the global KV cache to induce Head-of-Line blocking, while "Squeeze" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.</li>
</ul>

<h3>Title: GRAFT: Decoupling Ranking and Calibration for Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Ashhad, Robert Hoehndorf, Ricardo Henao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07884">https://arxiv.org/abs/2602.07884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07884">https://arxiv.org/pdf/2602.07884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07884]] GRAFT: Decoupling Ranking and Calibration for Survival Analysis(https://arxiv.org/abs/2602.07884)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Survival analysis is complicated by censored data, high-dimensional features, and non-linear interactions. Classical models are interpretable but restrictive, while deep learning models are flexible but often non-interpretable and sensitive to noise. We propose GRAFT (Gated Residual Accelerated Failure Time), a novel AFT model that decouples prognostic ranking from calibration. GRAFT's hybrid architecture combines a linear AFT model with a non-linear residual neural network, and it also integrates stochastic gates for automatic, end-to-end feature selection. The model is trained by directly optimizing a differentiable, C-index-aligned ranking loss using stochastic conditional imputation from local Kaplan-Meier estimators. In public benchmarks, GRAFT outperforms baselines in discrimination and calibration, while remaining robust and sparse in high-noise settings.</li>
</ul>

<h3>Title: Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection</h3>
<ul>
<li><strong>Authors: </strong>Guanglong Sun, Siyuan Zhang, Liyuan Wang, Jun Zhu, Hang Su, Yi Zhong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07892">https://arxiv.org/abs/2602.07892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07892">https://arxiv.org/pdf/2602.07892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07892]] Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection(https://arxiv.org/abs/2602.07892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{this https URL}{OGPSA}</li>
</ul>

<h3>Title: Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhao Shang, Haizhao Jing, Guoting Wei, Haokui Zhang, Rong Xiao, Jianqing Gao, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07899">https://arxiv.org/abs/2602.07899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07899">https://arxiv.org/pdf/2602.07899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07899]] Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models(https://arxiv.org/abs/2602.07899)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.</li>
</ul>

<h3>Title: Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Giang Ngo, Dat Phan Trong, Dang Nguyen, Sunil Gupta, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07904">https://arxiv.org/abs/2602.07904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07904">https://arxiv.org/pdf/2602.07904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07904]] Adaptive Acquisition Selection for Bayesian Optimization with Large Language Models(https://arxiv.org/abs/2602.07904)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Bayesian Optimization critically depends on the choice of acquisition function, but no single strategy is universally optimal; the best choice is non-stationary and problem-dependent. Existing adaptive portfolio methods often base their decisions on past function values while ignoring richer information like remaining budget or surrogate model characteristics. To address this, we introduce LMABO, a novel framework that casts a pre-trained Large Language Model (LLM) as a zero-shot, online strategist for the BO process. At each iteration, LMABO uses a structured state representation to prompt the LLM to select the most suitable acquisition function from a diverse portfolio. In an evaluation across 50 benchmark problems, LMABO demonstrates a significant performance improvement over strong static, adaptive portfolio, and other LLM-based baselines. We show that the LLM's behavior is a comprehensive strategy that adapts to real-time progress, proving its advantage stems from its ability to process and synthesize the complete optimization state into an effective, adaptive policy.</li>
</ul>

<h3>Title: AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering</h3>
<ul>
<li><strong>Authors: </strong>Yuzhu Cai, Zexi Liu, Xinyu Zhu, Cheng Wang, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Di Jin, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07906">https://arxiv.org/abs/2602.07906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07906">https://arxiv.org/pdf/2602.07906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07906]] AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering(https://arxiv.org/abs/2602.07906)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at this https URL.</li>
</ul>

<h3>Title: SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Hang Guo, Wang Lu, Tao Dai, Shu-Tao Xia, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07909">https://arxiv.org/abs/2602.07909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07909">https://arxiv.org/pdf/2602.07909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07909]] SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization(https://arxiv.org/abs/2602.07909)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$\tau$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {this https URL}.</li>
</ul>

<h3>Title: CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Huiyang Yi, Xiaojian Shen, Yonggang Wu, Duxin Chen, He Wang, Wenwu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07915">https://arxiv.org/abs/2602.07915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07915">https://arxiv.org/pdf/2602.07915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07915]] CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios(https://arxiv.org/abs/2602.07915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at this https URL.</li>
</ul>

<h3>Title: CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution</h3>
<ul>
<li><strong>Authors: </strong>Minbeom Kim, Mihir Parmar, Phillip Wallis, Lesly Miculicich, Kyomin Jung, Krishnamurthy Dj Dvijotham, Long T. Le, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07918">https://arxiv.org/abs/2602.07918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07918">https://arxiv.org/pdf/2602.07918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07918]] CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution(https://arxiv.org/abs/2602.07918)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, explainability</a></li>
<li><strong>Abstract: </strong>AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.</li>
</ul>

<h3>Title: A Kinetic-Energy Perspective of Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Li, Huancheng Hu, Soon Hoe Lim, Xuyu Li, Fei Gao, Enmao Diao, Zezhen Ding, Michalis Vazirgiannis, Henrik Bostrom</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07928">https://arxiv.org/abs/2602.07928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07928">https://arxiv.org/pdf/2602.07928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07928]] A Kinetic-Energy Perspective of Flow Matching(https://arxiv.org/abs/2602.07928)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.</li>
</ul>

<h3>Title: Patches of Nonlinearity: Instruction Vectors in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Irina Bigoulaeva, Jonas Rohweder, Subhabrata Dutta, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07930">https://arxiv.org/abs/2602.07930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07930">https://arxiv.org/pdf/2602.07930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07930]] Patches of Nonlinearity: Instruction Vectors in Large Language Models(https://arxiv.org/abs/2602.07930)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.</li>
</ul>

<h3>Title: Which private attributes do VLMs agree on and predict well?</h3>
<ul>
<li><strong>Authors: </strong>Olena Hrynenko, Darya Baranouskaya, Alina Elena Baia, Andrea Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07931">https://arxiv.org/abs/2602.07931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07931">https://arxiv.org/pdf/2602.07931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07931]] Which private attributes do VLMs agree on and predict well?(https://arxiv.org/abs/2602.07931)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.</li>
</ul>

<h3>Title: Privacy-Preserving Covert Communication Using Encrypted Wearable Gesture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Tasnia Ashrafi Heya, Sayed Erfan Arefin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07936">https://arxiv.org/abs/2602.07936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07936">https://arxiv.org/pdf/2602.07936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07936]] Privacy-Preserving Covert Communication Using Encrypted Wearable Gesture Recognition(https://arxiv.org/abs/2602.07936)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, biometric</a></li>
<li><strong>Abstract: </strong>Secure communication is essential in covert and safety-critical settings where verbal interactions may expose user intent or operational context. Wearable gesture-based communication enables low-effort, nonverbal interaction, but existing systems leak motion data, intermediate representations, or inference outputs to untrusted infrastructure, enabling intent inference, behavioral biometric leakage, and insider attacks. This work proposes a privacy-preserving gesture-based covert communication system that ensures, no raw sensor signals, learned features, or classification outputs are exposed to any third-party. The system employs a multi-party homomorphic learning pipeline for gesture recognition directly over encrypted motion data, preventing adversaries from inferring gesture semantics, replaying sensor traces, or accessing intermediate representations. To our knowledge, this work is the first to apply encrypted gesture recognition in a wearable-based covert communication setting. We design and evaluate haptic and visual feedback mechanisms for covert signal delivery and evaluate the system using 600 gesture samples from a commodity smartwatch, achieving over 94.44% classification accuracy and demonstrating the feasibility of the proposed system with practical deployability from high-performance systems to resource-constrained edge devices.</li>
</ul>

<h3>Title: Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps</h3>
<ul>
<li><strong>Authors: </strong>Rabbia Asghar, Lukas Rummelhard, Wenqian Liu, Anne Spalanzani, Christian Laugier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07938">https://arxiv.org/abs/2602.07938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07938">https://arxiv.org/pdf/2602.07938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07938]] Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps(https://arxiv.org/abs/2602.07938)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.</li>
</ul>

<h3>Title: Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Wróbel, Jan Maria Kowalski, Jerzy Surma, Igor Ciuciura, Maciej Szymański</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07954">https://arxiv.org/abs/2602.07954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07954">https://arxiv.org/pdf/2602.07954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07954]] Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation(https://arxiv.org/abs/2602.07954)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.</li>
</ul>

<h3>Title: D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning</h3>
<ul>
<li><strong>Authors: </strong>Changli Tang, Tianyi Wang, Fengyun Rao, Jing Lyu, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07960">https://arxiv.org/abs/2602.07960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07960">https://arxiv.org/pdf/2602.07960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07960]] D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning(https://arxiv.org/abs/2602.07960)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{this https URL}{this https URL}. Our code, data, and checkpoints will be available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms</h3>
<ul>
<li><strong>Authors: </strong>Vaibhav Shukla, Hardik Sharma, Adith N Reganti, Soham Wasmatkar, Bagesh Kumar, Vrijendra Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07963">https://arxiv.org/abs/2602.07963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07963">https://arxiv.org/pdf/2602.07963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07963]] Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms(https://arxiv.org/abs/2602.07963)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.</li>
</ul>

<h3>Title: EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Tan, Wanjiang Weng, Haodong Lei, Hongsong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07967">https://arxiv.org/abs/2602.07967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07967">https://arxiv.org/pdf/2602.07967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07967]] EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation(https://arxiv.org/abs/2602.07967)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {this https URL}.</li>
</ul>

<h3>Title: Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness</h3>
<ul>
<li><strong>Authors: </strong>Xin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07974">https://arxiv.org/abs/2602.07974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07974">https://arxiv.org/pdf/2602.07974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07974]] Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness(https://arxiv.org/abs/2602.07974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).</li>
</ul>

<h3>Title: Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection</h3>
<ul>
<li><strong>Authors: </strong>Rui Feng, Zhiyao Luo, Liuyu Wu, Wei Wang, Yuting Song, Yong Liu, Kok Pin Ng, Jianqing Li, Xingyao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07978">https://arxiv.org/abs/2602.07978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07978">https://arxiv.org/pdf/2602.07978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07978]] Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection(https://arxiv.org/abs/2602.07978)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.</li>
</ul>

<h3>Title: FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Peng Peng, Xinrui Zhang, Junlin Wang, Lei Li, Shaoyu Wang, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07979">https://arxiv.org/abs/2602.07979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07979">https://arxiv.org/pdf/2602.07979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07979]] FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction(https://arxiv.org/abs/2602.07979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.</li>
</ul>

<h3>Title: Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Junlin Wang, Jiancheng Fang, Peng Peng, Shaoyu Wang, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07980">https://arxiv.org/abs/2602.07980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07980">https://arxiv.org/pdf/2602.07980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07980]] Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction(https://arxiv.org/abs/2602.07980)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.</li>
</ul>

<h3>Title: Deepfake Synthesis vs. Detection: An Uneven Contest</h3>
<ul>
<li><strong>Authors: </strong>Md. Tarek Hasan, Sanjay Saha, Shaojing Fan, Swakkhar Shatabda, Terence Sim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07986">https://arxiv.org/abs/2602.07986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07986">https://arxiv.org/pdf/2602.07986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07986]] Deepfake Synthesis vs. Detection: An Uneven Contest(https://arxiv.org/abs/2602.07986)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.</li>
</ul>

<h3>Title: When Is Compositional Reasoning Learnable from Verifiable Rewards?</h3>
<ul>
<li><strong>Authors: </strong>Daniel Barzilai, Yotam Wolf, Ronen Basri</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07992">https://arxiv.org/abs/2602.07992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07992">https://arxiv.org/pdf/2602.07992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07992]] When Is Compositional Reasoning Learnable from Verifiable Rewards?(https://arxiv.org/abs/2602.07992)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.</li>
</ul>

<h3>Title: MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance</h3>
<ul>
<li><strong>Authors: </strong>Xuehai Bai, Xiaoling Gu, Akide Liu, Hangjie Yuan, YiFan Zhang, Jack Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07993">https://arxiv.org/abs/2602.07993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07993">https://arxiv.org/pdf/2602.07993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07993]] MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance(https://arxiv.org/abs/2602.07993)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.</li>
</ul>

<h3>Title: The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Arash Marioriyad, Omid Ghahroodi, Ehsaneddin Asgari, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.07996">https://arxiv.org/abs/2602.07996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.07996">https://arxiv.org/pdf/2602.07996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.07996]] The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation(https://arxiv.org/abs/2602.07996)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.</li>
</ul>

<h3>Title: Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection</h3>
<ul>
<li><strong>Authors: </strong>Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08003">https://arxiv.org/abs/2602.08003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08003">https://arxiv.org/pdf/2602.08003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08003]] Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection(https://arxiv.org/abs/2602.08003)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.</li>
</ul>

<h3>Title: ForecastOcc: Vision-based Semantic Occupancy Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Riya Mohan, Juana Valeria Hurtado, Rohit Mohan, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08006">https://arxiv.org/abs/2602.08006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08006">https://arxiv.org/pdf/2602.08006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08006]] ForecastOcc: Vision-based Semantic Occupancy Forecasting(https://arxiv.org/abs/2602.08006)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.</li>
</ul>

<h3>Title: A Unified Density Operator View of Flow Control and Merging</h3>
<ul>
<li><strong>Authors: </strong>Riccardo De Santi, Malte Franke, Ya-Ping Hsieh, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08012">https://arxiv.org/abs/2602.08012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08012">https://arxiv.org/pdf/2602.08012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08012]] A Unified Density Operator View of Flow Control and Merging(https://arxiv.org/abs/2602.08012)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.</li>
</ul>

<h3>Title: ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning</h3>
<ul>
<li><strong>Authors: </strong>Sadegh Sohani, Salar Ghazi, Farnaz Kamranfar, Sahar Pilehvar Moakhar, Mohammad Allahbakhsh, Haleh Amintoosi, Kaiwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08014">https://arxiv.org/abs/2602.08014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08014">https://arxiv.org/pdf/2602.08014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08014]] ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning(https://arxiv.org/abs/2602.08014)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy. The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data. For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.</li>
</ul>

<h3>Title: The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications</h3>
<ul>
<li><strong>Authors: </strong>Dong Pan, Bingtao Li, Yongsheng Zheng, Jiren Ma, Victor Fei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08019">https://arxiv.org/abs/2602.08019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08019">https://arxiv.org/pdf/2602.08019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08019]] The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications(https://arxiv.org/abs/2602.08019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.</li>
</ul>

<h3>Title: PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping</h3>
<ul>
<li><strong>Authors: </strong>Minghai Chen, Mingyuan Liu, Yuxiang Huan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08020">https://arxiv.org/abs/2602.08020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08020">https://arxiv.org/pdf/2602.08020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08020]] PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping(https://arxiv.org/abs/2602.08020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.</li>
</ul>

<h3>Title: CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment</h3>
<ul>
<li><strong>Authors: </strong>Nanda Rani, Kimberly Milner, Minghao Shao, Meet Udeshi, Haoran Xi, Venkata Sai Charan Putrevu, Saksham Aggarwal, Sandeep K. Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Muhammad Shafique, Ramesh Karri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08023">https://arxiv.org/abs/2602.08023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08023">https://arxiv.org/pdf/2602.08023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08023]] CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment(https://arxiv.org/abs/2602.08023)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.</li>
</ul>

<h3>Title: FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Fan, Keyu Chen, Ruilong Xing, Yulin Li, Li Jiang, Zhuotao Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08024">https://arxiv.org/abs/2602.08024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08024">https://arxiv.org/pdf/2602.08024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08024]] FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging(https://arxiv.org/abs/2602.08024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at this https URL.</li>
</ul>

<h3>Title: Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Po-Chun Chen, Hen-Hsen Huang, Hsin-Hsi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08028">https://arxiv.org/abs/2602.08028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08028">https://arxiv.org/pdf/2602.08028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08028]] Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning(https://arxiv.org/abs/2602.08028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.</li>
</ul>

<h3>Title: Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Chenwang Wu, Yiu-ming Cheung, Shuhai Zhang, Bo Han, Defu Lian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08031">https://arxiv.org/abs/2602.08031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08031">https://arxiv.org/pdf/2602.08031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08031]] Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection(https://arxiv.org/abs/2602.08031)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at this https URL.</li>
</ul>

<h3>Title: Horizon Imagination: Efficient On-Policy Training in Diffusion World Models</h3>
<ul>
<li><strong>Authors: </strong>Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08032">https://arxiv.org/abs/2602.08032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08032">https://arxiv.org/pdf/2602.08032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08032]] Horizon Imagination: Efficient On-Policy Training in Diffusion World Models(https://arxiv.org/abs/2602.08032)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at this https URL.</li>
</ul>

<h3>Title: The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring</h3>
<ul>
<li><strong>Authors: </strong>Julien Fageot, Matthias Grossglauser, Lê-Nguyên Hoang, Matteo Tacchi-Bénard, Oscar Villemaud</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08033">https://arxiv.org/abs/2602.08033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08033">https://arxiv.org/pdf/2602.08033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08033]] The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring(https://arxiv.org/abs/2602.08033)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.</li>
</ul>

<h3>Title: TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation</h3>
<ul>
<li><strong>Authors: </strong>Jingtao Liu, Xinming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08036">https://arxiv.org/abs/2602.08036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08036">https://arxiv.org/pdf/2602.08036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08036]] TAAM:Inductive Graph-Class Incremental Learning with Task-Aware Adaptive Modulation(https://arxiv.org/abs/2602.08036)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Graph Continual Learning (GCL) aims to solve the challenges of streaming graph data. However, current methods often depend on replay-based strategies, which raise concerns like memory limits and privacy issues, while also struggling to resolve the stability-plasticity dilemma. In this paper, we suggest that lightweight, task-specific modules can effectively guide the reasoning process of a fixed GNN backbone. Based on this idea, we propose Task-Aware Adaptive Modulation (TAAM). The key component of TAAM is its lightweight Neural Synapse Modulators (NSMs). For each new task, a dedicated NSM is trained and then frozen, acting as an "expert module." These modules perform detailed, node-attentive adaptive modulation on the computational flow of a shared GNN backbone. This setup ensures that new knowledge is kept within compact, task-specific modules, naturally preventing catastrophic forgetting without using any data replay. Additionally, to address the important challenge of unknown task IDs in real-world scenarios, we propose and theoretically prove a novel method named Anchored Multi-hop Propagation (AMP). Notably, we find that existing GCL benchmarks have flaws that can cause data leakage and biased evaluations. Therefore, we conduct all experiments in a more rigorous inductive learning scenario. Extensive experiments show that TAAM comprehensively outperforms state-of-the-art methods across eight datasets. Code and Datasets are available at: this https URL.</li>
</ul>

<h3>Title: Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments</h3>
<ul>
<li><strong>Authors: </strong>Boyang Xia, Weiyou Tian, Qingnan Ren, Jiaqi Huang, Jie Xiao, Shuo Lu, Kai Wang, Lynn Ai, Eric Yang, Bill Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08041">https://arxiv.org/abs/2602.08041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08041">https://arxiv.org/pdf/2602.08041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08041]] Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments(https://arxiv.org/abs/2602.08041)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.</li>
</ul>

<h3>Title: Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects</h3>
<ul>
<li><strong>Authors: </strong>Yahia Hamdi, Nicolas Andrialovanirina, Kélig Mahé, Emilie Poisson Caillault</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08046">https://arxiv.org/abs/2602.08046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08046">https://arxiv.org/pdf/2602.08046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08046]] Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects(https://arxiv.org/abs/2602.08046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.</li>
</ul>

<h3>Title: Vanilla Group Equivariant Vision Transformer: Simple and Effective</h3>
<ul>
<li><strong>Authors: </strong>Jiahong Fu, Qi Xie, Deyu Meng, Zongben Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08047">https://arxiv.org/abs/2602.08047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08047">https://arxiv.org/pdf/2602.08047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08047]] Vanilla Group Equivariant Vision Transformer: Simple and Effective(https://arxiv.org/abs/2602.08047)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.</li>
</ul>

<h3>Title: TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Arshia Hemmat, Philip Torr, Yongqiang Chen, Junchi Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08048">https://arxiv.org/abs/2602.08048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08048">https://arxiv.org/pdf/2602.08048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08048]] TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs(https://arxiv.org/abs/2602.08048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.</li>
</ul>

<h3>Title: Interpretable Fuzzy Systems For Forward Osmosis Desalination</h3>
<ul>
<li><strong>Authors: </strong>Qusai Khaled, Uzay Kaymak, Laura Genga</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08050">https://arxiv.org/abs/2602.08050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08050">https://arxiv.org/pdf/2602.08050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08050]] Interpretable Fuzzy Systems For Forward Osmosis Desalination(https://arxiv.org/abs/2602.08050)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.</li>
</ul>

<h3>Title: Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Manan Tayal, Mumuksh Tayal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08054">https://arxiv.org/abs/2602.08054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08054">https://arxiv.org/pdf/2602.08054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08054]] Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning(https://arxiv.org/abs/2602.08054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.</li>
</ul>

<h3>Title: Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yufei Wang, Haixu Liu, Tianxiang Xu, Chuancheng Shi, Hongsheng Xing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08057">https://arxiv.org/abs/2602.08057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08057">https://arxiv.org/pdf/2602.08057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08057]] Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks(https://arxiv.org/abs/2602.08057)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.</li>
</ul>

<h3>Title: DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tong Zhang, Ru Zhang, Jianyi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08059">https://arxiv.org/abs/2602.08059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08059">https://arxiv.org/pdf/2602.08059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08059]] DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models(https://arxiv.org/abs/2602.08059)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.</li>
</ul>

<h3>Title: Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Shayan Ali Hassan, Tao Ni, Zafar Ayyub Qazi, Marco Canini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08062">https://arxiv.org/abs/2602.08062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08062">https://arxiv.org/pdf/2602.08062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08062]] Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation(https://arxiv.org/abs/2602.08062)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability. To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.</li>
</ul>

<h3>Title: SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Li, Dongchen Han, Zixuan Cao, Haofeng Huang, Mengyu Zhou, Ming Chen, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08064">https://arxiv.org/abs/2602.08064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08064">https://arxiv.org/pdf/2602.08064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08064]] SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm(https://arxiv.org/abs/2602.08064)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, transformer</a></li>
<li><strong>Abstract: </strong>Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Chenglei Shen, Yi Zhan, Weijie Yu, Xiao Zhang, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08067">https://arxiv.org/abs/2602.08067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08067">https://arxiv.org/pdf/2602.08067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08067]] Enhancing Bandit Algorithms with LLMs for Time-varying User Preferences in Streaming Recommendations(https://arxiv.org/abs/2602.08067)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In real-world streaming recommender systems, user preferences evolve dynamically over time. Existing bandit-based methods treat time merely as a timestamp, neglecting its explicit relationship with user preferences and leading to suboptimal performance. Moreover, online learning methods often suffer from inefficient exploration-exploitation during the early online phase. To address these issues, we propose HyperBandit+, a novel contextual bandit policy that integrates a time-aware hypernetwork to adapt to time-varying user preferences and employs a large language model-assisted warm-start mechanism (LLM Start) to enhance exploration-exploitation efficiency in the early online phase. Specifically, HyperBandit+ leverages a neural network that takes time features as input and generates parameters for estimating time-varying rewards by capturing the correlation between time and user preferences. Additionally, the LLM Start mechanism employs multi-step data augmentation to simulate realistic interaction data for effective offline learning, providing warm-start parameters for the bandit policy in the early online phase. To meet real-time streaming recommendation demands, we adopt low-rank factorization to reduce hypernetwork training complexity. Theoretically, we rigorously establish a sublinear regret upper bound that accounts for both the hypernetwork and the LLM warm-start mechanism. Extensive experiments on real-world datasets demonstrate that HyperBandit+ consistently outperforms state-of-the-art baselines in terms of accumulated rewards.</li>
</ul>

<h3>Title: ReRoPE: Repurposing RoPE for Relative Camera Control</h3>
<ul>
<li><strong>Authors: </strong>Chunyang Li, Yuanbo Yang, Jiahao Shao, Hongyu Zhou, Katja Schwarz, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08068">https://arxiv.org/abs/2602.08068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08068">https://arxiv.org/pdf/2602.08068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08068]] ReRoPE: Repurposing RoPE for Relative Camera Control(https://arxiv.org/abs/2602.08068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: this https URL</li>
</ul>

<h3>Title: ViT-5: Vision Transformers for The Mid-2020s</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad, Cihang Xie, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08071">https://arxiv.org/abs/2602.08071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08071">https://arxiv.org/pdf/2602.08071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08071]] ViT-5: Vision Transformers for The Mid-2020s(https://arxiv.org/abs/2602.08071)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.</li>
</ul>

<h3>Title: IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports</h3>
<ul>
<li><strong>Authors: </strong>Md Nafiu Rahman, Sadif Ahmed, Zahin Wahab, Gias Uddin, Rifat Shahriyar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08072">https://arxiv.org/abs/2602.08072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08072">https://arxiv.org/pdf/2602.08072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08072]] IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports(https://arxiv.org/abs/2602.08072)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\% on a benchmark dataset, outperforming traditional regex-based scanners. \textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \href{this https URL}{this https URL}, and a demonstration video is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Sayantan Kumar, Peijie Qiu, Aristeidis Sotiras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08077">https://arxiv.org/abs/2602.08077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08077">https://arxiv.org/pdf/2602.08077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08077]] Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders(https://arxiv.org/abs/2602.08077)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.</li>
</ul>

<h3>Title: Online Domain-aware LLM Decoding for Continual Domain Evolution</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abu-Shaira, Weishi Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08088">https://arxiv.org/abs/2602.08088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08088">https://arxiv.org/pdf/2602.08088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08088]] Online Domain-aware LLM Decoding for Continual Domain Evolution(https://arxiv.org/abs/2602.08088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.</li>
</ul>

<h3>Title: VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Issar Tzachor, Dvir Samuel, Rami Ben-Ari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08099">https://arxiv.org/abs/2602.08099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08099">https://arxiv.org/pdf/2602.08099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08099]] VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval(https://arxiv.org/abs/2602.08099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.</li>
</ul>

<h3>Title: Emergent Search and Backtracking in Latent Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Jasmine Cui, Charles Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08100">https://arxiv.org/abs/2602.08100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08100">https://arxiv.org/pdf/2602.08100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08100]] Emergent Search and Backtracking in Latent Reasoning Models(https://arxiv.org/abs/2602.08100)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.</li>
</ul>

<h3>Title: MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery</h3>
<ul>
<li><strong>Authors: </strong>Sidike Paheding, Abel Reyes-Angulo, Leo Thomas Ramos, Angel D. Sappa, Rajaneesh A., Hiral P. B., Sajin Kumar K. S., Thomas Oommen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08112">https://arxiv.org/abs/2602.08112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08112">https://arxiv.org/pdf/2602.08112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08112]] MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery(https://arxiv.org/abs/2602.08112)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: this https URL</li>
</ul>

<h3>Title: Building Damage Detection using Satellite Images and Patch-Based Transformer Methods</h3>
<ul>
<li><strong>Authors: </strong>Smriti Siva, Jan Cross-Zamirski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08117">https://arxiv.org/abs/2602.08117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08117">https://arxiv.org/pdf/2602.08117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08117]] Building Damage Detection using Satellite Images and Patch-Based Transformer Methods(https://arxiv.org/abs/2602.08117)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data. In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.</li>
</ul>

<h3>Title: Gender and Race Bias in Consumer Product Recommendations by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ke Xu, Shera Potka, Alex Thomo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08124">https://arxiv.org/abs/2602.08124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08124">https://arxiv.org/pdf/2602.08124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08124]] Gender and Race Bias in Consumer Product Recommendations by Large Language Models(https://arxiv.org/abs/2602.08124)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.</li>
</ul>

<h3>Title: MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Venkatraman Narayanan, Bala Sai, Rahul Ahuja, Pratik Likhar, Varun Ravi Kumar, Senthil Yogamani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08126">https://arxiv.org/abs/2602.08126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08126">https://arxiv.org/pdf/2602.08126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08126]] MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection(https://arxiv.org/abs/2602.08126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.</li>
</ul>

<h3>Title: Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Zahir Alsulaimawi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08128">https://arxiv.org/abs/2602.08128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08128">https://arxiv.org/pdf/2602.08128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08128]] Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks(https://arxiv.org/abs/2602.08128)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\sqrt{T \log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.</li>
</ul>

<h3>Title: Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries</h3>
<ul>
<li><strong>Authors: </strong>Isaac Corley, Hannah Kerner, Caleb Robinson, Jennifer Marcus</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08131">https://arxiv.org/abs/2602.08131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08131">https://arxiv.org/pdf/2602.08131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08131]] Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries(https://arxiv.org/abs/2602.08131)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).</li>
</ul>

<h3>Title: Robustness of Vision Language Models Against Split-Image Harmful Input Attacks</h3>
<ul>
<li><strong>Authors: </strong>Md Rafi Ur Rashid, MD Sadik Hossain Shanto, Vishnu Asutosh Dasu, Shagufta Mehnaz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08136">https://arxiv.org/abs/2602.08136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08136">https://arxiv.org/pdf/2602.08136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08136]] Robustness of Vision Language Models Against Split-Image Harmful Input Attacks(https://arxiv.org/abs/2602.08136)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.</li>
</ul>

<h3>Title: Reliable and Responsible Foundation Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Yang, Junlin Han, Rishi Bommasani, Jinqi Luo, Wenjie Qu, Wangchunshu Zhou, Adel Bibi, Xiyao Wang, Jaehong Yoon, Elias Stengel-Eskin, Shengbang Tong, Lingfeng Shen, Rafael Rafailov, Runjia Li, Zhaoyang Wang, Yiyang Zhou, Chenhang Cui, Yu Wang, Wenhao Zheng, Huichi Zhou, Jindong Gu, Zhaorun Chen, Peng Xia, Tony Lee, Thomas Zollo, Vikash Sehwag, Jixuan Leng, Jiuhai Chen, Yuxin Wen, Huan Zhang, Zhun Deng, Linjun Zhang, Pavel Izmailov, Pang Wei Koh, Yulia Tsvetkov, Andrew Wilson, Jiaheng Zhang, James Zou, Cihang Xie, Hao Wang, Philip Torr, Julian McAuley, David Alvarez-Melis, Florian Tramèr, Kaidi Xu, Suman Jana, Chris Callison-Burch, Rene Vidal, Filippos Kokkinos, Mohit Bansal, Beidi Chen, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08145">https://arxiv.org/abs/2602.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08145">https://arxiv.org/pdf/2602.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08145]] Reliable and Responsible Foundation Models: A Comprehensive Survey(https://arxiv.org/abs/2602.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, fair, explainability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.</li>
</ul>

<h3>Title: DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries</h3>
<ul>
<li><strong>Authors: </strong>Sahana Ramnath, Nima Chitsazan, Mingyang Zhou, Chia-Hsuan Lee, Shi-Xiong Zhang, Stephen Rawls, Sambit Sahu, Sangwoo Cho, Xiang Ren, Genta Indra Winata, Akshaj Kumar Veldanda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08149">https://arxiv.org/abs/2602.08149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08149">https://arxiv.org/pdf/2602.08149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08149]] DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries(https://arxiv.org/abs/2602.08149)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.</li>
</ul>

<h3>Title: NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Campos, José Pedro Evans, José Miguel Isidro, Miguel Marques, Luís Filipe Cunha, Alípio Jorge, Sérgio Nunes, Nuno Guimarães</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08162">https://arxiv.org/abs/2602.08162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08162">https://arxiv.org/pdf/2602.08162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08162]] NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark(https://arxiv.org/abs/2602.08162)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.</li>
</ul>

<h3>Title: A Transfer Learning Approach to Unveil the Role of Windows Common Configuration Enumerations in IEC 62443 Compliance</h3>
<ul>
<li><strong>Authors: </strong>Miguel Bicudo, Estevão Rabello, Daniel Menasché, Paulo Segal, Claudio Segal, Anton Kocheturov, Priyanjan Sharma</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08165">https://arxiv.org/abs/2602.08165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08165">https://arxiv.org/pdf/2602.08165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08165]] A Transfer Learning Approach to Unveil the Role of Windows Common Configuration Enumerations in IEC 62443 Compliance(https://arxiv.org/abs/2602.08165)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Industrial control systems (ICS) depend on highly heterogeneous environments where Linux, proprietary real-time operating systems, and Windows coexist. Although the IEC 62443-3-3 standard provides a comprehensive framework for securing such systems, translating its requirements into concrete configuration checks remains challenging, especially for Windows platforms. In this paper, we propose a transfer learning methodology that maps Windows Common Configuration Enumerations (CCEs) to IEC 62443-3-3 System Security Requirements by leveraging labeled Linux datasets. The resulting labeled dataset enables automated compliance checks, analysis of requirement prevalence, and identification of cross-platform similarities and divergences. Our results highlight the role of CCEs as a bridge between abstract standards and concrete configurations, advancing automation, traceability, and clarity in IEC 62443-3-3 compliance for Windows environments.</li>
</ul>

<h3>Title: DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mei Ling Chee, Thangarajah Akilan, Aparna Ravindra Phalke, Kanchan Keisham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08168">https://arxiv.org/abs/2602.08168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08168">https://arxiv.org/pdf/2602.08168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08168]] DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation(https://arxiv.org/abs/2602.08168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: this http URL, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.</li>
</ul>

<h3>Title: Spherical Steering: Geometry-Aware Activation Rotation for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zejia You, Chunyuan Deng, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08169">https://arxiv.org/abs/2602.08169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08169">https://arxiv.org/pdf/2602.08169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08169]] Spherical Steering: Geometry-Aware Activation Rotation for Language Models(https://arxiv.org/abs/2602.08169)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.</li>
</ul>

<h3>Title: Evasion of IoT Malware Detection via Dummy Code Injection</h3>
<ul>
<li><strong>Authors: </strong>Sahar Zargarzadeh, Mohammad Islam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08170">https://arxiv.org/abs/2602.08170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08170">https://arxiv.org/pdf/2602.08170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08170]] Evasion of IoT Malware Detection via Dummy Code Injection(https://arxiv.org/abs/2602.08170)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) has revolutionized connectivity by linking billions of devices worldwide. However, this rapid expansion has also introduced severe security vulnerabilities, making IoT devices attractive targets for malware such as the Mirai botnet. Power side-channel analysis has recently emerged as a promising technique for detecting malware activity based on device power consumption patterns. However, the resilience of such detection systems under adversarial manipulation remains underexplored. This work presents a novel adversarial strategy against power side-channel-based malware detection. By injecting structured dummy code into the scanning phase of the Mirai botnet, we dynamically perturb power signatures to evade AI/ML-based anomaly detection without disrupting core functionality. Our approach systematically analyzes the trade-offs between stealthiness, execution overhead, and evasion effectiveness across multiple state-of-the-art models for side-channel analysis, using a custom dataset collected from smartphones of diverse manufacturers. Experimental results show that our adversarial modifications achieve an average attack success rate of 75.2\%, revealing practical vulnerabilities in power-based intrusion detection frameworks.</li>
</ul>

<h3>Title: A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis</h3>
<ul>
<li><strong>Authors: </strong>Cristian Minoccheri, Sophia Tesic, Kayvan Najarian, Ryan Stidham</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08171">https://arxiv.org/abs/2602.08171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08171">https://arxiv.org/pdf/2602.08171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08171]] A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis(https://arxiv.org/abs/2602.08171)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.</li>
</ul>

<h3>Title: Nansde-net: A neural sde framework for generating time series with memory</h3>
<ul>
<li><strong>Authors: </strong>Hiromu Ozai, Kei Nakagawa</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08182">https://arxiv.org/abs/2602.08182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08182">https://arxiv.org/pdf/2602.08182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08182]] Nansde-net: A neural sde framework for generating time series with memory(https://arxiv.org/abs/2602.08182)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with Itô calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an Itô-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the Itô calculus framework.</li>
</ul>

<h3>Title: PEGAsus: 3D Personalization of Geometry and Appearance</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Hu, Bin Hu, Ka-Hei Hui, Haipeng Li, Zhengzhe Liu, Daniel Cohen-Or, Chi-Wing Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08198">https://arxiv.org/abs/2602.08198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08198">https://arxiv.org/pdf/2602.08198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08198]] PEGAsus: 3D Personalization of Geometry and Appearance(https://arxiv.org/abs/2602.08198)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.</li>
</ul>

<h3>Title: Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video</h3>
<ul>
<li><strong>Authors: </strong>Jinrong Lv, Xun Gong, Zhaohuan Li, Weili Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08202">https://arxiv.org/abs/2602.08202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08202">https://arxiv.org/pdf/2602.08202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08202]] Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video(https://arxiv.org/abs/2602.08202)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.</li>
</ul>

<h3>Title: Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chufeng Zhou, Jian Wang, Xinyuan Liu, Xiaokang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08206">https://arxiv.org/abs/2602.08206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08206">https://arxiv.org/pdf/2602.08206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08206]] Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2602.08206)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.</li>
</ul>

<h3>Title: CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hyungseok Song, Deunsol Yoon, Kanghoon Lee, Han-Seul Jeong, Soonyoung Lee, Woohyung Lim</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08210">https://arxiv.org/abs/2602.08210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08210">https://arxiv.org/pdf/2602.08210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08210]] CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization(https://arxiv.org/abs/2602.08210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.</li>
</ul>

<h3>Title: Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension</h3>
<ul>
<li><strong>Authors: </strong>Yik Lung Pang, Changjae Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08211">https://arxiv.org/abs/2602.08211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08211">https://arxiv.org/pdf/2602.08211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08211]] Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension(https://arxiv.org/abs/2602.08211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.</li>
</ul>

<h3>Title: DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoran Liu, Zheni Zeng, Yukun Yan, Yuxuan Chen, Yunduo Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08213">https://arxiv.org/abs/2602.08213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08213">https://arxiv.org/pdf/2602.08213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08213]] DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning(https://arxiv.org/abs/2602.08213)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.</li>
</ul>

<h3>Title: Distribution-Free Robust Functional Predict-Then-Optimize</h3>
<ul>
<li><strong>Authors: </strong>Yash Patel, Ambuj Tewari</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08215">https://arxiv.org/abs/2602.08215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08215">https://arxiv.org/pdf/2602.08215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08215]] Distribution-Free Robust Functional Predict-Then-Optimize(https://arxiv.org/abs/2602.08215)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.</li>
</ul>

<h3>Title: Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Gunn Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08216">https://arxiv.org/abs/2602.08216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08216">https://arxiv.org/pdf/2602.08216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08216]] Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics(https://arxiv.org/abs/2602.08216)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.</li>
</ul>

<h3>Title: Pretraining with Token-Level Adaptive Latent Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Boyi Zeng, Yiqin Hao, He Li, Shixiang Song, Feichen Song, Zitong Wang, Siyuan Huang, Yi Xu, ZiWei He, Xinbing Wang, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08220">https://arxiv.org/abs/2602.08220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08220">https://arxiv.org/pdf/2602.08220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08220]] Pretraining with Token-Level Adaptive Latent Chain-of-Thought(https://arxiv.org/abs/2602.08220)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.</li>
</ul>

<h3>Title: Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jing Zhang, Zhikai Li, Xuewen Liu, Qingyi Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08224">https://arxiv.org/abs/2602.08224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08224">https://arxiv.org/pdf/2602.08224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08224]] Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval(https://arxiv.org/abs/2602.08224)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.</li>
</ul>

<h3>Title: Generating Adversarial Events: A Motion-Aware Point Cloud Framework</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Ren, Youxin Jiang, Qifei Gu, Xiangqian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08230">https://arxiv.org/abs/2602.08230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08230">https://arxiv.org/pdf/2602.08230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08230]] Generating Adversarial Events: A Motion-Aware Point Cloud Framework(https://arxiv.org/abs/2602.08230)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.</li>
</ul>

<h3>Title: SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Peng Xia, Jianwen Chen, Hanyang Wang, Jiaqi Liu, Kaide Zeng, Yu Wang, Siwei Han, Yiyang Zhou, Xujiang Zhao, Haifeng Chen, Zeyu Zheng, Cihang Xie, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08234">https://arxiv.org/abs/2602.08234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08234">https://arxiv.org/pdf/2602.08234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08234]] SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning(https://arxiv.org/abs/2602.08234)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this this https URL.</li>
</ul>

<h3>Title: When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08236">https://arxiv.org/abs/2602.08236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08236">https://arxiv.org/pdf/2602.08236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08236]] When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning(https://arxiv.org/abs/2602.08236)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.</li>
</ul>

<h3>Title: Document Reconstruction Unlocks Scalable Long-Context RLVR</h3>
<ul>
<li><strong>Authors: </strong>Yao Xiao, Lei Wang, Yue Deng, Guanzheng Chen, Ziqi Jin, Jung-jae Kim, Xiaoli Li, Roy Ka-wei Lee, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08237">https://arxiv.org/abs/2602.08237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08237">https://arxiv.org/pdf/2602.08237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08237]] Document Reconstruction Unlocks Scalable Long-Context RLVR(https://arxiv.org/abs/2602.08237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.</li>
</ul>

<h3>Title: Linearization Explains Fine-Tuning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zahra Rahimi Afzal, Tara Esmaeilbeig, Mojtaba Soltanalian, Mesrob I. Ohannessian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08239">https://arxiv.org/abs/2602.08239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08239">https://arxiv.org/pdf/2602.08239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08239]] Linearization Explains Fine-Tuning in Large Language Models(https://arxiv.org/abs/2602.08239)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.</li>
</ul>

<h3>Title: Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Dong, Bowen He, Moyang Guo, Ethan X. Fang, Zhuoran Yang, Vahid Tarokh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08244">https://arxiv.org/abs/2602.08244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08244">https://arxiv.org/pdf/2602.08244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08244]] Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers(https://arxiv.org/abs/2602.08244)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action this http URL on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.</li>
</ul>

<h3>Title: Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization</h3>
<ul>
<li><strong>Authors: </strong>Binglin Wu, Yingyi Zhang, Xianneng Li, Ruyue Deng, Chuan Yue, Weiru Zhang, Xiaoyi Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08261">https://arxiv.org/abs/2602.08261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08261">https://arxiv.org/pdf/2602.08261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08261]] Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization(https://arxiv.org/abs/2602.08261)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.</li>
</ul>

<h3>Title: Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification</h3>
<ul>
<li><strong>Authors: </strong>Guoqi Yu, Xiaowei Hu, Angelica I. Aviles-Rivero, Anqi Qiu, Shujun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08262">https://arxiv.org/abs/2602.08262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08262">https://arxiv.org/pdf/2602.08262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08262]] Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification(https://arxiv.org/abs/2602.08262)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at this https URL.</li>
</ul>

<h3>Title: Inverting Data Transformations via Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Kim, Sékou-Oumar Kaba, Jiyun Park, Seunghoon Hong, Siamak Ravanbakhsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08267">https://arxiv.org/abs/2602.08267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08267">https://arxiv.org/pdf/2602.08267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08267]] Inverting Data Transformations via Diffusion Sampling(https://arxiv.org/abs/2602.08267)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We study the problem of transformation inversion on general Lie groups: a datum is transformed by an unknown group element, and the goal is to recover an inverse transformation that maps it back to the original data distribution. Such unknown transformations arise widely in machine learning and scientific modeling, where they can significantly distort observations. We take a probabilistic view and model the posterior over transformations as a Boltzmann distribution defined by an energy function on data space. To sample from this posterior, we introduce a diffusion process on Lie groups that keeps all updates on-manifold and only requires computations in the associated Lie algebra. Our method, Transformation-Inverting Energy Diffusion (TIED), relies on a new trivialized target-score identity that enables efficient score-based sampling of the transformation posterior. As a key application, we focus on test-time equivariance, where the objective is to improve the robustness of pretrained neural networks to input transformations. Experiments on image homographies and PDE symmetries demonstrate that TIED can restore transformed inputs to the training distribution at test time, showing improved performance over strong canonicalization and sampling baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems</h3>
<ul>
<li><strong>Authors: </strong>Junwei Su, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08272">https://arxiv.org/abs/2602.08272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08272">https://arxiv.org/pdf/2602.08272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08272]] When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems(https://arxiv.org/abs/2602.08272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.</li>
</ul>

<h3>Title: PISCO: Precise Video Instance Insertion with Sparse Control</h3>
<ul>
<li><strong>Authors: </strong>Xiangbo Gao, Renjie Li, Xinghao Chen, Yuheng Wu, Suofei Feng, Qing Yin, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08277">https://arxiv.org/abs/2602.08277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08277">https://arxiv.org/pdf/2602.08277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08277]] PISCO: Precise Video Instance Insertion with Sparse Control(https://arxiv.org/abs/2602.08277)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: this http URL.</li>
</ul>

<h3>Title: New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Wang, Yafu Li, Shunkai Zhang, Zhi Wang, Haoran Zhang, Xiaoye Qu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08281">https://arxiv.org/abs/2602.08281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08281">https://arxiv.org/pdf/2602.08281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08281]] New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR(https://arxiv.org/abs/2602.08281)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($\rho \in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.</li>
</ul>

<h3>Title: Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Haixu Liu, Yufei Wang, Tianxiang Xu, Chuancheng Shi, Hongsheng Xing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08282">https://arxiv.org/abs/2602.08282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08282">https://arxiv.org/pdf/2602.08282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08282]] Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning(https://arxiv.org/abs/2602.08282)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.</li>
</ul>

<h3>Title: Noise Stability of Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Themistoklis Haris, Zihan Zhang, Yuichi Yoshida</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08287">https://arxiv.org/abs/2602.08287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08287">https://arxiv.org/pdf/2602.08287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08287]] Noise Stability of Transformer Models(https://arxiv.org/abs/2602.08287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Understanding simplicity biases in deep learning offers a promising path toward developing reliable AI. A common metric for this, inspired by Boolean function analysis, is average sensitivity, which captures a model's robustness to single-token perturbations. We argue that average sensitivity has two key limitations: it lacks a natural generalization to real-valued domains and fails to explain the "junta-like" input dependence we empirically observe in modern LLMs. To address these limitations, we propose noise stability as a more comprehensive simplicity metric. Noise stability expresses a model's robustness to correlated noise applied to all input coordinates simultaneously. We provide a theoretical analysis of noise stability for single-layer attention and ReLU MLP layers and tackle the multi-layer propagation problem with a covariance interval propagation approach. Building on this theory, we develop a practical noise stability regularization method. Experiments on algorithmic and next-token-prediction tasks show that our regularizer consistently catalyzes grokking and accelerates training by approximately $35\%$ and $75\%$ respectively. Our results sculpt a new connection between signal propagation in neural networks and interpretability, with noise stability emerging as a powerful tool for understanding and improving modern Transformers.</li>
</ul>

<h3>Title: Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Binglin Wu, Xianneng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08289">https://arxiv.org/abs/2602.08289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08289">https://arxiv.org/pdf/2602.08289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08289]] Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network(https://arxiv.org/abs/2602.08289)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.</li>
</ul>

<h3>Title: Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Ajay Kumar Shrestha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08290">https://arxiv.org/abs/2602.08290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08290">https://arxiv.org/pdf/2602.08290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08290]] Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems(https://arxiv.org/abs/2602.08290)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, fair</a></li>
<li><strong>Abstract: </strong>In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.</li>
</ul>

<h3>Title: When Does Context Help? Error Dynamics of Contextual Information in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08294">https://arxiv.org/abs/2602.08294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08294">https://arxiv.org/pdf/2602.08294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08294]] When Does Context Help? Error Dynamics of Contextual Information in Large Language Models(https://arxiv.org/abs/2602.08294)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.</li>
</ul>

<h3>Title: Grokking in Linear Models for Logistic Regression</h3>
<ul>
<li><strong>Authors: </strong>Nataraj Das, Atreya Vedantam, Chandrashekar Lakshminarayanan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08302">https://arxiv.org/abs/2602.08302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08302">https://arxiv.org/pdf/2602.08302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08302]] Grokking in Linear Models for Logistic Regression(https://arxiv.org/abs/2602.08302)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Grokking, the phenomenon of delayed generalization, is often attributed to the depth and compositional structure of deep neural networks. We study grokking in one of the simplest possible settings: the learning of a linear model with logistic loss for binary classification on data that are linearly (and max margin) separable about the origin. We investigate three testing regimes: (1) test data drawn from the same distribution as the training data, in which case grokking is not observed; (2) test data concentrated around the margin, in which case grokking is observed; and (3) adversarial test data generated via projected gradient descent (PGD) attacks, in which case grokking is also observed. We theoretically show that the implicit bias of gradient descent induces a three-phase learning process-population-dominated, support-vector-dominated unlearning, and support-vector-dominated generalization-during which delayed generalization can arise. Our analysis further relates the emergence of grokking to asymmetries in the data, both in the number of examples per class and in the distribution of support vectors across classes, and yields a characterization of the grokking time. We experimentally validate our theory by planting different distributions of population points and support vectors, and by analyzing accuracy curves and hyperplane dynamics. Overall, our results demonstrate that grokking does not require depth or representation learning, and can emerge even in linear models through the dynamics of the bias term.</li>
</ul>

<h3>Title: Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback</h3>
<ul>
<li><strong>Authors: </strong>Mengxiao Zhang, Yuheng Zhang, Haipeng Luo, Paul Mineiro</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08307">https://arxiv.org/abs/2602.08307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08307">https://arxiv.org/pdf/2602.08307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08307]] Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback(https://arxiv.org/abs/2602.08307)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.</li>
</ul>

<h3>Title: CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment</h3>
<ul>
<li><strong>Authors: </strong>Yunzuo Hu, Wen Li, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08309">https://arxiv.org/abs/2602.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08309">https://arxiv.org/pdf/2602.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08309]] CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment(https://arxiv.org/abs/2602.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.</li>
</ul>

<h3>Title: Improving Data and Reward Design for Scientific Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zijie Chen, Zhenghao Lin, Xiao Liu, Zhenzhong Lan, Yeyun Gong, Peng Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08321">https://arxiv.org/abs/2602.08321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08321">https://arxiv.org/pdf/2602.08321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08321]] Improving Data and Reward Design for Scientific Reasoning in Large Language Models(https://arxiv.org/abs/2602.08321)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using this http URL pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.</li>
</ul>

<h3>Title: An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08322">https://arxiv.org/abs/2602.08322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08322">https://arxiv.org/pdf/2602.08322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08322]] An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling(https://arxiv.org/abs/2602.08322)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.</li>
</ul>

<h3>Title: Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression</h3>
<ul>
<li><strong>Authors: </strong>Yuntian Tang, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Wenxi Li, Wei Li, Jie Hu, Xinghao Chen, Rongrong Ji, Shaohui Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08324">https://arxiv.org/abs/2602.08324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08324">https://arxiv.org/pdf/2602.08324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08324]] Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression(https://arxiv.org/abs/2602.08324)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.</li>
</ul>

<h3>Title: Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Yifei Gao, Lei Wang, Rong-Cheng Tu, Qixin Zhang, Jun Cheng, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08329">https://arxiv.org/abs/2602.08329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08329">https://arxiv.org/pdf/2602.08329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08329]] Near-Oracle KV Selection via Pre-hoc Sparsity for Long-Context Inference(https://arxiv.org/abs/2602.08329)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A core bottleneck in large language model (LLM) inference is the cost of attending over the ever-growing key-value (KV) cache. Although near-oracle top-k KV selection can preserve the quality of dense attention while sharply reducing computation and bandwidth, existing sparse methods generally rely on posterior heuristics, i.e., selectors conditioned on observed attention or proxy scores. Such conditioning introduces posterior bias: it tends to distort true token importance and miss salient tokens, thereby impairing long-range reasoning. To tackle this problem, we propose Pre-hoc Sparsity (PrHS), which selects KV entries before attention scoring and provides explicit accuracy control. Let the attention mass of discarded entries be delta (the dropped mass). Through a marginal-to-mutual-information analysis, we derive an upper bound on the mutual-information loss that depends only on the dropped mass. This relation explains failure modes of posterior heuristics and enables verifiable guarantees by controlling the dropped mass in advance. Within PrHS, we instantiate three orthogonal pre-hoc selectors along the axes of time, depth, and layer. Extensive experiments on LLaMA and Mistral families validate PrHS. Across GSM8K and CoQA, PrHS reduces retrieval overhead by over 90%, achieving 3x higher retrieval sparsity than HShare at matched or better accuracy. It incurs under 1% average degradation on LongBench, lowers attention FLOPs by about 15% versus prior sparse baselines, and yields a 9.9x speedup in attention-operator latency and 2.8x higher throughput on NVIDIA A100-80GB GPUs than the dense baseline.</li>
</ul>

<h3>Title: Latent Reasoning with Supervised Thinking States</h3>
<ul>
<li><strong>Authors: </strong>Ido Amos, Avi Caciularu, Mor Geva, Amir Globerson, Jonathan Herzig, Lior Shani, Idan Szpektor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08332">https://arxiv.org/abs/2602.08332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08332">https://arxiv.org/pdf/2602.08332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08332]] Latent Reasoning with Supervised Thinking States(https://arxiv.org/abs/2602.08332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.</li>
</ul>

<h3>Title: Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training</h3>
<ul>
<li><strong>Authors: </strong>Cristian Pérez-Corral, Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08333">https://arxiv.org/abs/2602.08333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08333">https://arxiv.org/pdf/2602.08333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08333]] Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training(https://arxiv.org/abs/2602.08333)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.</li>
</ul>

<h3>Title: Language-Guided Transformer Tokenizer for Human Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08337">https://arxiv.org/abs/2602.08337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08337">https://arxiv.org/pdf/2602.08337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08337]] Language-Guided Transformer Tokenizer for Human Motion Generation(https://arxiv.org/abs/2602.08337)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.</li>
</ul>

<h3>Title: ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection</h3>
<ul>
<li><strong>Authors: </strong>Debajyoti Datta, Trishala Neeraj, Bibek Paudel, Vyom Sharma, Subhabrata Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08343">https://arxiv.org/abs/2602.08343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08343">https://arxiv.org/pdf/2602.08343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08343]] ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection(https://arxiv.org/abs/2602.08343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations. On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.</li>
</ul>

<h3>Title: ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts</h3>
<ul>
<li><strong>Authors: </strong>Hung Quang Tran, Nam Tien Pham, Son T. Luu, Kiet Van Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08371">https://arxiv.org/abs/2602.08371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08371">https://arxiv.org/pdf/2602.08371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08371]] ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts(https://arxiv.org/abs/2602.08371)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.</li>
</ul>

<h3>Title: OJBKQ: Objective-Joint Babai-Klein Quantization</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Ziyu Zhao, Peng Lu, Yu Gu, Xiao-Wen Chang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08376">https://arxiv.org/abs/2602.08376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08376">https://arxiv.org/pdf/2602.08376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08376]] OJBKQ: Objective-Joint Babai-Klein Quantization(https://arxiv.org/abs/2602.08376)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.</li>
</ul>

<h3>Title: Reinforcement Learning with Backtracking Feedback</h3>
<ul>
<li><strong>Authors: </strong>Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08377">https://arxiv.org/abs/2602.08377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08377">https://arxiv.org/pdf/2602.08377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08377]] Reinforcement Learning with Backtracking Feedback(https://arxiv.org/abs/2602.08377)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.</li>
</ul>

<h3>Title: Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08382">https://arxiv.org/abs/2602.08382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08382">https://arxiv.org/pdf/2602.08382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08382]] Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning(https://arxiv.org/abs/2602.08382)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.</li>
</ul>

<h3>Title: Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Zhang, Fuyuan Zhang, Jiayi Lu, Jilin Hu, Xiaoyi Yin, Long Zhang, Feng Yang, Yongwang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08384">https://arxiv.org/abs/2602.08384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08384">https://arxiv.org/pdf/2602.08384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08384]] Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4(https://arxiv.org/abs/2602.08384)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.</li>
</ul>

<h3>Title: Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shuo Zhang, Wenzhuo Wu, Huayu Zhang, Jiarong Cheng, Xianghao Zang, Chao Ban, Hao Sun, Zhongjiang He, Tianwei Cao, Kongming Liang, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08388">https://arxiv.org/abs/2602.08388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08388">https://arxiv.org/pdf/2602.08388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08388]] Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers(https://arxiv.org/abs/2602.08388)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.</li>
</ul>

<h3>Title: D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng Liang, Shaocheng Shen, Botao Xu, Qiang Hu, Xiaoyun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08395">https://arxiv.org/abs/2602.08395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08395">https://arxiv.org/pdf/2602.08395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08395]] D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy(https://arxiv.org/abs/2602.08395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}</li>
</ul>

<h3>Title: RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications</h3>
<ul>
<li><strong>Authors: </strong>Chiara Lena, Davide Milesi, Alessandro Casella, Luca Carlini, Joseph C. Norton, James Martin, Bruno Scaglioni, Keith L. Obstein, Roberto De Sire, Marco Spadaccini, Cesare Hassan, Pietro Valdastri, Elena De Momi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08397">https://arxiv.org/abs/2602.08397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08397">https://arxiv.org/pdf/2602.08397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08397]] RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications(https://arxiv.org/abs/2602.08397)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.</li>
</ul>

<h3>Title: TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Linye Wei, Zixiang Luo, Pingzhi Tang, Meng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08404">https://arxiv.org/abs/2602.08404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08404">https://arxiv.org/pdf/2602.08404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08404]] TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration(https://arxiv.org/abs/2602.08404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at this https URL.</li>
</ul>

<h3>Title: LLMs + Security = Trouble</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Livshits</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08422">https://arxiv.org/abs/2602.08422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08422">https://arxiv.org/pdf/2602.08422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08422]] LLMs + Security = Trouble(https://arxiv.org/abs/2602.08422)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, diffusion</a></li>
<li><strong>Abstract: </strong>We argue that when it comes to producing secure code with AI, the prevailing "fighting fire with fire" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries. While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the "vibe coding" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees. In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.</li>
</ul>

<h3>Title: Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features</h3>
<ul>
<li><strong>Authors: </strong>Qiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08430">https://arxiv.org/abs/2602.08430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08430">https://arxiv.org/pdf/2602.08430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08430]] Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features(https://arxiv.org/abs/2602.08430)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.</li>
</ul>

<h3>Title: USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yingxu Wang, Kunyu Zhang, Mengzhu Wang, Siyang Gao, Nan Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08431">https://arxiv.org/abs/2602.08431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08431">https://arxiv.org/pdf/2602.08431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08431]] USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation(https://arxiv.org/abs/2602.08431)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.</li>
</ul>

<h3>Title: Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI</h3>
<ul>
<li><strong>Authors: </strong>Ziyan wang, Longlong Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08437">https://arxiv.org/abs/2602.08437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08437">https://arxiv.org/pdf/2602.08437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08437]] Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI(https://arxiv.org/abs/2602.08437)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.</li>
</ul>

<h3>Title: Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Dong, Shulin Tian, Shuai Liu, Shuangrui Ding, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08439">https://arxiv.org/abs/2602.08439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08439">https://arxiv.org/pdf/2602.08439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08439]] Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition(https://arxiv.org/abs/2602.08439)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.</li>
</ul>

<h3>Title: RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Pouria Arefijamal, Mahdi Ahmadlou, Bardia Safaei, Jörg Henkel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08446">https://arxiv.org/abs/2602.08446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08446">https://arxiv.org/pdf/2602.08446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08446]] RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks(https://arxiv.org/abs/2602.08446)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.</li>
</ul>

<h3>Title: Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Lu, Nan Zhang, Wei Tao, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08448">https://arxiv.org/abs/2602.08448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08448">https://arxiv.org/pdf/2602.08448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08448]] Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries(https://arxiv.org/abs/2602.08448)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.</li>
</ul>

<h3>Title: TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Cao, Yunze Deng, Ziyu Lin, Bin Feng, Xinggang Wang, Wenyu Liu, Dandan Zheng, Jingdong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08462">https://arxiv.org/abs/2602.08462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08462">https://arxiv.org/pdf/2602.08462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08462]] TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation(https://arxiv.org/abs/2602.08462)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: this https URL.</li>
</ul>

<h3>Title: Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Charalampos Shimillas, Kleanthis Malialis, Konstantinos Fokianos, Marios M. Polycarpou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08467">https://arxiv.org/abs/2602.08467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08467">https://arxiv.org/pdf/2602.08467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08467]] Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization(https://arxiv.org/abs/2602.08467)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.</li>
</ul>

<h3>Title: Learning Credal Ensembles via Distributionally Robust Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kaizheng Wang, Ghifari Adam Faza, Fabio Cuzzolin, Siu Lun Chau, David Moens, Hans Hallez</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08470">https://arxiv.org/abs/2602.08470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08470">https://arxiv.org/pdf/2602.08470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08470]] Learning Credal Ensembles via Distributionally Robust Optimization(https://arxiv.org/abs/2602.08470)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.</li>
</ul>

<h3>Title: Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Albert Alcalde, Markus Widhalm, Emre Yılmaz</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08478">https://arxiv.org/abs/2602.08478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08478">https://arxiv.org/pdf/2602.08478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08478]] Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics(https://arxiv.org/abs/2602.08478)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.</li>
</ul>

<h3>Title: Beyond Correctness: Learning Robust Reasoning via Transfer</h3>
<ul>
<li><strong>Authors: </strong>Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08489">https://arxiv.org/abs/2602.08489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08489">https://arxiv.org/pdf/2602.08489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08489]] Beyond Correctness: Learning Robust Reasoning via Transfer(https://arxiv.org/abs/2602.08489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.</li>
</ul>

<h3>Title: Enhanced Food Category Recognition under Illumination-Induced Domain Shift</h3>
<ul>
<li><strong>Authors: </strong>Keonvin Park, Aditya Pal, Jin Hong Mok</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08491">https://arxiv.org/abs/2602.08491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08491">https://arxiv.org/pdf/2602.08491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08491]] Enhanced Food Category Recognition under Illumination-Induced Domain Shift(https://arxiv.org/abs/2602.08491)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations. In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels. We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.</li>
</ul>

<h3>Title: Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Lu, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin, Zhijun Chen, Yu Luo, Fuzhen Zhuang, Yikun Ban, Deqing Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08499">https://arxiv.org/abs/2602.08499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08499">https://arxiv.org/pdf/2602.08499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08499]] Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards(https://arxiv.org/abs/2602.08499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.</li>
</ul>

<h3>Title: Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?</h3>
<ul>
<li><strong>Authors: </strong>Caterina Fuster-Barceló, Virginie Uhlmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08505">https://arxiv.org/abs/2602.08505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08505">https://arxiv.org/pdf/2602.08505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08505]] Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?(https://arxiv.org/abs/2602.08505)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.</li>
</ul>

<h3>Title: Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Yunhui Liu, Pengyu Qiu, Yu Xing, Yongchao Liu, Peng Du, Chuntao Hong, Jiajun Zheng, Tao Zheng, Tieke He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08519">https://arxiv.org/abs/2602.08519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08519">https://arxiv.org/pdf/2602.08519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08519]] Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering(https://arxiv.org/abs/2602.08519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (this https URL), PyPI (this https URL), and Documentation (this https URL).</li>
</ul>

<h3>Title: GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving</h3>
<ul>
<li><strong>Authors: </strong>Linger Deng, Yuliang Liu, Wenwen Yu, Zujia Zhang, Jianzhong Ju, Zhenbo Luo, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08524">https://arxiv.org/abs/2602.08524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08524">https://arxiv.org/pdf/2602.08524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08524]] GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving(https://arxiv.org/abs/2602.08524)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- this https URL</li>
</ul>

<h3>Title: Thegra: Graph-based SLAM for Thermal Imagery</h3>
<ul>
<li><strong>Authors: </strong>Anastasiia Kornilova, Ivan Moskalenko, Arabella Gromova, Gonzalo Ferrer, Alexander Menshchikov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08531">https://arxiv.org/abs/2602.08531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08531">https://arxiv.org/pdf/2602.08531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08531]] Thegra: Graph-based SLAM for Thermal Imagery(https://arxiv.org/abs/2602.08531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.</li>
</ul>

<h3>Title: Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Rui Wu, Li YongJun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08535">https://arxiv.org/abs/2602.08535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08535">https://arxiv.org/pdf/2602.08535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08535]] Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds(https://arxiv.org/abs/2602.08535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.</li>
</ul>

<h3>Title: TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation</h3>
<ul>
<li><strong>Authors: </strong>He Wu, Xia Yan, Yanghui Xu, Liegang Xia, Jiazhou Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08540">https://arxiv.org/abs/2602.08540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08540">https://arxiv.org/pdf/2602.08540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08540]] TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation(https://arxiv.org/abs/2602.08540)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.</li>
</ul>

<h3>Title: GISA: A Benchmark for General Information-Seeking Assistant</h3>
<ul>
<li><strong>Authors: </strong>Yutao Zhu, Xingshuo Zhang, Maosen Zhang, Jiajie Jin, Liancheng Zhang, Xiaoshuai Song, Kangzhi Zhao, Wencong Zeng, Ruiming Tang, Han Li, Ji-Rong Wen, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08543">https://arxiv.org/abs/2602.08543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08543">https://arxiv.org/pdf/2602.08543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08543]] GISA: A Benchmark for General Information-Seeking Assistant(https://arxiv.org/abs/2602.08543)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.</li>
</ul>

<h3>Title: How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location</h3>
<ul>
<li><strong>Authors: </strong>Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08548">https://arxiv.org/abs/2602.08548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08548">https://arxiv.org/pdf/2602.08548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08548]] How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location(https://arxiv.org/abs/2602.08548)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.</li>
</ul>

<h3>Title: GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08550">https://arxiv.org/abs/2602.08550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08550">https://arxiv.org/pdf/2602.08550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08550]] GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing(https://arxiv.org/abs/2602.08550)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.</li>
</ul>

<h3>Title: Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Salem, Andrew Paverd, Sahar Abdelnabi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08563">https://arxiv.org/abs/2602.08563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08563">https://arxiv.org/pdf/2602.08563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08563]] Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs(https://arxiv.org/abs/2602.08563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: this https URL.</li>
</ul>

<h3>Title: Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs</h3>
<ul>
<li><strong>Authors: </strong>Junsu Seo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08579">https://arxiv.org/abs/2602.08579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08579">https://arxiv.org/pdf/2602.08579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08579]] Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs(https://arxiv.org/abs/2602.08579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.</li>
</ul>

<h3>Title: SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Melany Yang, Yuhang Yu, Diwang Weng, Jinwei Chen, Wei Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08582">https://arxiv.org/abs/2602.08582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08582">https://arxiv.org/pdf/2602.08582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08582]] SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning(https://arxiv.org/abs/2602.08582)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at this https URL.</li>
</ul>

<h3>Title: FairRARI: A Plug and Play Framework for Fairness-Aware PageRank</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Kariotakis, Aritra Konar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08589">https://arxiv.org/abs/2602.08589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08589">https://arxiv.org/pdf/2602.08589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08589]] FairRARI: A Plug and Play Framework for Fairness-Aware PageRank(https://arxiv.org/abs/2602.08589)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.</li>
</ul>

<h3>Title: SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Di, Wei Yuan, Tieke He, Zhanjie Zhang, Ao Ma, Yuan Liu, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08590">https://arxiv.org/abs/2602.08590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08590">https://arxiv.org/pdf/2602.08590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08590]] SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning(https://arxiv.org/abs/2602.08590)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.</li>
</ul>

<h3>Title: TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyin Liao, Chunyu Hu, Yicheng Sui, Xingxuan Zhang, Peng Cui, Jianxin Li, Ziwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08592">https://arxiv.org/abs/2602.08592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08592">https://arxiv.org/pdf/2602.08592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08592]] TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models(https://arxiv.org/abs/2602.08592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.</li>
</ul>

<h3>Title: Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Archchana Sindhujan, Girish A. Koushik, Shenbin Qian, Diptesh Kanojia, Constantin Orăsan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08600">https://arxiv.org/abs/2602.08600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08600">https://arxiv.org/pdf/2602.08600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08600]] Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation(https://arxiv.org/abs/2602.08600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.</li>
</ul>

<h3>Title: VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Cheng, Yuhao Wang, Heyang Liu, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08607">https://arxiv.org/abs/2602.08607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08607">https://arxiv.org/pdf/2602.08607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08607]] VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling(https://arxiv.org/abs/2602.08607)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.</li>
</ul>

<h3>Title: Overview and Comparison of AVS Point Cloud Compression Standard</h3>
<ul>
<li><strong>Authors: </strong>Wei Gao, Wenxu Gao, Xingming Mu, Changhao Peng, Ge Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08613">https://arxiv.org/abs/2602.08613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08613">https://arxiv.org/pdf/2602.08613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08613]] Overview and Comparison of AVS Point Cloud Compression Standard(https://arxiv.org/abs/2602.08613)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.</li>
</ul>

<h3>Title: Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration</h3>
<ul>
<li><strong>Authors: </strong>Kfir Goldberg, Elad Richardson, Yael Vinker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08615">https://arxiv.org/abs/2602.08615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08615">https://arxiv.org/pdf/2602.08615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08615]] Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration(https://arxiv.org/abs/2602.08615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.</li>
</ul>

<h3>Title: ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Dario Fenoglio, Pasquale Polverino, Jacopo Quizi, Martin Gjoreski, Marc Langheinrich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08617">https://arxiv.org/abs/2602.08617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08617">https://arxiv.org/pdf/2602.08617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08617]] ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning(https://arxiv.org/abs/2602.08617)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer, federate, large language model</a></li>
<li><strong>Abstract: </strong>Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.</li>
</ul>

<h3>Title: Improving Reconstruction of Representation Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Siyu Liu, Chujie Qin, Hubery Yin, Qixin Yan, Zheng-Peng Duan, Chen Li, Jing Lyu, Chun-Le Guo, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08620">https://arxiv.org/abs/2602.08620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08620">https://arxiv.org/pdf/2602.08620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08620]] Improving Reconstruction of Representation Autoencoder(https://arxiv.org/abs/2602.08620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at this https URL.</li>
</ul>

<h3>Title: Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yukun Jiang, Hai Huang, Mingjie Li, Yage Zhang, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08621">https://arxiv.org/abs/2602.08621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08621">https://arxiv.org/pdf/2602.08621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08621]] Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs(https://arxiv.org/abs/2602.08621)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer, large language model</a></li>
<li><strong>Abstract: </strong>By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in this https URL.</li>
</ul>

<h3>Title: Do Multilingual LLMs have specialized language heads?</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Naufil</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08625">https://arxiv.org/abs/2602.08625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08625">https://arxiv.org/pdf/2602.08625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08625]] Do Multilingual LLMs have specialized language heads?(https://arxiv.org/abs/2602.08625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.</li>
</ul>

<h3>Title: Revisiting [CLS] and Patch Token Interaction in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Alexis Marouani, Oriane Siméoni, Hervé Jégou, Piotr Bojanowski, Huy V. Vo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08626">https://arxiv.org/abs/2602.08626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08626">https://arxiv.org/pdf/2602.08626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08626]] Revisiting [CLS] and Patch Token Interaction in Vision Transformers(https://arxiv.org/abs/2602.08626)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.</li>
</ul>

<h3>Title: Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jisung Hwang, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08646">https://arxiv.org/abs/2602.08646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08646">https://arxiv.org/pdf/2602.08646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08646]] Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models(https://arxiv.org/abs/2602.08646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.</li>
</ul>

<h3>Title: Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Liu, Shao-Bo Lin, Jun Fan, Ding-Xuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08657">https://arxiv.org/abs/2602.08657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08657">https://arxiv.org/pdf/2602.08657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08657]] Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction(https://arxiv.org/abs/2602.08657)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.</li>
</ul>

<h3>Title: Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingzi Cao, Xingwei Tan, Mahmud Akhter, Marco Valentino, Maria Liakata, Xi Wang, Nikolaos Aletras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08658">https://arxiv.org/abs/2602.08658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08658">https://arxiv.org/pdf/2602.08658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08658]] Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models(https://arxiv.org/abs/2602.08658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.</li>
</ul>

<h3>Title: Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Verine, Rafael Pinot, Florian Le Bronnec</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08660">https://arxiv.org/abs/2602.08660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08660">https://arxiv.org/pdf/2602.08660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08660]] Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models(https://arxiv.org/abs/2602.08660)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.</li>
</ul>

<h3>Title: Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion</h3>
<ul>
<li><strong>Authors: </strong>Scott Thornton</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08668">https://arxiv.org/abs/2602.08668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08668">https://arxiv.org/pdf/2602.08668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08668]] Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion(https://arxiv.org/abs/2602.08668)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved "seed" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure. We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition. We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.</li>
</ul>

<h3>Title: Learning to Judge: LLMs Designing and Applying Evaluation Rubrics</h3>
<ul>
<li><strong>Authors: </strong>Clemencia Siro, Pourya Aliannejadi, Mohammad Aliannejadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08672">https://arxiv.org/abs/2602.08672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08672">https://arxiv.org/pdf/2602.08672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08672]] Learning to Judge: LLMs Designing and Applying Evaluation Rubrics(https://arxiv.org/abs/2602.08672)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.</li>
</ul>

<h3>Title: LLaDA2.1: Speeding Up Text Diffusion via Token Editing</h3>
<ul>
<li><strong>Authors: </strong>Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08676">https://arxiv.org/abs/2602.08676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08676">https://arxiv.org/pdf/2602.08676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08676]] LLaDA2.1: Speeding Up Text Diffusion via Token Editing(https://arxiv.org/abs/2602.08676)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, diffusion</a></li>
<li><strong>Abstract: </strong>While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.</li>
</ul>

<h3>Title: Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yanzhang Fu, Zizheng Guo, Jizhou Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08679">https://arxiv.org/abs/2602.08679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08679">https://arxiv.org/pdf/2602.08679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08679]] Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks(https://arxiv.org/abs/2602.08679)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.</li>
</ul>

<h3>Title: CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation</h3>
<ul>
<li><strong>Authors: </strong>Ning Yang, Chengzhi Wang, Yibo Liu, Baoliang Tian, Haijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08686">https://arxiv.org/abs/2602.08686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08686">https://arxiv.org/pdf/2602.08686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08686]] CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation(https://arxiv.org/abs/2602.08686)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.</li>
</ul>

<h3>Title: Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement</h3>
<ul>
<li><strong>Authors: </strong>Hossein Kermani, Fatemeh Oudlajani, Pardis Yarahmadi, Hamideh Mahdi Soltani, Mohammad Makki, Zahra HosseiniKhoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08688">https://arxiv.org/abs/2602.08688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08688">https://arxiv.org/pdf/2602.08688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08688]] Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement(https://arxiv.org/abs/2602.08688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.</li>
</ul>

<h3>Title: Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Constant Bourdrez, Alexandre Vérine, Olivier Cappé</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08689">https://arxiv.org/abs/2602.08689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08689">https://arxiv.org/pdf/2602.08689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08689]] Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning(https://arxiv.org/abs/2602.08689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.</li>
</ul>

<h3>Title: SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Shae McFadden, Myles Foley, Elizabeth Bates, Ilias Tsingenopoulos, Sanyam Vyas, Vasilios Mavroudis, Chris Hicks, Fabio Pierazzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08690">https://arxiv.org/abs/2602.08690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08690">https://arxiv.org/pdf/2602.08690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08690]] SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity(https://arxiv.org/abs/2602.08690)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.</li>
</ul>

<h3>Title: Reasoning aligns language models to human cognition</h3>
<ul>
<li><strong>Authors: </strong>Gonçalo Guiomar, Elia Torre, Pehuen Moure, Victoria Shavina, Mario Giulianelli, Shih-Chii Liu, Valerio Mante</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08693">https://arxiv.org/abs/2602.08693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08693">https://arxiv.org/pdf/2602.08693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08693]] Reasoning aligns language models to human cognition(https://arxiv.org/abs/2602.08693)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Do language models make decisions under uncertainty like humans do, and what role does chain-of-thought (CoT) reasoning play in the underlying decision process? We introduce an active probabilistic reasoning task that cleanly separates sampling (actively acquiring evidence) from inference (integrating evidence toward a decision). Benchmarking humans and a broad set of contemporary large language models against near-optimal reference policies reveals a consistent pattern: extended reasoning is the key determinant of strong performance, driving large gains in inference and producing belief trajectories that become strikingly human-like, while yielding only modest improvements in active sampling. To explain these differences, we fit a mechanistic model that captures systematic deviations from optimal behavior via four interpretable latent variables: memory, strategy, choice bias, and occlusion awareness. This model places humans and models in a shared low-dimensional cognitive space, reproduces behavioral signatures across agents, and shows how chain-of-thought shifts language models toward human-like regimes of evidence accumulation and belief-to-choice mapping, tightening alignment in inference while leaving a persistent gap in information acquisition.</li>
</ul>

<h3>Title: Trapped by simplicity: When Transformers fail to learn from noisy features</h3>
<ul>
<li><strong>Authors: </strong>Evan Peters, Ando Deng, Matheus H. Zambianco, Devin Blankespoor, Achim Kempf</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08695">https://arxiv.org/abs/2602.08695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08695">https://arxiv.org/pdf/2602.08695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08695]] Trapped by simplicity: When Transformers fail to learn from noisy features(https://arxiv.org/abs/2602.08695)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.</li>
</ul>

<h3>Title: FactSim: Fact-Checking for Opinion Summarization</h3>
<ul>
<li><strong>Authors: </strong>Leandro Anghinoni, Jorge Sanchez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08709">https://arxiv.org/abs/2602.08709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08709">https://arxiv.org/pdf/2602.08709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08709]] FactSim: Fact-Checking for Opinion Summarization(https://arxiv.org/abs/2602.08709)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.</li>
</ul>

<h3>Title: Towards Understanding Multimodal Fine-Tuning: Spatial Features</h3>
<ul>
<li><strong>Authors: </strong>Lachin Naghashyar, Hunar Batra, Ashkan Khakzar, Philip Torr, Ronald Clark, Christian Schroeder de Witt, Constantin Venhoff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08713">https://arxiv.org/abs/2602.08713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08713">https://arxiv.org/pdf/2602.08713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08713]] Towards Understanding Multimodal Fine-Tuning: Spatial Features(https://arxiv.org/abs/2602.08713)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.</li>
</ul>

<h3>Title: PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments</h3>
<ul>
<li><strong>Authors: </strong>Shangrui Nie, Kian Omoomi, Lucie Flek, Zhixue Zhao, Charles Welch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08716">https://arxiv.org/abs/2602.08716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08716">https://arxiv.org/pdf/2602.08716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08716]] PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments(https://arxiv.org/abs/2602.08716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.</li>
</ul>

<h3>Title: Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images</h3>
<ul>
<li><strong>Authors: </strong>Farnaz Khun Jush, Grit Werner, Mark Klemens, Matthias Lenga</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08717">https://arxiv.org/abs/2602.08717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08717">https://arxiv.org/pdf/2602.08717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08717]] Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images(https://arxiv.org/abs/2602.08717)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.</li>
</ul>

<h3>Title: QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill</h3>
<ul>
<li><strong>Authors: </strong>Dalton Jones, Junyoung Park, Matthew Morse, Mingu Lee, Chris Lott, Harper Langston</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08722">https://arxiv.org/abs/2602.08722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08722">https://arxiv.org/pdf/2602.08722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08722]] QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill(https://arxiv.org/abs/2602.08722)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.</li>
</ul>

<h3>Title: SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training</h3>
<ul>
<li><strong>Authors: </strong>Khadija Iddrisu, Waseem Shariff, Suzanne Little, Noel OConnor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08726">https://arxiv.org/abs/2602.08726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08726">https://arxiv.org/pdf/2602.08726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08726]] SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training(https://arxiv.org/abs/2602.08726)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.</li>
</ul>

<h3>Title: Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework</h3>
<ul>
<li><strong>Authors: </strong>Johannes Thalhammer, Tina Dorosti, Sebastian Peterhansl, Daniela Pfeiffer, Franz Pfeiffer, Florian Schaff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08727">https://arxiv.org/abs/2602.08727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08727">https://arxiv.org/pdf/2602.08727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08727]] Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework(https://arxiv.org/abs/2602.08727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: this https URL.</li>
</ul>

<h3>Title: Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Shanshan Wang, Ziying Feng, Xiaozheng Shen, Xun Yang, Pichao Wang, Zhenwei He, Xingyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08730">https://arxiv.org/abs/2602.08730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08730">https://arxiv.org/pdf/2602.08730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08730]] Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation(https://arxiv.org/abs/2602.08730)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at this https URL</li>
</ul>

<h3>Title: From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08735">https://arxiv.org/abs/2602.08735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08735">https://arxiv.org/pdf/2602.08735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08735]] From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models(https://arxiv.org/abs/2602.08735)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.</li>
</ul>

<h3>Title: Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing</h3>
<ul>
<li><strong>Authors: </strong>Jona te Lintelo, Lichao Wu, Stjepan Picek</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08741">https://arxiv.org/abs/2602.08741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08741">https://arxiv.org/pdf/2602.08741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08741]] Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing(https://arxiv.org/abs/2602.08741)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L$^3$), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L$^3$ learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L$^3$ on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.</li>
</ul>

<h3>Title: Empirical Evaluation of SMOTE in Android Malware Detection with Machine Learning: Challenges and Performance in CICMalDroid 2020</h3>
<ul>
<li><strong>Authors: </strong>Diego Ferreira Duarte, Andre Augusto Bortoli</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08744">https://arxiv.org/abs/2602.08744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08744">https://arxiv.org/pdf/2602.08744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08744]] Empirical Evaluation of SMOTE in Android Malware Detection with Machine Learning: Challenges and Performance in CICMalDroid 2020(https://arxiv.org/abs/2602.08744)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Malware, malicious software designed to damage computer systems and perpetrate scams, is proliferating at an alarming rate, with thousands of new threats emerging daily. Android devices, prevalent in smartphones, smartwatches, tablets, and IoTs, represent a vast attack surface, making malware detection crucial. Although advanced analysis techniques exist, Machine Learning (ML) emerges as a promising tool to automate and accelerate the discovery of these threats. This work tests ML algorithms in detecting malicious code from dynamic execution characteristics. For this purpose, the CICMalDroid2020 dataset, composed of dynamically obtained Android malware behavior samples, was used with the algorithms XGBoost, Naıve Bayes (NB), Support Vector Classifier (SVC), and Random Forest (RF). The study focused on empirically evaluating the impact of the SMOTE technique, used to mitigate class imbalance in the data, on the performance of these models. The results indicate that, in 75% of the tested configurations, the application of SMOTE led to performance degradation or only marginal improvements, with an average loss of 6.14 percentage points. Tree-based algorithms, such as XGBoost and Random Forest, consistently outperformed the others, achieving weighted recall above 94%. It is inferred that SMOTE, although widely used, did not prove beneficial for Android malware detection in the CICMalDroid2020 dataset, possibly due to the complexity and sparsity of dynamic characteristics or the nature of malicious relationships. This work highlights the robustness of tree-ensemble models, such as XGBoost, and suggests that algorithmic data balancing approaches may be more effective than generating synthetic instances in certain cybersecurity scenarios</li>
</ul>

<h3>Title: Shifting the Breaking Point of Flow Matching for Multi-Instance Editing</h3>
<ul>
<li><strong>Authors: </strong>Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta Tintoré Gazulla, Rita Cucchiara, Alessio Tonioni, Silvia Cascianelli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08749">https://arxiv.org/abs/2602.08749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08749">https://arxiv.org/pdf/2602.08749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08749]] Shifting the Breaking Point of Flow Matching for Multi-Instance Editing(https://arxiv.org/abs/2602.08749)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.</li>
</ul>

<h3>Title: DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing</h3>
<ul>
<li><strong>Authors: </strong>Guy Farrelly, Michael Chesser, Seyit Camtepe, Damith C. Ranasinghe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08750">https://arxiv.org/abs/2602.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08750">https://arxiv.org/pdf/2602.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08750]] DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing(https://arxiv.org/abs/2602.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.</li>
</ul>

<h3>Title: Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Nobuyuki Ota</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08751">https://arxiv.org/abs/2602.08751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08751">https://arxiv.org/pdf/2602.08751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08751]] Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms(https://arxiv.org/abs/2602.08751)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure. By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing regulatory structure rather than merely optimizing predictions.</li>
</ul>

<h3>Title: MVAnimate: Enhancing Character Animation with Multi-View Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Zhoujie Fu, Bang Zhang, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08753">https://arxiv.org/abs/2602.08753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08753">https://arxiv.org/pdf/2602.08753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08753]] MVAnimate: Enhancing Character Animation with Multi-View Optimization(https://arxiv.org/abs/2602.08753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.</li>
</ul>

<h3>Title: HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training</h3>
<ul>
<li><strong>Authors: </strong>Wen Xu, Zhetao Li, Yong Xiao, Pengpeng Qiao, Mianxiong Dong, Kaoru Ota</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08762">https://arxiv.org/abs/2602.08762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08762">https://arxiv.org/pdf/2602.08762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08762]] HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training(https://arxiv.org/abs/2602.08762)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.</li>
</ul>

<h3>Title: FreqLens: Interpretable Frequency Attribution for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chi-Sheng Chen, Xinyu Zhang, En-Jui Kuo, Guan-Ying Chen, Qiuzhe Xie, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08768">https://arxiv.org/abs/2602.08768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08768">https://arxiv.org/pdf/2602.08768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08768]] FreqLens: Interpretable Frequency Attribution for Time Series Forecasting(https://arxiv.org/abs/2602.08768)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.</li>
</ul>

<h3>Title: VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars</h3>
<ul>
<li><strong>Authors: </strong>Vineet Kumar Rakesh, Ahana Bhattacharjee, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das, Sarbajit Pal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08775">https://arxiv.org/abs/2602.08775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08775">https://arxiv.org/pdf/2602.08775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08775]] VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars(https://arxiv.org/abs/2602.08775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: this https URL</li>
</ul>

<h3>Title: Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Dong, Eleni Chatzi, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08792">https://arxiv.org/abs/2602.08792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08792">https://arxiv.org/pdf/2602.08792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08792]] Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems(https://arxiv.org/abs/2602.08792)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.</li>
</ul>

<h3>Title: Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Liu, Cheng Ding, Daoqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08797">https://arxiv.org/abs/2602.08797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08797">https://arxiv.org/pdf/2602.08797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08797]] Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework(https://arxiv.org/abs/2602.08797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.</li>
</ul>

<h3>Title: CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Hedong Zhang, Neusha Javidnia, Shweta Pardeshi, Qian Lou, Farinaz Koushanfar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08798">https://arxiv.org/abs/2602.08798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08798">https://arxiv.org/pdf/2602.08798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08798]] CryptoGen: Secure Transformer Generation with Encrypted KV-Cache Reuse(https://arxiv.org/abs/2602.08798)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, transformer, generative</a></li>
<li><strong>Abstract: </strong>The widespread deployment of cloud-hosted generative models raises a fundamental challenge: enabling efficient autoregressive generation while preserving the privacy of both user prompts and model parameters in untrusted environments. We address this challenge in a client-server setting where an untrusted server hosts an autoregressive Transformer and the client requires cryptographic protection for both inputs and inference. We present CryptoGen, the first system to enable scalable privacy-preserving neural generation with persistent encrypted key-value (KV) cache reuse. Discriminative-task secure inference systems incur quadratic latency and memory growth when adapted to autoregressive decoding due to the lack of native encrypted KV-cache support. In contrast, CryptoGen achieves near-linear scaling by securely reusing and updating encrypted KV caches throughout generation. CryptoGen integrates homomorphic encryption and secret sharing to support both prefilling and generation. Key techniques include a unified encrypted KV-cache framework, heterogeneous SIMD encodings for different phases, optimized cipher-cipher matrix-matrix and matrix-vector operations, and efficient noise refresh and ciphertext concatenation mechanisms. Evaluation on generative Transformer models trained on WikiText-2, PTB, and LAMBADA shows that for input lengths of 128-512 tokens, CryptoGen achieves 4.4x-7.6x lower per-token latency than state-of-the-art discriminative secure inference systems, while maintaining near-linear latency and memory scaling, with advantages increasing for longer sequences. CryptoGen is released as an open-source library.</li>
</ul>

<h3>Title: How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yapei Chang, Kyle Lo, Mohit Iyyer, Luca Soldaini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08808">https://arxiv.org/abs/2602.08808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08808">https://arxiv.org/pdf/2602.08808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08808]] How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs(https://arxiv.org/abs/2602.08808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.</li>
</ul>

<h3>Title: Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI</h3>
<ul>
<li><strong>Authors: </strong>Karim Haroun, Aya Zitouni, Aicha Zenakhri, Meriem Amel Guessoum, Larbi Boubchir</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08809">https://arxiv.org/abs/2602.08809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08809">https://arxiv.org/pdf/2602.08809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08809]] Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI(https://arxiv.org/abs/2602.08809)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, biometric, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.</li>
</ul>

<h3>Title: Robust Policy Optimization to Prevent Catastrophic Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Sabbaghi, George Pappas, Adel Javanmard, Hamed Hassani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08813">https://arxiv.org/abs/2602.08813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08813">https://arxiv.org/pdf/2602.08813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08813]] Robust Policy Optimization to Prevent Catastrophic Forgetting(https://arxiv.org/abs/2602.08813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning. We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.</li>
</ul>

<h3>Title: Kirin: Improving ANN efficiency with SNN Hybridization</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Zhanglu Yan, Zhi Zhou, Xu Chen, Weng-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08817">https://arxiv.org/abs/2602.08817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08817">https://arxiv.org/pdf/2602.08817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08817]] Kirin: Improving ANN efficiency with SNN Hybridization(https://arxiv.org/abs/2602.08817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.</li>
</ul>

<h3>Title: FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Annemette Brok Pirchert, Jacob Nielsen, Mogens Henrik From, Lukas Galke Poech, Peter Schneider-Kamp</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08818">https://arxiv.org/abs/2602.08818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08818">https://arxiv.org/pdf/2602.08818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08818]] FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models(https://arxiv.org/abs/2602.08818)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.</li>
</ul>

<h3>Title: Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Zhiyu Tan, Jia Gong, Luozheng Qin, Hesen Chen, Xiaomeng Yang, Yuqing Sun, Yuetan Lin, Mengping Yang, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08820">https://arxiv.org/abs/2602.08820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08820">https://arxiv.org/pdf/2602.08820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08820]] Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing(https://arxiv.org/abs/2602.08820)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.</li>
</ul>

<h3>Title: Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications</h3>
<ul>
<li><strong>Authors: </strong>Yao Pu, Yiming Shi, Zhenxi Zhang, Peixin Yu, Yitao Zhuang, Xiang Wang, Hongzhao Chen, Jing Cai, Ge Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08822">https://arxiv.org/abs/2602.08822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08822">https://arxiv.org/pdf/2602.08822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08822]] Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications(https://arxiv.org/abs/2602.08822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.</li>
</ul>

<h3>Title: Affective Flow Language Model for Emotional Support Conversation</h3>
<ul>
<li><strong>Authors: </strong>Chenghui Zou, Ning Wang, Tiesunlong Shen, Luwei Xiao, Chuan Ma, Xiangpeng Li, Rui Mao, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08826">https://arxiv.org/abs/2602.08826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08826">https://arxiv.org/pdf/2602.08826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08826]] Affective Flow Language Model for Emotional Support Conversation(https://arxiv.org/abs/2602.08826)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains this http URL is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at this https URL.</li>
</ul>

<h3>Title: VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Tan, Jun Lan, Senyuan Shi, Zichang Tan, Zijian Yu, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08828">https://arxiv.org/abs/2602.08828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08828">https://arxiv.org/pdf/2602.08828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08828]] VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning(https://arxiv.org/abs/2602.08828)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.</li>
</ul>

<h3>Title: WildReward: Learning Reward Models from In-the-Wild Human Interactions</h3>
<ul>
<li><strong>Authors: </strong>Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08829">https://arxiv.org/abs/2602.08829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08829">https://arxiv.org/pdf/2602.08829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08829]] WildReward: Learning Reward Models from In-the-Wild Human Interactions(https://arxiv.org/abs/2602.08829)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at this https URL.</li>
</ul>

<h3>Title: Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization</h3>
<ul>
<li><strong>Authors: </strong>Yang Qiu, Yixiong Zou, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08855">https://arxiv.org/abs/2602.08855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08855">https://arxiv.org/pdf/2602.08855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08855]] Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization(https://arxiv.org/abs/2602.08855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.</li>
</ul>

<h3>Title: Discovering Interpretable Algorithms by Decompiling Transformers to RASP</h3>
<ul>
<li><strong>Authors: </strong>Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08857">https://arxiv.org/abs/2602.08857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08857">https://arxiv.org/pdf/2602.08857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08857]] Discovering Interpretable Algorithms by Decompiling Transformers to RASP(https://arxiv.org/abs/2602.08857)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.</li>
</ul>

<h3>Title: FlattenGPT: Depth Compression for Transformer with Layer Flattening</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Xu, Qingpei Guo, Yao Zhu, Xiangyang Ji, Ming Yang, Shiliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08858">https://arxiv.org/abs/2602.08858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08858">https://arxiv.org/pdf/2602.08858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08858]] FlattenGPT: Depth Compression for Transformer with Layer Flattening(https://arxiv.org/abs/2602.08858)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.</li>
</ul>

<h3>Title: Magnitude Distance: A Geometric Measure of Dataset Similarity</h3>
<ul>
<li><strong>Authors: </strong>Sahel Torkamani, Henry Gouk, Rik Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08859">https://arxiv.org/abs/2602.08859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08859">https://arxiv.org/pdf/2602.08859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08859]] Magnitude Distance: A Geometric Measure of Dataset Similarity(https://arxiv.org/abs/2602.08859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.</li>
</ul>

<h3>Title: TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiangtian Zheng, Zishuo Wang, Yuxin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08861">https://arxiv.org/abs/2602.08861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08861">https://arxiv.org/pdf/2602.08861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08861]] TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models(https://arxiv.org/abs/2602.08861)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.</li>
</ul>

<h3>Title: Understanding Dynamic Compute Allocation in Recurrent Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08864">https://arxiv.org/abs/2602.08864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08864">https://arxiv.org/pdf/2602.08864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08864]] Understanding Dynamic Compute Allocation in Recurrent Transformers(https://arxiv.org/abs/2602.08864)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.</li>
</ul>

<h3>Title: AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Junru Zhang, Lang Feng, Haoran Shi, Xu Guo, Han Yu, Yabo Dong, Duanqing Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08868">https://arxiv.org/abs/2602.08868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08868">https://arxiv.org/pdf/2602.08868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08868]] AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection(https://arxiv.org/abs/2602.08868)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.</li>
</ul>

<h3>Title: ZK-Rollup for Hyperledger Fabric: Architecture and Performance Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Sania Siddiqui, Neha, Hari Babu K</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08870">https://arxiv.org/abs/2602.08870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08870">https://arxiv.org/pdf/2602.08870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08870]] ZK-Rollup for Hyperledger Fabric: Architecture and Performance Evaluation(https://arxiv.org/abs/2602.08870)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>A big challenge posed in blockchain centric platforms is achieving scalability while also preserving user privacy. This report details the design, implementation and evaluation of a Layer-2 scaling solution for Hyperledger Fabric using Zero Knowledge Rollups (ZK Rollups). The proposed architecture introduces an off chain sequencer that accepts transactions immediately and sends them for batching into a Merkle tree based rollup, using ZK proofs to attest to the correctness and verifiability of the entire batch. The design aims to decouple transaction ingestion from actual on chain settlements to address Fabric scalability limitations and increase throughput under high load conditions. The baseline architecture in Hyperledger Fabric constrains transaction requests due to endorsement, ordering and validation phases, leading to a throughput of 5 to 7 TPS with an average latency of 4 seconds. Our Layer-2 solution achieves an ingestion throughput of 70 to 100 TPS, leading to an increase of nearly ten times due to the sequencer immediate acceptance of each transaction and reducing client perceived latency by nearly eighty percent to 700 to 1000 milliseconds. This work demonstrates that integrating ZK Rollups in Hyperledger Fabric enhances scalability while not compromising the security guarantees of a permissioned blockchain network.</li>
</ul>

<h3>Title: Large Language Models for Geolocation Extraction in Humanitarian Crisis Response</h3>
<ul>
<li><strong>Authors: </strong>G. Cafferata, T. Demarco, K. Kalimeri, Y. Mejova, M.G. Beiró</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08872">https://arxiv.org/abs/2602.08872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08872">https://arxiv.org/pdf/2602.08872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08872]] Large Language Models for Geolocation Extraction in Humanitarian Crisis Response(https://arxiv.org/abs/2602.08872)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair, large language model</a></li>
<li><strong>Abstract: </strong>Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.</li>
</ul>

<h3>Title: Is Reasoning Capability Enough for Safety in Long-Context Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yu Fu, Haz Sameen Shahgir, Huanli Gong, Zhipeng Wei, N. Benjamin Erichson, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08874">https://arxiv.org/abs/2602.08874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08874">https://arxiv.org/pdf/2602.08874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08874]] Is Reasoning Capability Enough for Safety in Long-Context Language Models?(https://arxiv.org/abs/2602.08874)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.</li>
</ul>

<h3>Title: Stress-Testing Alignment Audits With Prompt-Level Strategic Deception</h3>
<ul>
<li><strong>Authors: </strong>Oliver Daniels, Perusha Moodley, Ben Marlin, David Lindner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08877">https://arxiv.org/abs/2602.08877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08877">https://arxiv.org/pdf/2602.08877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08877]] Stress-Testing Alignment Audits With Prompt-Level Strategic Deception(https://arxiv.org/abs/2602.08877)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.</li>
</ul>

<h3>Title: Discrete Bridges for Mutual Information Estimation</h3>
<ul>
<li><strong>Authors: </strong>Iryna Zabarianska, Sergei Kholkin, Grigoriy Ksenofontov, Ivan Butakov, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08894">https://arxiv.org/abs/2602.08894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08894">https://arxiv.org/pdf/2602.08894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08894]] Discrete Bridges for Mutual Information Estimation(https://arxiv.org/abs/2602.08894)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.</li>
</ul>

<h3>Title: GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuanqi Zhang, Haoyang Shang, Xiaoxiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08901">https://arxiv.org/abs/2602.08901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08901">https://arxiv.org/pdf/2602.08901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08901]] GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs(https://arxiv.org/abs/2602.08901)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.</li>
</ul>

<h3>Title: Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang, Wei Wang, Wenjie Liu, Nam Ling</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08909">https://arxiv.org/abs/2602.08909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08909">https://arxiv.org/pdf/2602.08909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08909]] Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit(https://arxiv.org/abs/2602.08909)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.</li>
</ul>

<h3>Title: GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems</h3>
<ul>
<li><strong>Authors: </strong>Kateřina Henclová, Václav Šmídl</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08913">https://arxiv.org/abs/2602.08913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08913">https://arxiv.org/pdf/2602.08913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08913]] GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems(https://arxiv.org/abs/2602.08913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations. We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent. The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise. GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at this https URL also includes a free, easy-to-use application suitable for non-coders.</li>
</ul>

<h3>Title: Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration</h3>
<ul>
<li><strong>Authors: </strong>Manh Cuong Dao, Quang Hung Pham, Phi Le Nguyen, Thao Nguyen Truong, Bryan Kian Hsiang Low, Trong Nghia Hoang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08920">https://arxiv.org/abs/2602.08920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08920">https://arxiv.org/pdf/2602.08920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08920]] Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration(https://arxiv.org/abs/2602.08920)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.</li>
</ul>

<h3>Title: StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors</h3>
<ul>
<li><strong>Authors: </strong>Suraj Ranganath, Atharv Ramesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08934">https://arxiv.org/abs/2602.08934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08934">https://arxiv.org/pdf/2602.08934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08934]] StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors(https://arxiv.org/abs/2602.08934)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at this https URL.</li>
</ul>

<h3>Title: MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE</h3>
<ul>
<li><strong>Authors: </strong>Ruijie Zhu, Jiahao Lu, Wenbo Hu, Xiaoguang Han, Jianfei Cai, Ying Shan, Chuanxia Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CG, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08961">https://arxiv.org/abs/2602.08961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08961">https://arxiv.org/pdf/2602.08961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08961]] MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE(https://arxiv.org/abs/2602.08961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: this https URL</li>
</ul>

<h3>Title: A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Raghu Arghal, Fade Chen, Niall Dalton, Evgenii Kortukov, Calum McNamara, Angelos Nalmpantis, Moksh Nirvaan, Gabriele Sarti, Mario Giulianelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08964">https://arxiv.org/abs/2602.08964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08964">https://arxiv.org/pdf/2602.08964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08964]] A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents(https://arxiv.org/abs/2602.08964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.</li>
</ul>

<h3>Title: Distributionally Robust Optimization via Generative Ambiguity Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wen, Jianyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08976">https://arxiv.org/abs/2602.08976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08976">https://arxiv.org/pdf/2602.08976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08976]] Distributionally Robust Optimization via Generative Ambiguity Modeling(https://arxiv.org/abs/2602.08976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.</li>
</ul>

<h3>Title: StretchTime: Adaptive Time Series Forecasting via Symplectic Attention</h3>
<ul>
<li><strong>Authors: </strong>Yubin Kim, Viresh Pati, Jevon Twitty, Vinh Pham, Shihao Yang, Jiecheng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08983">https://arxiv.org/abs/2602.08983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08983">https://arxiv.org/pdf/2602.08983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08983]] StretchTime: Adaptive Time Series Forecasting via Symplectic Attention(https://arxiv.org/abs/2602.08983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.</li>
</ul>

<h3>Title: Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08984">https://arxiv.org/abs/2602.08984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08984">https://arxiv.org/pdf/2602.08984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08984]] Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models(https://arxiv.org/abs/2602.08984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.</li>
</ul>

<h3>Title: Reverse Online Guessing Attacks on PAKE Protocols</h3>
<ul>
<li><strong>Authors: </strong>Eloise Christian, Tejas Gadwalkar, Arthur Azevedo de Amorim, Edward V. Zieglar Jr</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08993">https://arxiv.org/abs/2602.08993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08993">https://arxiv.org/pdf/2602.08993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08993]] Reverse Online Guessing Attacks on PAKE Protocols(https://arxiv.org/abs/2602.08993)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Though not yet widely deployed, password-authenticated key exchange (PAKE) protocols have been the subject of several recent standardization efforts, partly because of their resistance against various guessing attacks, but also because they do not require a public-key infrastructure (PKI), making them naturally resistant against PKI failures. The goal of this paper is to reevaluate the PAKE model by noting that the absence of a PKI -- or, more generally, of a mechanism aside from the password for authenticating the server -- makes such protocols vulnerable to reverse online guessing attacks, in which an adversary attempts to validate password guesses by impersonating a server. While their logic is similar to traditional guessing, where the attacker impersonates a client, reverse guessing poses a unique risk because the burden of detection is shifted to the clients, rendering existing defenses against traditional guessing moot. Our results demonstrate that reverse guessing is particularly effective when an adversary attacks clients indiscriminately, such as in phishing or password-spraying attacks, or for applications with automated login processes or a universal password, such as WPA3-SAE. Our analysis suggests that stakeholders should, by default, authenticate the server using more stringent measures than just the user's password, and that a password-only mode of operation should be a last resort against catastrophic security failures when other authentication mechanisms are not available.</li>
</ul>

<h3>Title: When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Yuting Ning, Jaylen Jones, Zhehao Zhang, Chentao Ye, Weitong Ruan, Junyi Li, Rahul Gupta, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.08995">https://arxiv.org/abs/2602.08995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.08995">https://arxiv.org/pdf/2602.08995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.08995]] When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents(https://arxiv.org/abs/2602.08995)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.</li>
</ul>

<h3>Title: ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling</h3>
<ul>
<li><strong>Authors: </strong>Yilang Zhang, Bingcong Li, Niao He, Georgios B. Giannakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09009">https://arxiv.org/abs/2602.09009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09009">https://arxiv.org/pdf/2602.09009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09009]] ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling(https://arxiv.org/abs/2602.09009)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.</li>
</ul>

<h3>Title: Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Yaxin Luo, Jiacheng Cui, Xinyi Shang, Xiaohan Zhao, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09012">https://arxiv.org/abs/2602.09012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09012">https://arxiv.org/pdf/2602.09012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09012]] Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense(https://arxiv.org/abs/2602.09012)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, robust</a></li>
<li><strong>Abstract: </strong>The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.</li>
</ul>

<h3>Title: ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zihan Yang (1), Shuyuan Tu (1), Licheng Zhang (1), Qi Dai (2), Yu-Gang Jiang (1), Zuxuan Wu (1) ((1) Fudan University, (2) Microsoft Research Asia)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09014">https://arxiv.org/abs/2602.09014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09014">https://arxiv.org/pdf/2602.09014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09014]] ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation(https://arxiv.org/abs/2602.09014)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.</li>
</ul>

<h3>Title: CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Nejati, Mahdi Rabbani, Mansur Mirani, Gunjan Piya, Igor Opushnyev, Ali A. Ghorbani, Sajjad Dadkhah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09015">https://arxiv.org/abs/2602.09015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09015">https://arxiv.org/pdf/2602.09015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09015]] CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection(https://arxiv.org/abs/2602.09015)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.</li>
</ul>

<h3>Title: Autoregressive Image Generation with Masked Bit Modeling</h3>
<ul>
<li><strong>Authors: </strong>Qihang Yu, Qihao Liu, Ju He, Xinyang Zhang, Yang Liu, Liang-Chieh Chen, Xi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.09024">https://arxiv.org/abs/2602.09024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.09024">https://arxiv.org/pdf/2602.09024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.09024]] Autoregressive Image Generation with Masked Bit Modeling(https://arxiv.org/abs/2602.09024)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
