<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A New Approach to Post-Quantum Non-Malleability. (arXiv:2207.05861v1 [quant-ph])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05861">http://arxiv.org/abs/2207.05861</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We provide the first $\mathit{constant}$-$\mathit{round}$ construction of
post-quantum non-malleable commitments under the minimal assumption that
$\mathit{post}$-$\mathit{quantum}$ $\mathit{one}$-$\mathit{way}$
$\mathit{functions}$ exist. We achieve the standard notion of non-malleability
with respect to commitments. Prior constructions required
$\Omega(\log^*\lambda)$ rounds under the same assumption.
</p></li>
</ul>

<p>We achieve our results through a new technique for constant-round
non-malleable commitments which is easier to use in the post-quantum setting.
The technique also yields an almost elementary proof of security for
constant-round non-malleable commitments in the classical setting, which may be
of independent interest.
</p>
<p>As an application, when combined with existing work, our results yield the
first constant-round post-quantum secure multiparty computation under the
$\mathit{polynomial}$ hardness of quantum fully-homomorphic encryption and
quantum learning with errors.
</p>

<h3>Title: Achieving Almost All Blockchain Functionalities with Polylogarithmic Storage. (arXiv:2207.05869v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05869">http://arxiv.org/abs/2207.05869</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In current blockchain systems, full nodes that perform all of the available
functionalities need to store the entire blockchain. In addition to the
blockchain, full nodes also store a blockchain-summary, called the
\emph{state}, which is used to efficiently verify transactions. With the size
of popular blockchains and their states growing rapidly, full nodes require
massive storage resources in order to keep up with the scaling. This leads to a
tug-of-war between scaling and decentralization since fewer entities can afford
expensive resources. We present \emph{hybrid nodes} for proof-of-work (PoW)
cryptocurrencies which can validate transactions, validate blocks, validate
states, mine, select the main chain, bootstrap new hybrid nodes, and verify
payment proofs. With the use of a protocol called \emph{trimming}, hybrid nodes
only retain polylogarithmic number of blocks in the chain length in order to
represent the proof-of-work of the blockchain. Hybrid nodes are also optimized
for the storage of the state with the use of \emph{stateless blockchain}
protocols. The lowered storage requirements should enable more entities to join
as hybrid nodes and improve the decentralization of the system. We define novel
theoretical security models for hybrid nodes and show that they are provably
secure. We also show that the storage requirement of hybrid nodes is
near-optimal with respect to our security definitions.
</p></li>
</ul>

<h3>Title: E-Tenon: An Efficient Privacy-Preserving Secure Open Data Sharing Scheme for EHR System. (arXiv:2207.05890v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05890">http://arxiv.org/abs/2207.05890</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The transition from paper-based information to Electronic-Health-Records
(EHRs) has driven various advancements in the modern healthcare-industry. In
many cases, patients need to share their EHR with healthcare professionals.
Given the sensitive and security-critical nature of EHRs, it is essential to
consider the security and privacy issues of storing and sharing EHR. However,
existing security solutions excessively encrypt the whole database, thus
requiring the entire database to be decrypted for each access request, which is
a time-consuming process. On the other hand, the use of EHR for medical
research (e.g., development of precision-medicine, diagnostics-techniques), as
well as optimisation of practices in healthcare organisations, requires the EHR
to be analysed, and for that, they should be easily accessible without
compromising the privacy of the patient. In this paper, we propose an efficient
technique called E-Tenon that not only securely keeps all EHR publicly
accessible but also provides the desirable security features. To the best of
our knowledge, this is the first work in which an Open Database is used for
protecting EHR. The proposed E-Tenon empowers patients to securely share their
EHR under multi-level, fine-grained access policies defined by themselves.
Analyses show that our system outperforms existing solutions in terms of
computational-complexity.
</p></li>
</ul>

<h3>Title: A blockchain-based secure storage scheme for medical information. (arXiv:2207.06102v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06102">http://arxiv.org/abs/2207.06102</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Medical data involves a large amount of personal information and is highly
privacy sensitive. In the age of big data, the increasing informatization of
healthcare makes it vital that medical information is stored securely and
accurately. However, current medical information is subject to the risk of
privacy leakage and difficult to share. To address these issues, this paper
proposes a healthcare information security storage solution based on
Hyperledger Fabric and the Attribute-Based Access Control (ABAC) framework. The
scheme first utilizes attribute-based access control, which allows dynamic and
fine-grained access to medical information, and then stores the medical
information in the blockchain, which can be secured and tamper-proof by
formulating corresponding smart contracts. In addition, this solution also
incorporates IPFS technology to relieve the storage pressure of the blockchain.
Experiments show that the proposed scheme combining access control of
attributes and blockchain technology in this paper can not only ensure the
secure storage and integrity of medical information but also has a high
throughput when accessing medical information.
</p></li>
</ul>

<h3>Title: Secure Linear MDS Coded Matrix Inversion. (arXiv:2207.06271v1 [cs.IT])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06271">http://arxiv.org/abs/2207.06271</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A cumbersome operation in many scientific fields, is inverting large
full-rank matrices. In this paper, we propose a coded computing approach for
recovering matrix inverse approximations. We first present an approximate
matrix inversion algorithm which does not require a matrix factorization, but
uses a black-box least squares optimization solver as a subroutine, to give an
estimate of the inverse of a real full-rank matrix. We then present a
distributed framework for which our algorithm can be implemented, and show how
we can leverage sparsest-balanced MDS generator matrices to devise matrix
inversion coded computing schemes. We focus on balanced Reed-Solomon codes,
which are optimal in terms of computational load; and communication from the
workers to the master server. We also discuss how our algorithms can be used to
compute the pseudoinverse of a full-rank matrix, and how the communication is
secured from eavesdroppers.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding. (arXiv:2207.03608v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.03608">http://arxiv.org/abs/2207.03608</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.
</p></li>
</ul>

<h3>Title: Enhanced Security and Privacy via Fragmented Federated Learning. (arXiv:2207.05978v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05978">http://arxiv.org/abs/2207.05978</a></li>
<li>Code URL: <a href="https://github.com/anonymized30/ffl">https://github.com/anonymized30/ffl</a></li>
<li>Summary: <p>In federated learning (FL), a set of participants share updates computed on
their local data with an aggregator server that combines updates into a global
model. However, reconciling accuracy with privacy and security is a challenge
to FL. On the one hand, good updates sent by honest participants may reveal
their private local information, whereas poisoned updates sent by malicious
participants may compromise the model's availability and/or integrity. On the
other hand, enhancing privacy via update distortion damages accuracy, whereas
doing so via update aggregation damages security because it does not allow the
server to filter out individual poisoned updates. To tackle the
accuracy-privacy-security conflict, we propose {\em fragmented federated
learning} (FFL), in which participants randomly exchange and mix fragments of
their updates before sending them to the server. To achieve privacy, we design
a lightweight protocol that allows participants to privately exchange and mix
encrypted fragments of their updates so that the server can neither obtain
individual updates nor link them to their originators. To achieve security, we
design a reputation-based defense tailored for FFL that builds trust in
participants and their mixed updates based on the quality of the fragments they
exchange and the mixed updates they send. Since the exchanged fragments'
parameters keep their original coordinates and attackers can be neutralized,
the server can correctly reconstruct a global model from the received mixed
updates without accuracy loss. Experiments on four real data sets show that FFL
can prevent semi-honest servers from mounting privacy attacks, can effectively
counter poisoning attacks and can keep the accuracy of the global model.
</p></li>
</ul>

<h3>Title: A Personalised User Authentication System based on EEG Signals. (arXiv:2207.06109v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06109">http://arxiv.org/abs/2207.06109</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Conventional biometrics have been employed in high security user
authentication systems for over 20 years now. However, some of these modalities
face low security issues in common practice. Brain wave based user
authentication has emerged as a promising alternative method, as it overcomes
some of these drawbacks and allows for continuous user authentication. In the
present study we address the problem of individual user variability, by
proposing a data-driven Electroencephalography (EEG) based authentication
method. We introduce machine learning techniques, in order to reveal the
optimal classification algorithm that best fits the data of each individual
user, in a fast and efficient manner. A set of 15 power spectral features
(delta, theta, lower alpha, higher alpha, and alpha) is extracted from the
three EEG channels. The results show that our approach can reliably grant or
deny access to the user (mean accuracy 95,6%), while at the same time poses as
a viable option for real time applications, as the total time of the training
procedure was kept under one minute.
</p></li>
</ul>

<h3>Title: On Post-Quantum Perfect Forward Secrecy in 6G. (arXiv:2207.06144v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06144">http://arxiv.org/abs/2207.06144</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The standardized Authentication and Key Agreement protocol for 5G networks
(also known as 5G AKA) has several security and privacy vulnerabilities. For
example, the 5G AKA does not undertake perfect forward secrecy. In this paper,
we propose a novel quantum-safe authentication and key agreement protocol for
future generation of mobile communication networks (6G). Our protocol has
several privacy and security properties, e.g., it is resistant against
linkability attacks and it is quantum-safe. We use the Kyber algorithm, chosen
by NIST to become a standard and NIST Round 4 candidate algorithms to analyze
the performance of our protocol. The results for communication and computation
costs show that utilizing our protocol is feasible in practice. We further
prove the security of our protocol by utilizing the well-known formal verifier
ProVerif.
</p></li>
</ul>

<h3>Title: Explainable Intrusion Detection Systems (X-IDS): A Survey of Current Methods, Challenges, and Opportunities. (arXiv:2207.06236v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06236">http://arxiv.org/abs/2207.06236</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The application of Artificial Intelligence (AI) and Machine Learning (ML) to
cybersecurity challenges has gained traction in industry and academia,
partially as a result of widespread malware attacks on critical systems such as
cloud infrastructures and government institutions. Intrusion Detection Systems
(IDS), using some forms of AI, have received widespread adoption due to their
ability to handle vast amounts of data with a high prediction accuracy. These
systems are hosted in the organizational Cyber Security Operation Center (CSoC)
as a defense tool to monitor and detect malicious network flow that would
otherwise impact the Confidentiality, Integrity, and Availability (CIA). CSoC
analysts rely on these systems to make decisions about the detected threats.
However, IDSs designed using Deep Learning (DL) techniques are often treated as
black box models and do not provide a justification for their predictions. This
creates a barrier for CSoC analysts, as they are unable to improve their
decisions based on the model's predictions. One solution to this problem is to
design explainable IDS (X-IDS).
</p></li>
</ul>

<p>This survey reviews the state-of-the-art in explainable AI (XAI) for IDS, its
current challenges, and discusses how these challenges span to the design of an
X-IDS. In particular, we discuss black box and white box approaches
comprehensively. We also present the tradeoff between these approaches in terms
of their performance and ability to produce explanations. Furthermore, we
propose a generic architecture that considers human-in-the-loop which can be
used as a guideline when designing an X-IDS. Research recommendations are given
from three critical viewpoints: the need to define explainability for IDS, the
need to create explanations tailored to various stakeholders, and the need to
design metrics to evaluate explanations.
</p>

<h3>Title: Developing an NLP-based Recommender System for the Ethical, Legal, and Social Implications of Synthetic Biology. (arXiv:2207.06360v1 [cs.IR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06360">http://arxiv.org/abs/2207.06360</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Synthetic biology is an emerging field that involves the engineering and
re-design of organisms for purposes such as food security, health, and
environmental protection. As such, it poses numerous ethical, legal, and social
implications (ELSI) for researchers and policy makers. Various efforts to
ensure socially responsible synthetic biology are underway. Policy making is
one regulatory avenue, and other initiatives have sought to embed social
scientists and ethicists on synthetic biology projects. However, given the
nascency of synthetic biology, the number of heterogeneous domains it spans,
and the open nature of many ethical questions, it has proven challenging to
establish widespread concrete policies, and including social scientists and
ethicists on synthetic biology teams has met with mixed success.
</p></li>
</ul>

<p>This text proposes a different approach, asking instead is it possible to
develop a well-performing recommender model based upon natural language
processing (NLP) to connect synthetic biologists with information on the ELSI
of their specific research? This recommender was developed as part of a larger
project building a Synthetic Biology Knowledge System (SBKS) to accelerate
discovery and exploration of the synthetic biology design space. Our approach
aims to distill for synthetic biologists relevant ethical and social scientific
information and embed it into synthetic biology research workflows.
</p>

<h3>Title: On the Opportunities and Risks of Foundation Models. (arXiv:2108.07258v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2108.07258">http://arxiv.org/abs/2108.07258</a></li>
<li>Code URL: null</li>
<li>Summary: <p>AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature.
</p></li>
</ul>

<h3>Title: URANUS: Radio Frequency Tracking, Classification and Identification of Unmanned Aircraft Vehicles. (arXiv:2207.06025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06025">http://arxiv.org/abs/2207.06025</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Safety and security issues for Critical Infrastructures (CI) are growing as
attackers increasingly adopt drones as an attack vector flying in sensitive
airspace, such as airports, military bases, city centres, and crowded places.
The rapid proliferation of drones for merchandise, shipping recreations
activities, and other commercial applications poses severe concerns on the CI
operators due to the violations and the invasions of the restricted airspaces.
A cost-effective framework is needed to detect, classify and identify the
presence of drones in such cases. In this paper, we demonstrate that CI
operators can detect, classify and identify timely and efficiently drones
(multi-copter and fixed-wings) invading no-drone zones, with an inexpensive
RF-based detection framework named URANUS. Our experiments show that by using
Random Forest classifier, we achieved a classification accuracy of 93.4% in the
classification of one or multiple specific drones. The tracking performance
achieves an accuracy with an average of MAE=0.3650, MSE=0.9254 and R2 = 0.7502.
Our framework has been released as open-source, to enable the community to
verify our findings and use URANUS as a ready-to-use basis for further
analysis.
</p></li>
</ul>

<h3>Title: Machine Learning Assisted Approach for Security-Constrained Unit Commitment. (arXiv:2111.09824v2 [eess.SY] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2111.09824">http://arxiv.org/abs/2111.09824</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Security-constrained unit commitment (SCUC) is solved for power system
day-ahead generation scheduling, which is a large-scale mixed-integer linear
programming problem and is very computationally intensive. Model reduction of
SCUC may bring significant time savings. In this work, a novel approach is
proposed to effectively utilize machine learning (ML) to reduce the problem
size of SCUC. An ML model using logistic regression (LR) algorithm is proposed
and trained with historical nodal demand profiles and the respective commitment
schedules. The ML outputs are processed and analyzed to reduce variables and
constraints in SCUC. The proposed approach is validated on several standard
test systems including IEEE 24-bus system, IEEE 73-bus system, IEEE 118-bus
system, synthetic South Carolina 500-bus system and Polish 2383-bus system.
Simulation results demonstrate that the use of the prediction from the proposed
LR model in SCUC model reduction can substantially reduce the computing time
while maintaining solution quality.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy Preserving Image Registration. (arXiv:2205.10120v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.10120">http://arxiv.org/abs/2205.10120</a></li>
<li>Code URL: <a href="https://github.com/rtaiello/pp_image_registration">https://github.com/rtaiello/pp_image_registration</a></li>
<li>Summary: <p>Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</p></li>
</ul>

<h3>Title: DH-GAN: A Physics-driven Untrained Generative Adversarial Network for 3D Microscopic Imaging using Digital Holography. (arXiv:2205.12920v2 [cs.IR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.12920">http://arxiv.org/abs/2205.12920</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Digital holography is a 3D imaging technique by emitting a laser beam with a
plane wavefront to an object and measuring the intensity of the diffracted
waveform, called holograms. The object's 3D shape can be obtained by numerical
analysis of the captured holograms and recovering the incurred phase. Recently,
deep learning (DL) methods have been used for more accurate holographic
processing. However, most supervised methods require large datasets to train
the model, which is rarely available in most DH applications due to the
scarcity of samples or privacy concerns. A few one-shot DL-based recovery
methods exist with no reliance on large datasets of paired images. Still, most
of these methods often neglect the underlying physics law that governs wave
propagation. These methods offer a black-box operation, which is not
explainable, generalizable, and transferrable to other samples and
applications. In this work, we propose a new DL architecture based on
generative adversarial networks that uses a discriminative network for
realizing a semantic measure for reconstruction quality while using a
generative network as a function approximator to model the inverse of hologram
formation. We impose smoothness on the background part of the recovered image
using a progressive masking module powered by simulated annealing to enhance
the reconstruction quality. The proposed method is one of its kind that
exhibits high transferability to similar samples, which facilitates its fast
deployment in time-sensitive applications without the need for retraining the
network. The results show a considerable improvement to competitor methods in
reconstruction quality (about 5 dB PSNR gain) and robustness to noise (about
50% reduction in PSNR vs noise increase rate).
</p></li>
</ul>

<h3>Title: Exploiting Social Graph Networks for Emotion Prediction. (arXiv:2207.05820v1 [cs.SI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05820">http://arxiv.org/abs/2207.05820</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Emotion prediction plays an essential role in mental health and emotion-aware
computing. The complex nature of emotion resulting from its dependency on a
person's physiological health, mental state, and his surroundings makes its
prediction a challenging task. In this work, we utilize mobile sensing data to
predict happiness and stress. In addition to a person's physiological features,
we also incorporate the environment's impact through weather and social
network. To this end, we leverage phone data to construct social networks and
develop a machine learning architecture that aggregates information from
multiple users of the graph network and integrates it with the temporal
dynamics of data to predict emotion for all the users. The construction of
social networks does not incur additional cost in terms of EMAs or data
collection from users and doesn't raise privacy concerns. We propose an
architecture that automates the integration of a user's social network affect
prediction, is capable of dealing with the dynamic distribution of real-life
social networks, making it scalable to large-scale networks. Our extensive
evaluation highlights the improvement provided by the integration of social
networks. We further investigate the impact of graph topology on model's
performance.
</p></li>
</ul>

<h3>Title: Differentially Private Linear Bandits with Partial Distributed Feedback. (arXiv:2207.05827v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05827">http://arxiv.org/abs/2207.05827</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this paper, we study the problem of global reward maximization with only
partial distributed feedback. This problem is motivated by several real-world
applications (e.g., cellular network configuration, dynamic pricing, and policy
selection) where an action taken by a central entity influences a large
population that contributes to the global reward. However, collecting such
reward feedback from the entire population not only incurs a prohibitively high
cost but often leads to privacy concerns. To tackle this problem, we consider
differentially private distributed linear bandits, where only a subset of users
from the population are selected (called clients) to participate in the
learning process and the central server learns the global model from such
partial feedback by iteratively aggregating these clients' local feedback in a
differentially private fashion. We then propose a unified algorithmic learning
framework, called differentially private distributed phased elimination
(DP-DPE), which can be naturally integrated with popular differential privacy
(DP) models (including central DP, local DP, and shuffle DP). Furthermore, we
prove that DP-DPE achieves both sublinear regret and sublinear communication
cost. Interestingly, DP-DPE also achieves privacy protection "for free" in the
sense that the additional cost due to privacy guarantees is a lower-order
additive term. In addition, as a by-product of our techniques, the same results
of "free" privacy can also be achieved for the standard differentially private
linear bandits. Finally, we conduct simulations to corroborate our theoretical
results and demonstrate the effectiveness of DP-DPE.
</p></li>
</ul>

<h3>Title: Connected Vehicles: A Privacy Analysis. (arXiv:2207.06182v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06182">http://arxiv.org/abs/2207.06182</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Just as the world of consumer devices was forever changed by the introduction
of computer controlled solutions, the introduction of the engine control unit
(ECU) gave rise to the automobile's transformation from a transportation
product to a technology platform. A modern car is capable of processing,
analysing and transmitting data in ways that could not have been foreseen only
a few years ago. These cars often incorporate telematics systems, which are
used to provide navigation and internet connectivity over cellular networks, as
well as data-recording devices for insurance and product development purposes.
We examine the telematics system of a production vehicle, and aim to ascertain
some of the associated privacy-related threats. We also consider how this
analysis might underpin further research.
</p></li>
</ul>

<h3>Title: Smooth Anonymity for Sparse Binary Matrices. (arXiv:2207.06358v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06358">http://arxiv.org/abs/2207.06358</a></li>
<li>Code URL: null</li>
<li>Summary: <p>When working with user data providing well-defined privacy guarantees is
paramount. In this work we aim to manipulate and share an entire sparse dataset
with a third party privately. In fact, differential privacy has emerged as the
gold standard of privacy, however, when it comes to sharing sparse datasets, as
one of our main results, we prove that \emph{any} differentially private
mechanism that maintains a reasonable similarity with the initial dataset is
doomed to have a very weak privacy guarantee. Hence we need to opt for other
privacy notions such as $k$-anonymity are better at preserving utility in this
context. In this work we present a variation of $k$-anonymity, which we call
smooth $k$-anonymity and design simple algorithms that efficiently provide
smooth $k$-anonymity. We further perform an empirical evaluation to back our
theoretical guarantees, and show that our algorithm improves the performance in
downstream machine learning tasks on anonymized data.
</p></li>
</ul>

<h3>Title: Revealing the Landscape of Privacy-Enhancing Technologies in the Context of Data Markets for the IoT: A Systematic Literature Review. (arXiv:2107.11905v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2107.11905">http://arxiv.org/abs/2107.11905</a></li>
<li>Code URL: null</li>
<li>Summary: <p>IoT data markets in public and private institutions have become increasingly
relevant in recent years because of their potential to improve data
availability and unlock new business models. However, exchanging data in
markets bears considerable challenges related to disclosing sensitive
information. Despite considerable research focused on different aspects of
privacy-enhancing data markets for the IoT, none of the solutions proposed so
far seems to find a practical adoption. Thus, this study aims to organize the
state-of-the-art solutions, analyze and scope the technologies that have been
suggested in this context, and structure the remaining challenges to determine
areas where future research is required. To accomplish this goal, we conducted
a systematic literature review on privacy enhancement in data markets for the
IoT, covering 50 publications dated up to July 2020, and provided updates with
24 publications dated up to May 2022. Our results indicate that most research
in this area has emerged only recently, and no IoT data market architecture has
established itself as canonical. Existing solutions frequently lack the
required combination of anonymization and secure computation technologies.
Furthermore, there is no consensus on the appropriate use of blockchain
technology for IoT data markets and a low degree of leveraging existing
libraries or reusing generic data market architectures. We also identified
significant challenges remaining, such as the copy problem and the recursive
enforcement problem that-while solutions have been suggested to some extent-are
often not sufficiently addressed in proposed designs. We conclude that
privacy-enhancing technologies need further improvements to positively impact
data markets so that, ultimately, the value of data is preserved through data
scarcity and users' privacy and businesses-critical information are protected.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Aggregation in Federated Learning: A Survey. (arXiv:2203.17005v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.17005">http://arxiv.org/abs/2203.17005</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Over the recent years, with the increasing adoption of Federated Learning
(FL) algorithms and growing concerns over personal data privacy,
Privacy-Preserving Federated Learning (PPFL) has attracted tremendous attention
from both academia and industry. Practical PPFL typically allows multiple
participants to individually train their machine learning models, which are
then aggregated to construct a global model in a privacy-preserving manner. As
such, Privacy-Preserving Aggregation (PPAgg) as the key protocol in PPFL has
received substantial research interest. This survey aims to fill the gap
between a large number of studies on PPFL, where PPAgg is adopted to provide a
privacy guarantee, and the lack of a comprehensive survey on the PPAgg
protocols applied in FL systems. In this survey, we review the PPAgg protocols
proposed to address privacy and security issues in FL systems. The focus is
placed on the construction of PPAgg protocols with an extensive analysis of the
advantages and disadvantages of these selected PPAgg protocols and solutions.
Additionally, we discuss the open-source FL frameworks that support PPAgg.
Finally, we highlight important challenges and future research directions for
applying PPAgg to FL systems and the combination of PPAgg with other
technologies for further security improvement.
</p></li>
</ul>

<h3>Title: (Nearly) Optimal Private Linear Regression via Adaptive Clipping. (arXiv:2207.04686v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04686">http://arxiv.org/abs/2207.04686</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We study the problem of differentially private linear regression where each
data point is sampled from a fixed sub-Gaussian style distribution. We propose
and analyze a one-pass mini-batch stochastic gradient descent method
(DP-AMBSSGD) where points in each iteration are sampled without replacement.
Noise is added for DP but the noise standard deviation is estimated online.
Compared to existing $(\epsilon, \delta)$-DP techniques which have sub-optimal
error bounds, DP-AMBSSGD is able to provide nearly optimal error bounds in
terms of key parameters like dimensionality $d$, number of points $N$, and the
standard deviation $\sigma$ of the noise in observations. For example, when the
$d$-dimensional covariates are sampled i.i.d. from the normal distribution,
then the excess error of DP-AMBSSGD due to privacy is $\frac{\sigma^2
d}{N}(1+\frac{d}{\epsilon^2 N})$, i.e., the error is meaningful when number of
samples $N= \Omega(d \log d)$ which is the standard operative regime for linear
regression. In contrast, error bounds for existing efficient methods in this
setting are: $\mathcal{O}\big(\frac{d^3}{\epsilon^2 N^2}\big)$, even for
$\sigma=0$. That is, for constant $\epsilon$, the existing techniques require
$N=\Omega(d\sqrt{d})$ to provide a non-trivial result.
</p></li>
</ul>

<h3>Title: How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models. (arXiv:2102.08921v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2102.08921">http://arxiv.org/abs/2102.08921</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Devising domain- and model-agnostic evaluation metrics for generative models
is an important and as yet unresolved problem. Most existing metrics, which
were tailored solely to the image synthesis setup, exhibit a limited capacity
for diagnosing the different modes of failure of generative models across
broader application domains. In this paper, we introduce a 3-dimensional
evaluation metric, ($\alpha$-Precision, $\beta$-Recall, Authenticity), that
characterizes the fidelity, diversity and generalization performance of any
generative model in a domain-agnostic fashion. Our metric unifies statistical
divergence measures with precision-recall analysis, enabling sample- and
distribution-level diagnoses of model fidelity and diversity. We introduce
generalization as an additional, independent dimension (to the
fidelity-diversity trade-off) that quantifies the extent to which a model
copies training data -- a crucial performance indicator when modeling sensitive
data with requirements on privacy. The three metric components correspond to
(interpretable) probabilistic quantities, and are estimated via sample-level
binary classification. The sample-level nature of our metric inspires a novel
use case which we call model auditing, wherein we judge the quality of
individual samples generated by a (black-box) model, discarding low-quality
samples and hence improving the overall model performance in a post-hoc manner.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Trackers Bounce Back: Measuring Evasion of Partitioned Storage in the Wild. (arXiv:2203.10188v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.10188">http://arxiv.org/abs/2203.10188</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This work presents a systematic study of navigational tracking, the latest
development in the cat-and-mouse game between browsers and online trackers.
Navigational tracking allows trackers to 'aggregate users' activities and
behaviors across sites by modifying their navigation requests. This technique
is particularly important because it circumvents the increasing efforts by
browsers to partition or block third-party storage, which was previously
necessary for most cross-website tracking. While previous work has studied
specific navigational tracking techniques (i.e. "bounce tracking"), our work is
the first effort to systematically study and measure the entire category of
navigational tracking techniques. We describe and measure the frequency of two
different navigational tracking techniques on the Web, and find that
navigational tracking is present on slightly more than ten percent of all
navigations that we made. Our contributions include identifying 214 domains
belonging to at least 104 organizations tracking users across sites through
link decoration techniques using direct or indirect navigation flows. We
identify a further 23 domains belonging to at least 16 organizations tracking
users through bounce tracking (i.e. bouncing users through unrelated third
parties to generate user profiles). We also improve on prior techniques for
differenting user identifiers from non-sensitive information, which is
necessary to detect one class of navigational tracking. We discuss how our
findings can used to protect users from navigational tracking, and commit to
releasing both our complete dataset and our measurement pipeline
</p></li>
</ul>

<h3>Title: Confidentiality Protection in the 2020 US Census of Population and Housing. (arXiv:2206.03524v2 [stat.AP] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.03524">http://arxiv.org/abs/2206.03524</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In an era where external data and computational capabilities far exceed
statistical agencies' own resources and capabilities, they face the renewed
challenge of protecting the confidentiality of underlying microdata when
publishing statistics in very granular form and ensuring that these granular
data are used for statistical purposes only. Conventional statistical
disclosure limitation methods are too fragile to address this new challenge.
This article discusses the deployment of a differential privacy framework for
the 2020 US Census that was customized to protect confidentiality, particularly
the most detailed geographic and demographic categories, and deliver controlled
accuracy across the full geographic hierarchy.
</p></li>
</ul>

<h3>Title: A Conceptual Framework for Using Machine Learning to Support Child Welfare Decisions. (arXiv:2207.05855v1 [cs.CY])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05855">http://arxiv.org/abs/2207.05855</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Human services systems make key decisions that impact individuals in the
society. The U.S. child welfare system makes such decisions, from screening-in
hotline reports of suspected abuse or neglect for child protective
investigations, placing children in foster care, to returning children to
permanent home settings. These complex and impactful decisions on children's
lives rely on the judgment of child welfare decisionmakers. Child welfare
agencies have been exploring ways to support these decisions with empirical,
data-informed methods that include machine learning (ML). This paper describes
a conceptual framework for ML to support child welfare decisions. The ML
framework guides how child welfare agencies might conceptualize a target
problem that ML can solve; vet available administrative data for building ML;
formulate and develop ML specifications that mirror relevant populations and
interventions the agencies are undertaking; deploy, evaluate, and monitor ML as
child welfare context, policy, and practice change over time. Ethical
considerations, stakeholder engagement, and avoidance of common pitfalls
underpin the framework's impact and success. From abstract to concrete, we
describe one application of this framework to support a child welfare decision.
This ML framework, though child welfare-focused, is generalizable to solving
other public policy problems.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Perturbation Inactivation Based Adversarial Defense for Face Recognition. (arXiv:2207.06035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06035">http://arxiv.org/abs/2207.06035</a></li>
<li>Code URL: <a href="https://github.com/renmin1991/perturbation-inactivate">https://github.com/renmin1991/perturbation-inactivate</a></li>
<li>Summary: <p>Deep learning-based face recognition models are vulnerable to adversarial
attacks. To curb these attacks, most defense methods aim to improve the
robustness of recognition models against adversarial perturbations. However,
the generalization capacities of these methods are quite limited. In practice,
they are still vulnerable to unseen adversarial attacks. Deep learning models
are fairly robust to general perturbations, such as Gaussian noises. A
straightforward approach is to inactivate the adversarial perturbations so that
they can be easily handled as general perturbations. In this paper, a
plug-and-play adversarial defense method, named perturbation inactivation
(PIN), is proposed to inactivate adversarial perturbations for adversarial
defense. We discover that the perturbations in different subspaces have
different influences on the recognition model. There should be a subspace,
called the immune space, in which the perturbations have fewer adverse impacts
on the recognition model than in other subspaces. Hence, our method estimates
the immune space and inactivates the adversarial perturbations by restricting
them to this subspace. The proposed method can be generalized to unseen
adversarial perturbations since it does not rely on a specific kind of
adversarial attack method. This approach not only outperforms several
state-of-the-art adversarial defense methods but also demonstrates a superior
generalization capacity through exhaustive experiments. Moreover, the proposed
method can be successfully applied to four commercial APIs without additional
training, indicating that it can be easily generalized to existing face
recognition systems. The source code is available at
https://github.com/RenMin1991/Perturbation-Inactivate
</p></li>
</ul>

<h3>Title: Evaluating the Adversarial Robustness of Adaptive Test-time Defenses. (arXiv:2202.13711v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.13711">http://arxiv.org/abs/2202.13711</a></li>
<li>Code URL: <a href="https://github.com/fra31/evaluating-adaptive-test-time-defenses">https://github.com/fra31/evaluating-adaptive-test-time-defenses</a></li>
<li>Summary: <p>Adaptive defenses, which optimize at test time, promise to improve
adversarial robustness. We categorize such adaptive test-time defenses, explain
their potential benefits and drawbacks, and evaluate a representative variety
of the latest adaptive defenses for image classification. Unfortunately, none
significantly improve upon static defenses when subjected to our careful case
study evaluation. Some even weaken the underlying static model while
simultaneously increasing inference computation. While these results are
disappointing, we still believe that adaptive test-time defenses are a
promising avenue of research and, as such, we provide recommendations for their
thorough evaluation. We extend the checklist of Carlini et al. (2019) by
providing concrete steps specific to adaptive defenses.
</p></li>
</ul>

<h3>Title: Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.06761">http://arxiv.org/abs/2206.06761</a></li>
<li>Code URL: <a href="https://github.com/thobauma/aadefdino">https://github.com/thobauma/aadefdino</a></li>
<li>Summary: <p>This work conducts the first analysis on the robustness against adversarial
attacks on self-supervised Vision Transformers trained using DINO. First, we
evaluate whether features learned through self-supervision are more robust to
adversarial attacks than those emerging from supervised learning. Then, we
present properties arising for attacks in the latent space. Finally, we
evaluate whether three well-known defense strategies can increase adversarial
robustness in downstream tasks by only fine-tuning the classification head to
provide robustness even in view of limited compute resources. These defense
strategies are: Adversarial Training, Ensemble Adversarial Training and
Ensemble of Specialized Networks.
</p></li>
</ul>

<h3>Title: Towards Overcoming the Undercutting Problem. (arXiv:2007.11480v4 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2007.11480">http://arxiv.org/abs/2007.11480</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Mining processes of Bitcoin and similar cryptocurrencies are currently
incentivized with voluntary transaction fees and fixed block rewards which will
halve gradually to zero. In the setting where optional and arbitrary
transaction fee becomes the remaining incentive, Carlsten et al.\ [CCS~2016]
find that an undercutting attack can become the equilibrium strategy for
miners. In undercutting, the attacker deliberately forks an existing chain by
leaving wealthy transactions unclaimed to attract petty complaint miners to its
fork. We observe that two simplifying assumptions in [CCS~2016] of fees
arriving at fixed rates and miners collecting {\em all} accumulated fees
regardless of block size limit are often infeasible in practice and find that
they are inaccurately inflating the profitability of undercutting. Studying
Bitcoin and Monero blockchain data, we find that the fees deliberately left out
by an undercutter may not be attractive to other miners (hence to the attacker
itself): the deliberately left out transactions may not fit into a new block
without "squeezing out" some other to-be transactions, and thus claimable fees
in the next round cannot be raised arbitrarily.
</p></li>
</ul>

<p>This work views undercutting and shifting among chains rationally as mining
strategies of rational miners. We model profitability of undercutting strategy
with block size limit present, which bounds the claimable fees in a round and
gives rise to a pending (cushion) transaction set. In the proposed model, we
first identify the conditions necessary to make undercutting profitable. We
then present an easy-to-deploy defense against undercutting by selectively
assembling transactions into the new block to invalidate the identified
conditions. Under a typical setting with undercutters present, applying this
avoidance technique is a Nash Equilibrium. Finally, we complement the above
analytical results with experiments.
</p>

<h3>Title: Markov Decision Process For Automatic Cyber Defense. (arXiv:2207.05436v2 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05436">http://arxiv.org/abs/2207.05436</a></li>
<li>Code URL: null</li>
<li>Summary: <p>It is challenging for a security analyst to detect or defend against
cyber-attacks. Moreover, traditional defense deployment methods require the
security analyst to manually enforce the defenses in the presence of
uncertainties about the defense to deploy. As a result, it is essential to
develop an automated and resilient defense deployment mechanism to thwart the
new generation of attacks. In this paper, we propose a framework based on
Markov Decision Process (MDP) and Q-learning to automatically generate optimal
defense solutions for networked system states. The framework consists of four
phases namely; the model initialization phase, model generation phase,
Q-learning phase, and the conclusion phase. The proposed model collects real
network information as inputs and then builds them into structural data. We
implement a Q-learning process in the model to learn the quality of a defense
action in a particular state. To investigate the feasibility of the proposed
model, we perform simulation experiments and the result reveals that the model
can reduce the risk of network systems from cyber attacks. Furthermore, the
experiment shows that the model has shown a certain level of flexibility when
different parameters are used for Q-learning.
</p></li>
</ul>

<h3>Title: Interactive Machine Learning: A State of the Art Review. (arXiv:2207.06196v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06196">http://arxiv.org/abs/2207.06196</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Machine learning has proved useful in many software disciplines, including
computer vision, speech and audio processing, natural language processing,
robotics and some other fields. However, its applicability has been
significantly hampered due its black-box nature and significant resource
consumption. Performance is achieved at the expense of enormous computational
resource and usually compromising the robustness and trustworthiness of the
model. Recent researches have been identifying a lack of interactivity as the
prime source of these machine learning problems. Consequently, interactive
machine learning (iML) has acquired increased attention of researchers on
account of its human-in-the-loop modality and relatively efficient resource
utilization. Thereby, a state-of-the-art review of interactive machine learning
plays a vital role in easing the effort toward building human-centred models.
In this paper, we provide a comprehensive analysis of the state-of-the-art of
iML. We analyze salient research works using merit-oriented and
application/task oriented mixed taxonomy. We use a bottom-up clustering
approach to generate a taxonomy of iML research works. Research works on
adversarial black-box attacks and corresponding iML based defense system,
exploratory machine learning, resource constrained learning, and iML
performance evaluation are analyzed under their corresponding theme in our
merit-oriented taxonomy. We have further classified these research works into
technical and sectoral categories. Finally, research opportunities that we
believe are inspiring for future work in iML are discussed thoroughly.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving. (arXiv:2203.00858v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.00858">http://arxiv.org/abs/2203.00858</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the
sensor data and perceive the environment. However, DNN models are vulnerable to
different types of adversarial attacks, which pose significant risks to the
security and safety of the vehicles and passengers. One prominent threat is the
backdoor attack, where the adversary can compromise the DNN model by poisoning
the training samples. Although lots of effort has been devoted to the
investigation of the backdoor attack to conventional computer vision tasks, its
practicality and applicability to the autonomous driving scenario is rarely
explored, especially in the physical world.
</p></li>
</ul>

<p>In this paper, we target the lane detection system, which is an indispensable
module for many autonomous driving tasks, e.g., navigation, lane switching. We
design and realize the first physical backdoor attacks to such system. Our
attacks are comprehensively effective against different types of lane detection
algorithms. Specifically, we introduce two attack methodologies
(poison-annotation and clean-annotation) to generate poisoned samples. With
those samples, the trained lane detection model will be infected with the
backdoor, and can be activated by common objects (e.g., traffic cones) to make
wrong detections, leading the vehicle to drive off the road or onto the
opposite lane. Extensive evaluations on public datasets and physical autonomous
vehicles demonstrate that our backdoor attacks are effective, stealthy and
robust against various defense solutions. Our codes and experimental videos can
be found in https://sites.google.com/view/lane-detection-attack/lda.
</p>

<h3>Title: BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.00278">http://arxiv.org/abs/2207.00278</a></li>
<li>Code URL: <a href="https://github.com/cgcl-codes/badhash">https://github.com/cgcl-codes/badhash</a></li>
<li>Summary: <p>Due to its powerful feature learning capability and high efficiency, deep
hashing has achieved great success in large-scale image retrieval. Meanwhile,
extensive works have demonstrated that deep neural networks (DNNs) are
susceptible to adversarial examples, and exploring adversarial attack against
deep hashing has attracted many research efforts. Nevertheless, backdoor
attack, another famous threat to DNNs, has not been studied for deep hashing
yet. Although various backdoor attacks have been proposed in the field of image
classification, existing approaches failed to realize a truly imperceptive
backdoor attack that enjoys invisible triggers and clean label setting
simultaneously, and they also cannot meet the intrinsic demand of image
retrieval backdoor. In this paper, we propose BadHash, the first
generative-based imperceptible backdoor attack against deep hashing, which can
effectively generate invisible and input-specific poisoned images with clean
label. Specifically, we first propose a new conditional generative adversarial
network (cGAN) pipeline to effectively generate poisoned samples. For any given
benign image, it seeks to generate a natural-looking poisoned counterpart with
a unique invisible trigger. In order to improve the attack effectiveness, we
introduce a label-based contrastive learning network LabCLN to exploit the
semantic characteristics of different labels, which are subsequently used for
confusing and misleading the target model to learn the embedded trigger. We
finally explore the mechanism of backdoor attacks on image retrieval in the
hash space. Extensive experiments on multiple benchmark datasets verify that
BadHash can generate imperceptible poisoned samples with strong attack ability
and transferability over state-of-the-art deep hashing schemes.
</p></li>
</ul>

<h3>Title: PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch. (arXiv:2207.01795v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.01795">http://arxiv.org/abs/2207.01795</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and "zeros out" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.
</p></li>
</ul>

<h3>Title: Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05225">http://arxiv.org/abs/2207.05225</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider the task incremental learning (Task-IL) scenario and explore three
regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations highlight the potential
limitations of existing Task-IL approaches. Our empirical study recommends that
the research community consider the robustness of the proposed continual
learning approaches and invest extensive efforts in mitigating catastrophic
forgetting.
</p></li>
</ul>

<h3>Title: RelaxLoss: Defending Membership Inference Attacks without Losing Utility. (arXiv:2207.05801v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05801">http://arxiv.org/abs/2207.05801</a></li>
<li>Code URL: <a href="https://github.com/DingfanChen/RelaxLoss">https://github.com/DingfanChen/RelaxLoss</a></li>
<li>Summary: <p>As a long-term threat to the privacy of training data, membership inference
attacks (MIAs) emerge ubiquitously in machine learning models. Existing works
evidence strong connection between the distinguishability of the training and
testing loss distributions and the model's vulnerability to MIAs. Motivated by
existing results, we propose a novel training framework based on a relaxed loss
with a more achievable learning target, which leads to narrowed generalization
gap and reduced privacy leakage. RelaxLoss is applicable to any classification
model with added benefits of easy implementation and negligible overhead.
Through extensive evaluations on five datasets with diverse modalities (images,
medical data, transaction records), our approach consistently outperforms
state-of-the-art defense mechanisms in terms of resilience against MIAs as well
as model utility. Our defense is the first that can withstand a wide range of
attacks while preserving (or even improving) the target model's utility. Source
code is available at https://github.com/DingfanChen/RelaxLoss
</p></li>
</ul>

<h3>Title: Game of Trojans: A Submodular Byzantine Approach. (arXiv:2207.05937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05937">http://arxiv.org/abs/2207.05937</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Machine learning models in the wild have been shown to be vulnerable to
Trojan attacks during training. Although many detection mechanisms have been
proposed, strong adaptive attackers have been shown to be effective against
them. In this paper, we aim to answer the questions considering an intelligent
and adaptive adversary: (i) What is the minimal amount of instances required to
be Trojaned by a strong attacker? and (ii) Is it possible for such an attacker
to bypass strong detection mechanisms?
</p></li>
</ul>

<p>We provide an analytical characterization of adversarial capability and
strategic interactions between the adversary and detection mechanism that take
place in such models. We characterize adversary capability in terms of the
fraction of the input dataset that can be embedded with a Trojan trigger. We
show that the loss function has a submodular structure, which leads to the
design of computationally efficient algorithms to determine this fraction with
provable bounds on optimality. We propose a Submodular Trojan algorithm to
determine the minimal fraction of samples to inject a Trojan trigger. To evade
detection of the Trojaned model, we model strategic interactions between the
adversary and Trojan detection mechanism as a two-player game. We show that the
adversary wins the game with probability one, thus bypassing detection. We
establish this by proving that output probability distributions of a Trojan
model and a clean model are identical when following the Min-Max (MM) Trojan
algorithm.
</p>
<p>We perform extensive evaluations of our algorithms on MNIST, CIFAR-10, and
EuroSAT datasets. The results show that (i) with Submodular Trojan algorithm,
the adversary needs to embed a Trojan trigger into a very small fraction of
samples to achieve high accuracy on both Trojan and clean samples, and (ii) the
MM Trojan algorithm yields a trained Trojan model that evades detection with
probability 1.
</p>

<h3>Title: On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06154">http://arxiv.org/abs/2207.06154</a></li>
<li>Code URL: <a href="https://github.com/ginevracoal/robustBNNs">https://github.com/ginevracoal/robustBNNs</a></li>
<li>Summary: <p>Vulnerability to adversarial attacks is one of the principal hurdles to the
adoption of deep learning in safety-critical applications. Despite significant
efforts, both practical and theoretical, training deep learning models robust
to adversarial attacks is still an open problem. In this paper, we analyse the
geometry of adversarial attacks in the large-data, overparameterized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to
gradient-based attacks arises as a result of degeneracy in the data
distribution, i.e., when the data lies on a lower-dimensional submanifold of
the ambient space. As a direct consequence, we demonstrate that in this limit
BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we
prove that the expected gradient of the loss with respect to the BNN posterior
distribution is vanishing, even when each neural network sampled from the
posterior is vulnerable to gradient-based attacks. Experimental results on the
MNIST, Fashion MNIST, and half moons datasets, representing the finite data
regime, with BNNs trained with Hamiltonian Monte Carlo and Variational
Inference, support this line of arguments, showing that BNNs can display both
high accuracy on clean data and robustness to both gradient-based and
gradient-free based adversarial attacks.
</p></li>
</ul>

<h3>Title: A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Predictions. (arXiv:2205.01094v3 [cs.CR] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.01094">http://arxiv.org/abs/2205.01094</a></li>
<li>Code URL: <a href="https://github.com/yonxie/advfintweet">https://github.com/yonxie/advfintweet</a></li>
<li>Summary: <p>More and more investors and machine learning models rely on social media
(e.g., Twitter and Reddit) to gather real-time information and sentiment to
predict stock price movements. Although text-based models are known to be
vulnerable to adversarial attacks, whether stock prediction models have similar
vulnerability is underexplored. In this paper, we experiment with a variety of
adversarial attack configurations to fool three stock prediction victim models.
We address the task of adversarial generation by solving combinatorial
optimization problems with semantics and budget constraints. Our results show
that the proposed attack method can achieve consistent success rates and cause
significant monetary loss in trading simulation by simply concatenating a
perturbed but semantically similar tweet.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust and efficient computation of retinal fractal dimension through deep approximation. (arXiv:2207.05757v1 [q-bio.QM])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05757">http://arxiv.org/abs/2207.05757</a></li>
<li>Code URL: null</li>
<li>Summary: <p>A retinal trait, or phenotype, summarises a specific aspect of a retinal
image in a single number. This can then be used for further analyses, e.g. with
statistical methods. However, reducing an aspect of a complex image to a
single, meaningful number is challenging. Thus, methods for calculating retinal
traits tend to be complex, multi-step pipelines that can only be applied to
high quality images. This means that researchers often have to discard
substantial portions of the available data. We hypothesise that such pipelines
can be approximated with a single, simpler step that can be made robust to
common quality issues. We propose Deep Approximation of Retinal Traits (DART)
where a deep neural network is used predict the output of an existing pipeline
on high quality images from synthetically degraded versions of these images. We
demonstrate DART on retinal Fractal Dimension (FD) calculated by VAMPIRE, using
retinal images from UK Biobank that previous work identified as high quality.
Our method shows very high agreement with FD VAMPIRE on unseen test images
(Pearson r=0.9572). Even when those images are severely degraded, DART can
still recover an FD estimate that shows good agreement with FD VAMPIRE obtained
from the original images (Pearson r=0.8817). This suggests that our method
could enable researchers to discard fewer images in the future. Our method can
compute FD for over 1,000img/s using a single GPU. We consider these to be very
encouraging initial results and hope to develop this approach into a useful
tool for retinal analysis.
</p></li>
</ul>

<h3>Title: A Near Sensor Edge Computing System for Point Cloud Semantic Segmentation. (arXiv:2207.05888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05888">http://arxiv.org/abs/2207.05888</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Point cloud semantic segmentation has attracted attentions due to its
robustness to light condition. This makes it an ideal semantic solution for
autonomous driving. However, considering the large computation burden and
bandwidth demanding of neural networks, putting all the computing into vehicle
Electronic Control Unit (ECU) is not efficient or practical. In this paper, we
proposed a light weighted point cloud semantic segmentation network based on
range view. Due to its simple pre-processing and standard convolution, it is
efficient when running on deep learning accelerator like DPU. Furthermore, a
near sensor computing system is built for autonomous vehicles. In this system,
a FPGA-based deep learning accelerator core (DPU) is placed next to the LiDAR
sensor, to perform point cloud pre-processing and segmentation neural network.
By leaving only the post-processing step to ECU, this solution heavily
alleviate the computation burden of ECU and consequently shortens the decision
making and vehicles reaction latency. Our semantic segmentation network
achieved 10 frame per second (fps) on Xilinx DPU with computation efficiency
42.5 GOP/W.
</p></li>
</ul>

<h3>Title: Verifying Attention Robustness of Deep Neural Networks against Semantic Perturbations. (arXiv:2207.05902v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05902">http://arxiv.org/abs/2207.05902</a></li>
<li>Code URL: <a href="https://zenodo.org/record/6606257">https://zenodo.org/record/6606257</a></li>
<li>Summary: <p>It is known that deep neural networks (DNNs) classify an input image by
paying particular attention to certain specific pixels; a graphical
representation of the magnitude of attention to each pixel is called a
saliency-map. Saliency-maps are used to check the validity of the
classification decision basis, e.g., it is not a valid basis for classification
if a DNN pays more attention to the background rather than the subject of an
image. Semantic perturbations can significantly change the saliency-map. In
this work, we propose the first verification method for attention robustness,
i.e., the local robustness of the changes in the saliency-map against
combinations of semantic perturbations. Specifically, our method determines the
range of the perturbation parameters (e.g., the brightness change) that
maintains the difference between the actual saliency-map change and the
expected saliency-map change below a given threshold value. Our method is based
on activation region traversals, focusing on the outermost robust boundary for
scalability on larger DNNs. Experimental results demonstrate that our method
can show the extent to which DNNs can classify with the same basis regardless
of semantic perturbations and report on performance and performance factors of
activation region traversals.
</p></li>
</ul>

<h3>Title: Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition. (arXiv:2207.06020v1 [cs.SD])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06020">http://arxiv.org/abs/2207.06020</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3.
</p></li>
</ul>

<h3>Title: Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06058">http://arxiv.org/abs/2207.06058</a></li>
<li>Code URL: <a href="https://github.com/peterfws/structure-plp-slam">https://github.com/peterfws/structure-plp-slam</a></li>
<li>Summary: <p>This paper demonstrates a visual SLAM system that utilizes point and line
cloud for robust camera localization, simultaneously, with an embedded
piece-wise planar reconstruction (PPR) module which in all provides a
structural map. To build a scale consistent map in parallel with tracking, such
as employing a single camera brings the challenge of reconstructing geometric
primitives with scale ambiguity, and further introduces the difficulty in graph
optimization of bundle adjustment (BA). We address these problems by proposing
several run-time optimizations on the reconstructed lines and planes. The
system is then extended with depth and stereo sensors based on the design of
the monocular framework. The results show that our proposed SLAM tightly
incorporates the semantic features to boost both frontend tracking as well as
backend optimization. We evaluate our system exhaustively on various datasets,
and open-source our code for the community
(https://github.com/PeterFWS/Structure-PLP-SLAM).
</p></li>
</ul>

<h3>Title: Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06067">http://arxiv.org/abs/2207.06067</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Traffic sign detection is a vital task in the visual system of self-driving
cars and the automated driving system. Recently, novel Transformer-based models
have achieved encouraging results for various computer vision tasks. We still
observed that vanilla ViT could not yield satisfactory results in traffic sign
detection because the overall size of the datasets is very small and the class
distribution of traffic signs is extremely unbalanced. To overcome this
problem, a novel Pyramid Transformer with locality mechanisms is proposed in
this paper. Specifically, Pyramid Transformer has several spatial pyramid
reduction layers to shrink and embed the input image into tokens with rich
multi-scale context by using atrous convolutions. Moreover, it inherits an
intrinsic scale invariance inductive bias and is able to learn local feature
representation for objects at various scales, thereby enhancing the network
robustness against the size discrepancy of traffic signs. The experiments are
conducted on the German Traffic Sign Detection Benchmark (GTSDB). The results
demonstrate the superiority of the proposed model in the traffic sign detection
tasks. More specifically, Pyramid Transformer achieves 75.6% mAP in GTSDB when
applied to the Cascade RCNN as the backbone and surpassing most well-known and
widely used SOTAs.
</p></li>
</ul>

<h3>Title: Robust and accurate depth estimation by fusing LiDAR and Stereo. (arXiv:2207.06139v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06139">http://arxiv.org/abs/2207.06139</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Depth estimation is one of the key technologies in some fields such as
autonomous driving and robot navigation. However, the traditional method of
using a single sensor is inevitably limited by the performance of the sensor.
Therefore, a precision and robust method for fusing the LiDAR and stereo
cameras is proposed. This method fully combines the advantages of the LiDAR and
stereo camera, which can retain the advantages of the high precision of the
LiDAR and the high resolution of images respectively. Compared with the
traditional stereo matching method, the texture of the object and lighting
conditions have less influence on the algorithm. Firstly, the depth of the
LiDAR data is converted to the disparity of the stereo camera. Because the
density of the LiDAR data is relatively sparse on the y-axis, the converted
disparity map is up-sampled using the interpolation method. Secondly, in order
to make full use of the precise disparity map, the disparity map and stereo
matching are fused to propagate the accurate disparity. Finally, the disparity
map is converted to the depth map. Moreover, the converted disparity map can
also increase the speed of the algorithm. We evaluate the proposed pipeline on
the KITTI benchmark. The experiment demonstrates that our algorithm has higher
accuracy than several classic methods.
</p></li>
</ul>

<h3>Title: Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06202">http://arxiv.org/abs/2207.06202</a></li>
<li>Code URL: <a href="https://github.com/7eu7d7/robustdet">https://github.com/7eu7d7/robustdet</a></li>
<li>Summary: <p>Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.
</p></li>
</ul>

<h3>Title: Entry-Flipped Transformer for Inference and Prediction of Participant Behavior. (arXiv:2207.06235v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06235">http://arxiv.org/abs/2207.06235</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Some group activities, such as team sports and choreographed dances, involve
closely coupled interaction between participants. Here we investigate the tasks
of inferring and predicting participant behavior, in terms of motion paths and
actions, under such conditions. We narrow the problem to that of estimating how
a set target participants react to the behavior of other observed participants.
Our key idea is to model the spatio-temporal relations among participants in a
manner that is robust to error accumulation during frame-wise inference and
prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),
which models the relations of participants by attention mechanisms on both
spatial and temporal domains. Unlike typical transformers, we tackle the
problem of error accumulation by flipping the order of query, key, and value
entries, to increase the importance and fidelity of observed features in the
current frame. Comparative experiments show that our EF-Transformer achieves
the best performance on a newly-collected tennis doubles dataset, a Ceilidh
dance dataset, and two pedestrian datasets. Furthermore, it is also
demonstrated that our EF-Transformer is better at limiting accumulated errors
and recovering from wrong estimations.
</p></li>
</ul>

<h3>Title: Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach. (arXiv:2207.06267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06267">http://arxiv.org/abs/2207.06267</a></li>
<li>Code URL: <a href="https://github.com/neurai-lab/tarc">https://github.com/neurai-lab/tarc</a></li>
<li>Summary: <p>Continual learning (CL) over non-stationary data streams remains one of the
long-standing challenges in deep neural networks (DNNs) as they are prone to
catastrophic forgetting. CL models can benefit from self-supervised
pre-training as it enables learning more generalizable task-agnostic features.
However, the effect of self-supervised pre-training diminishes as the length of
task sequences increases. Furthermore, the domain shift between pre-training
data distribution and the task distribution reduces the generalizability of the
learned representations. To address these limitations, we propose Task Agnostic
Representation Consolidation (TARC), a two-stage training paradigm for CL that
intertwines task-agnostic and task-specific learning whereby self-supervised
training is followed by supervised learning for each task. To further restrict
the deviation from the learned representations in the self-supervised stage, we
employ a task-agnostic auxiliary loss during the supervised stage. We show that
our training paradigm can be easily added to memory- or regularization-based
approaches and provides consistent performance gain across more challenging CL
settings. We further show that it leads to more robust and well-calibrated
models.
</p></li>
</ul>

<h3>Title: 6D Camera Relocalization in Visually Ambiguous Extreme Environments. (arXiv:2207.06333v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06333">http://arxiv.org/abs/2207.06333</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We propose a novel method to reliably estimate the pose of a camera given a
sequence of images acquired in extreme environments such as deep seas or
extraterrestrial terrains. Data acquired under these challenging conditions are
corrupted by textureless surfaces, image degradation, and presence of
repetitive and highly ambiguous structures. When naively deployed, the
state-of-the-art methods can fail in those scenarios as confirmed by our
empirical analysis. In this paper, we attempt to make camera relocalization
work in these extreme situations. To this end, we propose: (i) a hierarchical
localization system, where we leverage temporal information and (ii) a novel
environment-aware image enhancement method to boost the robustness and
accuracy. Our extensive experimental results demonstrate superior performance
in favor of our method under two extreme settings: localizing an autonomous
underwater vehicle and localizing a planetary rover in a Mars-like desert. In
addition, our method achieves comparable performance with state-of-the-art
methods on the indoor benchmark (7-Scenes dataset) using only 20% training
data.
</p></li>
</ul>

<h3>Title: Joint Prediction of Monocular Depth and Structure using Planar and Parallax Geometry. (arXiv:2207.06351v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06351">http://arxiv.org/abs/2207.06351</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Supervised learning depth estimation methods can achieve good performance
when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can
only generate sparse 3D maps which causes losing information. Obtaining
high-quality ground-truth depth data per pixel is difficult to acquire. In
order to overcome this limitation, we propose a novel approach combining
structure information from a promising Plane and Parallax geometry pipeline
with depth information into a U-Net supervised learning network, which results
in quantitative and qualitative improvement compared to existing popular
learning-based methods. In particular, the model is evaluated on two
large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes
dataset and achieve the best performance in terms of relative error. Compared
with pure depth supervision models, our model has impressive performance on
depth prediction of thin objects and edges, and compared to structure
prediction baseline, our model performs more robustly.
</p></li>
</ul>

<h3>Title: Learning Interpretable Microscopic Features of Tumor by Multi-task Adversarial CNNs Improves Generalization. (arXiv:2008.01478v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2008.01478">http://arxiv.org/abs/2008.01478</a></li>
<li>Code URL: <a href="https://github.com/maragraziani/multitask_adversarial">https://github.com/maragraziani/multitask_adversarial</a></li>
<li>Summary: <p>Adopting Convolutional Neural Networks (CNNs) in the daily routine of primary
diagnosis requires not only near-perfect precision, but also a sufficient
degree of generalization to data acquisition shifts and transparency. Existing
CNN models act as black boxes, not ensuring to the physicians that important
diagnostic features are used by the model. Building on top of successfully
existing techniques such as multi-task learning, domain adversarial training
and concept-based interpretability, this paper addresses the challenge of
introducing diagnostic factors in the training objectives. Here we show that
our architecture, by learning end-to-end an uncertainty-based weighting
combination of multi-task and adversarial losses, is encouraged to focus on
pathology features such as density and pleomorphism of nuclei, e.g. variations
in size and appearance, while discarding misleading features such as staining
differences. Our results on breast lymph node tissue show significantly
improved generalization in the detection of tumorous tissue, with best average
AUC 0.89 (0.01) against the baseline AUC 0.86 (0.005). By applying the
interpretability technique of linearly probing intermediate representations, we
also demonstrate that interpretable pathology features such as nuclei density
are learned by the proposed CNN architecture, confirming the increased
transparency of this model. This result is a starting point towards building
interpretable multi-task architectures that are robust to data heterogeneity.
Our code is available at https://bit.ly/356yQ2u.
</p></li>
</ul>

<h3>Title: Multi-Label Noise Robust Collaborative Learning Method for Remote Sensing Image Classification. (arXiv:2012.10715v5 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2012.10715">http://arxiv.org/abs/2012.10715</a></li>
<li>Code URL: <a href="https://git.tu-berlin.de/rsim/RCML">https://git.tu-berlin.de/rsim/RCML</a></li>
<li>Summary: <p>The development of accurate methods for multi-label classification (MLC) of
remote sensing (RS) images is one of the most important research topics in RS.
The MLC methods based on Convolutional Neural Networks (CNNs) have shown strong
performance gains in RS. However, they usually require a high number of
reliable training images annotated with multiple land-cover class labels.
Collecting such data is time-consuming and costly. To address this problem, the
publicly available thematic products, which can include noisy labels, can be
used to annotate RS images with zero-labeling cost. However, multi-label noise
(which can be associated with wrong and missing label annotations) can distort
the learning process of the MLC methods. To address this problem, we propose a
novel multi-label noise robust collaborative learning (RCML) method to
alleviate the negative effects of multi-label noise during the training phase
of a CNN model. RCML identifies, ranks and excludes noisy multi-labels in RS
images based on three main modules: 1) the discrepancy module; 2) the group
lasso module; and 3) the swap module. The discrepancy module ensures that the
two networks learn diverse features, while producing the same predictions. The
task of the group lasso module is to detect the potentially noisy labels
assigned to the multi-labeled training images, while the swap module is devoted
to exchange the ranking information between two networks. Unlike existing
methods that make assumptions about the noise distribution, our proposed RCML
does not make any prior assumption about the type of noise in the training set.
The experiments conducted on two multi-label RS image archives confirm the
robustness of the proposed RCML under extreme multi-label noise rates. Our code
is publicly available at: <a href="http://www.noisy-labels-in-rs.org">this http URL</a>
</p></li>
</ul>

<h3>Title: Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting. (arXiv:2104.02527v4 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2104.02527">http://arxiv.org/abs/2104.02527</a></li>
<li>Code URL: <a href="https://github.com/aaronwool/rcvpose">https://github.com/aaronwool/rcvpose</a></li>
<li>Summary: <p>We propose a novel keypoint voting scheme based on intersecting spheres, that
is more accurate than existing schemes and allows for fewer, more disperse
keypoints. The scheme is based upon the distance between points, which as a 1D
quantity can be regressed more accurately than the 2D and 3D vector and offset
quantities regressed in previous work, yielding more accurate keypoint
localization. The scheme forms the basis of the proposed RCVPose method for 6
DoF pose estimation of 3D objects in RGB-D data, which is particularly
effective at handling occlusions. A CNN is trained to estimate the distance
between the 3D point corresponding to the depth mode of each RGB pixel, and a
set of 3 disperse keypoints defined in the object frame. At inference, a sphere
centered at each 3D point is generated, of radius equal to this estimated
distance. The surfaces of these spheres vote to increment a 3D accumulator
space, the peaks of which indicate keypoint locations. The proposed radial
voting scheme is more accurate than previous vector or offset schemes, and is
robust to disperse keypoints. Experiments demonstrate RCVPose to be highly
accurate and competitive, achieving state-of-the-art results on the LINEMOD
99.7% and YCB-Video 97.2% datasets, notably scoring +4.9% higher 71.1% than
previous methods on the challenging Occlusion LINEMOD dataset, and on average
outperforming all other published results from the BOP benchmark for these 3
datasets. Our code is available at <a href="http://www.github.com/aaronwool/rcvpose.">this http URL</a>
</p></li>
</ul>

<h3>Title: Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2107.12636">http://arxiv.org/abs/2107.12636</a></li>
<li>Code URL: <a href="https://github.com/encounter1997/SFA">https://github.com/encounter1997/SFA</a></li>
<li>Summary: <p>Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.
</p></li>
</ul>

<h3>Title: AVA-AVD: Audio-Visual Speaker Diarization in the Wild. (arXiv:2111.14448v4 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2111.14448">http://arxiv.org/abs/2111.14448</a></li>
<li>Code URL: <a href="https://github.com/showlab/ava-avd">https://github.com/showlab/ava-avd</a></li>
<li>Summary: <p>Audio-visual speaker diarization aims at detecting "who spoke when" using
both auditory and visual signals. Existing audio-visual diarization datasets
are mainly focused on indoor environments like meeting rooms or news studios,
which are quite different from in-the-wild videos in many scenarios such as
movies, documentaries, and audience sitcoms. To develop diarization methods for
these challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)
dataset. Our experiments demonstrate that adding AVA-AVD into training set can
produce significantly better diarization models for in-the-wild videos despite
that the data is relatively small. Moreover, this benchmark is challenging due
to the diverse scenes, complicated acoustic conditions, and completely
off-screen speakers. As a first step towards addressing the challenges, we
design the Audio-Visual Relation Network (AVR-Net) which introduces a simple
yet effective modality mask to capture discriminative information based on face
visibility. Experiments show that our method not only can outperform
state-of-the-art methods but is more robust as varying the ratio of off-screen
speakers. Our data and code has been made publicly available at
https://github.com/showlab/AVA-AVD.
</p></li>
</ul>

<h3>Title: Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound. (arXiv:2202.06599v2 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2202.06599">http://arxiv.org/abs/2202.06599</a></li>
<li>Code URL: <a href="https://github.com/wapbastiaansen/multi-atlas-seg-reg">https://github.com/wapbastiaansen/multi-atlas-seg-reg</a></li>
<li>Summary: <p>Segmentation and spatial alignment of ultrasound (US) imaging data acquired
in the in first trimester are crucial for monitoring human embryonic growth and
development throughout this crucial period of life. Current approaches are
either manual or semi-automatic and are therefore very time-consuming and prone
to errors. To automate these tasks, we propose a multi-atlas framework for
automatic segmentation and spatial alignment of the embryo using deep learning
with minimal supervision. Our framework learns to register the embryo to an
atlas, which consists of the US images acquired at a range of gestational age
(GA), segmented and spatially aligned to a predefined standard orientation.
From this, we can derive the segmentation of the embryo and put the embryo in
standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used
and eight subjects were selected as atlas. We evaluated different fusion
strategies to incorporate multiple atlases: 1) training the framework using
atlas images from a single subject, 2) training the framework with data of all
available atlases and 3) ensembling of the frameworks trained per subject. To
evaluate the performance, we calculated the Dice score over the test set. We
found that training the framework using all available atlases outperformed
ensembling and gave similar results compared to the best of all frameworks
trained on a single subject. Furthermore, we found that selecting images from
the four atlases closest in GA out of all available atlases, regardless of the
individual quality, gave the best results with a median Dice score of 0.72. We
conclude that our framework can accurately segment and spatially align the
embryo in first trimester 3D US images and is robust for the variation in
quality that existed in the available atlases. Our code is publicly available
at: https://github.com/wapbastiaansen/multi-atlas-seg-reg.
</p></li>
</ul>

<h3>Title: PointInst3D: Segmenting 3D Instances by Points. (arXiv:2204.11402v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.11402">http://arxiv.org/abs/2204.11402</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.
</p></li>
</ul>

<h3>Title: Object-Aware Self-supervised Multi-Label Learning. (arXiv:2205.07028v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.07028">http://arxiv.org/abs/2205.07028</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Multi-label Learning on Image data has been widely exploited with deep
learning models. However, supervised training on deep CNN models often cannot
discover sufficient discriminative features for classification. As a result,
numerous self-supervision methods are proposed to learn more robust image
representations. However, most self-supervised approaches focus on
single-instance single-label data and fall short on more complex images with
multiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS)
method to obtain more fine-grained representations for multi-label learning,
dynamically generating auxiliary tasks based on object locations. Secondly, the
robust representation learned by OASS can be leveraged to efficiently generate
Class-Specific Instances (CSI) in a proposal-free fashion to better guide
multi-label supervision signal transfer to instances. Extensive experiments on
the VOC2012 dataset for multi-label classification demonstrate the
effectiveness of the proposed method against the state-of-the-art counterparts.
</p></li>
</ul>

<h3>Title: RepMix: Representation Mixing for Robust Attribution of Synthesized Images. (arXiv:2207.02063v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.02063">http://arxiv.org/abs/2207.02063</a></li>
<li>Code URL: <a href="https://github.com/tubui/image_attribution">https://github.com/tubui/image_attribution</a></li>
<li>Summary: <p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.
</p></li>
</ul>

<h3>Title: Why Robust Natural Language Understanding is a Challenge. (arXiv:2206.14575v2 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.14575">http://arxiv.org/abs/2206.14575</a></li>
<li>Code URL: null</li>
<li>Summary: <p>With the proliferation of Deep Machine Learning into real-life applications,
a particular property of this technology has been brought to attention:
robustness Neural Networks notoriously present low robustness and can be highly
sensitive to small input perturbations. Recently, many methods for verifying
networks' general properties of robustness have been proposed, but they are
mostly applied in Computer Vision. In this paper we propose a Verification
specification for Natural Language Understanding classification based on larger
regions of interest, and we discuss the challenges of such task. We observe
that, although the data is almost linearly separable, the verifier struggles to
output positive results and we explain the problems and implications.
</p></li>
</ul>

<h3>Title: Forecasting COVID-19 spreading trough an ensemble of classical and machine learning models: Spain's case study. (arXiv:2207.05753v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05753">http://arxiv.org/abs/2207.05753</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this work we evaluate the applicability of an ensemble of population
models and machine learning models to predict the near future evolution of the
COVID-19 pandemic, with a particular use case in Spain. We rely solely in open
and public datasets, fusing incidence, vaccination, human mobility and weather
data to feed our machine learning models (Random Forest, Gradient Boosting,
k-Nearest Neighbours and Kernel Ridge Regression). We use the incidence data to
adjust classic population models (Gompertz, Logistic, Richards, Bertalanffy) in
order to be able to better capture the trend of the data. We then ensemble
these two families of models in order to obtain a more robust and accurate
prediction. Furthermore, we have observed an improvement in the predictions
obtained with machine learning models as we add new features (vaccines,
mobility, climatic conditions), analyzing the importance of each of them using
Shapley Additive Explanation values. As in any other modelling work, data and
predictions quality have several limitations and therefore they must be seen
from a critical standpoint, as we discuss in the text. Our work concludes that
the ensemble use of these models improves the individual predictions (using
only machine learning models or only population models) and can be applied,
with caution, in cases when compartmental models cannot be utilized due to the
lack of relevant data.
</p></li>
</ul>

<h3>Title: Exploring Adversarial Examples and Adversarial Robustness of Convolutional Neural Networks by Mutual Information. (arXiv:2207.05756v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05756">http://arxiv.org/abs/2207.05756</a></li>
<li>Code URL: <a href="https://github.com/wowotou1998/exploring_adv_by_mutual_info">https://github.com/wowotou1998/exploring_adv_by_mutual_info</a></li>
<li>Summary: <p>A counter-intuitive property of convolutional neural networks (CNNs) is their
inherent susceptibility to adversarial examples, which severely hinders the
application of CNNs in security-critical fields. Adversarial examples are
similar to original examples but contain malicious perturbations. Adversarial
training is a simple and effective training method to improve the robustness of
CNNs to adversarial examples. The mechanisms behind adversarial examples and
adversarial training are worth exploring. Therefore, this work investigates
similarities and differences between two types of CNNs (both normal and robust
ones) in information extraction by observing the trends towards the mutual
information. We show that 1) the amount of mutual information that CNNs extract
from original and adversarial examples is almost similar, whether CNNs are in
normal training or adversarial training; the reason why adversarial examples
mislead CNNs may be that they contain more texture-based information about
other categories; 2) compared with normal training, adversarial training is
more difficult and the amount of information extracted by the robust CNNs is
less; 3) the CNNs trained with different methods have different preferences for
certain types of information; normally trained CNNs tend to extract
texture-based information from the inputs, while adversarially trained models
prefer to shape-based information. Furthermore, we also analyze the mutual
information estimators used in this work, kernel-density-estimation and binning
methods, and find that these estimators outline the geometric properties of the
middle layer's output to a certain extent.
</p></li>
</ul>

<h3>Title: Long Term Fairness for Minority Groups via Performative Distributionally Robust Optimization. (arXiv:2207.05777v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05777">http://arxiv.org/abs/2207.05777</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Fairness researchers in machine learning (ML) have coalesced around several
fairness criteria which provide formal definitions of what it means for an ML
model to be fair. However, these criteria have some serious limitations. We
identify four key shortcomings of these formal fairness criteria, and aim to
help to address them by extending performative prediction to include a
distributionally robust objective.
</p></li>
</ul>

<h3>Title: GriddlyJS: A Web IDE for Reinforcement Learning. (arXiv:2207.06105v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06105">http://arxiv.org/abs/2207.06105</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Progress in reinforcement learning (RL) research is often driven by the
design of new, challenging environments -- a costly undertaking requiring
skills orthogonal to that of a typical machine learning researcher. The
complexity of environment development has only increased with the rise of
procedural-content generation (PCG) as the prevailing paradigm for producing
varied environments capable of testing the robustness and generalization of RL
agents. Moreover, existing environments often require complex build processes,
making reproducing results difficult. To address these issues, we introduce
GriddlyJS, a web-based Integrated Development Environment (IDE) based on the
Griddly engine. GriddlyJS allows researchers to visually design and debug
arbitrary, complex PCG grid-world environments using a convenient graphical
interface, as well as visualize, evaluate, and record the performance of
trained agent models. By connecting the RL workflow to the advanced
functionality enabled by modern web standards, GriddlyJS allows publishing
interactive agent-environment demos that reproduce experimental results
directly to the web. To demonstrate the versatility of GriddlyJS, we use it to
quickly develop a complex compositional puzzle-solving environment alongside
arbitrary human-designed environment configurations and their solutions for use
in automatic curriculum learning and offline RL. The GriddlyJS IDE is open
source and freely available at \url{https://griddly.ai}.
</p></li>
</ul>

<h3>Title: Probing the Robustness of Independent Mechanism Analysis for Representation Learning. (arXiv:2207.06137v1 [stat.ML])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06137">http://arxiv.org/abs/2207.06137</a></li>
<li>Code URL: null</li>
<li>Summary: <p>One aim of representation learning is to recover the original latent code
that generated the data, a task which requires additional information or
inductive biases. A recently proposed approach termed Independent Mechanism
Analysis (IMA) postulates that each latent source should influence the observed
mixtures independently, complementing standard nonlinear independent component
analysis, and taking inspiration from the principle of independent causal
mechanisms. While it was shown in theory and experiments that IMA helps
recovering the true latents, the method's performance was so far only
characterized when the modeling assumptions are exactly satisfied. Here, we
test the method's robustness to violations of the underlying assumptions. We
find that the benefits of IMA-based regularization for recovering the true
sources extend to mixing functions with various degrees of violation of the IMA
principle, while standard regularizers do not provide the same merits.
Moreover, we show that unregularized maximum likelihood recovers mixing
functions which systematically deviate from the IMA principle, and provide an
argument elucidating the benefits of IMA-based regularization.
</p></li>
</ul>

<h3>Title: DiverGet: A Search-Based Software Testing Approach for Deep Neural Network Quantization Assessment. (arXiv:2207.06282v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06282">http://arxiv.org/abs/2207.06282</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Quantization is one of the most applied Deep Neural Network (DNN) compression
strategies, when deploying a trained DNN model on an embedded system or a cell
phone. This is owing to its simplicity and adaptability to a wide range of
applications and circumstances, as opposed to specific Artificial Intelligence
(AI) accelerators and compilers that are often designed only for certain
specific hardware (e.g., Google Coral Edge TPU). With the growing demand for
quantization, ensuring the reliability of this strategy is becoming a critical
challenge. Traditional testing methods, which gather more and more genuine data
for better assessment, are often not practical because of the large size of the
input space and the high similarity between the original DNN and its quantized
counterpart. As a result, advanced assessment strategies have become of
paramount importance. In this paper, we present DiverGet, a search-based
testing framework for quantization assessment. DiverGet defines a space of
metamorphic relations that simulate naturally-occurring distortions on the
inputs. Then, it optimally explores these relations to reveal the disagreements
among DNNs of different arithmetic precision. We evaluate the performance of
DiverGet on state-of-the-art DNNs applied to hyperspectral remote sensing
images. We chose the remote sensing DNNs as they're being increasingly deployed
at the edge (e.g., high-lift drones) in critical domains like climate change
research and astronomy. Our results show that DiverGet successfully challenges
the robustness of established quantization techniques against
naturally-occurring shifted data, and outperforms its most recent concurrent,
DiffChaser, with a success rate that is (on average) four times higher.
</p></li>
</ul>

<h3>Title: Neural Network Robustness as a Verification Property: A Principled Case Study. (arXiv:2104.01396v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2104.01396">http://arxiv.org/abs/2104.01396</a></li>
<li>Code URL: <a href="https://github.com/aisec-private/training-with-constraints">https://github.com/aisec-private/training-with-constraints</a></li>
<li>Summary: <p>Neural networks are very successful at detecting patterns in noisy data, and
have become the technology of choice in many fields. However, their usefulness
is hampered by their susceptibility to adversarial attacks. Recently, many
methods for measuring and improving a network's robustness to adversarial
perturbations have been proposed, and this growing body of research has given
rise to numerous explicit or implicit notions of robustness. Connections
between these notions are often subtle, and a systematic comparison between
them is missing in the literature. In this paper we begin addressing this gap,
by setting up general principles for the empirical analysis and evaluation of a
network's robustness as a mathematical property - during the network's training
phase, its verification, and after its deployment. We then apply these
principles and conduct a case study that showcases the practical benefits of
our general approach.
</p></li>
</ul>

<h3>Title: Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2107.04086">http://arxiv.org/abs/2107.04086</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Massive deployment of Graph Neural Networks (GNNs) in high-stake applications
generates a strong demand for explanations that are robust to noise and align
well with human intuition. Most existing methods generate explanations by
identifying a subgraph of an input graph that has a strong correlation with the
prediction. These explanations are not robust to noise because independently
optimizing the correlation for a single input can easily overfit noise.
Moreover, they do not align well with human intuition because removing an
identified subgraph from an input graph does not necessarily change the
prediction result. In this paper, we propose a novel method to generate robust
counterfactual explanations on GNNs by explicitly modelling the common decision
logic of GNNs on similar input graphs. Our explanations are naturally robust to
noise because they are produced from the common decision boundaries of a GNN
that govern the predictions of many similar input graphs. The explanations also
align well with human intuition because removing the set of edges identified by
an explanation from the input graph changes the prediction significantly.
Exhaustive experiments on many public datasets demonstrate the superior
performance of our method.
</p></li>
</ul>

<h3>Title: Driving Style Recognition Using Interval Type-2 Fuzzy Inference System and Multiple Experts Decision Making. (arXiv:2110.13805v2 [cs.RO] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2110.13805">http://arxiv.org/abs/2110.13805</a></li>
<li>Code URL: <a href="https://github.com/iag0g0mes/t2fis_driving_style">https://github.com/iag0g0mes/t2fis_driving_style</a></li>
<li>Summary: <p>Driving styles summarize different driving behaviors that reflect in the
movements of the vehicles. These behaviors may indicate a tendency to perform
riskier maneuvers, consume more fuel or energy, break traffic rules, or drive
carefully. Therefore, this paper presents a driving style recognition using
Interval Type-2 Fuzzy Inference System with Multiple Experts Decision-Making
for classifying drivers into calm, moderate and aggressive. This system
receives as input features longitudinal and lateral kinematic parameters of the
vehicle motion. The type-2 fuzzy sets are more robust than type-1 fuzzy sets
when handling noisy data, because their membership function are also fuzzy
sets. In addition, a multiple experts approach can reduce the bias and
imprecision while building the fuzzy rulebase, which stores the knowledge of
the fuzzy system. The proposed approach was evaluated using descriptive
statistics analysis, and compared with clustering algorithms and a type-1 fuzzy
inference system. The results show the tendency to associate lower kinematic
profiles for the driving styles classified with the type-2 fuzzy inference
system when compared to other algorithms, which is in line with the more
conservative approach adopted in the aggregation of the experts' opinions.
</p></li>
</ul>

<h3>Title: Data-driven Control of Agent-based Models: an Equation/Variable-free Machine Learning Approach. (arXiv:2207.05779v1 [math.DS])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05779">http://arxiv.org/abs/2207.05779</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We present an Equation/Variable free machine learning (EVFML) framework for
the control of the collective dynamics of complex/multiscale systems modelled
via microscopic/agent-based simulators. The approach obviates the need for
construction of surrogate, reduced-order models.~The proposed implementation
consists of three steps: (A) from high-dimensional agent-based simulations,
machine learning (in particular, non-linear manifold learning (Diffusion Maps
(DMs)) helps identify a set of coarse-grained variables that parametrize the
low-dimensional manifold on which the emergent/collective dynamics evolve. The
out-of-sample extension and pre-image problems, i.e. the construction of
non-linear mappings from the high-dimensional input space to the
low-dimensional manifold and back, are solved by coupling DMs with the Nystrom
extension and Geometric Harmonics, respectively; (B) having identified the
manifold and its coordinates, we exploit the Equation-free approach to perform
numerical bifurcation analysis of the emergent dynamics; then (C) based on the
previous steps, we design data-driven embedded wash-out controllers that drive
the agent-based simulators to their intrinsic, imprecisely known, emergent
open-loop unstable steady-states, thus demonstrating that the scheme is robust
against numerical approximation errors and modelling uncertainty.~The
efficiency of the framework is illustrated by controlling emergent unstable (i)
traveling waves of a deterministic agent-based model of traffic dynamics, and
(ii) equilibria of a stochastic financial market agent model with mimesis.
</p></li>
</ul>

<h3>Title: Estimating Test Performance for AI Medical Devices under Distribution Shift with Conformal Prediction. (arXiv:2207.05796v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05796">http://arxiv.org/abs/2207.05796</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Estimating the test performance of software AI-based medical devices under
distribution shifts is crucial for evaluating the safety, efficiency, and
usability prior to clinical deployment. Due to the nature of regulated medical
device software and the difficulty in acquiring large amounts of labeled
medical datasets, we consider the task of predicting the test accuracy of an
arbitrary black-box model on an unlabeled target domain without modification to
the original training process or any distributional assumptions of the original
source data (i.e. we treat the model as a "black-box" and only use the
predicted output responses). We propose a "black-box" test estimation technique
based on conformal prediction and evaluate it against other methods on three
medical imaging datasets (mammography, dermatology, and histopathology) under
several clinically relevant types of distribution shift (institution, hardware
scanner, atlas, hospital). We hope that by promoting practical and effective
estimation techniques for black-box models, manufacturers of medical devices
will develop more standardized and realistic evaluation procedures to improve
the robustness and trustworthiness of clinical AI tools.
</p></li>
</ul>

<h3>Title: Goal-Oriented Sensitivity Analysis of Hyperparameters in Deep Learning. (arXiv:2207.06216v1 [stat.ML])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06216">http://arxiv.org/abs/2207.06216</a></li>
<li>Code URL: <a href="https://github.com/paulnovello/goal-oriented-ho">https://github.com/paulnovello/goal-oriented-ho</a></li>
<li>Summary: <p>Tackling new machine learning problems with neural networks always means
optimizing numerous hyperparameters that define their structure and strongly
impact their performances. In this work, we study the use of goal-oriented
sensitivity analysis, based on the Hilbert-Schmidt Independence Criterion
(HSIC), for hyperparameter analysis and optimization. Hyperparameters live in
spaces that are often complex and awkward. They can be of different natures
(categorical, discrete, boolean, continuous), interact, and have
inter-dependencies. All this makes it non-trivial to perform classical
sensitivity analysis. We alleviate these difficulties to obtain a robust
analysis index that is able to quantify hyperparameters' relative impact on a
neural network's final error. This valuable tool allows us to better understand
hyperparameters and to make hyperparameter optimization more interpretable. We
illustrate the benefits of this knowledge in the context of hyperparameter
optimization and derive an HSIC-based optimization algorithm that we apply on
MNIST and Cifar, classical machine learning data sets, but also on the
approximation of Runge function and Bateman equations solution, of interest for
scientific machine learning. This method yields neural networks that are both
competitive and cost-effective.
</p></li>
</ul>

<h3>Title: Stochastic Functional Analysis and Multilevel Vector Field Anomaly Detection. (arXiv:2207.06229v1 [stat.ML])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06229">http://arxiv.org/abs/2207.06229</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Massive vector field datasets are common in multi-spectral optical and radar
sensors and modern multimodal MRI data, among many other areas of application.
In this paper we develop a novel stochastic functional analysis approach for
detecting anomalies based on the covariance structure of nominal stochastic
behavior across a domain with multi-band vector field data. An optimal vector
field Karhunen-Loeve (KL) expansion is applied to such random field data. A
series of multilevel orthogonal functional subspaces is constructed from the
geometry of the domain, adapted from the KL expansion. Detection is achieved by
examining the projection of the random field on the multilevel basis. The
anomalies can be quantified in suitable normed spaces based on local and global
information. In addition, reliable hypothesis tests are formed with
controllable distributions that do not require prior assumptions on probability
distributions of the data. Only the covariance function is needed, which makes
for significantly simpler estimates. Furthermore this approach allows
stochastic vector-based fusion of anomalies without any loss of information.
The method is applied to the important problem of deforestation and degradation
in the Amazon forest. This is a complex non-monotonic process, as forests can
degrade and recover. This particular problem is further compounded by the
presence of clouds that are hard to remove with current masking algorithms.
Using multi-spectral satellite data from Sentinel 2, the multilevel filter is
constructed and anomalies are treated as deviations from the initial state of
the forest. Forest anomalies are quantified with robust hypothesis tests and
distinguished from false variations such as cloud cover. Our approach shows the
advantage of using multiple bands of data in a vectorized complex, leading to
better anomaly detection beyond the capabilities of scalar-based methods.
</p></li>
</ul>

<h3>Title: QT-Routenet: Improved GNN generalization to larger 5G networks by fine-tuning predictions from queueing theory. (arXiv:2207.06336v1 [cs.NI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06336">http://arxiv.org/abs/2207.06336</a></li>
<li>Code URL: <a href="https://github.com/itu-ai-ml-in-5g-challenge/itu-ml5g-ps-001-parana">https://github.com/itu-ai-ml-in-5g-challenge/itu-ml5g-ps-001-parana</a></li>
<li>Summary: <p>In order to promote the use of machine learning in 5G, the International
Telecommunication Union (ITU) proposed in 2021 the second edition of the ITU
AI/ML in 5G challenge, with over 1600 participants from 82 countries. This work
details the second place solution overall, which is also the winning solution
of the Graph Neural Networking Challenge 2021. We tackle the problem of
generalization when applying a model to a 5G network that may have longer paths
and larger link capacities than the ones observed in training. To achieve this,
we propose to first extract robust features related to Queueing Theory (QT),
and then fine-tune the analytical baseline prediction using a modification of
the Routenet Graph Neural Network (GNN) model. The proposed solution
generalizes much better than simply using Routenet, and manages to reduce the
analytical baseline's 10.42 mean absolute percent error to 1.45 (1.27 with an
ensemble). This suggests that making small changes to an approximate model that
is known to be robust can be an effective way to improve accuracy without
compromising generalization.
</p></li>
</ul>

<h3>Title: Learning robust marking policies for adaptive mesh refinement. (arXiv:2207.06339v1 [math.NA])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06339">http://arxiv.org/abs/2207.06339</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In this work, we revisit the marking decisions made in the standard adaptive
finite element method (AFEM). Experience shows that a na\"{i}ve marking policy
leads to inefficient use of computational resources for adaptive mesh
refinement (AMR). Consequently, using AFEM in practice often involves ad-hoc or
time-consuming offline parameter tuning to set appropriate parameters for the
marking subroutine. To address these practical concerns, we recast AMR as a
Markov decision process in which refinement parameters can be selected
on-the-fly at run time, without the need for pre-tuning by expert users. In
this new paradigm, the refinement parameters are also chosen adaptively via a
marking policy that can be optimized using methods from reinforcement learning.
We use the Poisson equation to demonstrate our techniques on $h$- and
$hp$-refinement benchmark problems, and our experiments suggest that superior
marking policies remain undiscovered for many classical AFEM applications.
Furthermore, an unexpected observation from this work is that marking policies
trained on one family of PDEs are sometimes robust enough to perform well on
problems far outside the training family. For illustration, we show that a
simple $hp$-refinement policy trained on 2D domains with only a single
re-entrant corner can be deployed on far more complicated 2D domains, and even
3D domains, without significant performance loss. For reproduction and broader
adoption, we accompany this work with an open-source implementation of our
methods.
</p></li>
</ul>

<h3>Title: Learning Approximately Optimal Contracts. (arXiv:1811.06736v2 [cs.GT] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/1811.06736">http://arxiv.org/abs/1811.06736</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In principal-agent models, a principal offers a contract to an agent to
perform a certain task. The agent exerts a level of effort that maximizes her
utility. The principal is oblivious to the agent's chosen level of effort, and
conditions her wage only on possible outcomes. In this work, we consider a
model in which the principal is unaware of the agent's utility and action
space: she sequentially offers contracts to identical agents, and observes the
resulting outcomes. We present an algorithm for learning the optimal contract
under mild assumptions. We bound the number of samples needed for the principal
to obtain a contract that is within $\eps$ of her optimal net profit for every
$\eps>0$. Our results are robust even when considering risk-averse agents.
Furthermore, we show that when there are only two possible outcomes or the
agent is risk-neutral, the algorithm's outcome approximates the optimal
contract described in the classical theory.
</p></li>
</ul>

<h3>Title: Optimal Network Compression. (arXiv:2008.08733v5 [q-fin.RM] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2008.08733">http://arxiv.org/abs/2008.08733</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper introduces a formulation of the optimal network compression
problem for financial systems. This general formulation is presented for
different levels of network compression or rerouting allowed from the initial
interbank network. We prove that this problem is, generically, NP-hard. We
focus on objective functions generated by systemic risk measures under shocks
to the financial network. We use this framework to study the (sub)optimality of
the maximally compressed network. We conclude by studying the optimal
compression problem for specific networks; this permits us to study, e.g., the
so-called robust fragility of certain network topologies more generally as well
as the potential benefits and costs of network compression. In particular,
under systematic shocks and heterogeneous financial networks the robust
fragility results of Acemoglu et al. (2015) no longer hold generally.
</p></li>
</ul>

<h3>Title: Electromagnetic Source Imaging via a Data-Synthesis-Based Convolutional Encoder-Decoder Network. (arXiv:2010.12876v6 [eess.IV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2010.12876">http://arxiv.org/abs/2010.12876</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Electromagnetic source imaging (ESI) requires solving a highly ill-posed
inverse problem. To seek a unique solution, traditional ESI methods impose
various forms of priors that may not accurately reflect the actual source
properties, which may hinder their broad applications. To overcome this
limitation, in this paper a novel data-synthesized spatio-temporally
convolutional encoder-decoder network method termed DST-CedNet is proposed for
ESI. DST-CedNet recasts ESI as a machine learning problem, where discriminative
learning and latent-space representations are integrated in a convolutional
encoder-decoder network (CedNet) to learn a robust mapping from the measured
electroencephalography/magnetoencephalography (E/MEG) signals to the brain
activity. In particular, by incorporating prior knowledge regarding dynamical
brain activities, a novel data synthesis strategy is devised to generate
large-scale samples for effectively training CedNet. This stands in contrast to
traditional ESI methods where the prior information is often enforced via
constraints primarily aimed for mathematical convenience. Extensive numerical
experiments as well as analysis of a real MEG and Epilepsy EEG dataset
demonstrate that DST-CedNet outperforms several state-of-the-art ESI methods in
robustly estimating source signals under a variety of source configurations.
</p></li>
</ul>

<h3>Title: Robust Data-Driven Predictive Control using Reachability Analysis. (arXiv:2103.14110v3 [eess.SY] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2103.14110">http://arxiv.org/abs/2103.14110</a></li>
<li>Code URL: <a href="https://github.com/aalanwar/Data-Driven-Predictive-Control">https://github.com/aalanwar/Data-Driven-Predictive-Control</a></li>
<li>Summary: <p>We present a robust data-driven control scheme for an unknown linear system
model with bounded process and measurement noise. Instead of depending on a
system model in traditional predictive control, a controller utilizing
data-driven reachable regions is proposed. The data-driven reachable regions
are based on a matrix zonotope recursion and are computed based on only noisy
input-output data of a trajectory of the system. We assume that measurement and
process noise are contained in bounded sets. While we assume knowledge of these
bounds, no knowledge about the statistical properties of the noise is assumed.
In the noise-free case, we prove that the presented purely data-driven control
scheme results in an equivalent closed-loop behavior to a nominal model
predictive control scheme. In the case of measurement and process noise, our
proposed scheme guarantees robust constraint satisfaction, which is essential
in safety-critical applications. Numerical experiments show the effectiveness
of the proposed data-driven controller in comparison to model-based control
schemes.
</p></li>
</ul>

<h3>Title: OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses. (arXiv:2204.02426v4 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.02426">http://arxiv.org/abs/2204.02426</a></li>
<li>Code URL: <a href="https://github.com/erobic/occam-nets-v1">https://github.com/erobic/occam-nets-v1</a></li>
<li>Summary: <p>Dataset bias and spurious correlations can significantly impair
generalization in deep neural networks. Many prior efforts have addressed this
problem using either alternative loss functions or sampling strategies that
focus on rare patterns. We propose a new direction: modifying the network
architecture to impose inductive biases that make the network robust to dataset
bias. Specifically, we propose OccamNets, which are biased to favor simpler
solutions by design. OccamNets have two inductive biases. First, they are
biased to use as little network depth as needed for an individual example.
Second, they are biased toward using fewer image locations for prediction.
While OccamNets are biased toward simpler hypotheses, they can learn more
complex hypotheses if necessary. In experiments, OccamNets outperform or rival
state-of-the-art methods run on architectures that do not incorporate these
inductive biases. Furthermore, we demonstrate that when the state-of-the-art
debiasing methods are combined with OccamNets results further improve.
</p></li>
</ul>

<h3>Title: Online Algorithms with Multiple Predictions. (arXiv:2205.03921v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.03921">http://arxiv.org/abs/2205.03921</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper studies online algorithms augmented with multiple machine-learned
predictions. While online algorithms augmented with a single prediction have
been extensively studied in recent years, the literature for the multiple
predictions setting is sparse. In this paper, we give a generic algorithmic
framework for online covering problems with multiple predictions that obtains
an online solution that is competitive against the performance of the best
predictor. Our algorithm incorporates the use of predictions in the classic
potential-based analysis of online algorithms. We apply our algorithmic
framework to solve classical problems such as online set cover, (weighted)
caching, and online facility location in the multiple predictions setting. Our
algorithm can also be robustified, i.e., the algorithm can be simultaneously
made competitive against the best prediction and the performance of the best
online algorithm (without prediction).
</p></li>
</ul>

<h3>Title: GraphMAE: Self-Supervised Masked Graph Autoencoders. (arXiv:2205.10803v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2205.10803">http://arxiv.org/abs/2205.10803</a></li>
<li>Code URL: <a href="https://github.com/thudm/graphmae">https://github.com/thudm/graphmae</a></li>
<li>Summary: <p>Self-supervised learning (SSL) has been extensively explored in recent years.
Particularly, generative SSL has seen emerging success in natural language
processing and other AI fields, such as the wide adoption of BERT and GPT.
Despite this, contrastive learning-which heavily relies on structural data
augmentation and complicated training strategies-has been the dominant approach
in graph SSL, while the progress of generative SSL on graphs, especially graph
autoencoders (GAEs), has thus far not reached the potential as promised in
other fields. In this paper, we identify and examine the issues that negatively
impact the development of GAEs, including their reconstruction objective,
training robustness, and error metric. We present a masked graph autoencoder
GraphMAE that mitigates these issues for generative self-supervised graph
pretraining. Instead of reconstructing graph structures, we propose to focus on
feature reconstruction with both a masking strategy and scaled cosine error
that benefit the robust training of GraphMAE. We conduct extensive experiments
on 21 public datasets for three different graph learning tasks. The results
manifest that GraphMAE-a simple graph autoencoder with careful designs-can
consistently generate outperformance over both contrastive and generative
state-of-the-art baselines. This study provides an understanding of graph
autoencoders and demonstrates the potential of generative self-supervised
pre-training on graphs.
</p></li>
</ul>

<h3>Title: Robust optimal well control using an adaptive multi-grid reinforcement learning framework. (arXiv:2207.03253v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.03253">http://arxiv.org/abs/2207.03253</a></li>
<li>Code URL: <a href="https://github.com/atishdixit16/ada_multigrid_ppo">https://github.com/atishdixit16/ada_multigrid_ppo</a></li>
<li>Summary: <p>Reinforcement learning (RL) is a promising tool to solve robust optimal well
control problems where the model parameters are highly uncertain, and the
system is partially observable in practice. However, RL of robust control
policies often relies on performing a large number of simulations. This could
easily become computationally intractable for cases with computationally
intensive simulations. To address this bottleneck, an adaptive multi-grid RL
framework is introduced which is inspired by principles of geometric multi-grid
methods used in iterative numerical algorithms. RL control policies are
initially learned using computationally efficient low fidelity simulations
using coarse grid discretization of the underlying partial differential
equations (PDEs). Subsequently, the simulation fidelity is increased in an
adaptive manner towards the highest fidelity simulation that correspond to
finest discretization of the model domain. The proposed framework is
demonstrated using a state-of-the-art, model-free policy-based RL algorithm,
namely the Proximal Policy Optimisation (PPO) algorithm. Results are shown for
two case studies of robust optimal well control problems which are inspired
from SPE-10 model 2 benchmark case studies. Prominent gains in the
computational efficiency is observed using the proposed framework saving around
60-70% of computational cost of its single fine-grid counterpart.
</p></li>
</ul>

<h3>Title: Training Robust Deep Models for Time-Series Domain: Novel Algorithms and Theoretical Analysis. (arXiv:2207.04305v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04305">http://arxiv.org/abs/2207.04305</a></li>
<li>Code URL: <a href="https://github.com/tahabelkhouja/robust-training-for-time-series">https://github.com/tahabelkhouja/robust-training-for-time-series</a></li>
<li>Summary: <p>Despite the success of deep neural networks (DNNs) for real-world
applications over time-series data such as mobile health, little is known about
how to train robust DNNs for time-series domain due to its unique
characteristics compared to images and text data. In this paper, we propose a
novel algorithmic framework referred as RObust Training for Time-Series (RO-TS)
to create robust DNNs for time-series classification tasks. Specifically, we
formulate a min-max optimization problem over the model parameters by
explicitly reasoning about the robustness criteria in terms of additive
perturbations to time-series inputs measured by the global alignment kernel
(GAK) based distance. We also show the generality and advantages of our
formulation using the summation structure over time-series alignments by
relating both GAK and dynamic time warping (DTW). This problem is an instance
of a family of compositional min-max optimization problems, which are
challenging and open with unclear theoretical guarantee. We propose a
principled stochastic compositional alternating gradient descent ascent
(SCAGDA) algorithm for this family of optimization problems. Unlike traditional
methods for time-series that require approximate computation of distance
measures, SCAGDA approximates the GAK based distance on-the-fly using a moving
average approach. We theoretically analyze the convergence rate of SCAGDA and
provide strong theoretical support for the estimation of GAK based distance.
Our experiments on real-world benchmarks demonstrate that RO-TS creates more
robust DNNs when compared to adversarial training using prior methods that rely
on data augmentation or new definitions of loss functions. We also demonstrate
the importance of GAK for time-series data over the Euclidean distance. The
source code of RO-TS algorithms is available at
https://github.com/tahabelkhouja/Robust-Training-for-Time-Series
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: A General Framework for Partial to Full Image Registration. (arXiv:2207.06387v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06387">http://arxiv.org/abs/2207.06387</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Image registration is a research field in which images must be compared and
aligned independently of the point of view or camera characteristics. In some
applications (such as forensic biometrics, satellite photography or outdoor
scene identification) classical image registration systems fail due to one of
the images compared represents a tiny piece of the other image. For instance,
in forensics palmprint recognition, it is usual to find only a small piece of
the palmprint, but in the database, the whole palmprint has been enrolled. The
main reason of the poor behaviour of classical image registration methods is
the gap between the amounts of salient points of both images, which is related
to the number of points to be considered as outliers. Usually, the difficulty
of finding a good match increases when the image that represents the tiny part
of the scene has been drastically rotated. Again, in the case of palmprint
forensics, it is difficult to decide a priori the orientation of the found tiny
palmprint image. We present a rotation invariant registration method that
explicitly considers that the image to be matched is a small piece of a larger
image. We have experimentally validated our method in two different scenarios;
palmprint identification and outdoor image registration.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Dam reservoir extraction from remote sensing imagery using tailored metric learning strategies. (arXiv:2207.05807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05807">http://arxiv.org/abs/2207.05807</a></li>
<li>Code URL: <a href="https://github.com/c8241998/dam-reservoir-extraction">https://github.com/c8241998/dam-reservoir-extraction</a></li>
<li>Summary: <p>Dam reservoirs play an important role in meeting sustainable development
goals and global climate targets. However, particularly for small dam
reservoirs, there is a lack of consistent data on their geographical location.
To address this data gap, a promising approach is to perform automated dam
reservoir extraction based on globally available remote sensing imagery. It can
be considered as a fine-grained task of water body extraction, which involves
extracting water areas in images and then separating dam reservoirs from
natural water bodies. We propose a novel deep neural network (DNN) based
pipeline that decomposes dam reservoir extraction into water body segmentation
and dam reservoir recognition. Water bodies are firstly separated from
background lands in a segmentation model and each individual water body is then
predicted as either dam reservoir or natural water body in a classification
model. For the former step, point-level metric learning with triplets across
images is injected into the segmentation model to address contour ambiguities
between water areas and land regions. For the latter step, prior-guided metric
learning with triplets from clusters is injected into the classification model
to optimize the image embedding space in a fine-grained level based on
reservoir clusters. To facilitate future research, we establish a benchmark
dataset with earth imagery data and human labelled reservoirs from river basins
in West Africa and India. Extensive experiments were conducted on this
benchmark in the water body segmentation task, dam reservoir recognition task,
and the joint dam reservoir extraction task. Superior performance has been
observed in the respective tasks when comparing our method with state of the
art approaches.
</p></li>
</ul>

<h3>Title: REZCR: A Zero-shot Character Recognition Method via Radical Extraction. (arXiv:2207.05842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05842">http://arxiv.org/abs/2207.05842</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The long-tail effect is a common issue that limits the performance of deep
learning models on real-world datasets. Character image dataset development is
also affected by such unbalanced data distribution due to differences in
character usage frequency. Thus, current character recognition methods are
limited when applying to real-world datasets, in particular to the character
categories in the tail which are lacking training samples, e.g., uncommon
characters, or characters from historical documents. In this paper, we propose
a zero-shot character recognition framework via radical extraction, i.e.,
REZCR, to improve the recognition performance of few-sample character
categories, in which we exploit information on radicals, the graphical units of
characters, by decomposing and reconstructing characters following orthography.
REZCR consists of an attention-based radical information extractor (RIE) and a
knowledge graph-based character reasoner (KGR). The RIE aims to recognize
candidate radicals and their possible structural relations from character
images. The results will be fed into KGR to recognize the target character by
reasoning with a pre-designed character knowledge graph. We validate our method
on multiple datasets, REZCR shows promising experimental results, especially
for few-sample character datasets.
</p></li>
</ul>

<h3>Title: Intra-Modal Constraint Loss For Image-Text Retrieval. (arXiv:2207.05024v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05024">http://arxiv.org/abs/2207.05024</a></li>
<li>Code URL: <a href="https://github.com/canonchen/imc">https://github.com/canonchen/imc</a></li>
<li>Summary: <p>Cross-modal retrieval has drawn much attention in both computer vision and
natural language processing domains. With the development of convolutional and
recurrent neural networks, the bottleneck of retrieval across image-text
modalities is no longer the extraction of image and text features but an
efficient loss function learning in embedding space. Many loss functions try to
closer pairwise features from heterogeneous modalities. This paper proposes a
method for learning joint embedding of images and texts using an intra-modal
constraint loss function to reduce the violation of negative pairs from the
same homogeneous modality. Experimental results show that our approach
outperforms state-of-the-art bi-directional image-text retrieval methods on
Flickr30K and Microsoft COCO datasets. Our code is publicly available:
https://github.com/CanonChen/IMC.
</p></li>
</ul>

<h3>Title: OSLAT: Open Set Label Attention Transformer for Medical Entity Span Extraction. (arXiv:2207.05817v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05817">http://arxiv.org/abs/2207.05817</a></li>
<li>Code URL: <a href="https://github.com/curai/curai-research">https://github.com/curai/curai-research</a></li>
<li>Summary: <p>Identifying spans in medical texts that correspond to medical entities is one
of the core steps for many healthcare NLP tasks such as ICD coding, medical
finding extraction, medical note contextualization, to name a few. Existing
entity extraction methods rely on a fixed and limited vocabulary of medical
entities and have difficulty with extracting entities represented by disjoint
spans. In this paper, we present a new transformer-based architecture called
OSLAT, Open Set Label Attention Transformer, that addresses many of the
limitations of the previous methods. Our approach uses the label-attention
mechanism to implicitly learn spans associated with entities of interest. These
entities can be provided as free text, including entities not seen during
OSLAT's training, and the model can extract spans even when they are disjoint.
To test the generalizability of our method, we train two separate models on two
different datasets, which have very low entity overlap: (1) a public discharge
notes dataset from hNLP, and (2) a much more challenging proprietary patient
text dataset "Reasons for Encounter" (RFE). We find that OSLAT models trained
on either dataset outperform rule-based and fuzzy string matching baselines
when applied to the RFE dataset as well as to the portion of hNLP dataset where
entities are represented by disjoint spans. Our code can be found at
https://github.com/curai/curai-research/tree/main/OSLAT.
</p></li>
</ul>

<h3>Title: Building a Relation Extraction Baseline for Gene-Disease Associations: A Reproducibility Study. (arXiv:2207.06226v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06226">http://arxiv.org/abs/2207.06226</a></li>
<li>Code URL: <a href="https://github.com/mntlra/dexter">https://github.com/mntlra/dexter</a></li>
<li>Summary: <p>Reproducibility is an important task in scientific research. It is crucial
for researchers to compare newly developed systems with the state-of-the-art to
assess whether they made a breakthrough. However previous works may not be
immediately reproducible, for example due to the lack of source code. In this
work we reproduce DEXTER, a system to automatically extract Gene-Disease
Associations (GDAs) from biomedical abstracts. The goal is to provide a
benchmark for future works regarding Relation Extraction (RE), enabling
researchers to test and compare their results.
</p></li>
</ul>

<h3>Title: A Transfer Learning Based Model for Text Readability Assessment in German. (arXiv:2207.06265v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06265">http://arxiv.org/abs/2207.06265</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Text readability assessment has a wide range of applications for different
target people, from language learners to people with disabilities. The fast
pace of textual content production on the web makes it impossible to measure
text complexity without the benefit of machine learning and natural language
processing techniques. Although various research addressed the readability
assessment of English text in recent years, there is still room for improvement
of the models for other languages. In this paper, we proposed a new model for
text complexity assessment for German text based on transfer learning. Our
results show that the model outperforms more classical solutions based on
linguistic features extraction from input text. The best model is based on the
BERT pre-trained language model achieved the Root Mean Square Error (RMSE) of
0.483.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v2 [eess.AS] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.02797">http://arxiv.org/abs/2206.02797</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Federated Learning (FL) enables training state-of-the-art Automatic Speech
Recognition (ASR) models on user devices (clients) in distributed systems,
hence preventing transmission of raw user data to a central server. A key
challenge facing practical adoption of FL for ASR is obtaining ground-truth
labels on the clients. Existing approaches rely on clients to manually
transcribe their speech, which is impractical for obtaining large training
corpora. A promising alternative is using semi-/self-supervised learning
approaches to leverage unlabelled user data. To this end, we propose FedNST, a
novel method for training distributed ASR models using private and unlabelled
user data. We explore various facets of FedNST, such as training models with
different proportions of labelled and unlabelled data, and evaluate the
proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech,
where 960 hours of speech data is split equally into server (labelled) and
client (unlabelled) data, showed a 22.5% relative word error rate reduction}
(WERR) over a supervised baseline trained only on server data.
</p></li>
</ul>

<h3>Title: FD-GATDR: A Federated-Decentralized-Learning Graph Attention Network for Doctor Recommendation Using EHR. (arXiv:2207.05750v1 [cs.IR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05750">http://arxiv.org/abs/2207.05750</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In the past decade, with the development of big data technology, an
increasing amount of patient information has been stored as electronic health
records (EHRs). Leveraging these data, various doctor recommendation systems
have been proposed. Typically, such studies process the EHR data in a
flat-structured manner, where each encounter was treated as an unordered set of
features. Nevertheless, the heterogeneous structured information such as
service sequence stored in claims shall not be ignored. This paper presents a
doctor recommendation system with time embedding to reconstruct the potential
connections between patients and doctors using heterogeneous graph attention
network. Besides, to address the privacy issue of patient data sharing crossing
hospitals, a federated decentralized learning method based on a minimization
optimization model is also proposed. The graph-based recommendation system has
been validated on a EHR dataset. Compared to baseline models, the proposed
method improves the AUC by up to 6.2%. And our proposed federated-based
algorithm not only yields the fictitious fusion center's performance but also
enjoys a convergence rate of O(1/T).
</p></li>
</ul>

<h3>Title: Federated Learning for THz Channel Estimation. (arXiv:2207.06017v1 [eess.SP])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06017">http://arxiv.org/abs/2207.06017</a></li>
<li>Code URL: null</li>
<li>Summary: <p>This paper addresses two major challenges in terahertz (THz) channel
estimation: the beam-split phenomenon, i.e., beam misalignment because of
frequency-independent analog beamformers, and computational complexity because
of the usage of ultra-massive number of antennas to compensate propagation
losses. Data-driven techniques are known to mitigate the complexity of this
problem but usually require the transmission of the datasets from the users to
a central server entailing huge communications overhead. In this work, we
employ federated learning (FL), wherein the users transmit only the model
parameters instead of the whole dataset, for THz channel estimation to improve
the communications-efficiency. In order to accurately estimate the channel
despite beam-split, we propose a beamspace support alignment technique without
requiring additional hardware. Compared to the previous works, our method
provides higher channel estimation accuracy as well as approximately $68$ times
lower communications overhead.
</p></li>
</ul>

<h3>Title: TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels. (arXiv:2207.06343v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06343">http://arxiv.org/abs/2207.06343</a></li>
<li>Code URL: null</li>
<li>Summary: <p>State-of-the-art federated learning methods can perform far worse than their
centralized counterparts when clients have dissimilar data distributions. For
neural networks, even when centralized SGD easily finds a solution that is
simultaneously performant for all clients, current federated optimization
methods fail to converge to a comparable solution. We show that this
performance disparity can largely be attributed to optimization challenges
presented by nonconvexity. Specifically, we find that the early layers of the
network do learn useful features, but the final layers fail to make use of
them. That is, federated optimization applied to this non-convex problem
distorts the learning of the final layers. Leveraging this observation, we
propose a Train-Convexify-Train (TCT) procedure to sidestep this issue: first,
learn features using off-the-shelf methods (e.g., FedAvg); then, optimize a
convexified problem obtained from the network's empirical neural tangent kernel
approximation. Our technique yields accuracy improvements of up to +36% on
FMNIST and +37% on CIFAR10 when clients have dissimilar data.
</p></li>
</ul>

<h3>Title: FedShuffle: Recipes for Better Use of Local Work in Federated Learning. (arXiv:2204.13169v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.13169">http://arxiv.org/abs/2204.13169</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The practice of applying several local updates before aggregation across
clients has been empirically shown to be a successful approach to overcoming
the communication bottleneck in Federated Learning (FL). In this work, we
propose a general recipe, FedShuffle, that better utilizes the local updates in
FL, especially in the heterogeneous regime. Unlike many prior works, FedShuffle
does not assume any uniformity in the number of updates per device. Our
FedShuffle recipe comprises four simple-yet-powerful ingredients: 1) local
shuffling of the data, 2) adjustment of the local learning rates, 3) update
weighting, and 4) momentum variance reduction (Cutkosky and Orabona, 2019). We
present a comprehensive theoretical analysis of FedShuffle and show that both
theoretically and empirically, our approach does not suffer from the objective
function mismatch that is present in FL methods which assume homogeneous
updates in heterogeneous FL setups, e.g., FedAvg (McMahan et al., 2017). In
addition, by combining the ingredients above, FedShuffle improves upon FedNova
(Wang et al., 2020), which was previously proposed to solve this mismatch. We
also show that FedShuffle with momentum variance reduction can improve upon
non-local methods under a Hessian similarity assumption. Finally, through
experiments on synthetic and real-world datasets, we illustrate how each of the
four ingredients used in FedShuffle helps improve the use of local updates in
FL.
</p></li>
</ul>

<h3>Title: Multi-Model Federated Learning with Provable Guarantees. (arXiv:2207.04330v3 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04330">http://arxiv.org/abs/2207.04330</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Federated Learning (FL) is a variant of distributed learning where edge
devices collaborate to learn a model without sharing their data with the
central server or each other. We refer to the process of training multiple
independent models simultaneously in a federated setting using a common pool of
clients as multi-model FL. In this work, we propose two variants of the popular
FedAvg algorithm for multi-model FL, with provable convergence guarantees. We
further show that for the same amount of computation, multi-model FL can have
better performance than training each model separately. We supplement our
theoretical results with experiments in strongly convex, convex, and non-convex
settings.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v3 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2203.07980">http://arxiv.org/abs/2203.07980</a></li>
<li>Code URL: <a href="https://github.com/georghess/pmb-nll">https://github.com/georghess/pmb-nll</a></li>
<li>Summary: <p>Accurate uncertainty estimates are essential for deploying deep object
detectors in safety-critical systems. The development and evaluation of
probabilistic object detectors have been hindered by shortcomings in existing
performance measures, which tend to involve arbitrary thresholds or limit the
detector's choice of distributions. In this work, we propose to view object
detection as a set prediction task where detectors predict the distribution
over the set of objects. Using the negative log-likelihood for random finite
sets, we present a proper scoring rule for evaluating and training
probabilistic object detectors. The proposed method can be applied to existing
probabilistic detectors, is free from thresholds, and enables fair comparison
between architectures. Three different types of detectors are evaluated on the
COCO dataset. Our results indicate that the training of existing detectors is
optimized toward non-probabilistic metrics. We hope to encourage the
development of new object detectors that can accurately estimate their own
uncertainty. Code available at https://github.com/georghess/pmb-nll.
</p></li>
</ul>

<h3>Title: A Closer Look at Invariances in Self-supervised Pre-training for 3D Vision. (arXiv:2207.04997v2 [cs.CV] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.04997">http://arxiv.org/abs/2207.04997</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Self-supervised pre-training for 3D vision has drawn increasing research
interest in recent years. In order to learn informative representations, a lot
of previous works exploit invariances of 3D features, e.g.,
perspective-invariance between views of the same scene, modality-invariance
between depth and RGB images, format-invariance between point clouds and
voxels. Although they have achieved promising results, previous researches lack
a systematic and fair comparison of these invariances. To address this issue,
our work, for the first time, introduces a unified framework, under which
various pre-training methods can be investigated. We conduct extensive
experiments and provide a closer look at the contributions of different
invariances in 3D pre-training. Also, we propose a simple but effective method
that jointly pre-trains a 3D encoder and a depth map encoder using contrastive
learning. Models pre-trained with our method gain significant performance boost
in downstream tasks. For instance, a pre-trained VoteNet outperforms previous
methods on SUN RGB-D and ScanNet object detection benchmarks with a clear
margin.
</p></li>
</ul>

<h3>Title: Degendering Resumes for Fair Algorithmic Resume Screening. (arXiv:2112.08910v3 [cs.CL] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2112.08910">http://arxiv.org/abs/2112.08910</a></li>
<li>Code URL: null</li>
<li>Summary: <p>We investigate whether it is feasible to remove gendered information from
resumes to mitigate potential bias in algorithmic resume screening. Using a
corpus of 709k resumes from IT firms, we first train a series of models to
classify the self-reported gender of the applicant, thereby measuring the
extent and nature of gendered information encoded in resumes. We then conduct a
series of gender obfuscation experiments, where we iteratively remove gendered
information from resumes. Finally, we train a resume screening algorithm and
investigate the trade-off between gender obfuscation and screening algorithm
performance. Results show: (1) There is a significant amount of gendered
information in resumes. (2) Lexicon-based gender obfuscation method (i.e.
removing tokens that are predictive of gender) can reduce the amount of
gendered information to a large extent. However, after a certain point, the
performance of the resume screening algorithm starts suffering. (3)
General-purpose gender debiasing methods for NLP models such as removing gender
subspace from embeddings are not effective in obfuscating gender.
</p></li>
</ul>

<h3>Title: Revealing Unfair Models by Mining Interpretable Evidence. (arXiv:2207.05811v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05811">http://arxiv.org/abs/2207.05811</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The popularity of machine learning has increased the risk of unfair models
getting deployed in high-stake applications, such as justice system,
drug/vaccination design, and medical diagnosis. Although there are effective
methods to train fair models from scratch, how to automatically reveal and
explain the unfairness of a trained model remains a challenging task. Revealing
unfairness of machine learning models in interpretable fashion is a critical
step towards fair and trustworthy AI. In this paper, we systematically tackle
the novel task of revealing unfair models by mining interpretable evidence
(RUMIE). The key idea is to find solid evidence in the form of a group of data
instances discriminated most by the model. To make the evidence interpretable,
we also find a set of human-understandable key attributes and decision rules
that characterize the discriminated data instances and distinguish them from
the other non-discriminated data. As demonstrated by extensive experiments on
many real-world data sets, our method finds highly interpretable and solid
evidence to effectively reveal the unfairness of trained models. Moreover, it
is much more scalable than all of the baseline methods.
</p></li>
</ul>

<h3>Title: Human-AI Collaboration in Decision-Making: Beyond Learning to Defer. (arXiv:2206.13202v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2206.13202">http://arxiv.org/abs/2206.13202</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Human-AI collaboration (HAIC) in decision-making aims to create synergistic
teaming between human decision-makers and AI systems. Learning to defer (L2D)
has been presented as a promising framework to determine who among humans and
AI should make which decisions in order to optimize the performance and
fairness of the combined system. Nevertheless, L2D entails several often
unfeasible requirements, such as the availability of predictions from humans
for every instance or ground-truth labels that are independent from said
humans. Furthermore, neither L2D nor alternative approaches tackle fundamental
issues of deploying HAIC systems in real-world settings, such as capacity
management or dealing with dynamic environments. In this paper, we aim to
identify and review these and other limitations, pointing to where
opportunities for future research in HAIC may lie.
</p></li>
</ul>

<h3>Title: High Per Parameter: A Large-Scale Study of Hyperparameter Tuning for Machine Learning Algorithms. (arXiv:2207.06028v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06028">http://arxiv.org/abs/2207.06028</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Hyperparameters in machine learning (ML) have received a fair amount of
attention, and hyperparameter tuning has come to be regarded as an important
step in the ML pipeline. But just how useful is said tuning? While
smaller-scale experiments have been previously conducted, herein we carry out a
large-scale investigation, specifically, one involving 26 ML algorithms, 250
datasets (regression and both binary and multinomial classification), 6 score
metrics, and 28,857,600 algorithm runs. Analyzing the results we conclude that
for many ML algorithms we should not expect considerable gains from
hyperparameter tuning on average, however, there may be some datasets for which
default hyperparameters perform poorly, this latter being truer for some
algorithms than others. By defining a single hp_score value, which combines an
algorithm's accumulated statistics, we are able to rank the 26 ML algorithms
from those expected to gain the most from hyperparameter tuning to those
expected to gain the least. We believe such a study may serve ML practitioners
at large.
</p></li>
</ul>

<h3>Title: Towards A Holistic View of Bias in Machine Learning: Bridging Algorithmic Fairness and Imbalanced Learning. (arXiv:2207.06084v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06084">http://arxiv.org/abs/2207.06084</a></li>
<li>Code URL: <a href="https://github.com/dd1github/fair-over-sampling">https://github.com/dd1github/fair-over-sampling</a></li>
<li>Summary: <p>Machine learning (ML) is playing an increasingly important role in rendering
decisions that affect a broad range of groups in society. ML models inform
decisions in criminal justice, the extension of credit in banking, and the
hiring practices of corporations. This posits the requirement of model
fairness, which holds that automated decisions should be equitable with respect
to protected features (e.g., gender, race, or age) that are often
under-represented in the data. We postulate that this problem of
under-representation has a corollary to the problem of imbalanced data
learning. This class imbalance is often reflected in both classes and protected
features. For example, one class (those receiving credit) may be
over-represented with respect to another class (those not receiving credit) and
a particular group (females) may be under-represented with respect to another
group (males). A key element in achieving algorithmic fairness with respect to
protected groups is the simultaneous reduction of class and protected group
imbalance in the underlying training data, which facilitates increases in both
model accuracy and fairness. We discuss the importance of bridging imbalanced
learning and group fairness by showing how key concepts in these fields overlap
and complement each other; and propose a novel oversampling algorithm, Fair
Oversampling, that addresses both skewed class distributions and protected
features. Our method: (i) can be used as an efficient pre-processing algorithm
for standard ML algorithms to jointly address imbalance and group equity; and
(ii) can be combined with fairness-aware learning algorithms to improve their
robustness to varying levels of class imbalance. Additionally, we take a step
toward bridging the gap between fairness and imbalanced learning with a new
metric, Fair Utility, that combines balanced accuracy with fairness.
</p></li>
</ul>

<h3>Title: Understanding Unfairness in Fraud Detection through Model and Data Bias Interactions. (arXiv:2207.06273v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06273">http://arxiv.org/abs/2207.06273</a></li>
<li>Code URL: null</li>
<li>Summary: <p>In recent years, machine learning algorithms have become ubiquitous in a
multitude of high-stakes decision-making applications. The unparalleled ability
of machine learning algorithms to learn patterns from data also enables them to
incorporate biases embedded within. A biased model can then make decisions that
disproportionately harm certain groups in society -- limiting their access to
financial services, for example. The awareness of this problem has given rise
to the field of Fair ML, which focuses on studying, measuring, and mitigating
unfairness in algorithmic prediction, with respect to a set of protected groups
(e.g., race or gender). However, the underlying causes for algorithmic
unfairness still remain elusive, with researchers divided between blaming
either the ML algorithms or the data they are trained on. In this work, we
maintain that algorithmic unfairness stems from interactions between models and
biases in the data, rather than from isolated contributions of either of them.
To this end, we propose a taxonomy to characterize data bias and we study a set
of hypotheses regarding the fairness-accuracy trade-offs that fairness-blind ML
algorithms exhibit under different data bias settings. On our real-world
account-opening fraud use case, we find that each setting entails specific
trade-offs, affecting fairness in expected value and variance -- the latter
often going unnoticed. Moreover, we show how algorithms compare differently in
terms of accuracy and fairness, depending on the biases affecting the data.
Finally, we note that under specific data bias conditions, simple
pre-processing interventions can successfully balance group-wise error rates,
while the same techniques fail in more complex settings.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer. (arXiv:2207.05749v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05749">http://arxiv.org/abs/2207.05749</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Pathologists have a rich vocabulary with which they can describe all the
nuances of cellular morphology. In their world, there is a natural pairing of
images and words. Recent advances demonstrate that machine learning models can
now be trained to learn high-quality image features and represent them as
discrete units of information. This enables natural language, which is also
discrete, to be jointly modelled alongside the imaging, resulting in a
description of the contents of the imaging. Here we present experiments in
applying discrete modelling techniques to the problem domain of non-melanoma
skin cancer, specifically, histological images of Intraepidermal Carcinoma
(IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256)
images of IEC images, we trained a sequence-to-sequence transformer to generate
natural language descriptions using pathologist terminology. Combined with the
idea of interactive concept vectors available by using continuous generative
methods, we demonstrate an additional angle of interpretability. The result is
a promising means of working towards highly expressive machine learning systems
which are not only useful as predictive/classification tools, but also means to
further our scientific understanding of disease.
</p></li>
</ul>

<h3>Title: A Novel DeBERTa-based Model for Financial Question Answering Task. (arXiv:2207.05875v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.05875">http://arxiv.org/abs/2207.05875</a></li>
<li>Code URL: null</li>
<li>Summary: <p>As a rising star in the field of natural language processing, question
answering systems (Q&amp;A Systems) are widely used in all walks of life. Compared
with other scenarios, the applicationin financial scenario has strong
requirements in the traceability and interpretability of the Q&amp;A systems. In
addition, since the demand for artificial intelligence technology has gradually
shifted from the initial computational intelligence to cognitive intelligence,
this research mainly focuses on the financial numerical reasoning dataset -
FinQA. In the shared task, the objective is to generate the reasoning program
and the final answer according to the given financial report containing text
and tables. We use the method based on DeBERTa pre-trained language model, with
additional optimization methods including multi-model fusion, training set
combination on this basis. We finally obtain an execution accuracy of 68.99 and
a program accuracy of 64.53, ranking No. 4 in the 2022 FinQA Challenge.
</p></li>
</ul>

<h3>Title: On NeuroSymbolic Solutions for PDEs. (arXiv:2207.06240v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.06240">http://arxiv.org/abs/2207.06240</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Physics Informed Neural Networks (PINNs) have gained immense popularity as an
alternate method for numerically solving PDEs. Despite their empirical success
we are still building an understanding of the convergence properties of
training on such constraints with gradient descent. It is known that, in the
absence of an explicit inductive bias, Neural Networks can struggle to learn or
approximate even simple and well known functions in a sample efficient manner.
Thus the numerical approximation induced from few collocation points may not
generalize over the entire domain. Meanwhile, a symbolic form can exhibit good
generalization, with interpretability as a useful byproduct. However, symbolic
approximations can struggle to simultaneously be concise and accurate.
Therefore in this work we explore a NeuroSymbolic approach to approximate the
solution for PDEs. We observe that our approach work for several simple cases.
We illustrate the efficacy of our approach on Navier Stokes: Kovasznay flow
where there are multiple physical quantities of interest governed with
non-linear coupled PDE system. Domain splitting is now becoming a popular trick
to help PINNs approximate complex functions. We observe that a NeuroSymbolic
approach can help such complex functions as well. We demonstrate
Domain-splitting assisted NeuroSymbolic approach on a temporally varying
two-dimensional Burger's equation. Finally we consider the scenario where PINNs
have to be solved for parameterized PDEs, for changing Initial-Boundary
Conditions and changes in the coefficient of the PDEs. Hypernetworks have shown
to hold promise to overcome these challenges. We show that one can design
Hyper-NeuroSymbolic Networks which can combine the benefits of speed and
increased accuracy. We observe that that the NeuroSymbolic approximations are
consistently 1-2 order of magnitude better than just the neural or symbolic
approximations.
</p></li>
</ul>

<h3>Title: Explainability in Deep Reinforcement Learning, a Review into Current Methods and Applications. (arXiv:2207.01911v2 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.01911">http://arxiv.org/abs/2207.01911</a></li>
<li>Code URL: null</li>
<li>Summary: <p>The use of Deep Reinforcement Learning (DRL) schemes has increased
dramatically since their first introduction in 2015. Though uses in many
different applications are being found they still have a problem with the lack
of interpretability. This has bread a lack of understanding and trust in the
use of DRL solutions from researchers and the general public. To solve this
problem the field of explainable artificial intelligence (XAI) has emerged.
This is a variety of different methods that look to open the DRL black boxes,
they range from the use of interpretable symbolic decision trees to numerical
methods like Shapley Values. This review looks at which methods are being used
and what applications they are being used. This is done to identify which
models are the best suited to each application or if a method is being
underutilised.
</p></li>
</ul>

<h3>Title: Modelling Evolutionary and Stationary User Preferences for Temporal Sets Prediction. (arXiv:2204.05490v5 [cs.LG] UPDATED)</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2204.05490">http://arxiv.org/abs/2204.05490</a></li>
<li>Code URL: null</li>
<li>Summary: <p>Given a sequence of sets, where each set is associated with a timestamp and
contains an arbitrary number of elements, the task of temporal sets prediction
aims to predict the elements in the subsequent set. Previous studies for
temporal sets prediction mainly capture each user's evolutionary preference by
learning from his/her own sequence. Although insightful, we argue that: 1) the
collaborative signals latent in different users' sequences are essential but
have not been exploited; 2) users also tend to show stationary preferences
while existing methods fail to consider. To this end, we propose an integrated
learning framework to model both the evolutionary and the stationary
preferences of users for temporal sets prediction, which first constructs a
universal sequence by chronologically arranging all the user-set interactions,
and then learns on each user-set interaction. In particular, for each user-set
interaction, we first design an evolutionary user preference modelling
component to track the user's time-evolving preference and exploit the latent
collaborative signals among different users. This component maintains a memory
bank to store memories of the related user and elements, and continuously
updates their memories based on the currently encoded messages and the past
memories. Then, we devise a stationary user preference modelling module to
discover each user's personalized characteristics according to the historical
sequence, which adaptively aggregates the previously interacted elements from
dual perspectives with the guidance of the user's and elements' embeddings.
Finally, we develop a set-batch algorithm to improve the model efficiency,
which can create time-consistent batches in advance and achieve 3.5x training
speedups on average. Experiments on real-world datasets demonstrate the
effectiveness and good interpretability of our approach.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
