<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-27</h1>
<h3>Title: TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Rahul Ghosh, Chun-Hao Liu, Gaurav Rele, Vidya Sagar Ravipati, Hazar Aouad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16984">https://arxiv.org/abs/2601.16984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16984">https://arxiv.org/pdf/2601.16984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16984]] TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation(https://arxiv.org/abs/2601.16984)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.</li>
</ul>

<h3>Title: Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Cheng Tang, Lei Gong, Cheng Li, Chao Wang, teng wang, Wenqi Lou, Xuehai Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16986">https://arxiv.org/abs/2601.16986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16986">https://arxiv.org/pdf/2601.16986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16986]] Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle(https://arxiv.org/abs/2601.16986)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.</li>
</ul>

<h3>Title: Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions</h3>
<ul>
<li><strong>Authors: </strong>Shunyang Luo, Peibei Cao, Zhihui Zhu, Kehua Feng, Zhihua Wang, Keyan Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16987">https://arxiv.org/abs/2601.16987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16987">https://arxiv.org/pdf/2601.16987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16987]] Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions(https://arxiv.org/abs/2601.16987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.</li>
</ul>

<h3>Title: Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16991">https://arxiv.org/abs/2601.16991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16991">https://arxiv.org/pdf/2601.16991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16991]] Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models(https://arxiv.org/abs/2601.16991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.</li>
</ul>

<h3>Title: RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Zhou, Ziqi Liu, Yan Wang, Yiming Lin, Yangbin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17002">https://arxiv.org/abs/2601.17002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17002">https://arxiv.org/pdf/2601.17002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17002]] RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection(https://arxiv.org/abs/2601.17002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.</li>
</ul>

<h3>Title: MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuchen Li, Jing Chen, Xuzhao Li, Hao Liang, Xiaohuan Zhou, Taifeng Wang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17006">https://arxiv.org/abs/2601.17006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17006">https://arxiv.org/pdf/2601.17006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17006]] MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning(https://arxiv.org/abs/2601.17006)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.</li>
</ul>

<h3>Title: Analysis of voice recordings features for Classification of Parkinson's Disease</h3>
<ul>
<li><strong>Authors: </strong>Beatriz Pérez-Sánchez, Noelia Sánchez-Maroño, Miguel A. Díaz-Freire</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17007">https://arxiv.org/abs/2601.17007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17007">https://arxiv.org/pdf/2601.17007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17007]] Analysis of voice recordings features for Classification of Parkinson's Disease(https://arxiv.org/abs/2601.17007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease. This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.</li>
</ul>

<h3>Title: Bayesian Robust Financial Trading with Adversarial Synthetic Market Data</h3>
<ul>
<li><strong>Authors: </strong>Haochong Xia, Simin Li, Ruixiao Xu, Zhixia Zhang, Hongxiang Wang, Zhiqian Liu, Teng Yao Long, Molei Qin, Chuqiao Zong, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17008">https://arxiv.org/abs/2601.17008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17008">https://arxiv.org/pdf/2601.17008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17008]] Bayesian Robust Financial Trading with Adversarial Synthetic Market Data(https://arxiv.org/abs/2601.17008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.</li>
</ul>

<h3>Title: Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study</h3>
<ul>
<li><strong>Authors: </strong>Hudson Golino</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17010">https://arxiv.org/abs/2601.17010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17010">https://arxiv.org/pdf/2601.17010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17010]] Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study(https://arxiv.org/abs/2601.17010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.</li>
</ul>

<h3>Title: Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Xu, Fuquan Zong, Yexuan Xing, Chulong Zhang, Guang Yang, Shilong Yang, Xiaokun Liang, Juan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17031">https://arxiv.org/abs/2601.17031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17031">https://arxiv.org/pdf/2601.17031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17031]] Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection(https://arxiv.org/abs/2601.17031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.</li>
</ul>

<h3>Title: AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs</h3>
<ul>
<li><strong>Authors: </strong>Aahana Basappa, Pranay Goel, Anusri Karra, Anish Karra, Asa Gilmore, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17037">https://arxiv.org/abs/2601.17037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17037">https://arxiv.org/pdf/2601.17037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17037]] AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs(https://arxiv.org/abs/2601.17037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.</li>
</ul>

<h3>Title: Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification</h3>
<ul>
<li><strong>Authors: </strong>Obai Alashram, Nejad Alagha, Mahmoud AlKakuri, Zeeshan Swaveel, Abigail Copiaco</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17038">https://arxiv.org/abs/2601.17038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17038">https://arxiv.org/pdf/2601.17038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17038]] Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification(https://arxiv.org/abs/2601.17038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.</li>
</ul>

<h3>Title: MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Junhyuk Heo, Beomkyu Choi, Hyunjin Shin, Darongsae Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17039">https://arxiv.org/abs/2601.17039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17039">https://arxiv.org/pdf/2601.17039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17039]] MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation(https://arxiv.org/abs/2601.17039)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.</li>
</ul>

<h3>Title: Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective</h3>
<ul>
<li><strong>Authors: </strong>Tianyuan Liu, Libin Hou, Linyuan Wang, Bin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17042">https://arxiv.org/abs/2601.17042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17042">https://arxiv.org/pdf/2601.17042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17042]] Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective(https://arxiv.org/abs/2601.17042)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.</li>
</ul>

<h3>Title: Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Matan Leibovich, Mai Tan, Adria Marcos-Morales, Sreyas Mohan, Peter A. Crozier, Carlos Fernandez-Granda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17046">https://arxiv.org/abs/2601.17046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17046">https://arxiv.org/pdf/2601.17046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17046]] Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning(https://arxiv.org/abs/2601.17046)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.</li>
</ul>

<h3>Title: A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Yuanjie Gu, Yiqun Wang, Chaohui Yu, Ang Xuan, Fan Wang, Zhi Lu, Biqin Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17047">https://arxiv.org/abs/2601.17047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17047">https://arxiv.org/pdf/2601.17047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17047]] A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities(https://arxiv.org/abs/2601.17047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.</li>
</ul>

<h3>Title: SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jing Jie Tan, Rupert Schreiner, Matthias Hausladen, Ali Asgharzade, Simon Edler, Julian Bartsch, Michael Bachmann, Andreas Schels, Ban-Hoe Kwan, Danny Wee-Kiat Ng, Yan-Chai Hum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17048">https://arxiv.org/abs/2601.17048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17048">https://arxiv.org/pdf/2601.17048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17048]] SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis(https://arxiv.org/abs/2601.17048)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at this https URL</li>
</ul>

<h3>Title: Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Hongjun An, Yiliang Song, Jiawei Shao, Zhe Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17050">https://arxiv.org/abs/2601.17050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17050">https://arxiv.org/pdf/2601.17050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17050]] Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence(https://arxiv.org/abs/2601.17050)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.</li>
</ul>

<h3>Title: Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults</h3>
<ul>
<li><strong>Authors: </strong>Shuhao Que, Dieuwke van Dartel, Ilse Heeringa, Han Hegeman, Miriam Vollenbroek-Hutten, Ying Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17053">https://arxiv.org/abs/2601.17053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17053">https://arxiv.org/pdf/2601.17053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17053]] Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults(https://arxiv.org/abs/2601.17053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.</li>
</ul>

<h3>Title: FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Byeongju Kim, Jungwan Lee, Donghyeon Han, Hoi-Jun Yoo, Sangyeob Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17063">https://arxiv.org/abs/2601.17063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17063">https://arxiv.org/pdf/2601.17063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17063]] FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices(https://arxiv.org/abs/2601.17063)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.</li>
</ul>

<h3>Title: A Mechanistic View on Video Generation as World Models: State and Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Luozhou Wang, Zhifei Chen, Yihua Du, Dongyu Yan, Wenhang Ge, Guibao Shen, Xinli Xu, Leyi Wu, Man Chen, Tianshuo Xu, Peiran Ren, Xin Tao, Pengfei Wan, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17067">https://arxiv.org/abs/2601.17067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17067">https://arxiv.org/pdf/2601.17067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17067]] A Mechanistic View on Video Generation as World Models: State and Dynamics(https://arxiv.org/abs/2601.17067)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.</li>
</ul>

<h3>Title: Multi-Agent Deep Reinforcement Learning Under Constrained Communications</h3>
<ul>
<li><strong>Authors: </strong>Shahil Shaik, Jonathon M. Smereka, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17069">https://arxiv.org/abs/2601.17069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17069">https://arxiv.org/pdf/2601.17069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17069]] Multi-Agent Deep Reinforcement Learning Under Constrained Communications(https://arxiv.org/abs/2601.17069)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.</li>
</ul>

<h3>Title: Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances</h3>
<ul>
<li><strong>Authors: </strong>Jisui Huang, Andreas Alpers, Ke Chen, Na Lei</a></li>
<li><strong>Subjects: </strong>cs.CV, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17071">https://arxiv.org/abs/2601.17071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17071">https://arxiv.org/pdf/2601.17071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17071]] Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances(https://arxiv.org/abs/2601.17071)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.</li>
</ul>

<h3>Title: Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zhang, Meimei Liu, Zhengwu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17073">https://arxiv.org/abs/2601.17073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17073">https://arxiv.org/pdf/2601.17073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17073]] Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis(https://arxiv.org/abs/2601.17073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.</li>
</ul>

<h3>Title: PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction</h3>
<ul>
<li><strong>Authors: </strong>Akila Sampath, Vandana Janeja, Jianwu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17074">https://arxiv.org/abs/2601.17074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17074">https://arxiv.org/pdf/2601.17074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17074]] PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction(https://arxiv.org/abs/2601.17074)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided this http URL core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.</li>
</ul>

<h3>Title: E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Chen, Yue Wu, Kai Huang, Wen Xi, Yangyang Wu, Xiaoye Miao, Mengying Zhu, Meng Xi, Guanjie Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17076">https://arxiv.org/abs/2601.17076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17076">https://arxiv.org/pdf/2601.17076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17076]] E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning(https://arxiv.org/abs/2601.17076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at this https URL.</li>
</ul>

<h3>Title: GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars</h3>
<ul>
<li><strong>Authors: </strong>Rui-Yang Ju, Jen-Shiun Chiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17088">https://arxiv.org/abs/2601.17088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17088">https://arxiv.org/pdf/2601.17088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17088]] GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars(https://arxiv.org/abs/2601.17088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at this https URL.</li>
</ul>

<h3>Title: GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Qigan Sun, Chaoning Zhang, Jianwei Zhang, Xudong Wang, Jiehui Xie, Pengcheng Zheng, Haoyu Wang, Sungyoung Lee, Chi-lok Andy Tai, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17089">https://arxiv.org/abs/2601.17089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17089">https://arxiv.org/pdf/2601.17089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17089]] GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing(https://arxiv.org/abs/2601.17089)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.</li>
</ul>

<h3>Title: SFO: Learning PDE Operators via Spectral Filtering</h3>
<ul>
<li><strong>Authors: </strong>Noam Koren, Rafael Moschopoulos, Kira Radinsky, Elad Hazan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17090">https://arxiv.org/abs/2601.17090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17090">https://arxiv.org/pdf/2601.17090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17090]] SFO: Learning PDE Operators via Spectral Filtering(https://arxiv.org/abs/2601.17090)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.</li>
</ul>

<h3>Title: CUROCKET: Optimizing ROCKET for GPU</h3>
<ul>
<li><strong>Authors: </strong>Ole Stüven, Keno Moenck, Thorsten Schüppstuhl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17091">https://arxiv.org/abs/2601.17091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17091">https://arxiv.org/pdf/2601.17091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17091]] CUROCKET: Optimizing ROCKET for GPU(https://arxiv.org/abs/2601.17091)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository this https URL on github.</li>
</ul>

<h3>Title: The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations</h3>
<ul>
<li><strong>Authors: </strong>Olha Sirikova, Alvin Chan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17093">https://arxiv.org/abs/2601.17093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17093">https://arxiv.org/pdf/2601.17093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17093]] The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations(https://arxiv.org/abs/2601.17093)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.</li>
</ul>

<h3>Title: Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Junichiro Niimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17094">https://arxiv.org/abs/2601.17094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17094">https://arxiv.org/pdf/2601.17094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17094]] Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation(https://arxiv.org/abs/2601.17094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.</li>
</ul>

<h3>Title: LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation</h3>
<ul>
<li><strong>Authors: </strong>Xusheng Du, Athiwat Kongkaeo, Ye Zhang, Haoran Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17095">https://arxiv.org/abs/2601.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17095">https://arxiv.org/pdf/2601.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17095]] LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation(https://arxiv.org/abs/2601.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.</li>
</ul>

<h3>Title: Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals</h3>
<ul>
<li><strong>Authors: </strong>Pascaline André (1), Charles Heitz (1), Evangelia Christodoulou (2, 5, 6), Annika Reinke (2, 4), Carole H. Sudre (3, 7, 8), Michela Antonelli (7, 8), Patrick Godau (2, 5), M. Jorge Cardoso (7), Antoine Gilson (1), Sophie Tezenas du Montcel (1), Gaël Varoquaux (9), Lena Maier-Hein (2, 4, 5, 10, 11), Olivier Colliot (1) ((1) Sorbonne Université, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP, Hôpital de la Pitié-Salpêtrière, F-75013, Paris, France (2) German Cancer Research Center (DKFZ) Heidelberg, Division of Intelligent Medical Systems, Germany (3) Unit for Lifelong Health and Ageing at UCL, Department of Population Science and Experimental Medicine and Hawkes InstituteCentre for Medical Image Computing, Department of Computer Science, University College London, UK (4) DKFZ Heidelberg, Helmholtz Imaging, Germany (5) National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and Heidelberg University Hospital, Germany (6) AI Health Innovation Cluster, Germany (7) School of Biomedical Engineering and Imaging Science, King's College London, UK (8) Hawkes Institute, Department of Computer Science, University College London, UK (9) SODA project team, Inria Saclay-Île-de-France, France (10) Faculty of Mathematics and Computer Science, Heidelberg University, Germany (11) Medical Faculty, Heidelberg University, Germany)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17103">https://arxiv.org/abs/2601.17103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17103">https://arxiv.org/pdf/2601.17103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17103]] Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals(https://arxiv.org/abs/2601.17103)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.</li>
</ul>

<h3>Title: StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors</h3>
<ul>
<li><strong>Authors: </strong>Qinkai Yu, Chong Zhang, Gaojie Jin, Tianjin Huang, Wei Zhou, Wenhui Li, Xiaobo Jin, Bo Huang, Yitian Zhao, Guang Yang, Gregory Y.H. Lip, Yalin Zheng, Aline Villavicencio, Yanda Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17107">https://arxiv.org/abs/2601.17107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17107">https://arxiv.org/pdf/2601.17107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17107]] StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors(https://arxiv.org/abs/2601.17107)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, steal, watermark, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: this https URL.</li>
</ul>

<h3>Title: MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Dianxin Luan, Chengsi Liang, Jie Huang, Zheng Lin, Kaitao Meng, John Thompson, Cheng-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17108">https://arxiv.org/abs/2601.17108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17108">https://arxiv.org/pdf/2601.17108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17108]] MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism(https://arxiv.org/abs/2601.17108)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.</li>
</ul>

<h3>Title: Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization</h3>
<ul>
<li><strong>Authors: </strong>A. El Ichi, K. Jbilou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17112">https://arxiv.org/abs/2601.17112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17112">https://arxiv.org/pdf/2601.17112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17112]] Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization(https://arxiv.org/abs/2601.17112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.</li>
</ul>

<h3>Title: iFSQ: Improving FSQ for Image Generation with 1 Line of Code</h3>
<ul>
<li><strong>Authors: </strong>Bin Lin, Zongjian Li, Yuwei Niu, Kaixiong Gong, Yunyang Ge, Yunlong Lin, Mingzhe Zheng, JianWei Zhang, Miles Yang, Zhao Zhong, Liefeng Bo, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17124">https://arxiv.org/abs/2601.17124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17124">https://arxiv.org/pdf/2601.17124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17124]] iFSQ: Improving FSQ for Image Generation with 1 Line of Code(https://arxiv.org/abs/2601.17124)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at this https URL</li>
</ul>

<h3>Title: How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?</h3>
<ul>
<li><strong>Authors: </strong>Megha Khosla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17130">https://arxiv.org/abs/2601.17130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17130">https://arxiv.org/pdf/2601.17130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17130]] How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?(https://arxiv.org/abs/2601.17130)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, membership infer</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.</li>
</ul>

<h3>Title: Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation</h3>
<ul>
<li><strong>Authors: </strong>Inderjeet Singh, Eleonore Vissol-Gaudin, Andikan Otung, Motoyoshi Sekiya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17133">https://arxiv.org/abs/2601.17133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17133">https://arxiv.org/pdf/2601.17133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17133]] Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation(https://arxiv.org/abs/2601.17133)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.</li>
</ul>

<h3>Title: ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Jakob Karalus, Friedhelm Schwenker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17135">https://arxiv.org/abs/2601.17135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17135">https://arxiv.org/pdf/2601.17135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17135]] ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning(https://arxiv.org/abs/2601.17135)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.</li>
</ul>

<h3>Title: Dynamic Role Assignment for Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Miao Zhang, Junsik Kim, Siyuan Xiang, Jian Gao, Cheng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17152">https://arxiv.org/abs/2601.17152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17152">https://arxiv.org/pdf/2601.17152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17152]] Dynamic Role Assignment for Multi-Agent Debate(https://arxiv.org/abs/2601.17152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.</li>
</ul>

<h3>Title: Interpretability of the Intent Detection Problem: A New Approach</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Gutiérrez-Naranjo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17156">https://arxiv.org/abs/2601.17156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17156">https://arxiv.org/pdf/2601.17156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17156]] Interpretability of the Intent Detection Problem: A New Approach(https://arxiv.org/abs/2601.17156)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.</li>
</ul>

<h3>Title: Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text</h3>
<ul>
<li><strong>Authors: </strong>Tunazzina Islam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17172">https://arxiv.org/abs/2601.17172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17172">https://arxiv.org/pdf/2601.17172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17172]] Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text(https://arxiv.org/abs/2601.17172)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.</li>
</ul>

<h3>Title: TrojanGYM: A Detector-in-the-Loop LLM for Adaptive RTL Hardware Trojan Insertion</h3>
<ul>
<li><strong>Authors: </strong>Saideep Sreekumar, Zeng Wang, Akashdeep Saha, Weihua Xiao, Minghao Shao, Muhammad Shafique, Ozgur Sinanoglu, Ramesh Karri, Johann Knechtel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17178">https://arxiv.org/abs/2601.17178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17178">https://arxiv.org/pdf/2601.17178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17178]] TrojanGYM: A Detector-in-the-Loop LLM for Adaptive RTL Hardware Trojan Insertion(https://arxiv.org/abs/2601.17178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Hardware Trojans (HTs) remain a critical threat because learning-based detectors often overfit to narrow trigger/payload patterns and small, stylized benchmarks. We introduce TrojanGYM, an agentic, LLM-driven framework that automatically curates HT insertions to expose detector blind spots while preserving design correctness. Given high-level HT specifications, a suite of cooperating LLM agents (instantiated with GPT-4, LLaMA-3.3-70B, and Gemini-2.5Pro) proposes and refines RTL modifications that realize diverse triggers and payloads without impacting normal functionality. TrojanGYM implements a feedback-driven benchmark generation loop co-designed with HT detectors, in which constraint-aware syntactic checking and GNN-based HT detectors provide feedback that iteratively refines HT specifications and insertion strategies to better surface detector blind spots. We further propose Robust-GNN4TJ, a new implementation of the GNN4TJ with improved graph extraction, training robustness, and prediction reliability, especially on LLM-generated HT designs. On the most challenging TrojanGYM-generated benchmarks, Robust-GNN4TJ raises HT detection rates from 0% to 60% relative to a prior GNN-based detector. We instantiate TrojanGYM on SRAM, AES-128, and UART designs at RTL level, and show that it systematically produces diverse, functionally correct HTs that reach up to 83.33% evasion rates against modern GNN-based detectors, revealing robustness gaps that are not apparent when these detectors are evaluated solely on existing TrustHub-style benchmarks. Post peer-review, we will release all codes and artifacts.</li>
</ul>

<h3>Title: Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Farzam Asad, Junaid Saif Khan, Maria Tariq, Sundus Munir, Muhammad Adnan Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17183">https://arxiv.org/abs/2601.17183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17183">https://arxiv.org/pdf/2601.17183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17183]] Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data(https://arxiv.org/abs/2601.17183)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.</li>
</ul>

<h3>Title: Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments</h3>
<ul>
<li><strong>Authors: </strong>Cheyu Lin, Katherine A. Flanigan, Sirajum Munir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17194">https://arxiv.org/abs/2601.17194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17194">https://arxiv.org/pdf/2601.17194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17194]] Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments(https://arxiv.org/abs/2601.17194)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.</li>
</ul>

<h3>Title: Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Seyyed Saeid Cheshmi, Hahnemann Ortiz, James Mooney, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17197">https://arxiv.org/abs/2601.17197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17197">https://arxiv.org/pdf/2601.17197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17197]] Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding(https://arxiv.org/abs/2601.17197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at this https URL.</li>
</ul>

<h3>Title: SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yinkai Wang, Yan Zhou Chen, Xiaohui Chen, Li-Ping Liu, Soha Hassoun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17204">https://arxiv.org/abs/2601.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17204">https://arxiv.org/pdf/2601.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17204]] SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment(https://arxiv.org/abs/2601.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at this https URL.</li>
</ul>

<h3>Title: Structural Complexity of Brain MRI reveals age-associated patterns</h3>
<ul>
<li><strong>Authors: </strong>Anzhe Cheng, Italo Ivo Lima Dias Pinto, Paul Bogdan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17211">https://arxiv.org/abs/2601.17211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17211">https://arxiv.org/pdf/2601.17211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17211]] Structural Complexity of Brain MRI reveals age-associated patterns(https://arxiv.org/abs/2601.17211)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.</li>
</ul>

<h3>Title: JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers</h3>
<ul>
<li><strong>Authors: </strong>Ruoqing Zheng, Chang Sun, Qibin Liu, Lauri Laatu, Arianna Cox, Benedikt Maier, Alexander Tapper, Jose G. F. Coutinho, Wayne Luk, Zhiqiang Que</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17215">https://arxiv.org/abs/2601.17215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17215">https://arxiv.org/pdf/2601.17215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17215]] JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers(https://arxiv.org/abs/2601.17215)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Massimiliano Pronesti, Anya Belz, Yufang Hou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17223">https://arxiv.org/abs/2601.17223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17223">https://arxiv.org/pdf/2601.17223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17223]] Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning(https://arxiv.org/abs/2601.17223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.</li>
</ul>

<h3>Title: Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Torbunov, Yihui Ren, Lijun Wu, Yimei Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17224">https://arxiv.org/abs/2601.17224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17224">https://arxiv.org/pdf/2601.17224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17224]] Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning(https://arxiv.org/abs/2601.17224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.</li>
</ul>

<h3>Title: Toward Risk Thresholds for AI-Enabled Cyber Threats: Enhancing Decision-Making Under Uncertainty with Bayesian Networks</h3>
<ul>
<li><strong>Authors: </strong>Krystal Jackson, Deepika Raman, Jessica Newman, Nada Madkour, Charlotte Yuan, Evan R. Murphy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17225">https://arxiv.org/abs/2601.17225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17225">https://arxiv.org/pdf/2601.17225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17225]] Toward Risk Thresholds for AI-Enabled Cyber Threats: Enhancing Decision-Making Under Uncertainty with Bayesian Networks(https://arxiv.org/abs/2601.17225)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is increasingly being used to augment and automate cyber operations, altering the scale, speed, and accessibility of malicious activity. These shifts raise urgent questions about when AI systems introduce unacceptable or intolerable cyber risk, and how risk thresholds should be identified before harms materialize at scale. In recent years, industry, government, and civil society actors have begun to articulate such thresholds for advanced AI systems, with the goal of signaling when models meaningfully amplify cyber threats, for example, by automating multi-stage intrusions, enabling zero-day discovery, or lowering the expertise required for sophisticated attacks. However, current approaches to determine these thresholds remain fragmented and limited. Many thresholds rely solely on capability benchmarks or narrow threat scenarios, and are weakly connected to empirical evidence. This paper proposes a structured approach to developing and evaluating AI cyber risk thresholds that is probabilistic, evidence-based, and operationalizable. In this paper we make three core contributions that build on our prior work that highlights the limitations of relying solely on capability assessments. First, we analyze existing industry cyber thresholds and identify common threshold elements as well as recurring methodological shortcomings. Second, we propose the use of Bayesian networks as a tool for modeling AI-enabled cyber risk, enabling the integration of heterogeneous evidence, explicit representation of uncertainty, and continuous updating as new information emerges. Third, we illustrate this approach through a focused case study on AI-augmented phishing, demonstrating how qualitative threat insights can be decomposed into measurable variables and recombined into structured risk estimates that better capture how AI changes attacker behavior and outcomes.</li>
</ul>

<h3>Title: Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Tengyue Zhang, Ruiwen Ding, Luoting Zhuang, Yuxiao Wu, Erika F. Rodriguez, William Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17228">https://arxiv.org/abs/2601.17228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17228">https://arxiv.org/pdf/2601.17228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17228]] Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification(https://arxiv.org/abs/2601.17228)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.</li>
</ul>

<h3>Title: CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Akshith Reddy Putta, Jacob Devasier, Chengkai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17230">https://arxiv.org/abs/2601.17230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17230">https://arxiv.org/pdf/2601.17230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17230]] CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval(https://arxiv.org/abs/2601.17230)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.</li>
</ul>

<h3>Title: Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization</h3>
<ul>
<li><strong>Authors: </strong>Takato Yasuno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17254">https://arxiv.org/abs/2601.17254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17254">https://arxiv.org/pdf/2601.17254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17254]] Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization(https://arxiv.org/abs/2601.17254)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.</li>
</ul>

<h3>Title: A Constrained Optimization Perspective of Unrolled Transformers</h3>
<ul>
<li><strong>Authors: </strong>Javier Porras-Valenzuela, Samar Hadou, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17257">https://arxiv.org/abs/2601.17257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17257">https://arxiv.org/pdf/2601.17257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17257]] A Constrained Optimization Perspective of Unrolled Transformers(https://arxiv.org/abs/2601.17257)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.</li>
</ul>

<h3>Title: Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Angad Singh Ahuja, Aarush Ram Anandh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17259">https://arxiv.org/abs/2601.17259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17259">https://arxiv.org/pdf/2601.17259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17259]] Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling(https://arxiv.org/abs/2601.17259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.</li>
</ul>

<h3>Title: Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales</h3>
<ul>
<li><strong>Authors: </strong>Kun Huang, Fang-Lue Zhang, Neil Dodgson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17271">https://arxiv.org/abs/2601.17271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17271">https://arxiv.org/pdf/2601.17271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17271]] Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales(https://arxiv.org/abs/2601.17271)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at this https URL.</li>
</ul>

<h3>Title: Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17275">https://arxiv.org/abs/2601.17275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17275">https://arxiv.org/pdf/2601.17275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17275]] Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning(https://arxiv.org/abs/2601.17275)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.</li>
</ul>

<h3>Title: PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Rifqi Farhansyah, Hanif Muhammad Zhafran, Farid Adilazuarda, Shamsuddeen Hassan Muhammad, Maryam Ibrahim Mukhtar, Nedjma Ousidhoum, Genta Indra Winata, Ayu Purwarianti, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17277">https://arxiv.org/abs/2601.17277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17277">https://arxiv.org/pdf/2601.17277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17277]] PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues(https://arxiv.org/abs/2601.17277)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.</li>
</ul>

<h3>Title: On the Insecurity of Keystroke-Based AI Authorship Detection: Timing-Forgery Attacks Against Motor-Signal Verification</h3>
<ul>
<li><strong>Authors: </strong>David Condrey</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17280">https://arxiv.org/abs/2601.17280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17280">https://arxiv.org/pdf/2601.17280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17280]] On the Insecurity of Keystroke-Based AI Authorship Detection: Timing-Forgery Attacks Against Motor-Signal Verification(https://arxiv.org/abs/2601.17280)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Recent proposals advocate using keystroke timing signals, specifically the coefficient of variation ($\delta$) of inter-keystroke intervals, to distinguish human-composed text from AI-generated content. We demonstrate that this class of defenses is insecure against two practical attack classes: the copy-type attack, in which a human transcribes LLM-generated text producing authentic motor signals, and timing-forgery attacks, in which automated agents sample inter-keystroke intervals from empirical human distributions. Using 13,000 sessions from the SBU corpus and three timing-forgery variants (histogram sampling, statistical impersonation, and generative LSTM), we show all attacks achieve $\ge$99.8% evasion rates against five classifiers. While detectors achieve AUC=1.000 against fully-automated injection, they classify $\ge$99.8% of attack samples as human with mean confidence $\ge$0.993. We formalize a non-identifiability result: when the detector observes only timing, the mutual information between features and content provenance is zero for copy-type attacks. Although composition and transcription produce statistically distinguishable motor patterns (Cohen's d=1.28), both yield $\delta$ values 2-4x above detection thresholds, rendering the distinction security-irrelevant. These systems confirm a human operated the keyboard, but not whether that human originated the text. Securing provenance requires architectures that bind the writing process to semantic content.</li>
</ul>

<h3>Title: Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yaokun Liu, Yifan Liu, Phoebe Mbuvi, Zelin Li, Ruichen Yao, Gawon Lim, Dong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17284">https://arxiv.org/abs/2601.17284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17284">https://arxiv.org/pdf/2601.17284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17284]] Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering(https://arxiv.org/abs/2601.17284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at this https URL, and the CV-MedBench dataset is released on Hugging Face at this https URL.</li>
</ul>

<h3>Title: Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Jin Bai, Huiyao Zhang, Qi Wen, Shengyang Li, Xiaolin Tian, Atta ur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17288">https://arxiv.org/abs/2601.17288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17288">https://arxiv.org/pdf/2601.17288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17288]] Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing(https://arxiv.org/abs/2601.17288)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.</li>
</ul>

<h3>Title: Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Samaresh Kumar Singh, Joyjit Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17303">https://arxiv.org/abs/2601.17303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17303">https://arxiv.org/pdf/2601.17303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17303]] Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach(https://arxiv.org/abs/2601.17303)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.</li>
</ul>

<h3>Title: Meta-Judging with Large Language Models: Concepts, Methods, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Hugo Silva, Mateus Mendes, Hugo Gonçalo Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17312">https://arxiv.org/abs/2601.17312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17312">https://arxiv.org/pdf/2601.17312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17312]] Meta-Judging with Large Language Models: Concepts, Methods, and Challenges(https://arxiv.org/abs/2601.17312)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.</li>
</ul>

<h3>Title: SkyReels-V3 Technique Report</h3>
<ul>
<li><strong>Authors: </strong>Debang Li, Zhengcong Fei, Tuanhui Li, Yikun Dou, Zheng Chen, Jiangping Yang, Mingyuan Fan, Jingtao Xu, Jiahua Wang, Baoxuan Gu, Mingshan Chang, Yuqiang Xie, Binjie Mao, Youqiang Zhang, Nuo Pang, Hao Zhang, Yuzhe Jin, Zhiheng Xu, Dixuan Lin, Guibin Chen, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17323">https://arxiv.org/abs/2601.17323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17323">https://arxiv.org/pdf/2601.17323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17323]] SkyReels-V3 Technique Report(https://arxiv.org/abs/2601.17323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized. Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: this https URL.</li>
</ul>

<h3>Title: Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tiejin Chen, Xiaoou Liu, Vishnu Nandam, Kuan-Ru Liou, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17329">https://arxiv.org/abs/2601.17329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17329">https://arxiv.org/pdf/2601.17329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17329]] Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment(https://arxiv.org/abs/2601.17329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.</li>
</ul>

<h3>Title: Learning with Geometric Priors in U-Net Variants for Polyp Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fabian Vazquez, Jose A. Nuñez, Diego Adame, Alissen Moreno, Augustin Zhan, Huimin Li, Jinghao Yang, Haoteng Tang, Bin Fu, Pengfei Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17331">https://arxiv.org/abs/2601.17331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17331">https://arxiv.org/pdf/2601.17331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17331]] Learning with Geometric Priors in U-Net Variants for Polyp Segmentation(https://arxiv.org/abs/2601.17331)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: this https URL</li>
</ul>

<h3>Title: Power-based Partial Attention: Bridging Linear-Complexity and Full Attention</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17334">https://arxiv.org/abs/2601.17334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17334">https://arxiv.org/pdf/2601.17334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17334]] Power-based Partial Attention: Bridging Linear-Complexity and Full Attention(https://arxiv.org/abs/2601.17334)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.</li>
</ul>

<h3>Title: AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Li, Runni Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17336">https://arxiv.org/abs/2601.17336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17336">https://arxiv.org/pdf/2601.17336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17336]] AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading(https://arxiv.org/abs/2601.17336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.</li>
</ul>

<h3>Title: TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Haodong He, Xin Zhan, Yancheng Bai, Rui Lan, Lei Sun, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17340">https://arxiv.org/abs/2601.17340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17340">https://arxiv.org/pdf/2601.17340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17340]] TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution(https://arxiv.org/abs/2601.17340)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.</li>
</ul>

<h3>Title: STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tong Wang, Xiaodong Zhang, Guanzhou Chen, Jiaqi Wang, Chenxi Liu, Xiaoliang Tan, Wenchao Guo, Xuyang Li, Xuanrui Wang, Zifan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17342">https://arxiv.org/abs/2601.17342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17342">https://arxiv.org/pdf/2601.17342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17342]] STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2601.17342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.</li>
</ul>

<h3>Title: The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Kim Young Il, Yuan Yang, Wenhao Su, Yilin Zhang, Xueluan Gong, Qian Wang, Yongsen Zheng, Ziyao Liu, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17344">https://arxiv.org/abs/2601.17344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17344">https://arxiv.org/pdf/2601.17344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17344]] The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents(https://arxiv.org/abs/2601.17344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.</li>
</ul>

<h3>Title: HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data</h3>
<ul>
<li><strong>Authors: </strong>M. L. Mamud, Piyoosh Jaysaval, Frederick D Day-Lewis, M. K. Mudunuru</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17352">https://arxiv.org/abs/2601.17352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17352">https://arxiv.org/pdf/2601.17352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17352]] HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data(https://arxiv.org/abs/2601.17352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.</li>
</ul>

<h3>Title: Safeguard: Security Controls at the Software Defined Network Layer</h3>
<ul>
<li><strong>Authors: </strong>Yi Lyu, Shichun Yu, Joe Catudal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17355">https://arxiv.org/abs/2601.17355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17355">https://arxiv.org/pdf/2601.17355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17355]] Safeguard: Security Controls at the Software Defined Network Layer(https://arxiv.org/abs/2601.17355)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Improvements in software defined networking allow for policy to be informed and modified by data-driven applications that can adjust policy to accommodate fluctuating requirements at line speed. However, there is some concern that over-correction can occur and cause unintended consequences depending on the data received. This is particularly problematic for network security features, such as machine-learning intrusion detection systems. We present Safeguard, a rule-based policy that overlaps a data-driven policy to prevent unintended responses for edge cases in network traffic. We develop a reference implementation of a network traffic classifier that enforces firewall rules for malicious traffic, and show how additional rulesets to allow known-good traffic are essential in utilizing a data-driven network policy.</li>
</ul>

<h3>Title: From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits</h3>
<ul>
<li><strong>Authors: </strong>Yao Zhao, Zhang Sheng, Shengchen Duan, Shen Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17356">https://arxiv.org/abs/2601.17356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17356">https://arxiv.org/pdf/2601.17356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17356]] From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits(https://arxiv.org/abs/2601.17356)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion</a></li>
<li><strong>Abstract: </strong>Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.</li>
</ul>

<h3>Title: Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory</h3>
<ul>
<li><strong>Authors: </strong>Davide Ettori</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17357">https://arxiv.org/abs/2601.17357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17357">https://arxiv.org/pdf/2601.17357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17357]] Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory(https://arxiv.org/abs/2601.17357)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.</li>
</ul>

<h3>Title: Robust Privacy: Inference-Time Privacy through Certified Robustness</h3>
<ul>
<li><strong>Authors: </strong>Jiankai Jin, Xiangzheng Zhang, Zhao Liu, Deyue Zhang, Quanchen Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17360">https://arxiv.org/abs/2601.17360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17360">https://arxiv.org/pdf/2601.17360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17360]] Robust Privacy: Inference-Time Privacy through Certified Robustness(https://arxiv.org/abs/2601.17360)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\sigma=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.</li>
</ul>

<h3>Title: Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Fasha, Bassam Hammo, Bilal Sowan, Husam Barham, Esam Nsour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17364">https://arxiv.org/abs/2601.17364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17364">https://arxiv.org/pdf/2601.17364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17364]] Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws(https://arxiv.org/abs/2601.17364)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.</li>
</ul>

<h3>Title: UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chengbo Ding, Fenghe Tang, Shaohua Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17366">https://arxiv.org/abs/2601.17366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17366">https://arxiv.org/pdf/2601.17366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17366]] UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation(https://arxiv.org/abs/2601.17366)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:this https URL.</li>
</ul>

<h3>Title: Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17367">https://arxiv.org/abs/2601.17367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17367">https://arxiv.org/pdf/2601.17367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17367]] Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers(https://arxiv.org/abs/2601.17367)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.</li>
</ul>

<h3>Title: Diversified Scaling Inference in Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ruijin Hua, Zichuan Liu, Kun Zhang, Yiyuan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17376">https://arxiv.org/abs/2601.17376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17376">https://arxiv.org/pdf/2601.17376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17376]] Diversified Scaling Inference in Time Series Foundation Models(https://arxiv.org/abs/2601.17376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.</li>
</ul>

<h3>Title: Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zare, Pirooz Shamsinejadbabaki</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17378">https://arxiv.org/abs/2601.17378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17378">https://arxiv.org/pdf/2601.17378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17378]] Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models(https://arxiv.org/abs/2601.17378)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.</li>
</ul>

<h3>Title: Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art</h3>
<ul>
<li><strong>Authors: </strong>Khoi Trinh, Scott Seidenberger, Joseph Spracklen, Raveen Wijewickrama, Bimal Viswanath, Murtuza Jadliwala, Anindya Maiti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17379">https://arxiv.org/abs/2601.17379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17379">https://arxiv.org/pdf/2601.17379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17379]] Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art(https://arxiv.org/abs/2601.17379)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.</li>
</ul>

<h3>Title: Physical Prompt Injection Attacks on Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang, Changhai Ou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17383">https://arxiv.org/abs/2601.17383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17383">https://arxiv.org/pdf/2601.17383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17383]] Physical Prompt Injection Attacks on Large Vision-Language Models(https://arxiv.org/abs/2601.17383)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: ONRW: Optimizing inversion noise for high-quality and robust watermark</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ding, Xiu Yan, Chuanlong Xie, Yao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17388">https://arxiv.org/abs/2601.17388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17388">https://arxiv.org/pdf/2601.17388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17388]] ONRW: Optimizing inversion noise for high-quality and robust watermark(https://arxiv.org/abs/2601.17388)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at this https URL.</li>
</ul>

<h3>Title: SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Rui Fan, Weidong Hao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17391">https://arxiv.org/abs/2601.17391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17391">https://arxiv.org/pdf/2601.17391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17391]] SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition(https://arxiv.org/abs/2601.17391)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.</li>
</ul>

<h3>Title: GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems</h3>
<ul>
<li><strong>Authors: </strong>Vashista Nobaub</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17396">https://arxiv.org/abs/2601.17396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17396">https://arxiv.org/pdf/2601.17396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17396]] GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems(https://arxiv.org/abs/2601.17396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.</li>
</ul>

<h3>Title: CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Hu, Wei Zhou, Juesi Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17397">https://arxiv.org/abs/2601.17397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17397">https://arxiv.org/pdf/2601.17397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17397]] CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing(https://arxiv.org/abs/2601.17397)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.</li>
</ul>

<h3>Title: ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Fang, Jian Li, Wei Chen, Bin Hu, Ying-Cong Chen, Xin Tang, Liang Diao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17399">https://arxiv.org/abs/2601.17399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17399">https://arxiv.org/pdf/2601.17399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17399]] ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs(https://arxiv.org/abs/2601.17399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $\rho=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.</li>
</ul>

<h3>Title: Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Prajwal Chauhan, Salah Eddine Choutri, Saif Eddin Jabari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17407">https://arxiv.org/abs/2601.17407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17407">https://arxiv.org/pdf/2601.17407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17407]] Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations(https://arxiv.org/abs/2601.17407)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.</li>
</ul>

<h3>Title: Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase</h3>
<ul>
<li><strong>Authors: </strong>Abdul Hasib, A. S. M. Ahsanul Sarkar Akib</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17414">https://arxiv.org/abs/2601.17414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17414">https://arxiv.org/pdf/2601.17414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17414]] Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase(https://arxiv.org/abs/2601.17414)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.</li>
</ul>

<h3>Title: CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17420">https://arxiv.org/abs/2601.17420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17420">https://arxiv.org/pdf/2601.17420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17420]] CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction(https://arxiv.org/abs/2601.17420)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.</li>
</ul>

<h3>Title: Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jaehui Hwang, Dongyoon Han, Sangdoo Yun, Byeongho Heo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17421">https://arxiv.org/abs/2601.17421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17421">https://arxiv.org/pdf/2601.17421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17421]] Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning(https://arxiv.org/abs/2601.17421)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.</li>
</ul>

<h3>Title: Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Yousefzadeh, Siavash Shirzadeh Barough, Ashkan Fakharifar, Yashar Tayyarazad, Narges Eghbali, Mohaddeseh Mozaffari, Hoda Taeb, Negar Sadat Rafiee Tabatabaee, Parsa Esfahanian, Ghazaleh Sadeghi Gohar, Amineh Safavirad, Saeideh Mazloomzadeh, Ehsan khalilipur, Armin Elahifar, Majid Maleki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17429">https://arxiv.org/abs/2601.17429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17429">https://arxiv.org/pdf/2601.17429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17429]] Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography(https://arxiv.org/abs/2601.17429)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.</li>
</ul>

<h3>Title: Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zichuan Yang, Yiming Xing</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17430">https://arxiv.org/abs/2601.17430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17430">https://arxiv.org/pdf/2601.17430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17430]] Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection(https://arxiv.org/abs/2601.17430)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on this https URL</li>
</ul>

<h3>Title: Data-driven Clustering and Merging of Adapters for On-device Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ondrej Bohdal, Taha Ceritli, Mete Ozay, Jijoong Moon, Kyeng-Hun Lee, Hyeonmok Ko, Umberto Michieli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17441">https://arxiv.org/abs/2601.17441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17441">https://arxiv.org/pdf/2601.17441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17441]] Data-driven Clustering and Merging of Adapters for On-device Large Language Models(https://arxiv.org/abs/2601.17441)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.</li>
</ul>

<h3>Title: Clustering-driven Memory Compression for On-device Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ondrej Bohdal, Pramit Saha, Umberto Michieli, Mete Ozay, Taha Ceritli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17443">https://arxiv.org/abs/2601.17443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17443">https://arxiv.org/pdf/2601.17443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17443]] Clustering-driven Memory Compression for On-device Large Language Models(https://arxiv.org/abs/2601.17443)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.</li>
</ul>

<h3>Title: ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation</h3>
<ul>
<li><strong>Authors: </strong>Chia-Ming Lee, Yu-Fan Lin, Jing-Hui Jung, Yu-Jou Hsiao, Chih-Chung Hsu, Yu-Lun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17468">https://arxiv.org/abs/2601.17468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17468">https://arxiv.org/pdf/2601.17468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17468]] ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation(https://arxiv.org/abs/2601.17468)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at this https URL.</li>
</ul>

<h3>Title: Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction</h3>
<ul>
<li><strong>Authors: </strong>Wei Ju, Wei Zhang, Siyu Yi, Zhengyang Mao, Yifan Wang, Jingyang Yuan, Zhiping Xiao, Ziyue Qiao, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17469">https://arxiv.org/abs/2601.17469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17469">https://arxiv.org/pdf/2601.17469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17469]] Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction(https://arxiv.org/abs/2601.17469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.</li>
</ul>

<h3>Title: PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors</h3>
<ul>
<li><strong>Authors: </strong>Chia-Ming Lee, Yu-Fan Lin, Yu-Jou Hsiao, Jing-Hui Jung, Yu-Lun Liu, Chih-Chung Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17470">https://arxiv.org/abs/2601.17470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17470">https://arxiv.org/pdf/2601.17470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17470]] PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors(https://arxiv.org/abs/2601.17470)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at this https URL.</li>
</ul>

<h3>Title: PatchIsland: Orchestration of LLM Agents for Continuous Vulnerability Repair</h3>
<ul>
<li><strong>Authors: </strong>Wonyoung Kim, Seunggi Min, Minjae Gwon, Dowoo Baik, Haein Lee, Hyeon Heo, Minjae Lee, Min Woo Baek, Yonghwi Jin, Younggi Park, Yunjae Choi, Taesoo Kim, Sangdon Park, Insu Yun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17471">https://arxiv.org/abs/2601.17471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17471">https://arxiv.org/pdf/2601.17471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17471]] PatchIsland: Orchestration of LLM Agents for Continuous Vulnerability Repair(https://arxiv.org/abs/2601.17471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continuous fuzzing platforms such as OSS-Fuzz uncover large numbers of vulnerabilities, yet the subsequent repair process remains largely manual. Unfortunately, existing Automated Vulnerability Repair (AVR) techniques -- including recent LLM-based systems -- are not directly applicable to continuous fuzzing. This is because these systems are designed and evaluated on a static, single-run benchmark setting, making them ill-suited for the diverse, noisy, and failure-prone environments in continuous fuzzing. To address these issues, we introduce PatchIsland, a system for Continuous Vulnerability Repair (CVR) that tightly integrates with continuous fuzzing pipelines. PatchIsland employs an ensemble of diverse LLM agents. By leveraging multiple LLM agents, PatchIsland can cover a wider range of settings (e.g., different projects, bug types, and programming languages) and also improve operational robustness. In addition, PatchIsland utilizes a two-phase patch-based deduplication to mitigate duplicate crashes and patches, which can be problematic in continuous fuzzing. In our internal evaluation, PatchIsland repaired 84 of 92 vulnerabilities, demonstrating strong repair capability. In the official AIxCC competition, the system operated with no human intervention in a fully autonomous environment and successfully patched 31 out of 43 vulnerabilities, achieving a repair rate of 72.1\%.</li>
</ul>

<h3>Title: LeanTutor: Towards a Verified AI Mathematical Proof Tutor</h3>
<ul>
<li><strong>Authors: </strong>Manooshree Patel, Rayna Bhattacharyya, Thomas Lu, Arnav Mehta, Niels Voss, Narges Norouzi, Gireeja Ranade</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17473">https://arxiv.org/abs/2601.17473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17473">https://arxiv.org/pdf/2601.17473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17473]] LeanTutor: Towards a Verified AI Mathematical Proof Tutor(https://arxiv.org/abs/2601.17473)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.</li>
</ul>

<h3>Title: Unintended Memorization of Sensitive Information in Fine-Tuned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Marton Szep, Jorge Marin Ruiz, Georgios Kaissis, Paulina Seidl, Rüdiger von Eisenhart-Rothe, Florian Hinterwimmer, Daniel Rueckert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17480">https://arxiv.org/abs/2601.17480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17480">https://arxiv.org/pdf/2601.17480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17480]] Unintended Memorization of Sensitive Information in Fine-Tuned Language Models(https://arxiv.org/abs/2601.17480)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.</li>
</ul>

<h3>Title: SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Bajpai, Akshat Bhandari, Akshay Nambi, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17489">https://arxiv.org/abs/2601.17489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17489">https://arxiv.org/pdf/2601.17489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17489]] SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving(https://arxiv.org/abs/2601.17489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.</li>
</ul>

<h3>Title: On the Impossibility of Simulation Security for Quantum Functional Encryption</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Barhoush, Arthur Mehta, Anne Müller, Louis Salvail</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17497">https://arxiv.org/abs/2601.17497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17497">https://arxiv.org/pdf/2601.17497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17497]] On the Impossibility of Simulation Security for Quantum Functional Encryption(https://arxiv.org/abs/2601.17497)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Functional encryption is a powerful cryptographic primitive that enables fine-grained access to encrypted data and underlies numerous applications. Although the ideal security notion for FE (simulation security) has been shown to be impossible in the classical setting, those impossibility results rely on inherently classical arguments. This leaves open the question of whether simulation-secure functional encryption can be achieved in the quantum regime. In this work, we rule out this possibility by showing that the classical impossibility results largely extend to the quantum world. In particular, when the adversary can issue an unbounded number of challenge messages, we prove an unconditional impossibility, matching the classical barrier. In the case where the adversary may obtain many functional keys, classical arguments only yield impossibility under the assumption of pseudorandom functions; we strengthen this by proving impossibility under the potentially weaker assumption of pseudorandom quantum states. In the same setting, we also establish an alternative impossibility based on public-key encryption. Since public-key encryption is not known to imply pseudorandom quantum states, this provides independent evidence of the barrier. As part of our proofs, we show a novel incompressibility property for pseudorandom states, which may be of independent interest.</li>
</ul>

<h3>Title: BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang, Zehua Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17504">https://arxiv.org/abs/2601.17504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17504">https://arxiv.org/pdf/2601.17504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17504]] BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation(https://arxiv.org/abs/2601.17504)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL.</li>
</ul>

<h3>Title: One-Shot Federated Clustering of Non-Independent Completely Distributed Data</h3>
<ul>
<li><strong>Authors: </strong>Yiqun Zhang, Shenghong Cai, Zihua Yang, Sen Feng, Yuzhu Ji, Haijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17512">https://arxiv.org/abs/2601.17512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17512">https://arxiv.org/pdf/2601.17512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17512]] One-Shot Federated Clustering of Non-Independent Completely Distributed Data(https://arxiv.org/abs/2601.17512)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.</li>
</ul>

<h3>Title: FMIR, a foundation model-based Image Registration Framework for Robust Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Fengting Zhang, Yue He, Qinghao Liu, Yaonan Wang, Xiang Chen, Hang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17529">https://arxiv.org/abs/2601.17529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17529">https://arxiv.org/pdf/2601.17529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17529]] FMIR, a foundation model-based Image Registration Framework for Robust Image Registration(https://arxiv.org/abs/2601.17529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this this http URL a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain this http URL approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at this https URL.</li>
</ul>

<h3>Title: Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes</h3>
<ul>
<li><strong>Authors: </strong>Gautam Siddharth Kashyap, Harsh Joshi, Niharika Jain, Ebad Shabbir, Jiechao Gao, Nipun Joshi, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17530">https://arxiv.org/abs/2601.17530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17530">https://arxiv.org/pdf/2601.17530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17530]] Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes(https://arxiv.org/abs/2601.17530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.</li>
</ul>

<h3>Title: Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17532">https://arxiv.org/abs/2601.17532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17532">https://arxiv.org/pdf/2601.17532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17532]] Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection(https://arxiv.org/abs/2601.17532)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.</li>
</ul>

<h3>Title: Reconstructing Training Data from Adapter-based Federated Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17533">https://arxiv.org/abs/2601.17533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17533">https://arxiv.org/pdf/2601.17533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17533]] Reconstructing Training Data from Adapter-based Federated Large Language Models(https://arxiv.org/abs/2601.17533)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, federate, large language model</a></li>
<li><strong>Abstract: </strong>Adapter-based Federated Large Language Models (FedLLMs) are widely adopted to reduce the computational, storage, and communication overhead of full-parameter fine-tuning for web-scale applications while preserving user privacy. By freezing the backbone and training only compact low-rank adapters, these methods appear to limit gradient leakage and thwart existing Gradient Inversion Attacks (GIAs). Contrary to this assumption, we show that low-rank adapters create new, exploitable leakage channels. We propose the Unordered-word-bag-based Text Reconstruction (UTR) attack, a novel GIA tailored to the unique structure of adapter-based FedLLMs. UTR overcomes three core challenges: low-dimensional gradients, frozen backbones, and combinatorially large reconstruction spaces by: (i) inferring token presence from attention patterns in frozen layers, (ii) performing sentence-level inversion within the low-rank subspace of adapter gradients, and (iii) enforcing semantic coherence through constrained greedy decoding guided by language priors. Extensive experiments across diverse models (GPT2-Large, BERT, Qwen2.5-7B) and datasets (CoLA, SST-2, Rotten Tomatoes) demonstrate that UTR achieves near-perfect reconstruction accuracy (ROUGE-1/2 > 99), even with large batch size settings where prior GIAs fail completely. Our results reveal a fundamental tension between parameter efficiency and privacy in FedLLMs, challenging the prevailing belief that lightweight adaptation inherently enhances security. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: OTI: A Model-free and Visually Interpretable Measure of Image Attackability</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Liang, Haowei Liu, Chi-Man Pun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17536">https://arxiv.org/abs/2601.17536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17536">https://arxiv.org/pdf/2601.17536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17536]] OTI: A Model-free and Visually Interpretable Measure of Image Attackability(https://arxiv.org/abs/2601.17536)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability</a></li>
<li><strong>Abstract: </strong>Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.</li>
</ul>

<h3>Title: CTF for education</h3>
<ul>
<li><strong>Authors: </strong>Yi Lyu, Luke Dotson, Nic Draves, Andy Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17543">https://arxiv.org/abs/2601.17543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17543">https://arxiv.org/pdf/2601.17543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17543]] CTF for education(https://arxiv.org/abs/2601.17543)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>In this paper, we take a close look at how CTF can be used in cybersecurity education. We divide the CTF competitions into four different categories, which are attack-based CTFs, defense-based CTFs, jeopardy CTFs and gamified and wargames CTFs. We start our analysis by summarizing the main characteristics of different CTF types. We then compare them with each other in both learning objectives and other aspects like accessibility. We conclude that combining all four CTF formats can help participants build one's cybersecurity knowledge. By doing that, we hope that our findings will provide some useful insights for future CTF educators.</li>
</ul>

<h3>Title: Prompt Injection Attacks on Agentic Coding Assistants: A Systematic Analysis of Vulnerabilities in Skills, Tools, and Protocol Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Narek Maloyan, Dmitry Namiot</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17548">https://arxiv.org/abs/2601.17548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17548">https://arxiv.org/pdf/2601.17548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17548]] Prompt Injection Attacks on Agentic Coding Assistants: A Systematic Analysis of Vulnerabilities in Skills, Tools, and Protocol Ecosystems(https://arxiv.org/abs/2601.17548)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of agentic AI coding assistants, including Claude Code, GitHub Copilot, Cursor, and emerging skill-based architectures, has fundamentally transformed software development workflows. These systems leverage Large Language Models (LLMs) integrated with external tools, file systems, and shell access through protocols like the Model Context Protocol (MCP). However, this expanded capability surface introduces critical security vulnerabilities. In this \textbf{Systematization of Knowledge (SoK)} paper, we present a comprehensive analysis of prompt injection attacks targeting agentic coding assistants. We propose a novel three-dimensional taxonomy categorizing attacks across \textit{delivery vectors}, \textit{attack modalities}, and \textit{propagation behaviors}. Our meta-analysis synthesizes findings from 78 recent studies (2021--2026), consolidating evidence that attack success rates against state-of-the-art defenses exceed 85\% when adaptive attack strategies are employed. We systematically catalog 42 distinct attack techniques spanning input manipulation, tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning. Through critical analysis of 18 defense mechanisms reported in prior work, we identify that most achieve less than 50\% mitigation against sophisticated adaptive attacks. We contribute: (1) a unified taxonomy bridging disparate attack classifications, (2) the first systematic analysis of skill-based architecture vulnerabilities with concrete exploit chains, and (3) a defense-in-depth framework grounded in the limitations we identify. Our findings indicate that the security community must treat prompt injection as a first-class vulnerability class requiring architectural-level mitigations rather than ad-hoc filtering approaches.</li>
</ul>

<h3>Title: Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Narek Maloyan, Dmitry Namiot</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17549">https://arxiv.org/abs/2601.17549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17549">https://arxiv.org/pdf/2601.17549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17549]] Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents(https://arxiv.org/abs/2601.17549)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) has emerged as a de facto standard for integrating Large Language Models with external tools, yet no formal security analysis of the protocol specification exists. We present the first rigorous security analysis of MCP's architectural design, identifying three fundamental protocol-level vulnerabilities: (1) absence of capability attestation allowing servers to claim arbitrary permissions, (2) bidirectional sampling without origin authentication enabling server-side prompt injection, and (3) implicit trust propagation in multi-server configurations. We implement \textsc{MCPBench}, a novel framework bridging existing agent security benchmarks to MCP-compliant infrastructure, enabling direct measurement of protocol-specific attack surfaces. Through controlled experiments on 847 attack scenarios across five MCP server implementations, we demonstrate that MCP's architectural choices amplify attack success rates by 23--41\% compared to equivalent non-MCP integrations. We propose \textsc{MCPSec}, a backward-compatible protocol extension adding capability attestation and message authentication, reducing attack success rates from 52.8\% to 12.4\% with median latency overhead of 8.3ms per message. Our findings establish that MCP's security weaknesses are architectural rather than implementation-specific, requiring protocol-level remediation.</li>
</ul>

<h3>Title: Private Iris Recognition with High-Performance FHE</h3>
<ul>
<li><strong>Authors: </strong>Jincheol Ha, Guillaume Hanrot, Taeyeong Noh, Jung Hee Cheon, Jung Woo Kim, Damien Stehlé</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17561">https://arxiv.org/abs/2601.17561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17561">https://arxiv.org/pdf/2601.17561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17561]] Private Iris Recognition with High-Performance FHE(https://arxiv.org/abs/2601.17561)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, biometric</a></li>
<li><strong>Abstract: </strong>Among biometric verification systems, irises stand out because they offer high accuracy even in large-scale databases. For example, the World ID project aims to provide authentication to all humans via iris recognition, with millions already registered. Storing such biometric data raises privacy concerns, which can be addressed using privacy-enhancing techniques. Bloemen et al. describe a solution based on 2-out-of-3 Secret-Sharing Multiparty Computation (SS-MPC), for the World ID setup. In terms of security, unless an adversary corrupts 2~servers, the iris codes remain confidential and nothing leaks beyond the result of the computation. Their solution is able to match~$32$ users against a database of~$2^{22}$ iris codes in~$\approx 2$s , using~24 H100 GPUs, more than 40~communication rounds and $81$GB/party of data transferred (the timing assumes a network speed above~3Tb/s). In the present work, we explore the use of Threshold Fully Homomorphic Encryption (ThFHE) for the same task. The ThFHE solution brings a number of security advantages: no trusted setup, the encrypted database and queries can be public, the secret can be distributed among many parties, and active security can be added without significant performance degradation. Our proof-of-concept implementation of the computation phase handles $32$~eyes against a database of $7\cdot 2^{14}$ iris codes in~$\approx 1.8$s ($\approx 0.33s$ for 4 eyes against the same database), using 8 RTX-5090 GPUs. To this, one should add~2 to 3 rounds of communication (depending on deployment choice). We perform the matching using the CKKS (Th)FHE scheme. Our main technical ingredients are the use of recent progress on FHE-based linear algebra boosted using int8 GPU operations, and the introduction of a technique reducing the number of ciphertexts to be processed as early as possible.</li>
</ul>

<h3>Title: Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17566">https://arxiv.org/abs/2601.17566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17566">https://arxiv.org/pdf/2601.17566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17566]] Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning(https://arxiv.org/abs/2601.17566)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.</li>
</ul>

<h3>Title: Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17569">https://arxiv.org/abs/2601.17569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17569">https://arxiv.org/pdf/2601.17569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17569]] Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations(https://arxiv.org/abs/2601.17569)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.</li>
</ul>

<h3>Title: Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Doerrich, Francesco Di Salvo, Jonas Alle, Christian Ledig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17586">https://arxiv.org/abs/2601.17586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17586">https://arxiv.org/pdf/2601.17586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17586]] Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization(https://arxiv.org/abs/2601.17586)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at this https URL .</li>
</ul>

<h3>Title: From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianjun Zhong, Linyang He, Nima Mesgarani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17593">https://arxiv.org/abs/2601.17593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17593">https://arxiv.org/pdf/2601.17593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17593]] From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs(https://arxiv.org/abs/2601.17593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question. In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.</li>
</ul>

<h3>Title: Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout</h3>
<ul>
<li><strong>Authors: </strong>Xuanzhou Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17602">https://arxiv.org/abs/2601.17602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17602">https://arxiv.org/pdf/2601.17602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17602]] Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout(https://arxiv.org/abs/2601.17602)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.</li>
</ul>

<h3>Title: What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization</h3>
<ul>
<li><strong>Authors: </strong>Sara Rezaeimanesh, Mohammad M. Ghassemi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17609">https://arxiv.org/abs/2601.17609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17609">https://arxiv.org/pdf/2601.17609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17609]] What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization(https://arxiv.org/abs/2601.17609)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.</li>
</ul>

<h3>Title: Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Fatema Siddika, Md Anwar Hossen, Tanwi Mallick, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17616">https://arxiv.org/abs/2601.17616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17616">https://arxiv.org/pdf/2601.17616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17616]] Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning(https://arxiv.org/abs/2601.17616)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.</li>
</ul>

<h3>Title: Reconstructing Protected Biometric Templates from Binary Authentication Results</h3>
<ul>
<li><strong>Authors: </strong>Eliron Rahimi, Margarita Osadchy, Orr Dunkelman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17620">https://arxiv.org/abs/2601.17620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17620">https://arxiv.org/pdf/2601.17620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17620]] Reconstructing Protected Biometric Templates from Binary Authentication Results(https://arxiv.org/abs/2601.17620)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, biometric, extraction, generative</a></li>
<li><strong>Abstract: </strong>Biometric data is considered to be very private and highly sensitive. As such, many methods for biometric template protection were considered over the years -- from biohashing and specially crafted feature extraction procedures, to the use of cryptographic solutions such as Fuzzy Commitments or the use of Fully Homomorphic Encryption (FHE). A key question that arises is how much protection these solutions can offer when the adversary can inject samples, and observe the outputs of the system. While for systems that return the similarity score, one can use attacks such as hill-climbing, for systems where the adversary can only learn whether the authentication attempt was successful, this question remained open. In this paper, we show that it is indeed possible to reconstruct the biometric template by just observing the success/failure of the authentication attempt (given the ability to inject a sufficient amount of templates). Our attack achieves negligible template reconstruction loss and enables full recovery of facial images through a generative inversion method, forming a pipeline from binary scores to high-resolution facial images that successfully pass the system more than 98\% of the time. Our results, of course, are applicable for any protection mechanism that maintains the accuracy of the recognition.</li>
</ul>

<h3>Title: BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Xie, Jinhan Liu, Xiaoyong Ni, Fei Tan, Icare Sakr, Thibault Collin, Shiqi Sun, Alejandro Rodriguez Guajardo, Demon Fanny, Charles-francois Vincent Latchoumane, Henri Lorach, Jocelyne Bloch, Gregoire Courtine, Mahsa Shoaran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17625">https://arxiv.org/abs/2601.17625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17625">https://arxiv.org/pdf/2601.17625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17625]] BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation(https://arxiv.org/abs/2601.17625)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.</li>
</ul>

<h3>Title: RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Ryan A. Canfield, Tomohiro Ouchi, Beatrice Macagno, Eli Shlizerman, Amy L. Orsborn</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17641">https://arxiv.org/abs/2601.17641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17641">https://arxiv.org/pdf/2601.17641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17641]] RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding(https://arxiv.org/abs/2601.17641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.</li>
</ul>

<h3>Title: A Systemic Evaluation of Multimodal RAG Privacy</h3>
<ul>
<li><strong>Authors: </strong>Ali Al-Lawati, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17644">https://arxiv.org/abs/2601.17644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17644">https://arxiv.org/pdf/2601.17644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17644]] A Systemic Evaluation of Multimodal RAG Privacy(https://arxiv.org/abs/2601.17644)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.</li>
</ul>

<h3>Title: SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Taewan Cho, Taeryang Kim, Andrew Jaeyong Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17657">https://arxiv.org/abs/2601.17657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17657">https://arxiv.org/pdf/2601.17657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17657]] SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation(https://arxiv.org/abs/2601.17657)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at this https URL</li>
</ul>

<h3>Title: A PUF-Based Security Framework for Fault and Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Oun, Rishabh Das, Clay Hess, Aakriti Barat, Savas Kaya</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17661">https://arxiv.org/abs/2601.17661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17661">https://arxiv.org/pdf/2601.17661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17661]] A PUF-Based Security Framework for Fault and Intrusion Detection(https://arxiv.org/abs/2601.17661)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Industrial Control Systems (ICS) rely on sensor feedback to keep safety-critical processes within operational limits. This research presents a hardware-root-of-trust that embeds a Physically Unclonable Function (PUF) at the measurement layer to authenticate sensor readings. The architecture combines voltage fingerprinting with a temporal authentication that integrates with standard industrial control system architecture. The research prototypes the PUF integration on a hardware-in-the-loop (HIL) water tank testbed using a Simulink-based PUF emulator. The system maintains 99.97% accuracy over a 5.18-hour period of normal operation and flags all injected anomalies, including spike faults, hard-over faults, and hardware trojan scenarios that push the system over to an unsafe operational state. The proposed architecture provides a process-aware, vendor-agnostic approach that can integrate with legacy plants to detect sensor signal degradation or sophisticated supply chain attacks.</li>
</ul>

<h3>Title: UrduLM: A Resource-Efficient Monolingual Urdu Language Model</h3>
<ul>
<li><strong>Authors: </strong>Syed Muhammad Ali, Hammad Sajid, Zainab Haider, Ali Muhammad Asad, Haya Fatima, Abdul Samad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17664">https://arxiv.org/abs/2601.17664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17664">https://arxiv.org/pdf/2601.17664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17664]] UrduLM: A Resource-Efficient Monolingual Urdu Language Model(https://arxiv.org/abs/2601.17664)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.</li>
</ul>

<h3>Title: Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Pan, Yuhao Chen, Fengqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17666">https://arxiv.org/abs/2601.17666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17666">https://arxiv.org/pdf/2601.17666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17666]] Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting(https://arxiv.org/abs/2601.17666)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.</li>
</ul>

<h3>Title: Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction</h3>
<ul>
<li><strong>Authors: </strong>Jang-Hyun Kim, Dongyoon Han, Sangdoo Yun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17668">https://arxiv.org/abs/2601.17668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17668">https://arxiv.org/pdf/2601.17668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17668]] Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction(https://arxiv.org/abs/2601.17668)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.</li>
</ul>

<h3>Title: Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chunxu Zhao, Xin Huang, Xue Han, Shujian Huang, Chao Deng, Junlan Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17671">https://arxiv.org/abs/2601.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17671">https://arxiv.org/pdf/2601.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17671]] Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning(https://arxiv.org/abs/2601.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.</li>
</ul>

<h3>Title: Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng, Yu Wang, Zhiyuan Yan, Yonghong Tian, Yu Li, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17687">https://arxiv.org/abs/2601.17687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17687">https://arxiv.org/pdf/2601.17687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17687]] Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis(https://arxiv.org/abs/2601.17687)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.</li>
</ul>

<h3>Title: REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization</h3>
<ul>
<li><strong>Authors: </strong>Shanu Saklani, Tushar M. Athawale, Nairita Pal, David Pugmire, Christopher R. Johnson, Soumya Dutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17689">https://arxiv.org/abs/2601.17689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17689">https://arxiv.org/pdf/2601.17689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17689]] REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization(https://arxiv.org/abs/2601.17689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.</li>
</ul>

<h3>Title: StyleDecoupler: Generalizable Artistic Style Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Zexi Jia, Jinchao Zhang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17697">https://arxiv.org/abs/2601.17697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17697">https://arxiv.org/pdf/2601.17697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17697]] StyleDecoupler: Generalizable Artistic Style Disentanglement(https://arxiv.org/abs/2601.17697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.</li>
</ul>

<h3>Title: S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Qingsen Ma, Dianyun Wang, Yaoye Wang, Lechen Ning, Sujie Zhu, Xiaohang Zhang, Jiaming Lyu, Linhao Ren, Zhenbo Xu, Zhaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17702">https://arxiv.org/abs/2601.17702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17702">https://arxiv.org/pdf/2601.17702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17702]] S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference(https://arxiv.org/abs/2601.17702)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages. We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size. At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.</li>
</ul>

<h3>Title: An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Kadivar, Guansheng Li, Jianlu Zheng, John M. Higgins, Ming Dao, George Em Karniadakis, Mengjia Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17703">https://arxiv.org/abs/2601.17703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17703">https://arxiv.org/pdf/2601.17703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17703]] An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays(https://arxiv.org/abs/2601.17703)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.</li>
</ul>

<h3>Title: A Computational Approach to Visual Metonymy</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17706">https://arxiv.org/abs/2601.17706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17706">https://arxiv.org/pdf/2601.17706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17706]] A Computational Approach to Visual Metonymy(https://arxiv.org/abs/2601.17706)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices</h3>
<ul>
<li><strong>Authors: </strong>Kaile Wang, Jiannong Cao, Yu Yang, Xiaoyin Li, Yinfeng Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17713">https://arxiv.org/abs/2601.17713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17713">https://arxiv.org/pdf/2601.17713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17713]] FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices(https://arxiv.org/abs/2601.17713)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, federate</a></li>
<li><strong>Abstract: </strong>With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.</li>
</ul>

<h3>Title: Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games</h3>
<ul>
<li><strong>Authors: </strong>Daniel M. Pedrozo, Telma W. de L. Soares, Bryan L. M. de Oliveira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17716">https://arxiv.org/abs/2601.17716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17716">https://arxiv.org/pdf/2601.17716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17716]] Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games(https://arxiv.org/abs/2601.17716)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.</li>
</ul>

<h3>Title: Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles</h3>
<ul>
<li><strong>Authors: </strong>Junran Lu, Yuanqi Li, Hengji Li, Jie Guo, Yanwen Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17733">https://arxiv.org/abs/2601.17733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17733">https://arxiv.org/pdf/2601.17733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17733]] Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles(https://arxiv.org/abs/2601.17733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Boundary Representation (B-Rep) is the widely adopted standard in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness. We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.</li>
</ul>

<h3>Title: ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Park, Sanghyeok Lee, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17755">https://arxiv.org/abs/2601.17755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17755">https://arxiv.org/pdf/2601.17755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17755]] ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation(https://arxiv.org/abs/2601.17755)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.</li>
</ul>

<h3>Title: AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongjie Cheng, Ruifeng Yuan, Yongqi Li, Runyang You, Wenjie Wang, Liqiang Nie, Lei Zhang, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17761">https://arxiv.org/abs/2601.17761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17761">https://arxiv.org/pdf/2601.17761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17761]] AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation(https://arxiv.org/abs/2601.17761)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.</li>
</ul>

<h3>Title: Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali</h3>
<ul>
<li><strong>Authors: </strong>Md Asgor Hossain Reaj, Rajan Das Gupta, Jui Saha Pritha, Abdullah Al Noman, Abir Ahmed, Golam Md Mohiuddin, Tze Hui Liew</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17764">https://arxiv.org/abs/2601.17764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17764">https://arxiv.org/pdf/2601.17764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17764]] Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali(https://arxiv.org/abs/2601.17764)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.</li>
</ul>

<h3>Title: DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Liu, Xiaoyu Guan, Di Liang, Xianjie Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17777">https://arxiv.org/abs/2601.17777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17777">https://arxiv.org/pdf/2601.17777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17777]] DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning(https://arxiv.org/abs/2601.17777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.</li>
</ul>

<h3>Title: Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics</h3>
<ul>
<li><strong>Authors: </strong>Md Sahidullah, Hye-jin Shim, Rosa Gonzalez Hautamäki, Tomi H. Kinnunen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17782">https://arxiv.org/abs/2601.17782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17782">https://arxiv.org/pdf/2601.17782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17782]] Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics(https://arxiv.org/abs/2601.17782)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.</li>
</ul>

<h3>Title: Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Tushar Jain</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17785">https://arxiv.org/abs/2601.17785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17785">https://arxiv.org/pdf/2601.17785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17785]] Performance Analysis of Quantum-Secure Digital Signature Algorithms in Blockchain(https://arxiv.org/abs/2601.17785)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The long-term security of public blockchains strictly depends on the hardness assumptions of the underlying digital signature schemes. In the current scenario, most deployed cryptocurrencies and blockchain platforms rely on elliptic-curve cryptography, which is vulnerable to quantum attacks due to Shor's algorithm. Therefore, it is important to understand how post-quantum (PQ) digital signatures behave when integrated into real blockchain systems. This report presents a blockchain prototype that supports multiple quantum-secure signature algorithms, focusing on CRYSTALS-Dilithium, Falcon and Hawk as lattice-based schemes. This report also describes the design of the prototype and discusses the performance metrics, which include key generation, signing, verification times, key sizes and signature sizes. This report covers the problem, background, and experimental methodology, also providing a detailed comparison of quantum-secure signatures in a blockchain context and extending the analysis to schemes such as HAETAE.</li>
</ul>

<h3>Title: Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data</h3>
<ul>
<li><strong>Authors: </strong>A. Brawanski, Th. Schaffer, F. Raab, K.-M. Schebesch, M. Schrey, Chr. Doenitz, A. M. Tomé, E. W. Lang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17802">https://arxiv.org/abs/2601.17802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17802">https://arxiv.org/pdf/2601.17802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17802]] Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data(https://arxiv.org/abs/2601.17802)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.</li>
</ul>

<h3>Title: Multi-Agent Collaborative Intrusion Detection for Low-Altitude Economy IoT: An LLM-Enhanced Agentic AI Framework</h3>
<ul>
<li><strong>Authors: </strong>Hongjuan Li, Hui Kang, Jiahui Li, Geng Sun, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Wei Ni, Abbas Jamalipour</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17817">https://arxiv.org/abs/2601.17817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17817">https://arxiv.org/pdf/2601.17817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17817]] Multi-Agent Collaborative Intrusion Detection for Low-Altitude Economy IoT: An LLM-Enhanced Agentic AI Framework(https://arxiv.org/abs/2601.17817)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of low-altitude economy Internet of Things (LAE-IoT) networks has created unprecedented security challenges due to dynamic three-dimensional mobility patterns, distributed autonomous operations, and severe resource constraints. Traditional intrusion detection systems designed for static ground-based networks prove inadequate for tackling the unique characteristics of aerial IoT environments, including frequent topology changes, real-time detection requirements, and energy limitations. In this article, we analyze the intrusion detection requirements for LAE-IoT networks, complemented by a comprehensive review of evaluation metrics that cover detection effectiveness, response time, and resource consumption. Then, we investigate transformative potential of agentic artificial intelligence (AI) paradigms and introduce a large language model (LLM)-enabled agentic AI framework for enhancing intrusion detection in LAE-IoT networks. This leads to our proposal of a novel multi-agent collaborative intrusion detection framework that leverages specialized LLM-enhanced agents for intelligent data processing and adaptive classification. Through experimental validation, our framework demonstrates superior performance of over 90\% classification accuracy across multiple benchmark datasets. These results highlight the transformative potential of combining agentic AI principles with LLMs for next-generation LAE-IoT security systems.</li>
</ul>

<h3>Title: ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning</h3>
<ul>
<li><strong>Authors: </strong>Wen Luo, Peng Chen, Xiaotao Huang, LiQun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17818">https://arxiv.org/abs/2601.17818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17818">https://arxiv.org/pdf/2601.17818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17818]] ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning(https://arxiv.org/abs/2601.17818)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.</li>
</ul>

<h3>Title: DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation</h3>
<ul>
<li><strong>Authors: </strong>Pranav Kasela, Marco Braga, Alessandro Ghiotto, Andrea Pilzer, Marco Viviani, Alessandro Raganato</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17823">https://arxiv.org/abs/2601.17823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17823">https://arxiv.org/pdf/2601.17823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17823]] DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation(https://arxiv.org/abs/2601.17823)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. this https URL</li>
</ul>

<h3>Title: Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents</h3>
<ul>
<li><strong>Authors: </strong>Dan Greenstein, Zohar Karnin, Chen Amiraz, Oren Somekh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17829">https://arxiv.org/abs/2601.17829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17829">https://arxiv.org/pdf/2601.17829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17829]] Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents(https://arxiv.org/abs/2601.17829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.</li>
</ul>

<h3>Title: VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Wang, Dengyang Jiang, Liuzhuozheng Li, Yucheng Lin, Guojiang Shen, Xiangjie Kong, Yong Liu, Guang Dai, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17830">https://arxiv.org/abs/2601.17830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17830">https://arxiv.org/pdf/2601.17830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17830]] VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training(https://arxiv.org/abs/2601.17830)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</li>
</ul>

<h3>Title: An Effective and Cost-Efficient Agentic Framework for Ethereum Smart Contract Auditing</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Hu, Wun Yu Chan, Yuejie Shi, Qumeng Sun, Wei-Cheng Wang, Chiachih Wu, Haoyu Wang, Ningyu He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17833">https://arxiv.org/abs/2601.17833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17833">https://arxiv.org/pdf/2601.17833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17833]] An Effective and Cost-Efficient Agentic Framework for Ethereum Smart Contract Auditing(https://arxiv.org/abs/2601.17833)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Smart contract security is paramount, but identifying intricate business logic vulnerabilities remains a persistent challenge because existing solutions consistently fall short: manual auditing is unscalable, static analysis tools are plagued by false positives, and fuzzers struggle to navigate deep logic states within complex systems. Even emerging AI-based methods suffer from hallucinations, context constraints, and a heavy reliance on expensive, proprietary Large Language Models. In this paper, we introduce Heimdallr, an automated auditing agent designed to overcome these hurdles through four core innovations. By reorganizing code at the function level, Heimdallr minimizes context overhead while preserving essential business logic. It then employs heuristic reasoning to detect complex vulnerabilities and automatically chain functional exploits. Finally, a cascaded verification layer validates these findings to eliminate false positives. Notably, this approach achieves high performance on lightweight, open-source models like GPToss-120B without relying on proprietary systems. Our evaluations demonstrate exceptional performance, as Heimdallr successfully reconstructed 17 out of 20 real-world attacks post June 2025, resulting in total losses of $384M, and uncovered 4 confirmed zero-day vulnerabilities that safeguarded $400M in TVL. Compared to SOTA baselines including both official industrial tools and academic tools, Heimdallr at most reduces analysis time by 97.59% and financial costs by 98.77% while boosting detection precision by over 93.66%. Notably, when applied to auditing contests, Heimdallr can achieve a 92.45% detection rate at a negligible cost of $2.31 per 10K LOC. We provide production-ready auditing services and release valuable benchmarks for future work.</li>
</ul>

<h3>Title: Geometry-Grounded Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17835">https://arxiv.org/abs/2601.17835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17835">https://arxiv.org/pdf/2601.17835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17835]] Geometry-Grounded Gaussian Splatting(https://arxiv.org/abs/2601.17835)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.</li>
</ul>

<h3>Title: EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy</h3>
<ul>
<li><strong>Authors: </strong>Lanqing Du, Yunong Li, YuJie Long, Shihong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17842">https://arxiv.org/abs/2601.17842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17842">https://arxiv.org/pdf/2601.17842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17842]] EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy(https://arxiv.org/abs/2601.17842)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.</li>
</ul>

<h3>Title: SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Lan Yang, Minghan Yang, Ke Li, Honggang Zhang, Kaiyue Pang, Yi-Zhe Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17857">https://arxiv.org/abs/2601.17857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17857">https://arxiv.org/pdf/2601.17857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17857]] SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction(https://arxiv.org/abs/2601.17857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.</li>
</ul>

<h3>Title: MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Jiapeng Wang, Changxin Tian, Kunlong Chen, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17858">https://arxiv.org/abs/2601.17858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17858">https://arxiv.org/pdf/2601.17858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17858]] MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging(https://arxiv.org/abs/2601.17858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $\rho > 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.</li>
</ul>

<h3>Title: Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment</h3>
<ul>
<li><strong>Authors: </strong>Jingsong Xia, Siqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17862">https://arxiv.org/abs/2601.17862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17862">https://arxiv.org/pdf/2601.17862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17862]] Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment(https://arxiv.org/abs/2601.17862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.</li>
</ul>

<h3>Title: D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17865">https://arxiv.org/abs/2601.17865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17865">https://arxiv.org/pdf/2601.17865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17865]] D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models(https://arxiv.org/abs/2601.17865)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.</li>
</ul>

<h3>Title: MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yoonwoo Jeong, Cheng Sun, Yu-Chiang Frank Wang, Minsu Cho, Jaesung Choe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17866">https://arxiv.org/abs/2601.17866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17866">https://arxiv.org/pdf/2601.17866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17866]] MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance(https://arxiv.org/abs/2601.17866)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</li>
</ul>

<h3>Title: VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao, Chaofan Gan, Shijie Li, Zuxuan Wu, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17868">https://arxiv.org/abs/2601.17868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17868">https://arxiv.org/pdf/2601.17868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17868]] VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding(https://arxiv.org/abs/2601.17868)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at this https URL.</li>
</ul>

<h3>Title: On the Emergence and Test-Time Use of Structural Information in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michelle Chao Chen, Moritz Miller, Bernhard Schölkopf, Siyuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17869">https://arxiv.org/abs/2601.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17869">https://arxiv.org/pdf/2601.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17869]] On the Emergence and Test-Time Use of Structural Information in Large Language Models(https://arxiv.org/abs/2601.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.</li>
</ul>

<h3>Title: The Stateless Pattern: Ephemeral Coordination as the Third Pillar of Digital Sovereignty</h3>
<ul>
<li><strong>Authors: </strong>Sean Carlin, Kevin Curran</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17875">https://arxiv.org/abs/2601.17875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17875">https://arxiv.org/pdf/2601.17875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17875]] The Stateless Pattern: Ephemeral Coordination as the Third Pillar of Digital Sovereignty(https://arxiv.org/abs/2601.17875)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>For the past three decades, the architecture of the internet has rested on two primary pillars - communication on the World Wide Web and Value such as Bitcoin/Distributed ledgers). However, a third critical pillar, Private Coordination has remained dependent on centralized intermediaries, effectively creating a surveillance architecture by default. This paper introduces the 'Stateless Pattern', a novel network topology that replaces the traditional 'Fortress' security model (database-centric) with a 'Mist' model (ephemeral relays). By utilizing client-side cryptography and self-destructing server instances, we demonstrate a protocol where the server acts as a blind medium rather than a custodian of state. We present empirical data from a live deployment (this http URL), analyzing over 1,900 requests and cache-hit ratios to validate the system's 'Zero-Knowledge' properties and institutional utility. The findings suggest that digital privacy can be commoditized as a utility, technically enforcing specific articles of the universal declaration of human rights not through policy, but through physics.</li>
</ul>

<h3>Title: EEG Foundation Models: Progresses, Benchmarking, and Open Problems</h3>
<ul>
<li><strong>Authors: </strong>Dingkun Liu, Yuheng Chen, Zhu Chen, Zhenyao Cui, Yaozhi Wen, Jiayu An, Jingwei Luo, Dongrui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17883">https://arxiv.org/abs/2601.17883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17883">https://arxiv.org/pdf/2601.17883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17883]] EEG Foundation Models: Progresses, Benchmarking, and Open Problems(https://arxiv.org/abs/2601.17883)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.</li>
</ul>

<h3>Title: Assessment of Generative Named Entity Recognition in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhan, Yile Wang, Hui Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17898">https://arxiv.org/abs/2601.17898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17898">https://arxiv.org/pdf/2601.17898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17898]] Assessment of Generative Named Entity Recognition in the Era of Large Language Models(https://arxiv.org/abs/2601.17898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at this https URL.</li>
</ul>

<h3>Title: Feature-Space Generative Models for One-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jack Foster, Kirill Paramonov, Mete Ozay, Umberto Michieli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17905">https://arxiv.org/abs/2601.17905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17905">https://arxiv.org/pdf/2601.17905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17905]] Feature-Space Generative Models for One-Shot Class-Incremental Learning(https://arxiv.org/abs/2601.17905)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.</li>
</ul>

<h3>Title: FARM: Few-shot Adaptive Malware Family Classification under Concept Drift</h3>
<ul>
<li><strong>Authors: </strong>Numan Halit Guldemir, Oluwafemi Olukoya, Jesús Martínez-del-Rincón</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17907">https://arxiv.org/abs/2601.17907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17907">https://arxiv.org/pdf/2601.17907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17907]] FARM: Few-shot Adaptive Malware Family Classification under Concept Drift(https://arxiv.org/abs/2601.17907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.</li>
</ul>

<h3>Title: From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Adriana Watson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17909">https://arxiv.org/abs/2601.17909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17909">https://arxiv.org/pdf/2601.17909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17909]] From Statistical Disclosure Control to Fair AI: Navigating Fundamental Tradeoffs in Differential Privacy(https://arxiv.org/abs/2601.17909)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Differential privacy has become the gold standard for privacy-preserving machine learning systems. Unfortunately, subsequent work has primarily fixated on the privacy-utility tradeoff, leaving the subject of fairness constraints undervalued and under-researched. This paper provides a systematic treatment connecting three threads: (1) Dalenius's impossibility results for semantic privacy, (2) Dwork's differential privacy as an achievable alternative, and (3) emerging impossibility results from the addition of a fairness requirement. Through concrete examples and technical analysis, the three-way Pareto frontier between privacy, utility, and fairness is demonstrated to showcase the fundamental limits on what can be simultaneously achieved. In this work, these limits are characterized, the impact on minority groups is demonstrated, and practical guidance for navigating these tradeoffs are provided. This forms a unified framework synthesizing scattered results to help practitioners and policymakers make informed decisions when deploying private fair learning systems.</li>
</ul>

<h3>Title: Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization</h3>
<ul>
<li><strong>Authors: </strong>Aaron R. Flouro, Shawn P. Chadwick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17910">https://arxiv.org/abs/2601.17910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17910">https://arxiv.org/pdf/2601.17910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17910]] Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization(https://arxiv.org/abs/2601.17910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.</li>
</ul>

<h3>Title: Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Heverin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17911">https://arxiv.org/abs/2601.17911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17911">https://arxiv.org/pdf/2601.17911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17911]] Prompt Injection Evaluations: Refusal Boundary Instability and Artifact-Dependent Compliance in GPT-4-Series Models(https://arxiv.org/abs/2601.17911)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prompt injection evaluations typically treat refusal as a stable, binary indicator of safety. This study challenges that paradigm by modeling refusal as a local decision boundary and examining its stability under structured perturbations. We evaluated two models, GPT-4.1 and GPT-4o, using 3,274 perturbation runs derived from refusal-inducing prompt injection attempts. Each base prompt was subjected to 25 perturbations across five structured families, with outcomes manually coded as Refusal, Partial Compliance, or Full Compliance. Using chi-square tests, logistic regression, mixed-effects modeling, and a novel Refusal Boundary Entropy (RBE) metric, we demonstrate that while both models refuse >94% of attempts, refusal instability is persistent and non-uniform. Approximately one-third of initial refusal-inducing prompts exhibited at least one "refusal escape," a transition to compliance under perturbation. We find that artifact type is a stronger predictor of refusal failure than perturbation style. Textual artifacts, such as ransomware notes, exhibited significantly higher instability, with flip rates exceeding 20%. Conversely, executable malware artifacts showed zero refusal escapes in both models. While GPT-4o demonstrated tighter refusal enforcement and lower RBE than GPT-4.1, it did not eliminate artifact-dependent risks. These findings suggest that single-prompt evaluations systematically overestimate safety robustness. We conclude that refusal behavior is a probabilistic, artifact-dependent boundary phenomenon rather than a stable binary property, requiring a shift in how LLM safety is measured and audited.</li>
</ul>

<h3>Title: Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN</h3>
<ul>
<li><strong>Authors: </strong>Qinyi Liu, Mohammad Khalil, Naman Goel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17912">https://arxiv.org/abs/2601.17912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17912">https://arxiv.org/pdf/2601.17912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17912]] Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN(https://arxiv.org/abs/2601.17912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.</li>
</ul>

<h3>Title: UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR</h3>
<ul>
<li><strong>Authors: </strong>Jialu Tang, Tong Xia, Yuan Lu, Aaqib Saeed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17916">https://arxiv.org/abs/2601.17916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17916">https://arxiv.org/pdf/2601.17916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17916]] UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR(https://arxiv.org/abs/2601.17916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.</li>
</ul>

<h3>Title: treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Xiao, Zhiwei Hao, Jianyuan Guo, Yong Luo, Jia Liu, Jie Xu, Han Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17917">https://arxiv.org/abs/2601.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17917">https://arxiv.org/pdf/2601.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17917]] treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding(https://arxiv.org/abs/2601.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at this https URL.</li>
</ul>

<h3>Title: ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhao, Qinghua Yao, Xinyuan song, Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17921">https://arxiv.org/abs/2601.17921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17921">https://arxiv.org/pdf/2601.17921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17921]] ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation(https://arxiv.org/abs/2601.17921)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.</li>
</ul>

<h3>Title: RemEdit: Efficient Diffusion Editing with Riemannian Geometry</h3>
<ul>
<li><strong>Authors: </strong>Eashan Adhikarla, Brian D. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17927">https://arxiv.org/abs/2601.17927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17927">https://arxiv.org/pdf/2601.17927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17927]] RemEdit: Efficient Diffusion Editing with Riemannian Geometry(https://arxiv.org/abs/2601.17927)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: this https URL.</li>
</ul>

<h3>Title: From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17934">https://arxiv.org/abs/2601.17934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17934">https://arxiv.org/pdf/2601.17934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17934]] From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images(https://arxiv.org/abs/2601.17934)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at this https URL.</li>
</ul>

<h3>Title: FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering</h3>
<ul>
<li><strong>Authors: </strong>Daniel Commey, Matilda Nkoom, Yousef Alsenani, Sena G. Hounsinou, Garth V. Crosby</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17935">https://arxiv.org/abs/2601.17935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17935">https://arxiv.org/pdf/2601.17935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17935]] FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering(https://arxiv.org/abs/2601.17935)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, federate, generative</a></li>
<li><strong>Abstract: </strong>Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.</li>
</ul>

<h3>Title: DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chengkun Sun, Jinqian Pan, Renjie Liang, Zhengkang Fan, Xin Miao, Jiang Bian, Jie Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17939">https://arxiv.org/abs/2601.17939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17939">https://arxiv.org/pdf/2601.17939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17939]] DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation(https://arxiv.org/abs/2601.17939)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.</li>
</ul>

<h3>Title: FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos</h3>
<ul>
<li><strong>Authors: </strong>Bora Yimenicioglu, Vishal Manikanden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17947">https://arxiv.org/abs/2601.17947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17947">https://arxiv.org/pdf/2601.17947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17947]] FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos(https://arxiv.org/abs/2601.17947)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow. Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.</li>
</ul>

<h3>Title: UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders</h3>
<ul>
<li><strong>Authors: </strong>Matthew Walmer, Saksham Suri, Anirud Aggarwal, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17950">https://arxiv.org/abs/2601.17950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17950">https://arxiv.org/pdf/2601.17950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17950]] UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders(https://arxiv.org/abs/2601.17950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.</li>
</ul>

<h3>Title: A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, Zhongtian Sun, Richard Bethlehem, Pietro Lio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17952">https://arxiv.org/abs/2601.17952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17952">https://arxiv.org/pdf/2601.17952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17952]] A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models(https://arxiv.org/abs/2601.17952)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.</li>
</ul>

<h3>Title: Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Nikos Georgoudios, Konstantinos Spiliopoulos, Justin Sirignano</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17954">https://arxiv.org/abs/2601.17954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17954">https://arxiv.org/pdf/2601.17954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17954]] Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms(https://arxiv.org/abs/2601.17954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.</li>
</ul>

<h3>Title: TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors</h3>
<ul>
<li><strong>Authors: </strong>Ido Andrew Atad, Itamar Zimerman, Shahar Katz, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17958">https://arxiv.org/abs/2601.17958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17958">https://arxiv.org/pdf/2601.17958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17958]] TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors(https://arxiv.org/abs/2601.17958)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.</li>
</ul>

<h3>Title: Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer</h3>
<ul>
<li><strong>Authors: </strong>Alon Hillel-Tuch</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17967">https://arxiv.org/abs/2601.17967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17967">https://arxiv.org/pdf/2601.17967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17967]] Data Siphoning Through Advanced Persistent Transmission Attacks At The Physical Layer(https://arxiv.org/abs/2601.17967)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Data at the physical layer transmits via media such as copper cable, fiber optic, or wireless. Physical attack vectors exist that challenge data confidentiality and availability. Protocols and encryption standards help obfuscate but often cannot keep the data type and destination secure, with limited insight into confidentiality and integrity. We will investigate the feasibility of developing an awareness and integrity protocol to help mitigate physical side-channel attacks that lead to eavesdropping of data communication and denial-of-service. Keywords: data confidentiality, siphoning, eavesdropping, person-in-the-middle, denial-of-service, physical layer attacks, nation-states</li>
</ul>

<h3>Title: LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction</h3>
<ul>
<li><strong>Authors: </strong>Junior Cedric Tonga, Chen Cecilia Liu, Iryna Gurevych, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17971">https://arxiv.org/abs/2601.17971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17971">https://arxiv.org/pdf/2601.17971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17971]] LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction(https://arxiv.org/abs/2601.17971)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.</li>
</ul>

<h3>Title: Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors</h3>
<ul>
<li><strong>Authors: </strong>Jinchen Gu, Nan Zhao, Lei Qiu, Lu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17977">https://arxiv.org/abs/2601.17977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17977">https://arxiv.org/pdf/2601.17977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17977]] Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors(https://arxiv.org/abs/2601.17977)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.</li>
</ul>

<h3>Title: Federated learning for unpaired multimodal data through a homogeneous transformer model</h3>
<ul>
<li><strong>Authors: </strong>Anders Eklund</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17986">https://arxiv.org/abs/2601.17986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17986">https://arxiv.org/pdf/2601.17986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17986]] Federated learning for unpaired multimodal data through a homogeneous transformer model(https://arxiv.org/abs/2601.17986)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, transformer</a></li>
<li><strong>Abstract: </strong>Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.</li>
</ul>

<h3>Title: Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Zheng, Huizhi Liang, Vaclav Snasel, Vito Latora, Panos Pardalos, Giuseppe Nicosia, Varun Ojha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17987">https://arxiv.org/abs/2601.17987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17987">https://arxiv.org/pdf/2601.17987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17987]] Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization(https://arxiv.org/abs/2601.17987)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.</li>
</ul>

<h3>Title: Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Shudi Weng, Ming Xiao, Mikael Skoglund</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.17995">https://arxiv.org/abs/2601.17995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.17995">https://arxiv.org/pdf/2601.17995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.17995]] Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning(https://arxiv.org/abs/2601.17995)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees</li>
</ul>

<h3>Title: MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images</h3>
<ul>
<li><strong>Authors: </strong>Aqsa Yousaf, Sint Sint Win, Megan Coffee, Habeeb Olufowobi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18001">https://arxiv.org/abs/2601.18001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18001">https://arxiv.org/pdf/2601.18001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18001]] MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images(https://arxiv.org/abs/2601.18001)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.</li>
</ul>

<h3>Title: Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection</h3>
<ul>
<li><strong>Authors: </strong>Asiegbu Miracle Kanu-Asiegbu, Nitin Jotwani, Xiaoxiao Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18008">https://arxiv.org/abs/2601.18008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18008">https://arxiv.org/pdf/2601.18008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18008]] Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection(https://arxiv.org/abs/2601.18008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.</li>
</ul>

<h3>Title: Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems</h3>
<ul>
<li><strong>Authors: </strong>Hendrika Maclean, Mert Can Cakmak, Muzakkiruddin Ahmed Mohammed, Shames Al Mandalawi, John Talburt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18012">https://arxiv.org/abs/2601.18012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18012">https://arxiv.org/pdf/2601.18012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18012]] Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems(https://arxiv.org/abs/2601.18012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.</li>
</ul>

<h3>Title: A System for Name and Address Parsing with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adeeba Tarannum, Muzakkiruddin Ahmed Mohammed, Mert Can Cakmak, Shames Al Mandalawi, John Talburt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18014">https://arxiv.org/abs/2601.18014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18014">https://arxiv.org/pdf/2601.18014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18014]] A System for Name and Address Parsing with Large Language Models(https://arxiv.org/abs/2601.18014)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.</li>
</ul>

<h3>Title: Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhuangzhi Gao, Feixiang Zhou, He Zhao, Xiuju Chen, Xiaoxin Li, Qinkai Yu, Yitian Zhao, Alena Shantsila, Gregory Y. H. Lip, Eduard Shantsila, Yalin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18045">https://arxiv.org/abs/2601.18045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18045">https://arxiv.org/pdf/2601.18045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18045]] Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation(https://arxiv.org/abs/2601.18045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.</li>
</ul>

<h3>Title: Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling</h3>
<ul>
<li><strong>Authors: </strong>Yunfei Qiu, Qiqiong Ma, Tianhua Lv, Li Fang, Shudong Zhou, Wei Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18049">https://arxiv.org/abs/2601.18049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18049">https://arxiv.org/pdf/2601.18049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18049]] Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling(https://arxiv.org/abs/2601.18049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.</li>
</ul>

<h3>Title: Addressing LLM Diversity by Infusing Random Concepts</h3>
<ul>
<li><strong>Authors: </strong>Pulin Agrawal, Prasoon Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18053">https://arxiv.org/abs/2601.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18053">https://arxiv.org/pdf/2601.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18053]] Addressing LLM Diversity by Infusing Random Concepts(https://arxiv.org/abs/2601.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.</li>
</ul>

<h3>Title: Resonant Sparse Geometry Networks</h3>
<ul>
<li><strong>Authors: </strong>Hasi Hays</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18064">https://arxiv.org/abs/2601.18064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18064">https://arxiv.org/pdf/2601.18064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18064]] Resonant Sparse Geometry Networks(https://arxiv.org/abs/2601.18064)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8% accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible neural architectures.</li>
</ul>

<h3>Title: Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aryan Roy, Zekun Wang, Christopher J. MacLellan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18065">https://arxiv.org/abs/2601.18065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18065">https://arxiv.org/pdf/2601.18065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18065]] Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models(https://arxiv.org/abs/2601.18065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.</li>
</ul>

<h3>Title: XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhang, Chenxin Sun, Chenxiong Qian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18068">https://arxiv.org/abs/2601.18068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18068">https://arxiv.org/pdf/2601.18068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18068]] XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games(https://arxiv.org/abs/2601.18068)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.</li>
</ul>

<h3>Title: Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Chouldechova, A. Feder Cooper, Solon Barocas, Abhinav Palia, Dan Vann, Hanna Wallach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18076">https://arxiv.org/abs/2601.18076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18076">https://arxiv.org/pdf/2601.18076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18076]] Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming(https://arxiv.org/abs/2601.18076)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.</li>
</ul>

<h3>Title: Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents</h3>
<ul>
<li><strong>Authors: </strong>Mahesh Ramesh, Kaousheik Jayakumar, Aswinkumar Ramkumar, Pavan Thodima, Aniket Rege</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18077">https://arxiv.org/abs/2601.18077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18077">https://arxiv.org/pdf/2601.18077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18077]] Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents(https://arxiv.org/abs/2601.18077)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.</li>
</ul>

<h3>Title: DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal</h3>
<ul>
<li><strong>Authors: </strong>Peixuan Han, Yingjie Yu, Jingjun Xu, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18081">https://arxiv.org/abs/2601.18081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18081">https://arxiv.org/pdf/2601.18081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18081]] DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal(https://arxiv.org/abs/2601.18081)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at this https URL.</li>
</ul>

<h3>Title: Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Jianshu Chao, Tianhua Lv, Qiqiong Ma, Yunfei Qiu, Li Fang, Huifang Shen, Wei Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18088">https://arxiv.org/abs/2601.18088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18088">https://arxiv.org/pdf/2601.18088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18088]] Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification(https://arxiv.org/abs/2601.18088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.</li>
</ul>

<h3>Title: LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Venmugil Elango, Nidhi Bhatia, Roger Waleffe, Rasoul Shafipour, Tomer Asida, Abhinav Khattar, Nave Assaf, Maximilian Golub, Joey Guman, Tiyasa Mitra, Ritchie Zhao, Ritika Borkar, Ran Zilberstein, Mostofa Patwary, Mohammad Shoeybi, Bita Rouhani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18089">https://arxiv.org/abs/2601.18089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18089">https://arxiv.org/pdf/2601.18089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18089]] LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts(https://arxiv.org/abs/2601.18089)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).</li>
</ul>

<h3>Title: From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models</h3>
<ul>
<li><strong>Authors: </strong>Longwei Ding, Anhao Zhao, Fanghua Ye, Ziyang Chen, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18091">https://arxiv.org/abs/2601.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18091">https://arxiv.org/pdf/2601.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18091]] From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models(https://arxiv.org/abs/2601.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Fong, Zimu Wang, Guilherme C. Oliveira, Xiangyu Zhao, Yiwen Jiang, Jiahe Liu, Beau-Luke Colton, Scott Woods, Martha E. Shenton, Barnaby Nelson, Zongyuan Ge, Dominic Dwyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18102">https://arxiv.org/abs/2601.18102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18102">https://arxiv.org/pdf/2601.18102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18102]] CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations(https://arxiv.org/abs/2601.18102)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.</li>
</ul>

<h3>Title: Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Fasha, Faisal Abul Rub, Nasim Matar, Bilal Sowan, Mohammad Al Khaldy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18105">https://arxiv.org/abs/2601.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18105">https://arxiv.org/pdf/2601.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18105]] Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents(https://arxiv.org/abs/2601.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.</li>
</ul>

<h3>Title: GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health</h3>
<ul>
<li><strong>Authors: </strong>Jiatan Huang, Zheyuan Zhang, Tianyi Ma, Mingchen Li, Yaning Zheng, Yanfang Ye, Chuxu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18106">https://arxiv.org/abs/2601.18106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18106">https://arxiv.org/pdf/2601.18106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18106]] GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health(https://arxiv.org/abs/2601.18106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.</li>
</ul>

<h3>Title: Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions</h3>
<ul>
<li><strong>Authors: </strong>Pedram Agand, Mo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18107">https://arxiv.org/abs/2601.18107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18107">https://arxiv.org/pdf/2601.18107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18107]] Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions(https://arxiv.org/abs/2601.18107)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.</li>
</ul>

<h3>Title: AttenMIA: LLM Membership Inference Attack through Attention Signals</h3>
<ul>
<li><strong>Authors: </strong>Pedram Zaree, Md Abdullah Al Mamun, Yue Dong, Ihsen Alouani, Nael Abu-Ghazaleh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18110">https://arxiv.org/abs/2601.18110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18110">https://arxiv.org/pdf/2601.18110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18110]] AttenMIA: LLM Membership Inference Attack through Attention Signals(https://arxiv.org/abs/2601.18110)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, extraction, membership infer, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.</li>
</ul>

<h3>Title: Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jean Kossaifi, Nikola Kovachki, Morteza Mardani, Daniel Leibovici, Suman Ravuri, Ira Shokar, Edoardo Calvello, Mohammad Shoaib Abbas, Peter Harrington, Ashay Subramaniam, Noah Brenowitz, Boris Bonev, Wonmin Byeon, Karsten Kreis, Dale Durran, Arash Vahdat, Mike Pritchard, Jan Kautz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18111">https://arxiv.org/abs/2601.18111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18111">https://arxiv.org/pdf/2601.18111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18111]] Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting(https://arxiv.org/abs/2601.18111)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.</li>
</ul>

<h3>Title: MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs</h3>
<ul>
<li><strong>Authors: </strong>Dezhang Kong, Zhuxi Wu, Shiqi Liu, Zhicheng Tan, Kuichen Lu, Minghao Li, Qichen Liu, Shengyu Chu, Zhenhua Xu, Xuan Liu, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18113">https://arxiv.org/abs/2601.18113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18113">https://arxiv.org/pdf/2601.18113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18113]] MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs(https://arxiv.org/abs/2601.18113)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at this https URL.</li>
</ul>

<h3>Title: Robust Learning of a Group DRO Neuron</h3>
<ul>
<li><strong>Authors: </strong>Guyang Cao, Shuyao Li, Sushrut Karmalkar, Jelena Diakonikolas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18115">https://arxiv.org/abs/2601.18115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18115">https://arxiv.org/pdf/2601.18115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18115]] Robust Learning of a Group DRO Neuron(https://arxiv.org/abs/2601.18115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbol{\lambda} \in \Delta_K$, where the objective is $\sum_{i \in [K]}\lambda_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(\sigma(\mathbf w\cdot\mathbf x)-y)^2 - \nu d_f(\boldsymbol\lambda,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $\nu \geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.</li>
</ul>

<h3>Title: FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lin Sun, Linglin Zhang, Jingang Huang, Change Jia, Zhengwei Cheng, Xiangzheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18116">https://arxiv.org/abs/2601.18116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18116">https://arxiv.org/pdf/2601.18116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18116]] FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning(https://arxiv.org/abs/2601.18116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis. We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs. Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.</li>
</ul>

<h3>Title: LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment</h3>
<ul>
<li><strong>Authors: </strong>Daeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18118">https://arxiv.org/abs/2601.18118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18118">https://arxiv.org/pdf/2601.18118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18118]] LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment(https://arxiv.org/abs/2601.18118)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.</li>
</ul>

<h3>Title: Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kunat Pipatanakul, Pittawat Taveekitworachai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18129">https://arxiv.org/abs/2601.18129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18129">https://arxiv.org/pdf/2601.18129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18129]] Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models(https://arxiv.org/abs/2601.18129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.</li>
</ul>

<h3>Title: Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Lyu, Minghua Zhao, Xuewen Huang, Yifei Chen, Shuangli Du, Jing Hu, Cheng Shi, Zhiyong Lv</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18135">https://arxiv.org/abs/2601.18135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18135">https://arxiv.org/pdf/2601.18135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18135]] Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection(https://arxiv.org/abs/2601.18135)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.</li>
</ul>

<h3>Title: Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods</h3>
<ul>
<li><strong>Authors: </strong>Mingxu Zhang, Huicheng Zhang, Jiaming Ji, Yaodong Yang, Ying Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18142">https://arxiv.org/abs/2601.18142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18142">https://arxiv.org/pdf/2601.18142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18142]] Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods(https://arxiv.org/abs/2601.18142)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.</li>
</ul>

<h3>Title: FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhaopeng Qiu, Shuang Yu, Jingqi Zhang, Shuai Zhang, Xue Huang, Jingyi Yang, Junjie Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18150">https://arxiv.org/abs/2601.18150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18150">https://arxiv.org/pdf/2601.18150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18150]] FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning(https://arxiv.org/abs/2601.18150)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.</li>
</ul>

<h3>Title: Agentic Very Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Aniket Rege, Arka Sadhu, Yuliang Li, Kejie Li, Ramya Korlakai Vinayak, Yuning Chai, Yong Jae Lee, Hyo Jin Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18157">https://arxiv.org/abs/2601.18157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18157">https://arxiv.org/pdf/2601.18157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18157]] Agentic Very Long Video Understanding(https://arxiv.org/abs/2601.18157)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</li>
</ul>

<h3>Title: Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Ani Harutyunyan, Sachin Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18162">https://arxiv.org/abs/2601.18162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18162">https://arxiv.org/pdf/2601.18162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18162]] Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models(https://arxiv.org/abs/2601.18162)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.</li>
</ul>

<h3>Title: TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration</h3>
<ul>
<li><strong>Authors: </strong>Zehua Liu, Shihao Zou, Jincai Huang, Yanfang Zhang, Chao Tong, Weixin Si</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18168">https://arxiv.org/abs/2601.18168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18168">https://arxiv.org/pdf/2601.18168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18168]] TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration(https://arxiv.org/abs/2601.18168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{this https URL}</li>
</ul>

<h3>Title: Learning Fair Domain Adaptation with Virtual Label Distribution</h3>
<ul>
<li><strong>Authors: </strong>Yuguang Zhang, Lijun Sheng, Jian Liang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18171">https://arxiv.org/abs/2601.18171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18171">https://arxiv.org/pdf/2601.18171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18171]] Learning Fair Domain Adaptation with Virtual Label Distribution(https://arxiv.org/abs/2601.18171)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.</li>
</ul>

<h3>Title: Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yifan Li, Shiying Wang, Jianqiang Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18190">https://arxiv.org/abs/2601.18190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18190">https://arxiv.org/pdf/2601.18190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18190]] Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval(https://arxiv.org/abs/2601.18190)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at this https URL.</li>
</ul>

<h3>Title: QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding</h3>
<ul>
<li><strong>Authors: </strong>Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang, Jun Jia, Dandan Zhu, Guangtao Zhai, Xiongkuo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18195">https://arxiv.org/abs/2601.18195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18195">https://arxiv.org/pdf/2601.18195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18195]] QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding(https://arxiv.org/abs/2601.18195)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL.</li>
</ul>

<h3>Title: HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zhang, Xinchen Lyu, Chenshan Ren, Shuhan Liu, Qimei Cui, Xiaofeng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18200">https://arxiv.org/abs/2601.18200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18200">https://arxiv.org/pdf/2601.18200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18200]] HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models(https://arxiv.org/abs/2601.18200)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.</li>
</ul>

<h3>Title: MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Juexiang Ye, Xue Li, Xinyu Yang, Chengkai Huang, Lanshun Nie, Lina Yao, Dechen Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18204">https://arxiv.org/abs/2601.18204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18204">https://arxiv.org/pdf/2601.18204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18204]] MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning(https://arxiv.org/abs/2601.18204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.</li>
</ul>

<h3>Title: Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud</h3>
<ul>
<li><strong>Authors: </strong>Beom Heyn Kim, Seok Min Hong, Mohammad Mannan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18216">https://arxiv.org/abs/2601.18216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18216">https://arxiv.org/pdf/2601.18216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18216]] Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud(https://arxiv.org/abs/2601.18216)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.</li>
</ul>

<h3>Title: HomoFM: Deep Homography Estimation with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18222">https://arxiv.org/abs/2601.18222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18222">https://arxiv.org/pdf/2601.18222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18222]] HomoFM: Deep Homography Estimation with Flow Matching(https://arxiv.org/abs/2601.18222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL.</li>
</ul>

<h3>Title: V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Mengyuan Jin, Zehui Liao, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18240">https://arxiv.org/abs/2601.18240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18240">https://arxiv.org/pdf/2601.18240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18240]] V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering(https://arxiv.org/abs/2601.18240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.</li>
</ul>

<h3>Title: Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity</h3>
<ul>
<li><strong>Authors: </strong>Santanu Das, Jatin Batra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18245">https://arxiv.org/abs/2601.18245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18245">https://arxiv.org/pdf/2601.18245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18245]] Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity(https://arxiv.org/abs/2601.18245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + \zeta_i$ (where $\zeta_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.</li>
</ul>

<h3>Title: A multimodal vision foundation model for generalizable knee pathology</h3>
<ul>
<li><strong>Authors: </strong>Kang Yu, Dingyu Wang, Zimu Yuan, Nan Zhou, Jiajun Liu, Jiaxin Liu, Shanggui Liu, Yaoyan Zheng, Huishu Yuan, Di Huang, Dong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18250">https://arxiv.org/abs/2601.18250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18250">https://arxiv.org/pdf/2601.18250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18250]] A multimodal vision foundation model for generalizable knee pathology(https://arxiv.org/abs/2601.18250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.</li>
</ul>

<h3>Title: Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing</h3>
<ul>
<li><strong>Authors: </strong>Chao Wang, Xuanying Li, Cheng Dai, Jinglei Feng, Yuxiang Luo, Yuqi Ouyang, Hao Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18252">https://arxiv.org/abs/2601.18252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18252">https://arxiv.org/pdf/2601.18252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18252]] Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing(https://arxiv.org/abs/2601.18252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.</li>
</ul>

<h3>Title: BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Peng Sun, Xiangyu Zhang, Duan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18253">https://arxiv.org/abs/2601.18253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18253">https://arxiv.org/pdf/2601.18253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18253]] BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation(https://arxiv.org/abs/2601.18253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.</li>
</ul>

<h3>Title: Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fei Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18255">https://arxiv.org/abs/2601.18255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18255">https://arxiv.org/pdf/2601.18255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18255]] Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs(https://arxiv.org/abs/2601.18255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.</li>
</ul>

<h3>Title: Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images</h3>
<ul>
<li><strong>Authors: </strong>Eytan Kats, Kai Geissler, Daniel Mensing, Jochen G. Hirsch, Stefan Heldman, Mattias P. Heinrich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18260">https://arxiv.org/abs/2601.18260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18260">https://arxiv.org/pdf/2601.18260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18260]] Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images(https://arxiv.org/abs/2601.18260)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.</li>
</ul>

<h3>Title: FGGM: Fisher-Guided Gradient Masking for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Chao-Hong Tan, Qian Chen, Wen Wang, Yukun Ma, Chong Zhang, Chong Deng, Qinglin Zhang, Xiangang Li, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18261">https://arxiv.org/abs/2601.18261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18261">https://arxiv.org/pdf/2601.18261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18261]] FGGM: Fisher-Guided Gradient Masking for Continual Learning(https://arxiv.org/abs/2601.18261)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.</li>
</ul>

<h3>Title: Revisiting Aerial Scene Classification on the AID Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Subhajeet Das, Susmita Ghosh, Abhiroop Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18263">https://arxiv.org/abs/2601.18263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18263">https://arxiv.org/pdf/2601.18263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18263]] Revisiting Aerial Scene Classification on the AID Benchmark(https://arxiv.org/abs/2601.18263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.</li>
</ul>

<h3>Title: What Do Learned Models Measure?</h3>
<ul>
<li><strong>Authors: </strong>Indrė Žliobaitė</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18278">https://arxiv.org/abs/2601.18278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18278">https://arxiv.org/pdf/2601.18278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18278]] What Do Learned Models Measure?(https://arxiv.org/abs/2601.18278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.</li>
</ul>

<h3>Title: U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents</h3>
<ul>
<li><strong>Authors: </strong>Jin Su, Runnan Fang, Yeqiu Li, Xiaobin Wang, Shihao Cai, Pengjun Xie, Ningyu Zhang, Fajie Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18285">https://arxiv.org/abs/2601.18285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18285">https://arxiv.org/pdf/2601.18285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18285]] U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents(https://arxiv.org/abs/2601.18285)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $\tau$-bench, $\tau^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.</li>
</ul>

<h3>Title: TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhewen Tan, Wenhan Yu, Jianfeng Si, Tongxin Liu, Kaiqi Guan, Huiyan Jin, Jiawen Tao, Xiaokun Yuan, Duohe Ma, Xiangzheng Zhang, Tong Yang, Lin Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18292">https://arxiv.org/abs/2601.18292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18292">https://arxiv.org/pdf/2601.18292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18292]] TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment(https://arxiv.org/abs/2601.18292)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.</li>
</ul>

<h3>Title: Suppressing Final Layer Hidden State Jumps in Transformer Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Keigo Shibata, Kazuki Yano, Ryosuke Takahashi, Jaesung Lee, Wataru Ikeda, Jun Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18302">https://arxiv.org/abs/2601.18302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18302">https://arxiv.org/pdf/2601.18302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18302]] Suppressing Final Layer Hidden State Jumps in Transformer Pretraining(https://arxiv.org/abs/2601.18302)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.</li>
</ul>

<h3>Title: Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM</h3>
<ul>
<li><strong>Authors: </strong>Everlyn Asiko Chimoto, Mostafa Elhoushi, Bruce A. Bassett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18306">https://arxiv.org/abs/2601.18306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18306">https://arxiv.org/pdf/2601.18306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18306]] Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM(https://arxiv.org/abs/2601.18306)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.</li>
</ul>

<h3>Title: A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods</h3>
<ul>
<li><strong>Authors: </strong>Lina Felsner, Sevgi G. Kafali, Hannah Eichhorn, Agnes A. J. Leth, Aidas Batvinskas, Andre Datchev, Fabian Klemm, Jan Aulich, Puntika Leepagorn, Ruben Klinger, Daniel Rueckert, Julia A. Schnabel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18314">https://arxiv.org/abs/2601.18314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18314">https://arxiv.org/pdf/2601.18314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18314]] A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods(https://arxiv.org/abs/2601.18314)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.</li>
</ul>

<h3>Title: Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals</h3>
<ul>
<li><strong>Authors: </strong>Jie Li, Jing Li, Lu Lv, Zhanyu Ju, Fengkui Gong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18326">https://arxiv.org/abs/2601.18326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18326">https://arxiv.org/pdf/2601.18326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18326]] Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals(https://arxiv.org/abs/2601.18326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.</li>
</ul>

<h3>Title: Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection</h3>
<ul>
<li><strong>Authors: </strong>Chuhan Feng, Jing Li, Jie Li, Lu Lv, Fengkui Gong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18329">https://arxiv.org/abs/2601.18329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18329">https://arxiv.org/pdf/2601.18329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18329]] Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection(https://arxiv.org/abs/2601.18329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.</li>
</ul>

<h3>Title: A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Ali Shah (1), Muhammad Mansoor Alam (1,2), Saddam Hussain Khan (3) ((1) Riphah International University, Islamabad, Pakistan, (2) Multimedia University, Malaysia, (3) University of Engineering and Applied Sciences, Swat, Kanju Township, Pakistan)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18330">https://arxiv.org/abs/2601.18330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18330">https://arxiv.org/pdf/2601.18330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18330]] A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification(https://arxiv.org/abs/2601.18330)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.</li>
</ul>

<h3>Title: Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Clément Christophe, Wadood Mohammed Abdul, Prateek Munjal, Tathagata Raha, Ronnie Rajan, Praveenkumar Kanithi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18334">https://arxiv.org/abs/2601.18334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18334">https://arxiv.org/pdf/2601.18334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18334]] Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare(https://arxiv.org/abs/2601.18334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.</li>
</ul>

<h3>Title: PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Isaac Deutsch, Nicolas Moënne-Loccoz, Gavriel State, Zan Gojcic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18336">https://arxiv.org/abs/2601.18336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18336">https://arxiv.org/pdf/2601.18336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18336]] PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction(https://arxiv.org/abs/2601.18336)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: this https URL</li>
</ul>

<h3>Title: Structural Gender Bias in Credit Scoring: Proxy Leakage</h3>
<ul>
<li><strong>Authors: </strong>Navya SD, Sreekanth D, SS Uma Sankari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18342">https://arxiv.org/abs/2601.18342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18342">https://arxiv.org/pdf/2601.18342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18342]] Structural Gender Bias in Credit Scoring: Proxy Leakage(https://arxiv.org/abs/2601.18342)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.</li>
</ul>

<h3>Title: Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception</h3>
<ul>
<li><strong>Authors: </strong>Sijing Wu, Yunhao Li, Zicheng Zhang, Qi Jia, Xinyue Li, Huiyu Duan, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18346">https://arxiv.org/abs/2601.18346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18346">https://arxiv.org/pdf/2601.18346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18346]] Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception(https://arxiv.org/abs/2601.18346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.</li>
</ul>

<h3>Title: When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junyi Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18350">https://arxiv.org/abs/2601.18350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18350">https://arxiv.org/pdf/2601.18350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18350]] When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs(https://arxiv.org/abs/2601.18350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.</li>
</ul>

<h3>Title: Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weiqin Yang, Haowen Xue, Qingyi Peng, Hexuan Hu, Qian Huang, Tingbo Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18356">https://arxiv.org/abs/2601.18356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18356">https://arxiv.org/pdf/2601.18356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18356]] Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning(https://arxiv.org/abs/2601.18356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.</li>
</ul>

<h3>Title: OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI</h3>
<ul>
<li><strong>Authors: </strong>Caterina Fuster-Barceló, Claudia Castrillón, Laura Rodrigo-Muñoz, Victor Manuel Vega-Suárez, Nicolás Pérez-Fernández, Gorka Bastarrika, Arrate Muñoz-Barrutia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18368">https://arxiv.org/abs/2601.18368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18368">https://arxiv.org/pdf/2601.18368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18368]] OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI(https://arxiv.org/abs/2601.18368)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention. Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical this http URL software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with this http URL, while endolymphatic volumes were systematically smaller and more physiologically realistic. These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.</li>
</ul>

<h3>Title: Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues</h3>
<ul>
<li><strong>Authors: </strong>Christos Petrou, Harris Partaourides, Athanasios Balomenos, Yannis Kopsinis, Sotirios Chatzis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18372">https://arxiv.org/abs/2601.18372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18372">https://arxiv.org/pdf/2601.18372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18372]] Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues(https://arxiv.org/abs/2601.18372)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.</li>
</ul>

<h3>Title: CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Silva, José Evans, José Isidro, Miguel Marques, Afonso Fonseca, Ricardo Morais, João Canavilhas, Arian Pasquali, Purificação Silvano, Alípio Jorge, Nuno Guimarães, Sérgio Nunes, Ricardo Campos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18374">https://arxiv.org/abs/2601.18374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18374">https://arxiv.org/pdf/2601.18374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18374]] CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes(https://arxiv.org/abs/2601.18374)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.</li>
</ul>

<h3>Title: Hierarchical Text Classification with LLM-Refined Taxonomies</h3>
<ul>
<li><strong>Authors: </strong>Jonas Golde, Nicolaas Jedema, Ravi Krishnan, Phong Le</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18375">https://arxiv.org/abs/2601.18375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18375">https://arxiv.org/pdf/2601.18375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18375]] Hierarchical Text Classification with LLM-Refined Taxonomies(https://arxiv.org/abs/2601.18375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.</li>
</ul>

<h3>Title: Estimation of geometric transformation matrices using grid-shaped pilot signals</h3>
<ul>
<li><strong>Authors: </strong>Rinka Kawano, Masaki Kawamura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18385">https://arxiv.org/abs/2601.18385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18385">https://arxiv.org/pdf/2601.18385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18385]] Estimation of geometric transformation matrices using grid-shaped pilot signals(https://arxiv.org/abs/2601.18385)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.</li>
</ul>

<h3>Title: ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Lee Jun Rong, Christos Korgialas, Dion Jia Xu Ho, Pai Chet Ng, Xiaoxiao Miao, Konstantinos N. Plataniotis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18386">https://arxiv.org/abs/2601.18386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18386">https://arxiv.org/pdf/2601.18386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18386]] ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks(https://arxiv.org/abs/2601.18386)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.</li>
</ul>

<h3>Title: Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space</h3>
<ul>
<li><strong>Authors: </strong>Moritz Rempe, Lukas T. Rotkopf, Marco Schlimbach, Helmut Becker, Fabian Hörst, Johannes Haubold, Philipp Dammann, Kevin Kröninger, Jens Kleesiek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18392">https://arxiv.org/abs/2601.18392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18392">https://arxiv.org/pdf/2601.18392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18392]] Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space(https://arxiv.org/abs/2601.18392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.</li>
</ul>

<h3>Title: Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mikel Zubillaga, Oscar Sainz, Oier Lopez de Lacalle, Eneko Agirre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18395">https://arxiv.org/abs/2601.18395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18395">https://arxiv.org/pdf/2601.18395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18395]] Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction(https://arxiv.org/abs/2601.18395)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.</li>
</ul>

<h3>Title: Superlinear Multi-Step Attention</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18401">https://arxiv.org/abs/2601.18401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18401">https://arxiv.org/pdf/2601.18401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18401]] Superlinear Multi-Step Attention(https://arxiv.org/abs/2601.18401)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.</li>
</ul>

<h3>Title: Larger than memory image processing</h3>
<ul>
<li><strong>Authors: </strong>Jon Sporring, David Stansby</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18407">https://arxiv.org/abs/2601.18407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18407">https://arxiv.org/pdf/2601.18407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18407]] Larger than memory image processing(https://arxiv.org/abs/2601.18407)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.</li>
</ul>

<h3>Title: Fundamentals, Recent Advances, and Challenges Regarding Cryptographic Algorithms for the Quantum Computing Era</h3>
<ul>
<li><strong>Authors: </strong>Darlan Noetzold, Valderi Reis Quietinho Leithardt</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18413">https://arxiv.org/abs/2601.18413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18413">https://arxiv.org/pdf/2601.18413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18413]] Fundamentals, Recent Advances, and Challenges Regarding Cryptographic Algorithms for the Quantum Computing Era(https://arxiv.org/abs/2601.18413)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>This book arises from the need to provide a clear and up-to-date overview of the impacts of quantum computing on cryptography. The goal is to provide a reference in Portuguese for undergraduate, master's, and doctoral students in the field of data security and cryptography. Throughout the chapters, we present fundamentals, we discuss classical and post-quantum algorithms, evaluate emerging patterns, and point out real-world implementation challenges. The initial objective is to serve as a guide for students, researchers, and professionals who need to understand not only the mathematics involved, but also its practical implications in security systems and policies. For more advanced professionals, the main objective is to present content and ideas so that they can assess the changes and perspectives in the era of quantum cryptographic algorithms. To that end, the text's structure was designed to be progressive: we begin with essential concepts, move on to quantum algorithms and their consequences (with emphasis on Shor's algorithm), present issues focusing on "families" of post-quantum schemes (based on lattices, codes, hash functions, multivariate, isogenies), analyze the state of the art in standardization (highlighting the NIST process), and finally, discuss migration, interoperability, performance, and cryptographic governance. We hope that this work will assist in the formation of critical thinking and informed technical decision-making, fostering secure transition strategies for the post-quantum era.</li>
</ul>

<h3>Title: Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings</h3>
<ul>
<li><strong>Authors: </strong>Aura Loredana Dan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18414">https://arxiv.org/abs/2601.18414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18414">https://arxiv.org/pdf/2601.18414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18414]] Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings(https://arxiv.org/abs/2601.18414)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.</li>
</ul>

<h3>Title: Pisets: A Robust Speech Recognition System for Lectures and Interviews</h3>
<ul>
<li><strong>Authors: </strong>Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets, Lyudmila Budneva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18415">https://arxiv.org/abs/2601.18415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18415">https://arxiv.org/pdf/2601.18415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18415]] Pisets: A Robust Speech Recognition System for Lectures and Interviews(https://arxiv.org/abs/2601.18415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: this https URL.</li>
</ul>

<h3>Title: Gradient Regularized Natural Gradients</h3>
<ul>
<li><strong>Authors: </strong>Satya Prakash Dash, Hossein Abdi, Wei Pan, Samuel Kaski, Mingfei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18420">https://arxiv.org/abs/2601.18420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18420">https://arxiv.org/pdf/2601.18420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18420]] Gradient Regularized Natural Gradients(https://arxiv.org/abs/2601.18420)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.</li>
</ul>

<h3>Title: KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE</h3>
<ul>
<li><strong>Authors: </strong>Eymen Ünay, Björn Franke, Jackson Woodruff</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18445">https://arxiv.org/abs/2601.18445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18445">https://arxiv.org/pdf/2601.18445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18445]] KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE(https://arxiv.org/abs/2601.18445)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required. In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.</li>
</ul>

<h3>Title: GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level</h3>
<ul>
<li><strong>Authors: </strong>Jinlong Hu, Jiacheng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18447">https://arxiv.org/abs/2601.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18447">https://arxiv.org/pdf/2601.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18447]] GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level(https://arxiv.org/abs/2601.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.</li>
</ul>

<h3>Title: On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics</h3>
<ul>
<li><strong>Authors: </strong>Lloyd Austin Courtenay</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18448">https://arxiv.org/abs/2601.18448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18448">https://arxiv.org/pdf/2601.18448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18448]] On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics(https://arxiv.org/abs/2601.18448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.</li>
</ul>

<h3>Title: 3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control</h3>
<ul>
<li><strong>Authors: </strong>Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Naoya Chiba, Yuki Uranishi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18451">https://arxiv.org/abs/2601.18451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18451">https://arxiv.org/pdf/2601.18451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18451]] 3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control(https://arxiv.org/abs/2601.18451)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.</li>
</ul>

<h3>Title: Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Wei, Suyuan Yao, Cheng Huang, Xiangyu Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18464">https://arxiv.org/abs/2601.18464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18464">https://arxiv.org/pdf/2601.18464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18464]] Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System(https://arxiv.org/abs/2601.18464)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.</li>
</ul>

<h3>Title: Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel B. Hier, Tayo Obafemi-Ajayi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18468">https://arxiv.org/abs/2601.18468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18468">https://arxiv.org/pdf/2601.18468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18468]] Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models(https://arxiv.org/abs/2601.18468)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.</li>
</ul>

<h3>Title: Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18483">https://arxiv.org/abs/2601.18483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18483">https://arxiv.org/pdf/2601.18483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18483]] Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs(https://arxiv.org/abs/2601.18483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.</li>
</ul>

<h3>Title: Demographic Probing of Large Language Models Lacks Construct Validity</h3>
<ul>
<li><strong>Authors: </strong>Manuel Tonneau, Neil K. R. Seghal, Niyati Malhotra, Victor Orozco-Olvera, Ana María Muñoz Boudet, Lakshmi Subramanian, Sharath Chandra Guntuku, Valentin Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18486">https://arxiv.org/abs/2601.18486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18486">https://arxiv.org/pdf/2601.18486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18486]] Demographic Probing of Large Language Models Lacks Construct Validity(https://arxiv.org/abs/2601.18486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.</li>
</ul>

<h3>Title: DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment</h3>
<ul>
<li><strong>Authors: </strong>Sara Tehrani, Yonghao Xu, Leif Haglund, Amanda Berg, Michael Felsberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18493">https://arxiv.org/abs/2601.18493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18493">https://arxiv.org/pdf/2601.18493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18493]] DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment(https://arxiv.org/abs/2601.18493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines. To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</li>
</ul>

<h3>Title: Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates</h3>
<ul>
<li><strong>Authors: </strong>Yibo Li, Zijie Lin, Ailin Deng, Xuan Zhang, Yufei He, Shuo Ji, Tri Cao, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18510">https://arxiv.org/abs/2601.18510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18510">https://arxiv.org/pdf/2601.18510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18510]] Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates(https://arxiv.org/abs/2601.18510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at this https URL.</li>
</ul>

<h3>Title: Scaling up Privacy-Preserving ML: A CKKS Implementation of Llama-2-7B</h3>
<ul>
<li><strong>Authors: </strong>Jaiyoung Park, Sejin Park, Jai Hyun Park, Jung Ho Ahn, Jung Hee Cheon, Guillaume Hanrot, Jung Woo Kim, Minje Park, Damien Stehlé</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18511">https://arxiv.org/abs/2601.18511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18511">https://arxiv.org/pdf/2601.18511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18511]] Scaling up Privacy-Preserving ML: A CKKS Implementation of Llama-2-7B(https://arxiv.org/abs/2601.18511)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become ubiquitous, privacy concerns pertaining to inference inputs keep growing. In this context, fully homomorphic encryption (FHE) has emerged as a primary cryptographic solution to provide non-interactive confidential LLM inference. Existing solutions scale poorly with the input token length, and hence focus either on small models or larger models with a small number of input tokens. They also suffer from the existence of large outlier values. These values have a strong impact on the evaluation of non-linear layers, leading to large-degree polynomial approximation and thus heavy evaluation costs. We propose an FHE-based private LLM inference solution that allows thousands of input tokens with only a part of them being encrypted: this fits with a scenario where the context is benign and only part of the input is sensitive. To do so, we suggest an unbalanced chunked prefill framework that processes the private and public parts of the input tokens differently. Our framework contains plaintext-plaintext, plaintext-ciphertext and ciphertext-ciphertext computational components. We adopt different strategies and ingredients for each component. We also devise new homomorphic algorithms for specific matrix multiplication and polynomial evaluation tasks encountered during LLM inference. Furthermore, without retraining, we tailor the LLM inference algorithm to reduce the ranges of outlier values: we leverage machine learning strategies (token prepending and rotations) to mitigate the impact of the outliers on non-linear layers. Based on these ingredients, we describe a CKKS-based end-to-end implementation of Llama-2-7B private inference for up to 4096 input tokens, of which the last 128 are encrypted. On a cluster of 8~NVIDIA RTX-4090 GPUs, inference takes 85s for summarization and 33s for generation per output token.</li>
</ul>

<h3>Title: Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research</h3>
<ul>
<li><strong>Authors: </strong>Antonio Garzon-Vico, Krithika Sharon Komalapati, Arsalan Shahid, Jan Rosier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18512">https://arxiv.org/abs/2601.18512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18512">https://arxiv.org/pdf/2601.18512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18512]] Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research(https://arxiv.org/abs/2601.18512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.</li>
</ul>

<h3>Title: LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models</h3>
<ul>
<li><strong>Authors: </strong>Kai Hu, Haoqi Hu, Matt Fredrikson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18513">https://arxiv.org/abs/2601.18513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18513">https://arxiv.org/pdf/2601.18513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18513]] LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models(https://arxiv.org/abs/2601.18513)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $\beta$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.</li>
</ul>

<h3>Title: Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Emna Boudabbous, Mohamed Karaa, Lokman Sboui, Julio Montecinos, Omar Alam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18521">https://arxiv.org/abs/2601.18521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18521">https://arxiv.org/pdf/2601.18521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18521]] Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning(https://arxiv.org/abs/2601.18521)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture. We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training. We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.</li>
</ul>

<h3>Title: From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale</h3>
<ul>
<li><strong>Authors: </strong>Yongqi Jin, Yecheng Wang, Jun-jie Wang, Rong Zhu, Guolin Ke, Weinan E</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18524">https://arxiv.org/abs/2601.18524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18524">https://arxiv.org/pdf/2601.18524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18524]] From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale(https://arxiv.org/abs/2601.18524)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.</li>
</ul>

<h3>Title: Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models</h3>
<ul>
<li><strong>Authors: </strong>Francesco Maria Molfese, Momchil Hardalov, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18527">https://arxiv.org/abs/2601.18527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18527">https://arxiv.org/pdf/2601.18527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18527]] Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models(https://arxiv.org/abs/2601.18527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.</li>
</ul>

<h3>Title: From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Devon Levy, Bar Assayag, Laura Gaspar, Ilan Shimshoni, Bella Specktor-Fadida</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18532">https://arxiv.org/abs/2601.18532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18532">https://arxiv.org/pdf/2601.18532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18532]] From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation(https://arxiv.org/abs/2601.18532)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.</li>
</ul>

<h3>Title: Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features</h3>
<ul>
<li><strong>Authors: </strong>Abishek Stephen, Jindřich Libovický</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18536">https://arxiv.org/abs/2601.18536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18536">https://arxiv.org/pdf/2601.18536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18536]] Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features(https://arxiv.org/abs/2601.18536)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.</li>
</ul>

<h3>Title: Information Hidden in Gradients of Regression with Target Noise</h3>
<ul>
<li><strong>Authors: </strong>Arash Jamshidi, Katsiaryna Haitsiukevich, Kai Puolamäki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18546">https://arxiv.org/abs/2601.18546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18546">https://arxiv.org/pdf/2601.18546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18546]] Information Hidden in Gradients of Regression with Target Noise(https://arxiv.org/abs/2601.18546)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $\Sigma$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $\Omega(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $\Sigma$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.</li>
</ul>

<h3>Title: Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection</h3>
<ul>
<li><strong>Authors: </strong>Devansh Srivastav, David Pape, Lea Schönherr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18552">https://arxiv.org/abs/2601.18552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18552">https://arxiv.org/pdf/2601.18552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18552]] Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection(https://arxiv.org/abs/2601.18552)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.</li>
</ul>

<h3>Title: Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jingsong Xia, Siqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18556">https://arxiv.org/abs/2601.18556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18556">https://arxiv.org/pdf/2601.18556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18556]] Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis(https://arxiv.org/abs/2601.18556)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.</li>
</ul>

<h3>Title: An Unsupervised Tensor-Based Domain Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chong Hyun Lee, Kibae Lee, Hyun Hee Yim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18564">https://arxiv.org/abs/2601.18564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18564">https://arxiv.org/pdf/2601.18564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18564]] An Unsupervised Tensor-Based Domain Alignment(https://arxiv.org/abs/2601.18564)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.</li>
</ul>

<h3>Title: One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization</h3>
<ul>
<li><strong>Authors: </strong>Franziska Weeber, Vera Neplenbroek, Jan Batzner, Sebastian Padó</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18572">https://arxiv.org/abs/2601.18572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18572">https://arxiv.org/pdf/2601.18572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18572]] One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization(https://arxiv.org/abs/2601.18572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.</li>
</ul>

<h3>Title: K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents</h3>
<ul>
<li><strong>Authors: </strong>Vincenzo De Paola, Mirco Mutti, Riccardo Zamboni, Marcello Restelli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18580">https://arxiv.org/abs/2601.18580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18580">https://arxiv.org/pdf/2601.18580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18580]] K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents(https://arxiv.org/abs/2601.18580)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.</li>
</ul>

<h3>Title: From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuan Cao, Feixiang Liu, Xinyue Wang, Yihan Zhu, Hui Xu, Zheng Wang, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18582">https://arxiv.org/abs/2601.18582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18582">https://arxiv.org/pdf/2601.18582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18582]] From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection(https://arxiv.org/abs/2601.18582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.</li>
</ul>

<h3>Title: GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Selena Ling, Alec Jacobson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18585">https://arxiv.org/abs/2601.18585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18585">https://arxiv.org/pdf/2601.18585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18585]] GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization(https://arxiv.org/abs/2601.18585)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.</li>
</ul>

<h3>Title: Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Miguel Costa, Arthur Vandervoort, Carolin Schmidt, Morten W. Petersen, Martin Drews, Karyn Morrissey, Francisco C. Pereira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18586">https://arxiv.org/abs/2601.18586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18586">https://arxiv.org/pdf/2601.18586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18586]] Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning(https://arxiv.org/abs/2601.18586)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.</li>
</ul>

<h3>Title: AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment</h3>
<ul>
<li><strong>Authors: </strong>KV Karthikeya, Ashok Kumar Das, Shantanu Pal, Vivekananda Bhat K, Arun Sekar Rajasekaran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18589">https://arxiv.org/abs/2601.18589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18589">https://arxiv.org/pdf/2601.18589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18589]] AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment(https://arxiv.org/abs/2601.18589)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.</li>
</ul>

<h3>Title: EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery</h3>
<ul>
<li><strong>Authors: </strong>Yu Xia, Chang Liu, Tianqi Xiang, Zhigang Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18597">https://arxiv.org/abs/2601.18597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18597">https://arxiv.org/pdf/2601.18597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18597]] EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery(https://arxiv.org/abs/2601.18597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.</li>
</ul>

<h3>Title: LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Zheng, Kevin Bryson</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18604">https://arxiv.org/abs/2601.18604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18604">https://arxiv.org/pdf/2601.18604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18604]] LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation(https://arxiv.org/abs/2601.18604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited. Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis. Availability and implementation: this https URL</li>
</ul>

<h3>Title: Multimodal Privacy-Preserving Entity Resolution with Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Susim Roy, Nalini Ratha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18612">https://arxiv.org/abs/2601.18612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18612">https://arxiv.org/pdf/2601.18612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18612]] Multimodal Privacy-Preserving Entity Resolution with Fully Homomorphic Encryption(https://arxiv.org/abs/2601.18612)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>The canonical challenge of entity resolution within high-compliance sectors, where secure identity reconciliation is frequently confounded by significant data heterogeneity, including syntactic variations in personal identifiers, is a longstanding and complex problem. To this end, we introduce a novel multimodal framework operating with the voluminous data sets typical of government and financial institutions. Specifically, our methodology is designed to address the tripartite challenge of data volume, matching fidelity, and privacy. Consequently, the underlying plaintext of personally identifiable information remains computationally inaccessible throughout the matching lifecycle, empowering institutions to rigorously satisfy stringent regulatory mandates with cryptographic assurances of client confidentiality while achieving a demonstrably low equal error rate and maintaining computational tractability at scale.</li>
</ul>

<h3>Title: Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem</h3>
<ul>
<li><strong>Authors: </strong>Ramiro Valdes Jara, Adam Meyers</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18615">https://arxiv.org/abs/2601.18615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18615">https://arxiv.org/pdf/2601.18615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18615]] Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem(https://arxiv.org/abs/2601.18615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.</li>
</ul>

<h3>Title: Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures</h3>
<ul>
<li><strong>Authors: </strong>Jorge Quesada, Ghassan AlRegib</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18619">https://arxiv.org/abs/2601.18619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18619">https://arxiv.org/pdf/2601.18619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18619]] Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures(https://arxiv.org/abs/2601.18619)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.</li>
</ul>

<h3>Title: Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Yuzhou Chen, Shaogang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18623">https://arxiv.org/abs/2601.18623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18623">https://arxiv.org/pdf/2601.18623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18623]] Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation(https://arxiv.org/abs/2601.18623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.</li>
</ul>

<h3>Title: CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search</h3>
<ul>
<li><strong>Authors: </strong>Zequn Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18625">https://arxiv.org/abs/2601.18625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18625">https://arxiv.org/pdf/2601.18625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18625]] CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search(https://arxiv.org/abs/2601.18625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at this https URL.</li>
</ul>

<h3>Title: Physics-Informed Uncertainty Enables Reliable AI-driven Design</h3>
<ul>
<li><strong>Authors: </strong>Tingkai Xue, Chin Chun Ooi, Yang Jiang, Luu Trung Pham Duong, Pao-Hsiung Chiu, Weijiang Zhao, Nagarajan Raghavan, My Ha Dao</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18638">https://arxiv.org/abs/2601.18638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18638">https://arxiv.org/pdf/2601.18638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18638]] Physics-Informed Uncertainty Enables Reliable AI-driven Design(https://arxiv.org/abs/2601.18638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.</li>
</ul>

<h3>Title: FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Liheng Yu, Zhe Zhao, Yuxuan Wang, Pengkun Wang, Binwu Wang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18650">https://arxiv.org/abs/2601.18650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18650">https://arxiv.org/pdf/2601.18650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18650]] FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning(https://arxiv.org/abs/2601.18650)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.</li>
</ul>

<h3>Title: A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Spyros Rigas, Thanasis Papaioannou, Panagiotis Trakadas, Georgios Alexandridis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18672">https://arxiv.org/abs/2601.18672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18672">https://arxiv.org/pdf/2601.18672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18672]] A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks(https://arxiv.org/abs/2601.18672)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.</li>
</ul>

<h3>Title: Quasi Monte Carlo methods enable extremely low-dimensional deep generative models</h3>
<ul>
<li><strong>Authors: </strong>Miles Martinez, Alex H. Williams</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18676">https://arxiv.org/abs/2601.18676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18676">https://arxiv.org/pdf/2601.18676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18676]] Quasi Monte Carlo methods enable extremely low-dimensional deep generative models(https://arxiv.org/abs/2601.18676)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.</li>
</ul>

<h3>Title: Counterfactual Explanations on Robust Perceptual Geodesics</h3>
<ul>
<li><strong>Authors: </strong>Eslam Zaher, Maciej Trzaskowski, Quan Nguyen, Fred Roosta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.HC, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18678">https://arxiv.org/abs/2601.18678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18678">https://arxiv.org/pdf/2601.18678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18678]] Counterfactual Explanations on Robust Perceptual Geodesics(https://arxiv.org/abs/2601.18678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.</li>
</ul>

<h3>Title: ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule</h3>
<ul>
<li><strong>Authors: </strong>Yilie Huang, Wenpin Tang, Xunyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18681">https://arxiv.org/abs/2601.18681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18681">https://arxiv.org/pdf/2601.18681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18681]] ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule(https://arxiv.org/abs/2601.18681)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.</li>
</ul>

<h3>Title: Explainability Methods for Hardware Trojan Detection: A Systematic Comparison</h3>
<ul>
<li><strong>Authors: </strong>Paul Whitten, Francis Wolff, Chris Papachristou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18696">https://arxiv.org/abs/2601.18696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18696">https://arxiv.org/pdf/2601.18696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18696]] Explainability Methods for Hardware Trojan Detection: A Systematic Comparison(https://arxiv.org/abs/2601.18696)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient). Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation. XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights. This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.</li>
</ul>

<h3>Title: Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18698">https://arxiv.org/abs/2601.18698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18698">https://arxiv.org/pdf/2601.18698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18698]] Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge(https://arxiv.org/abs/2601.18698)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.</li>
</ul>

<h3>Title: Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Olaf Yunus Laitinen Imanov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18699">https://arxiv.org/abs/2601.18699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18699">https://arxiv.org/pdf/2601.18699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18699]] Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning(https://arxiv.org/abs/2601.18699)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.</li>
</ul>

<h3>Title: From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Hansheng Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18702">https://arxiv.org/abs/2601.18702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18702">https://arxiv.org/pdf/2601.18702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18702]] From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic(https://arxiv.org/abs/2601.18702)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.</li>
</ul>

<h3>Title: SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model</h3>
<ul>
<li><strong>Authors: </strong>Jan Hagnberger, Mathias Niepert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18707">https://arxiv.org/abs/2601.18707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18707">https://arxiv.org/pdf/2601.18707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18707]] SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model(https://arxiv.org/abs/2601.18707)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.</li>
</ul>

<h3>Title: Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Judith Vilella-Cantos, Mauro Martini, Marcello Chiaberge, Mónica Ballesta, David Valiente</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18714">https://arxiv.org/abs/2601.18714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18714">https://arxiv.org/pdf/2601.18714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18714]] Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning(https://arxiv.org/abs/2601.18714)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.</li>
</ul>

<h3>Title: Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lintang Sutawika, Gokul Swamy, Zhiwei Steven Wu, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18722">https://arxiv.org/abs/2601.18722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18722">https://arxiv.org/pdf/2601.18722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18722]] Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning(https://arxiv.org/abs/2601.18722)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than of the training data across the single-language, multilingual, and generalization to unseen language settings.</li>
</ul>

<h3>Title: Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data</h3>
<ul>
<li><strong>Authors: </strong>Willem Diepeveen, Oscar Leong</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG, math.OC, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18728">https://arxiv.org/abs/2601.18728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18728">https://arxiv.org/pdf/2601.18728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18728]] Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data(https://arxiv.org/abs/2601.18728)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.</li>
</ul>

<h3>Title: Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale</h3>
<ul>
<li><strong>Authors: </strong>Henry Bell, Caroline Zhang, Mohammed Mobasserul Haque, Dhaval Potdar, Samia Zaman, Brandon Fain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18730">https://arxiv.org/abs/2601.18730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18730">https://arxiv.org/pdf/2601.18730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18730]] Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale(https://arxiv.org/abs/2601.18730)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.</li>
</ul>

<h3>Title: One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hongru Cai, Yongqi Li, Tiezheng Yu, Fengbin Zhu, Wenjie Wang, Fuli Feng, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18731">https://arxiv.org/abs/2601.18731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18731">https://arxiv.org/pdf/2601.18731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18731]] One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment(https://arxiv.org/abs/2601.18731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.</li>
</ul>

<h3>Title: Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18734">https://arxiv.org/abs/2601.18734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18734">https://arxiv.org/pdf/2601.18734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18734]] Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models(https://arxiv.org/abs/2601.18734)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.</li>
</ul>

<h3>Title: Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift</h3>
<ul>
<li><strong>Authors: </strong>Jake Lyon, Ehsan Saeedizade, Shamik Sengupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18736">https://arxiv.org/abs/2601.18736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18736">https://arxiv.org/pdf/2601.18736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18736]] Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift(https://arxiv.org/abs/2601.18736)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.</li>
</ul>

<h3>Title: SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Antequera-Sánchez, Juan Luis Suárez-Díaz, Rosana Montes, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18739">https://arxiv.org/abs/2601.18739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18739">https://arxiv.org/pdf/2601.18739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18739]] SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification(https://arxiv.org/abs/2601.18739)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.</li>
</ul>

<h3>Title: Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18751">https://arxiv.org/abs/2601.18751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18751">https://arxiv.org/pdf/2601.18751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18751]] Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback(https://arxiv.org/abs/2601.18751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.</li>
</ul>

<h3>Title: HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18753">https://arxiv.org/abs/2601.18753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18753">https://arxiv.org/pdf/2601.18753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18753]] HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs(https://arxiv.org/abs/2601.18753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.</li>
</ul>

<h3>Title: $α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18754">https://arxiv.org/abs/2601.18754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18754">https://arxiv.org/pdf/2601.18754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18754]] $α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks(https://arxiv.org/abs/2601.18754)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings. We introduce $\alpha^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $\alpha^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $\alpha^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage). We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $\alpha^{3}$-SecBench on GitHub: this https URL</li>
</ul>

<h3>Title: Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values</h3>
<ul>
<li><strong>Authors: </strong>Henry Bell, Lara Neubauer da Costa Schertel, Bochu Ding, Brandon Fain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18760">https://arxiv.org/abs/2601.18760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18760">https://arxiv.org/pdf/2601.18760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18760]] Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values(https://arxiv.org/abs/2601.18760)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.</li>
</ul>

<h3>Title: Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</h3>
<ul>
<li><strong>Authors: </strong>Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18771">https://arxiv.org/abs/2601.18771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18771">https://arxiv.org/pdf/2601.18771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18771]] Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory(https://arxiv.org/abs/2601.18771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</li>
</ul>

<h3>Title: PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Divekar, Anirban Majumder</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18777">https://arxiv.org/abs/2601.18777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18777">https://arxiv.org/pdf/2601.18777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18777]] PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation(https://arxiv.org/abs/2601.18777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.</li>
</ul>

<h3>Title: POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</h3>
<ul>
<li><strong>Authors: </strong>Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18779">https://arxiv.org/abs/2601.18779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18779">https://arxiv.org/pdf/2601.18779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18779]] POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration(https://arxiv.org/abs/2601.18779)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</li>
</ul>

<h3>Title: Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic</h3>
<ul>
<li><strong>Authors: </strong>Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18783">https://arxiv.org/abs/2601.18783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18783">https://arxiv.org/pdf/2601.18783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18783]] Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic(https://arxiv.org/abs/2601.18783)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.</li>
</ul>

<h3>Title: Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Mumin Jia, Jairo Diaz-Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18788">https://arxiv.org/abs/2601.18788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18788">https://arxiv.org/pdf/2601.18788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18788]] Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings(https://arxiv.org/abs/2601.18788)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.</li>
</ul>

<h3>Title: MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</h3>
<ul>
<li><strong>Authors: </strong>Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18790">https://arxiv.org/abs/2601.18790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18790">https://arxiv.org/pdf/2601.18790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18790]] MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts(https://arxiv.org/abs/2601.18790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.</li>
</ul>

<h3>Title: Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets</h3>
<ul>
<li><strong>Authors: </strong>Iaroslav Chelombitko, Mika Hämäläinen, Aleksey Komissarov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18791">https://arxiv.org/abs/2601.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18791">https://arxiv.org/pdf/2601.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18791]] Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets(https://arxiv.org/abs/2601.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.</li>
</ul>

<h3>Title: ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brian Ondov, Chia-Hsuan Chang, Yujia Zhou, Mauro Giuffrè, Hua Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18796">https://arxiv.org/abs/2601.18796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18796">https://arxiv.org/pdf/2601.18796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18796]] ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models(https://arxiv.org/abs/2601.18796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
