<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Architecture of Data Anomaly Detection-Enhanced Decentralized Expert System for Early-Stage Alzheimer's Disease Prediction. (arXiv:2311.00373v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00373">http://arxiv.org/abs/2311.00373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00373]] Architecture of Data Anomaly Detection-Enhanced Decentralized Expert System for Early-Stage Alzheimer's Disease Prediction(http://arxiv.org/abs/2311.00373)</code></li>
<li>Summary: <p>Alzheimer's Disease is a global health challenge that requires early and
accurate detection to improve patient outcomes. Magnetic Resonance Imaging
(MRI) holds significant diagnostic potential, but its effective analysis
remains a formidable task. This study introduces a groundbreaking decentralized
expert system that cleverly combines blockchain technology with Artificial
Intelligence (AI) to integrate robust anomaly detection for patient-submitted
data.
</p>
<p>Traditional diagnostic methods often lead to delayed and imprecise
predictions, especially in the early stages of the disease. Centralized data
repositories struggle to manage the immense volumes of MRI data, and persistent
privacy concerns hinder collaborative efforts. Our innovative solution
harnesses decentralization to protect data integrity and patient privacy,
facilitated by blockchain technology. It not only emphasizes AI-driven MRI
analysis but also incorporates a sophisticated data anomaly detection
architecture. These mechanisms scrutinize patient-contributed data for various
issues, including data quality problems and atypical findings within MRI
images.
</p>
<p>Conducting an exhaustive check of MRI image correctness and quality directly
on the blockchain is impractical due to computational complexity and cost
constraints. Typically, such checks are performed off-chain, and the blockchain
securely records the results. This comprehensive approach empowers our
decentralized app to provide more precise early-stage Alzheimer's Disease
predictions. By merging the strengths of blockchain, AI, and anomaly detection,
our system represents a pioneering step towards revolutionizing disease
diagnostics.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Intell-dragonfly: A Cybersecurity Attack Surface Generation Engine Based On Artificial Intelligence-generated Content Technology. (arXiv:2311.00240v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00240">http://arxiv.org/abs/2311.00240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00240]] Intell-dragonfly: A Cybersecurity Attack Surface Generation Engine Based On Artificial Intelligence-generated Content Technology(http://arxiv.org/abs/2311.00240)</code></li>
<li>Summary: <p>With the rapid development of the Internet, cyber security issues have become
increasingly prominent. Traditional cyber security defense methods are limited
in the face of ever-changing threats, so it is critical to seek innovative
attack surface generation methods. This study proposes Intell-dragonfly, a
cyber security attack surface generation engine based on artificial
intelligence generation technology, to meet the challenges of cyber security.
Based on ChatGPT technology, this paper designs an automated attack surface
generation process, which can generate diversified and personalized attack
scenarios, targets, elements and schemes. Through experiments in a real network
environment, the effect of the engine is verified and compared with traditional
methods, which improves the authenticity and applicability of the attack
surface. The experimental results show that the ChatGPT-based method has
significant advantages in the accuracy, diversity and operability of attack
surface generation. Furthermore, we explore the strengths and limitations of
the engine and discuss its potential applications in the field of cyber
security. This research provides a novel approach to the field of cyber
security that is expected to have a positive impact on defense and prevention
of cyberthreats.
</p></li>
</ul>

<h3>Title: smart contract, security, vulnerabilities, attacks, defenses. (arXiv:2311.00270v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00270">http://arxiv.org/abs/2311.00270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00270]] smart contract, security, vulnerabilities, attacks, defenses(http://arxiv.org/abs/2311.00270)</code></li>
<li>Summary: <p>With the increasing adoption of smart contracts, ensuring their security has
become a critical concern. Numerous vulnerabilities and attacks have been
identified and exploited, resulting in significant financial losses. In
response, researchers have developed various tools and techniques to identify
and prevent vulnerabilities in smart contracts. In this survey, we present a
systematic overview of the quality assurance of smart contracts, covering
vulnerabilities, attacks, defenses, and tool support. By classifying
vulnerabilities based on known attacks, we can identify patterns and common
weaknesses that need to be addressed. Moreover, in order to effectively protect
smart contracts, we have created a labeled dataset to evaluate various
vulnerability detection tools and compare their effectiveness.
</p></li>
</ul>

<h3>Title: On the Integration of Self-Sovereign Identity with TLS 1.3 Handshake to Build Trust in IoT Systems. (arXiv:2311.00386v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00386">http://arxiv.org/abs/2311.00386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00386]] On the Integration of Self-Sovereign Identity with TLS 1(http://arxiv.org/abs/2311.00386)</code></li>
<li>Summary: <p>The centralized PKI is not a suitable solution to provide identities in
large-scale IoT systems. The main problem is the high cost of managing X.509
certificates throughout their lifecycle, from installation to regular updates
and revocation. The Self-Sovereign Identity (SSI) is a decentralised option
that reduces the need for human intervention, and therefore has the potential
to significantly reduce the complexity and cost associated to identity
management in large-scale IoT systems. However, to leverage the full potential
of SSI, the authentication of IoT nodes needs to be moved from the application
to the Transport Layer Security (TLS) level. This paper contributes to the
adoption of SSI in large-scale IoT systems by addressing, for the first time,
the extension of the original TLS 1.3 handshake to support two new SSI
authentication modes while maintaining the interoperability with nodes
implementing the original handshake protocol. The open source implementation of
the new TLS 1.3 handshake protocol in OpenSSL is used to experimentally prove
the feasibility of the approach.
</p></li>
</ul>

<h3>Title: Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators. (arXiv:2311.00579v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00579">http://arxiv.org/abs/2311.00579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00579]] Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators(http://arxiv.org/abs/2311.00579)</code></li>
<li>Summary: <p>Convolution Neural Networks (CNNs) are widely used in various domains. Recent
advances in dataflow-based CNN accelerators have enabled CNN inference in
resource-constrained edge devices. These dataflow accelerators utilize inherent
data reuse of convolution layers to process CNN models efficiently. Concealing
the architecture of CNN models is critical for privacy and security. This paper
evaluates memory-based side-channel information to recover CNN architectures
from dataflow-based CNN inference accelerators. The proposed attack exploits
spatial and temporal data reuse of the dataflow mapping on CNN accelerators and
architectural hints to recover the structure of CNN models. Experimental
results demonstrate that our proposed side-channel attack can recover the
structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Analyzing Head Orientation of Neurotypical and Autistic Individuals in Triadic Conversations. (arXiv:2311.00343v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00343">http://arxiv.org/abs/2311.00343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00343]] Analyzing Head Orientation of Neurotypical and Autistic Individuals in Triadic Conversations(http://arxiv.org/abs/2311.00343)</code></li>
<li>Summary: <p>We propose a system that estimates people's body and head orientations using
low-resolution point cloud data from two LiDAR sensors. Our models make
accurate estimations in real-world conversation settings where the subject
moves naturally with varying head and body poses. The body orientation
estimation model uses ellipse fitting while the head orientation estimation
model is a pipeline of geometric feature extraction and an ensemble of neural
network regressors. Compared with other body and head orientation estimation
systems using RGB cameras, our proposed system uses LiDAR sensors to preserve
user privacy, while achieving comparable accuracy. Unlike other body/head
orientation estimation systems, our sensors do not require a specified
placement in front of the subject. Our models achieve a mean absolute
estimation error of 5.2 degrees for body orientation and 13.7 degrees for head
orientation. We use our models to quantify behavioral differences between
neurotypical and autistic individuals in triadic conversations. Tests of
significance show that people with autism spectrum disorder display
significantly different behavior compared to neurotypical individuals in terms
of distributing attention between participants in a conversation, suggesting
that the approach could be a component of a behavioral analysis or coaching
system.
</p></li>
</ul>

<h3>Title: Assessing Mobile Application Privacy: A Quantitative Framework for Privacy Measurement. (arXiv:2311.00066v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00066">http://arxiv.org/abs/2311.00066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00066]] Assessing Mobile Application Privacy: A Quantitative Framework for Privacy Measurement(http://arxiv.org/abs/2311.00066)</code></li>
<li>Summary: <p>The proliferation of mobile applications and the subsequent sharing of
personal data with service and application providers have given rise to
substantial privacy concerns. Application marketplaces have introduced
mechanisms to conform to regulations and provide individuals with control over
their data. However, a notable absence persists regarding clear indications,
labels or scores elucidating the privacy implications of these applications. In
response to this challenge, this paper introduces a privacy quantification
framework. The purpose of this framework is to systematically evaluate the
level of privacy risk when using particular Android applications. The main goal
is to provide individuals with qualitative labels to make informed decisions
about their privacy. This work aims to contribute to a digital environment that
prioritizes privacy, promotes informed decision-making, and endorses the
privacy-preserving design principles incorporation.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Stacking an autoencoder for feature selection of zero-day threats. (arXiv:2311.00304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00304">http://arxiv.org/abs/2311.00304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00304]] Stacking an autoencoder for feature selection of zero-day threats(http://arxiv.org/abs/2311.00304)</code></li>
<li>Summary: <p>Zero-day attack detection plays a critical role in mitigating risks,
protecting assets, and staying ahead in the evolving threat landscape. This
study explores the application of stacked autoencoder (SAE), a type of
artificial neural network, for feature selection and zero-day threat
classification using a Long Short-Term Memory (LSTM) scheme. The process
involves preprocessing the UGRansome dataset and training an unsupervised SAE
for feature extraction. Finetuning with supervised learning is then performed
to enhance the discriminative capabilities of this model. The learned weights
and activations of the autoencoder are analyzed to identify the most important
features for discriminating between zero-day threats and normal system
behavior. These selected features form a reduced feature set that enables
accurate classification. The results indicate that the SAE-LSTM performs well
across all three attack categories by showcasing high precision, recall, and F1
score values, emphasizing the model's strong predictive capabilities in
identifying various types of zero-day attacks. Additionally, the balanced
average scores of the SAE-LSTM suggest that the model generalizes effectively
and consistently across different attack categories.
</p></li>
</ul>

<h3>Title: EXTRACT: Explainable Transparent Control of Bias in Embeddings. (arXiv:2311.00115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00115">http://arxiv.org/abs/2311.00115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00115]] EXTRACT: Explainable Transparent Control of Bias in Embeddings(http://arxiv.org/abs/2311.00115)</code></li>
<li>Summary: <p>Knowledge Graphs are a widely used method to represent relations between
entities in various AI applications, and Graph Embedding has rapidly become a
standard technique to represent Knowledge Graphs in such a way as to facilitate
inferences and decisions. As this representation is obtained from behavioural
data, and is not in a form readable by humans, there is a concern that it might
incorporate unintended information that could lead to biases. We propose
EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in
knowledge graph embeddings, so as to assess and decrease the implicit presence
of protected information. Our method uses Canonical Correlation Analysis (CCA)
to investigate the presence, extent and origins of information leaks during
training, then decomposes embeddings into a sum of their private attributes by
solving a linear system. Our experiments, performed on the MovieLens1M dataset,
show that a range of personal attributes can be inferred from a user's viewing
behaviour and preferences, including gender, age, and occupation. Further
experiments, performed on the KG20C citation dataset, show that the information
about the conference in which a paper was published can be inferred from the
citation network of that article. We propose four transparent methods to
maintain the capability of the embedding to make the intended predictions
without retaining unwanted information. A trade-off between these two goals is
observed.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks. (arXiv:2311.00508v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00508">http://arxiv.org/abs/2311.00508</a></li>
<li>Code URL: https://github.com/i-need-sleep/eval_attack</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00508]] Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks(http://arxiv.org/abs/2311.00508)</code></li>
<li>Summary: <p>We investigate MT evaluation metric performance on adversarially-synthesized
texts, to shed light on metric robustness. We experiment with word- and
character-level attacks on three popular machine translation metrics:
BERTScore, BLEURT, and COMET. Our human experiments validate that automatic
metrics tend to overpenalize adversarially-degraded translations. We also
identify inconsistencies in BERTScore ratings, where it judges the original
sentence and the adversarially-degraded one as similar, while judging the
degraded translation as notably worse than the original with respect to the
reference. We identify patterns of brittleness that motivate more robust metric
development.
</p></li>
</ul>

<h3>Title: Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems. (arXiv:2311.00207v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00207">http://arxiv.org/abs/2311.00207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00207]] Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems(http://arxiv.org/abs/2311.00207)</code></li>
<li>Summary: <p>Machine Learning (ML) has been instrumental in enabling joint transceiver
optimization by merging all physical layer blocks of the end-to-end wireless
communication systems. Although there have been a number of adversarial attacks
on ML-based wireless systems, the existing methods do not provide a
comprehensive view including multi-modality of the source data, common physical
layer components, and wireless domain constraints. This paper proposes Magmaw,
the first black-box attack methodology capable of generating universal
adversarial perturbations for any multimodal signal transmitted over a wireless
channel. We further introduce new objectives for adversarial attacks on
ML-based downstream applications. The resilience of the attack to the existing
widely used defense methods of adversarial training and perturbation signal
subtraction is experimentally verified. For proof-of-concept evaluation, we
build a real-time wireless attack platform using a software-defined radio
system. Experimental results demonstrate that Magmaw causes significant
performance degradation even in the presence of the defense mechanisms.
Surprisingly, Magmaw is also effective against encrypted communication channels
and conventional communications.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: YOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers, and Manholes. (arXiv:2311.00073v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00073">http://arxiv.org/abs/2311.00073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00073]] YOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers, and Manholes(http://arxiv.org/abs/2311.00073)</code></li>
<li>Summary: <p>Effective detection of road hazards plays a pivotal role in road
infrastructure maintenance and ensuring road safety. This research paper
provides a comprehensive evaluation of YOLOv8, an object detection model, in
the context of detecting road hazards such as potholes, Sewer Covers, and Man
Holes. A comparative analysis with previous iterations, YOLOv5 and YOLOv7, is
conducted, emphasizing the importance of computational efficiency in various
applications. The paper delves into the architecture of YOLOv8 and explores
image preprocessing techniques aimed at enhancing detection accuracy across
diverse conditions, including variations in lighting, road types, hazard sizes,
and types. Furthermore, hyperparameter tuning experiments are performed to
optimize model performance through adjustments in learning rates, batch sizes,
anchor box sizes, and augmentation strategies. Model evaluation is based on
Mean Average Precision (mAP), a widely accepted metric for object detection
performance. The research assesses the robustness and generalization
capabilities of the models through mAP scores calculated across the diverse
test scenarios, underlining the significance of YOLOv8 in road hazard detection
and infrastructure maintenance.
</p></li>
</ul>

<h3>Title: DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing. (arXiv:2311.00230v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00230">http://arxiv.org/abs/2311.00230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00230]] DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing(http://arxiv.org/abs/2311.00230)</code></li>
<li>Summary: <p>Utilizing visual place recognition (VPR) technology to ascertain the
geographical location of publicly available images is a pressing issue for
real-world VPR applications. Although most current VPR methods achieve
favorable results under ideal conditions, their performance in complex
environments, characterized by lighting variations, seasonal changes, and
occlusions caused by moving objects, is generally unsatisfactory. In this
study, we utilize the DINOv2 model as the backbone network for trimming and
fine-tuning to extract robust image features. We propose a novel VPR
architecture called DINO-Mix, which combines a foundational vision model with
feature aggregation. This architecture relies on the powerful image feature
extraction capabilities of foundational vision models. We employ an
MLP-Mixer-based mix module to aggregate image features, resulting in globally
robust and generalizable descriptors that enable high-precision VPR. We
experimentally demonstrate that the proposed DINO-Mix architecture
significantly outperforms current state-of-the-art (SOTA) methods. In test sets
having lighting variations, seasonal changes, and occlusions (Tokyo24/7,
Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1
accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA
methods, our architecture exhibited an average accuracy improvement of 5.14%.
</p></li>
</ul>

<h3>Title: RAUNE-Net: A Residual and Attention-Driven Underwater Image Enhancement Method. (arXiv:2311.00246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00246">http://arxiv.org/abs/2311.00246</a></li>
<li>Code URL: https://github.com/fansuregrin/raune-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00246]] RAUNE-Net: A Residual and Attention-Driven Underwater Image Enhancement Method(http://arxiv.org/abs/2311.00246)</code></li>
<li>Summary: <p>Underwater image enhancement (UIE) poses challenges due to distinctive
properties of the underwater environment, including low contrast, high
turbidity, visual blurriness, and color distortion. In recent years, the
application of deep learning has quietly revolutionized various areas of
scientific research, including UIE. However, existing deep learning-based UIE
methods generally suffer from issues of weak robustness and limited
adaptability. In this paper, inspired by residual and attention mechanisms, we
propose a more reliable and reasonable UIE network called RAUNE-Net by
employing residual learning of high-level features at the network's bottle-neck
and two aspects of attention manipulations in the down-sampling procedure.
Furthermore, we collect and create two datasets specifically designed for
evaluating UIE methods, which contains different types of underwater
distortions and degradations. The experimental validation demonstrates that our
method obtains promising objective performance and consistent visual results
across various real-world underwater images compared to other eight UIE
methods. Our example code and datasets are publicly available at
https://github.com/fansuregrin/RAUNE-Net.
</p></li>
</ul>

<h3>Title: NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function. (arXiv:2311.00389v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00389">http://arxiv.org/abs/2311.00389</a></li>
<li>Code URL: https://github.com/leoqli/neuralgf</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00389]] NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function(http://arxiv.org/abs/2311.00389)</code></li>
<li>Summary: <p>Normal estimation for 3D point clouds is a fundamental task in 3D geometry
processing. The state-of-the-art methods rely on priors of fitting local
surfaces learned from normal supervision. However, normal supervision in
benchmarks comes from synthetic shapes and is usually not available from real
scans, thereby limiting the learned priors of these methods. In addition,
normal orientation consistency across shapes remains difficult to achieve
without a separate post-processing procedure. To resolve these issues, we
propose a novel method for estimating oriented normals directly from point
clouds without using ground truth normals as supervision. We achieve this by
introducing a new paradigm for learning neural gradient functions, which
encourages the neural network to fit the input point clouds and yield unit-norm
gradients at the points. Specifically, we introduce loss functions to
facilitate query points to iteratively reach the moving targets and aggregate
onto the approximated surface, thereby learning a global surface representation
of the data. Meanwhile, we incorporate gradients into the surface approximation
to measure the minimum signed deviation of queries, resulting in a consistent
gradient field associated with the surface. These techniques lead to our deep
unsupervised oriented normal estimator that is robust to noise, outliers and
density variations. Our excellent results on widely used benchmarks demonstrate
that our method can learn more accurate normals for both unoriented and
oriented normal estimation tasks than the latest methods. The source code and
pre-trained model are publicly available at https://github.com/LeoQLi/NeuralGF.
</p></li>
</ul>

<h3>Title: Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion. (arXiv:2311.00436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00436">http://arxiv.org/abs/2311.00436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00436]] Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion(http://arxiv.org/abs/2311.00436)</code></li>
<li>Summary: <p>Traffic object detection under variable illumination is challenging due to
the information loss caused by the limited dynamic range of conventional
frame-based cameras. To address this issue, we introduce bio-inspired event
cameras and propose a novel Structure-aware Fusion Network (SFNet) that
extracts sharp and complete object structures from the event stream to
compensate for the lost information in images through cross-modality fusion,
enabling the network to obtain illumination-robust representations for traffic
object detection. Specifically, to mitigate the sparsity or blurriness issues
arising from diverse motion states of traffic objects in fixed-interval event
sampling methods, we propose the Reliable Structure Generation Network (RSGNet)
to generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness
of object structures. Next, we design a novel Adaptive Feature Complement
Module (AFCM) which guides the adaptive fusion of two modality features to
compensate for the information loss in the images by perceiving the global
lightness distribution of the images, thereby generating illumination-robust
representations. Finally, considering the lack of large-scale and high-quality
annotations in the existing event-based object detection datasets, we build a
DSEC-Det dataset, which consists of 53 sequences with 63,931 images and more
than 208,000 labels for 8 classes. Extensive experimental results demonstrate
that our proposed SFNet can overcome the perceptual boundaries of conventional
cameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in
mAP50:95. Our code and dataset will be available at
https://github.com/YN-Yang/SFNet.
</p></li>
</ul>

<h3>Title: Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation. (arXiv:2311.00441v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00441">http://arxiv.org/abs/2311.00441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00441]] Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation(http://arxiv.org/abs/2311.00441)</code></li>
<li>Summary: <p>Vision Transformer (ViT) has demonstrated promising performance in computer
vision tasks, comparable to state-of-the-art neural networks. Yet, this new
type of deep neural network architecture is vulnerable to adversarial attacks
limiting its capabilities in terms of robustness. This article presents a novel
contribution aimed at further improving the accuracy and robustness of ViT,
particularly in the face of adversarial attacks. We propose an augmentation
technique called `Dynamic Scanning Augmentation' that leverages dynamic input
sequences to adaptively focus on different patches, thereby maintaining
performance and robustness. Our detailed investigations reveal that this
adaptability to the input sequence induces significant changes in the attention
mechanism of ViT, even for the same image. We introduce four variations of
Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to
adversarial attacks and accuracy against natural images, with one variant
showing comparable results. By integrating our augmentation technique, we
observe a substantial increase in ViT's robustness, improving it from $17\%$ to
$92\%$ measured across different types of adversarial attacks. These findings,
together with other comprehensive tests, indicate that Dynamic Scanning
Augmentation enhances accuracy and robustness by promoting a more adaptive type
of attention. In conclusion, this work contributes to the ongoing research on
Vision Transformers by introducing Dynamic Scanning Augmentation as a technique
for improving the accuracy and robustness of ViT. The observed results
highlight the potential of this approach in advancing computer vision tasks and
merit further exploration in future studies.
</p></li>
</ul>

<h3>Title: Group Distributionally Robust Knowledge Distillation. (arXiv:2311.00476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00476">http://arxiv.org/abs/2311.00476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00476]] Group Distributionally Robust Knowledge Distillation(http://arxiv.org/abs/2311.00476)</code></li>
<li>Summary: <p>Knowledge distillation enables fast and effective transfer of features
learned from a bigger model to a smaller one. However, distillation objectives
are susceptible to sub-population shifts, a common scenario in medical imaging
analysis which refers to groups/domains of data that are underrepresented in
the training set. For instance, training models on health data acquired from
multiple scanners or hospitals can yield subpar performance for minority
groups. In this paper, inspired by distributionally robust optimization (DRO)
techniques, we address this shortcoming by proposing a group-aware distillation
loss. During optimization, a set of weights is updated based on the per-group
losses at a given iteration. This way, our method can dynamically focus on
groups that have low performance during training. We empirically validate our
method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs)
and show consistent improvement in terms of worst-group accuracy.
</p></li>
</ul>

<h3>Title: ProcSim: Proxy-based Confidence for Robust Similarity Learning. (arXiv:2311.00668v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00668">http://arxiv.org/abs/2311.00668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00668]] ProcSim: Proxy-based Confidence for Robust Similarity Learning(http://arxiv.org/abs/2311.00668)</code></li>
<li>Summary: <p>Deep Metric Learning (DML) methods aim at learning an embedding space in
which distances are closely related to the inherent semantic similarity of the
inputs. Previous studies have shown that popular benchmark datasets often
contain numerous wrong labels, and DML methods are susceptible to them.
Intending to study the effect of realistic noise, we create an ontology of the
classes in a dataset and use it to simulate semantically coherent labeling
mistakes. To train robust DML models, we propose ProcSim, a simple framework
that assigns a confidence score to each sample using the normalized distance to
its class representative. The experimental results show that the proposed
method achieves state-of-the-art performance on the DML benchmark datasets
injected with uniform and the proposed semantically coherent noise.
</p></li>
</ul>

<h3>Title: Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00172">http://arxiv.org/abs/2311.00172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00172]] Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield(http://arxiv.org/abs/2311.00172)</code></li>
<li>Summary: <p>Large Language Models' safety remains a critical concern due to their
vulnerability to adversarial attacks, which can prompt these systems to produce
harmful responses. In the heart of these systems lies a safety classifier, a
computational model trained to discern and mitigate potentially harmful,
offensive, or unethical outputs. However, contemporary safety classifiers,
despite their potential, often fail when exposed to inputs infused with
adversarial noise. In response, our study introduces the Adversarial Prompt
Shield (APS), a lightweight model that excels in detection accuracy and
demonstrates resilience against adversarial prompts. Additionally, we propose
novel strategies for autonomously generating adversarial training datasets,
named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are
designed to fortify the safety classifier's robustness, and we investigate the
consequences of incorporating adversarial examples into the training process.
Through evaluations involving Large Language Models, we demonstrate that our
classifier has the potential to decrease the attack success rate resulting from
adversarial attacks by up to 60%. This advancement paves the way for the next
generation of more reliable and resilient conversational agents.
</p></li>
</ul>

<h3>Title: Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00258">http://arxiv.org/abs/2311.00258</a></li>
<li>Code URL: https://github.com/hiroki39/noisy-exemplars-make-large-language-models-more-robust</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00258]] Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis(http://arxiv.org/abs/2311.00258)</code></li>
<li>Summary: <p>Recent advances in prompt engineering enable large language models (LLMs) to
solve multi-hop logical reasoning problems with impressive accuracy. However,
there is little existing work investigating the robustness of LLMs with
few-shot prompting techniques. Therefore, we introduce a systematic approach to
test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic
perturbations. We include perturbations at multiple levels of abstractions
(e.g. lexical perturbations such as typos, and semantic perturbations such as
the inclusion of intermediate reasoning steps in the questions) to conduct
behavioral analysis on the LLMs. Throughout our experiments, we find that
models are more sensitive to certain perturbations such as replacing words with
their synonyms. We also demonstrate that increasing the proportion of perturbed
exemplars in the prompts improves the robustness of few-shot prompting methods.
</p></li>
</ul>

<h3>Title: Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00367">http://arxiv.org/abs/2311.00367</a></li>
<li>Code URL: https://github.com/lalalamdbf/plse_idrr</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00367]] Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition(http://arxiv.org/abs/2311.00367)</code></li>
<li>Summary: <p>Implicit Discourse Relation Recognition (IDRR), which infers discourse
relations without the help of explicit connectives, is still a crucial and
challenging task for discourse parsing. Recent works tend to exploit the
hierarchical structure information from the annotated senses, which demonstrate
enhanced discourse relation representations can be obtained by integrating
sense hierarchy. Nevertheless, the performance and robustness for IDRR are
significantly constrained by the availability of annotated data. Fortunately,
there is a wealth of unannotated utterances with explicit connectives, that can
be utilized to acquire enriched discourse relation features. In light of such
motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)
method for IDRR. Essentially, our method seamlessly injects knowledge relevant
to discourse relation into pre-trained language models through prompt-based
connective prediction. Furthermore, considering the prompt-based connective
prediction exhibits local dependencies due to the deficiency of masked language
model (MLM) in capturing global semantics, we design a novel self-supervised
learning objective based on mutual information maximization to derive enhanced
representations of logical semantics for IDRR. Experimental results on PDTB 2.0
and CoNLL16 datasets demonstrate that our method achieves outstanding and
consistent performance against the current state-of-the-art models.
</p></li>
</ul>

<h3>Title: AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification. (arXiv:2311.00408v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00408">http://arxiv.org/abs/2311.00408</a></li>
<li>Code URL: https://github.com/ukplab/adasent</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00408]] AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification(http://arxiv.org/abs/2311.00408)</code></li>
<li>Summary: <p>Recent work has found that few-shot sentence classification based on
pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In
this work, we investigate strategies for domain-specialization in the context
of few-shot sentence classification with SEs. We first establish that
unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language
Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot
sentence classification by up to 8.4 points. However, applying DAPT on SEs, on
the one hand, disrupts the effects of their (general-domain) Sentence Embedding
Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of
a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient,
since the computationally expensive SEPT needs to be executed on top of a
DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples
SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be
inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent's
effectiveness in extensive experiments on 17 different few-shot sentence
classification datasets. AdaSent matches or surpasses the performance of full
SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code
for AdaSent is available.
</p></li>
</ul>

<h3>Title: Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. (arXiv:2311.00430v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00430">http://arxiv.org/abs/2311.00430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00430]] Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling(http://arxiv.org/abs/2311.00430)</code></li>
<li>Summary: <p>As the size of pre-trained speech recognition models increases, running these
large models in low-latency or resource-constrained environments becomes
challenging. In this work, we leverage pseudo-labelling to assemble a
large-scale open-source dataset which we use to distill the Whisper model into
a smaller variant, called Distil-Whisper. Using a simple word error rate (WER)
heuristic, we select only the highest quality pseudo-labels for training. The
distilled model is 5.8 times faster with 51% fewer parameters, while performing
to within 1% WER on out-of-distribution test data in a zero-shot transfer
setting. Distil-Whisper maintains the robustness of the Whisper model to
difficult acoustic conditions, while being less prone to hallucination errors
on long-form audio. Distil-Whisper is designed to be paired with Whisper for
speculative decoding, yielding a 2 times speed-up while mathematically ensuring
the same outputs as the original model. To facilitate further research in this
domain, we make our training code, inference code and models publicly
accessible.
</p></li>
</ul>

<h3>Title: Bandit-Driven Batch Selection for Robust Learning under Label Noise. (arXiv:2311.00096v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00096">http://arxiv.org/abs/2311.00096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00096]] Bandit-Driven Batch Selection for Robust Learning under Label Noise(http://arxiv.org/abs/2311.00096)</code></li>
<li>Summary: <p>We introduce a novel approach for batch selection in Stochastic Gradient
Descent (SGD) training, leveraging combinatorial bandit algorithms. Our
methodology focuses on optimizing the learning process in the presence of label
noise, a prevalent issue in real-world datasets. Experimental evaluations on
the CIFAR-10 dataset reveal that our approach consistently outperforms existing
methods across various levels of label corruption. Importantly, we achieve this
superior performance without incurring the computational overhead commonly
associated with auxiliary neural network models. This work presents a balanced
trade-off between computational efficiency and model efficacy, offering a
scalable solution for complex machine learning applications.
</p></li>
</ul>

<h3>Title: Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00322">http://arxiv.org/abs/2311.00322</a></li>
<li>Code URL: https://github.com/hyeonsoojo/metagc</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00322]] Robust Graph Clustering via Meta Weighting for Noisy Graphs(http://arxiv.org/abs/2311.00322)</code></li>
<li>Summary: <p>How can we find meaningful clusters in a graph robustly against noise edges?
Graph clustering (i.e., dividing nodes into groups of similar ones) is a
fundamental problem in graph analysis with applications in various fields.
Recent studies have demonstrated that graph neural network (GNN) based
approaches yield promising results for graph clustering. However, we observe
that their performance degenerates significantly on graphs with noise edges,
which are prevalent in practice. In this work, we propose MetaGC for robust
GNN-based graph clustering. MetaGC employs a decomposable clustering loss
function, which can be rephrased as a sum of losses over node pairs. We add a
learnable weight to each node pair, and MetaGC adaptively adjusts the weights
of node pairs using meta-weighting so that the weights of meaningful node pairs
increase and the weights of less-meaningful ones (e.g., noise edges) decrease.
We show empirically that MetaGC learns weights as intended and consequently
outperforms the state-of-the-art GNN-based competitors, even when they are
equipped with separate denoising schemes, on five real-world graphs under
varying levels of noise. Our code and datasets are available at
https://github.com/HyeonsooJo/MetaGC.
</p></li>
</ul>

<h3>Title: NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks. (arXiv:2311.00428v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00428">http://arxiv.org/abs/2311.00428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00428]] NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks(http://arxiv.org/abs/2311.00428)</code></li>
<li>Summary: <p>While multi-exit neural networks are regarded as a promising solution for
making efficient inference via early exits, combating adversarial attacks
remains a challenging problem. In multi-exit networks, due to the high
dependency among different submodels, an adversarial example targeting a
specific exit not only degrades the performance of the target exit but also
reduces the performance of all other exits concurrently. This makes multi-exit
networks highly vulnerable to simple adversarial attacks. In this paper, we
propose NEO-KD, a knowledge-distillation-based adversarial training strategy
that tackles this fundamental challenge based on two key contributions. NEO-KD
first resorts to neighbor knowledge distillation to guide the output of the
adversarial examples to tend to the ensemble outputs of neighbor exits of clean
data. NEO-KD also employs exit-wise orthogonal knowledge distillation for
reducing adversarial transferability across different submodels. The result is
a significantly improved robustness against adversarial attacks. Experimental
results on various datasets/models show that our method achieves the best
adversarial accuracy with reduced computation budgets, compared to the
baselines relying on existing adversarial training or knowledge distillation
techniques for multi-exit networks.
</p></li>
</ul>

<h3>Title: Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00619">http://arxiv.org/abs/2311.00619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00619]] Loss Modeling for Multi-Annotator Datasets(http://arxiv.org/abs/2311.00619)</code></li>
<li>Summary: <p>Accounting for the opinions of all annotators of a dataset is critical for
fairness. However, when annotating large datasets, individual annotators will
frequently provide thousands of ratings which can lead to fatigue.
Additionally, these annotation processes can occur over multiple days which can
lead to an inaccurate representation of an annotator's opinion over time. To
combat this, we propose to learn a more accurate representation of diverse
opinions by utilizing multitask learning in conjunction with loss-based label
correction. We show that using our novel formulation, we can cleanly separate
agreeing and disagreeing annotations. Furthermore, we demonstrate that this
modification can improve prediction performance in a single or multi-annotator
setting. Lastly, we show that this method remains robust to additional label
noise that is applied to subjective data.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning. (arXiv:2311.00068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00068">http://arxiv.org/abs/2311.00068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00068]] View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning(http://arxiv.org/abs/2311.00068)</code></li>
<li>Summary: <p>Echocardiography provides an important tool for clinicians to observe the
function of the heart in real time, at low cost, and without harmful radiation.
Automated localization and classification of heart valves enables automatic
extraction of quantities associated with heart mechanical function and related
blood flow measurements. We propose a machine learning pipeline that uses deep
neural networks for separate classification and localization steps. As the
first step in the pipeline, we apply view classification to echocardiograms
with ten unique anatomic views of the heart. In the second step, we apply deep
learning-based object detection to both localize and identify the valves. Image
segmentation based object detection in echocardiography has been shown in many
earlier studies but, to the best of our knowledge, this is the first study that
predicts the bounding boxes around the valves along with classification from 2D
ultrasound images with the help of deep neural networks. Our object detection
experiments applied to the Apical views suggest that it is possible to localize
and identify multiple valves precisely.
</p></li>
</ul>

<h3>Title: Graph Representation Learning for Infrared and Visible Image Fusion. (arXiv:2311.00291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00291">http://arxiv.org/abs/2311.00291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00291]] Graph Representation Learning for Infrared and Visible Image Fusion(http://arxiv.org/abs/2311.00291)</code></li>
<li>Summary: <p>Infrared and visible image fusion aims to extract complementary features to
synthesize a single fused image. Many methods employ convolutional neural
networks (CNNs) to extract local features due to its translation invariance and
locality. However, CNNs fail to consider the image's non-local self-similarity
(NLss), though it can expand the receptive field by pooling operations, it
still inevitably leads to information loss. In addition, the transformer
structure extracts long-range dependence by considering the correlativity among
all image patches, leading to information redundancy of such transformer-based
methods. However, graph representation is more flexible than grid (CNN) or
sequence (transformer structure) representation to address irregular objects,
and graph can also construct the relationships among the spatially repeatable
details or texture with far-space distance. Therefore, to address the above
issues, it is significant to convert images into the graph space and thus adopt
graph convolutional networks (GCNs) to extract NLss. This is because the graph
can provide a fine structure to aggregate features and propagate information
across the nearest vertices without introducing redundant information.
Concretely, we implement a cascaded NLss extraction pattern to extract NLss of
intra- and inter-modal by exploring interactions of different image pixels in
intra- and inter-image positional distance. We commence by preforming GCNs on
each intra-modal to aggregate features and propagate information to extract
independent intra-modal NLss. Then, GCNs are performed on the concatenate
intra-modal NLss features of infrared and visible images, which can explore the
cross-domain NLss of inter-modal to reconstruct the fused image. Ablation
studies and extensive experiments illustrates the effectiveness and superiority
of the proposed method on three datasets.
</p></li>
</ul>

<h3>Title: From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities. (arXiv:2311.00308v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00308">http://arxiv.org/abs/2311.00308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00308]] From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities(http://arxiv.org/abs/2311.00308)</code></li>
<li>Summary: <p>The multimodal task of Visual Question Answering (VQA) encompassing elements
of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate
answers to questions on any visual input. Over time, the scope of VQA has
expanded from datasets focusing on an extensive collection of natural images to
datasets featuring synthetic images, video, 3D environments, and various other
visual inputs. The emergence of large pre-trained networks has shifted the
early VQA approaches relying on feature extraction and fusion schemes to vision
language pre-training (VLP) techniques. However, there is a lack of
comprehensive surveys that encompass both traditional VQA architectures and
contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of
VQA haven't been thoroughly explored, leaving room for potential open problems
to emerge. Our work presents a survey in the domain of VQA that delves into the
intricacies of VQA datasets and methods over the field's history, introduces a
detailed taxonomy to categorize the facets of VQA, and highlights the recent
trends, challenges, and scopes for improvement. We further generalize VQA to
multimodal question answering, explore tasks related to VQA, and present a set
of open problems for future investigation. The work aims to navigate both
beginners and experts by shedding light on the potential avenues of research
and expanding the boundaries of the field.
</p></li>
</ul>

<h3>Title: Progressive Recurrent Network for Shadow Removal. (arXiv:2311.00455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00455">http://arxiv.org/abs/2311.00455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00455]] Progressive Recurrent Network for Shadow Removal(http://arxiv.org/abs/2311.00455)</code></li>
<li>Summary: <p>Single-image shadow removal is a significant task that is still unresolved.
Most existing deep learning-based approaches attempt to remove the shadow
directly, which can not deal with the shadow well. To handle this issue, we
consider removing the shadow in a coarse-to-fine fashion and propose a simple
but effective Progressive Recurrent Network (PRNet). The network aims to remove
the shadow progressively, enabing us to flexibly adjust the number of
iterations to strike a balance between performance and time. Our network
comprises two parts: shadow feature extraction and progressive shadow removal.
Specifically, the first part is a shallow ResNet which constructs the
representations of the input shadow image on its original size, preventing the
loss of high-frequency details caused by the downsampling operation. The second
part has two critical components: the re-integration module and the update
module. The proposed re-integration module can fully use the outputs of the
previous iteration, providing input for the update module for further shadow
removal. In this way, the proposed PRNet makes the whole process more concise
and only uses 29% network parameters than the best published method. Extensive
experiments on the three benchmarks, ISTD, ISTD+, and SRD, demonstrate that our
method can effectively remove shadows and achieve superior performance.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00201">http://arxiv.org/abs/2311.00201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00201]] Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning(http://arxiv.org/abs/2311.00201)</code></li>
<li>Summary: <p>Federated reinforcement learning (RL) enables collaborative decision making
of multiple distributed agents without sharing local data trajectories. In this
work, we consider a multi-task setting, in which each agent has its own private
reward function corresponding to different tasks, while sharing the same
transition kernel of the environment. Focusing on infinite-horizon tabular
Markov decision processes, the goal is to learn a globally optimal policy that
maximizes the sum of the discounted total rewards of all the agents in a
decentralized manner, where each agent only communicates with its neighbors
over some prescribed graph topology. We develop federated vanilla and
entropy-regularized natural policy gradient (NPG) methods under softmax
parameterization, where gradient tracking is applied to the global Q-function
to mitigate the impact of imperfect information sharing. We establish
non-asymptotic global convergence guarantees under exact policy evaluation,
which are nearly independent of the size of the state-action space and
illuminate the impacts of network size and connectivity. To the best of our
knowledge, this is the first time that global convergence is established for
federated multi-task RL using policy optimization. Moreover, the convergence
behavior of the proposed algorithms is robust against inexactness of policy
evaluation.
</p></li>
</ul>

<h3>Title: StableFDG: Style and Attention Based Learning for Federated Domain Generalization. (arXiv:2311.00227v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00227">http://arxiv.org/abs/2311.00227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00227]] StableFDG: Style and Attention Based Learning for Federated Domain Generalization(http://arxiv.org/abs/2311.00227)</code></li>
<li>Summary: <p>Traditional federated learning (FL) algorithms operate under the assumption
that the data distributions at training (source domains) and testing (target
domain) are the same. The fact that domain shifts often occur in practice
necessitates equipping FL methods with a domain generalization (DG) capability.
However, existing DG algorithms face fundamental challenges in FL setups due to
the lack of samples/domains in each client's local dataset. In this paper, we
propose StableFDG, a style and attention based learning strategy for
accomplishing federated domain generalization, introducing two key
contributions. The first is style-based learning, which enables each client to
explore novel styles beyond the original source domains in its local dataset,
improving domain diversity based on the proposed style sharing, shifting, and
exploration strategies. Our second contribution is an attention-based feature
highlighter, which captures the similarities between the features of data
samples in the same class, and emphasizes the important/common characteristics
to better learn the domain-invariant characteristics of each class in data-poor
FL scenarios. Experimental results show that StableFDG outperforms existing
baselines on various DG benchmark datasets, demonstrating its efficacy.
</p></li>
</ul>

<h3>Title: Federated Topic Model and Model Pruning Based on Variational Autoencoder. (arXiv:2311.00314v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00314">http://arxiv.org/abs/2311.00314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00314]] Federated Topic Model and Model Pruning Based on Variational Autoencoder(http://arxiv.org/abs/2311.00314)</code></li>
<li>Summary: <p>Topic modeling has emerged as a valuable tool for discovering patterns and
topics within large collections of documents. However, when cross-analysis
involves multiple parties, data privacy becomes a critical concern. Federated
topic modeling has been developed to address this issue, allowing multiple
parties to jointly train models while protecting pri-vacy. However, there are
communication and performance challenges in the federated sce-nario. In order
to solve the above problems, this paper proposes a method to establish a
federated topic model while ensuring the privacy of each node, and use neural
network model pruning to accelerate the model, where the client periodically
sends the model neu-ron cumulative gradients and model weights to the server,
and the server prunes the model. To address different requirements, two
different methods are proposed to determine the model pruning rate. The first
method involves slow pruning throughout the entire model training process,
which has limited acceleration effect on the model training process, but can
ensure that the pruned model achieves higher accuracy. This can significantly
reduce the model inference time during the inference process. The second
strategy is to quickly reach the target pruning rate in the early stage of
model training in order to accelerate the model training speed, and then
continue to train the model with a smaller model size after reaching the target
pruning rate. This approach may lose more useful information but can complete
the model training faster. Experimental results show that the federated topic
model pruning based on the variational autoencoder proposed in this paper can
greatly accelerate the model training speed while ensuring the model's
performance.
</p></li>
</ul>

<h3>Title: MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00334">http://arxiv.org/abs/2311.00334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00334]] MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows(http://arxiv.org/abs/2311.00334)</code></li>
<li>Summary: <p>A Federated Learning (FL) system typically consists of two core processing
entities: the federation controller and the learners. The controller is
responsible for managing the execution of FL workflows across learners and the
learners for training and evaluating federated models over their private
datasets. While executing an FL workflow, the FL system has no control over the
computational resources or data of the participating learners. Still, it is
responsible for other operations, such as model aggregation, task dispatching,
and scheduling. These computationally heavy operations generally need to be
handled by the federation controller. Even though many FL systems have been
recently proposed to facilitate the development of FL workflows, most of these
systems overlook the scalability of the controller. To meet this need, we
designed and developed a novel FL system called MetisFL, where the federation
controller is the first-class citizen. MetisFL re-engineers all the operations
conducted by the federation controller to accelerate the training of
large-scale FL workflows. By quantitatively comparing MetisFL against other
state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a
10-fold wall-clock time execution boost across a wide range of challenging FL
workflows with increasing model sizes and federation sites.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation. (arXiv:2311.00306v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00306">http://arxiv.org/abs/2311.00306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00306]] Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation(http://arxiv.org/abs/2311.00306)</code></li>
<li>Summary: <p>Large Language Models (LLMs) can generate biased and toxic responses. Yet
most prior work on LLM gender bias evaluation requires predefined
gender-related phrases or gender stereotypes, which are challenging to be
comprehensively collected and are limited to explicit bias evaluation. In
addition, we believe that instances devoid of gender-related language or
explicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in
this work, we propose a conditional text generation mechanism without the need
for predefined gender phrases and stereotypes. This approach employs three
types of inputs generated through three distinct strategies to probe LLMs,
aiming to show evidence of explicit and implicit gender biases in LLMs. We also
utilize explicit and implicit evaluation metrics to evaluate gender bias in
LLMs under different strategies. Our experiments demonstrate that an increased
model size does not consistently lead to enhanced fairness and all tested LLMs
exhibit explicit and/or implicit gender bias, even when explicit gender
stereotypes are absent in the inputs.
</p></li>
</ul>

<h3>Title: FairWASP: Fast and Optimal Fair Wasserstein Pre-processing. (arXiv:2311.00109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00109">http://arxiv.org/abs/2311.00109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00109]] FairWASP: Fast and Optimal Fair Wasserstein Pre-processing(http://arxiv.org/abs/2311.00109)</code></li>
<li>Summary: <p>Recent years have seen a surge of machine learning approaches aimed at
reducing disparities in model outputs across different subgroups. In many
settings, training data may be used in multiple downstream applications by
different users, which means it may be most effective to intervene on the
training data itself. In this work, we present FairWASP, a novel pre-processing
approach designed to reduce disparities in classification datasets without
modifying the original data. FairWASP returns sample-level weights such that
the reweighted dataset minimizes the Wasserstein distance to the original
dataset while satisfying (an empirical version of) demographic parity, a
popular fairness criterion. We show theoretically that integer weights are
optimal, which means our method can be equivalently understood as duplicating
or eliminating samples. FairWASP can therefore be used to construct datasets
which can be fed into any classification method, not just methods which accept
sample weights. Our work is based on reformulating the pre-processing task as a
large-scale mixed-integer program (MIP), for which we propose a highly
efficient algorithm based on the cutting plane method. Experiments on synthetic
datasets demonstrate that our proposed optimization algorithm significantly
outperforms state-of-the-art commercial solvers in solving both the MIP and its
linear program relaxation. Further experiments highlight the competitive
performance of FairWASP in reducing disparities while preserving accuracy in
downstream classification settings.
</p></li>
</ul>

<h3>Title: FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00638">http://arxiv.org/abs/2311.00638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00638]] FAIRLABEL: Correcting Bias in Labels(http://arxiv.org/abs/2311.00638)</code></li>
<li>Summary: <p>There are several algorithms for measuring fairness of ML models. A
fundamental assumption in these approaches is that the ground truth is fair or
unbiased. In real-world datasets, however, the ground truth often contains data
that is a result of historical and societal biases and discrimination. Models
trained on these datasets will inherit and propagate the biases to the model
outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases
in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across
groups while maintaining high accuracy in predictions. We propose metrics to
measure the quality of bias correction and validate FAIRLABEL on synthetic
datasets and show that the label correction is correct 86.7% of the time vs.
71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such
as UCI Adult, German Credit Risk, and Compas datasets and show that the
Disparate Impact Ratio increases by as much as 54.2%.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00237">http://arxiv.org/abs/2311.00237</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00237]] The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities(http://arxiv.org/abs/2311.00237)</code></li>
<li>Summary: <p>Understanding emergent abilities, such as in-context learning (ICL) and
chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost
importance. This importance stems not only from the better utilization of these
capabilities across various tasks, but also from the proactive identification
and mitigation of potential risks, including concerns of truthfulness, bias,
and toxicity, that may arise alongside these capabilities. In this paper, we
present a thorough survey on the interpretation and analysis of emergent
abilities of LLMs. First, we provide a concise introduction to the background
and definition of emergent abilities. Then, we give an overview of advancements
from two perspectives: 1) a macro perspective, emphasizing studies on the
mechanistic interpretability and delving into the mathematical foundations
behind emergent abilities; and 2) a micro-perspective, concerning studies that
focus on empirical interpretability by examining factors associated with these
abilities. We conclude by highlighting the challenges encountered and
suggesting potential avenues for future research. We believe that our work
establishes the basis for further exploration into the interpretation of
emergent abilities.
</p></li>
</ul>

<h3>Title: Emotion Detection for Misinformation: A Review. (arXiv:2311.00671v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00671">http://arxiv.org/abs/2311.00671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00671]] Emotion Detection for Misinformation: A Review(http://arxiv.org/abs/2311.00671)</code></li>
<li>Summary: <p>With the advent of social media, an increasing number of netizens are sharing
and reading posts and news online. However, the huge volumes of misinformation
(e.g., fake news and rumors) that flood the internet can adversely affect
people's lives, and have resulted in the emergence of rumor and fake news
detection as a hot research topic. The emotions and sentiments of netizens, as
expressed in social media posts and news, constitute important factors that can
help to distinguish fake news from genuine news and to understand the spread of
rumors. This article comprehensively reviews emotion-based methods for
misinformation detection. We begin by explaining the strong links between
emotions and misinformation. We subsequently provide a detailed analysis of a
range of misinformation detection methods that employ a variety of emotion,
sentiment and stance-based features, and describe their strengths and
weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based
misinformation detection based on large language models and suggest future
research directions, including data collection (multi-platform, multilingual),
annotation, benchmark, multimodality, and interpretability.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00189">http://arxiv.org/abs/2311.00189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00189]] XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision(http://arxiv.org/abs/2311.00189)</code></li>
<li>Summary: <p>Text classification aims to effectively categorize documents into pre-defined
categories. Traditional methods for text classification often rely on large
amounts of manually annotated training data, making the process time-consuming
and labor-intensive. To address this issue, recent studies have focused on
weakly-supervised and extremely weakly-supervised settings, which require
minimal or no human annotation, respectively. In previous methods of weakly
supervised text classification, pseudo-training data is generated by assigning
pseudo-labels to documents based on their alignment (e.g., keyword matching)
with specific classes. However, these methods ignore the importance of
incorporating the explanations of the generated pseudo-labels, or saliency of
individual words, as additional guidance during the text classification
training process. To address this limitation, we propose XAI-CLASS, a novel
explanation-enhanced extremely weakly-supervised text classification method
that incorporates word saliency prediction as an auxiliary task. XAI-CLASS
begins by employing a multi-round question-answering process to generate
pseudo-training data that promotes the mutual enhancement of class labels and
corresponding explanation word generation. This pseudo-training data is then
used to train a multi-task framework that simultaneously learns both text
classification and word saliency prediction. Extensive experiments on several
weakly-supervised text classification datasets show that XAI-CLASS outperforms
other weakly-supervised text classification methods significantly. Moreover,
experiments demonstrate that XAI-CLASS enhances both model performance and
explainability.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion. (arXiv:2311.00056v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00056">http://arxiv.org/abs/2311.00056</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00056]] Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion(http://arxiv.org/abs/2311.00056)</code></li>
<li>Summary: <p>Recent progress in text-to-image (TTI) systems, such as StableDiffusion,
Imagen, and DALL-E 2, have made it possible to create realistic images with
simple text prompts. It is tempting to use these systems to eliminate the
manual task of obtaining natural images for training a new machine learning
classifier. However, in all of the experiments performed to date, classifiers
trained solely with synthetic images perform poorly at inference, despite the
images used for training appearing realistic. Examining this apparent
incongruity in detail gives insight into the limitations of the underlying
image generation processes. Through the lens of diversity in image creation
vs.accuracy of what is created, we dissect the differences in semantic
mismatches in what is modeled in synthetic vs. natural images. This will
elucidate the roles of the image-languag emodel, CLIP, and the image generation
model, diffusion. We find four issues that limit the usefulness of TTI systems
for this task: ambiguity, adherence to prompt, lack of diversity, and inability
to represent the underlying concept. We further present surprising insights
into the geometry of CLIP embeddings.
</p></li>
</ul>

<h3>Title: Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00157">http://arxiv.org/abs/2311.00157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00157]] Score Normalization for a Faster Diffusion Exponential Integrator Sampler(http://arxiv.org/abs/2311.00157)</code></li>
<li>Summary: <p>Recently, zhang et al have proposed the Diffusion Exponential Integrator
Sampler (DEIS) for fast generation of samples from Diffusion Models. It
leverages the semi-linear nature of the probability flow ordinary differential
equation (ODE) in order to greatly reduce integration error and improve
generation quality at low numbers of function evaluations (NFEs). Key to this
approach is the score function reparameterisation, which reduces the
integration error incurred from using a fixed score function estimate over each
integration step. The original authors use the default parameterisation used by
models trained for noise prediction -- multiply the score by the standard
deviation of the conditional forward noising distribution. We find that
although the mean absolute value of this score parameterisation is close to
constant for a large portion of the reverse sampling process, it changes
rapidly at the end of sampling. As a simple fix, we propose to instead
reparameterise the score (at inference) by dividing it by the average absolute
value of previous score estimates at that time step collected from offline high
NFE generations. We find that our score normalisation (DEIS-SN) consistently
improves FID compared to vanilla DEIS, showing an FID improvement from 6.44 to
5.57 at 10 NFEs for our CIFAR-10 experiments. Our code is available at
https://github.com/mtkresearch/Diffusion-DEIS-SN.
</p></li>
</ul>

<h3>Title: Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning. (arXiv:2311.00339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00339">http://arxiv.org/abs/2311.00339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00339]] Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning(http://arxiv.org/abs/2311.00339)</code></li>
<li>Summary: <p>The consistent mapping from poems to paintings is essential for the research
and restoration of traditional Chinese gardens. But the lack of firsthand
ma-terial is a great challenge to the reconstruction work. In this paper, we
pro-pose a method to generate garden paintings based on text descriptions using
deep learning method. Our image-text pair dataset consists of more than one
thousand Ming Dynasty Garden paintings and their inscriptions and post-scripts.
A latent text-to-image diffusion model learns the mapping from de-scriptive
texts to garden paintings of the Ming Dynasty, and then the text description of
Jichang Garden guides the model to generate new garden paintings. The cosine
similarity between the guide text and the generated image is the evaluation
criterion for the generated images. Our dataset is used to fine-tune the
pre-trained diffusion model using Low-Rank Adapta-tion of Large Language Models
(LoRA). We also transformed the generated images into a panorama and created a
free-roam scene in Unity 3D. Our post-trained model is capable of generating
garden images in the style of Ming Dynasty landscape paintings based on textual
descriptions. The gener-ated images are compatible with three-dimensional
presentation in Unity 3D.
</p></li>
</ul>

<h3>Title: LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation. (arXiv:2311.00353v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00353">http://arxiv.org/abs/2311.00353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00353]] LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation(http://arxiv.org/abs/2311.00353)</code></li>
<li>Summary: <p>Leveraging the generative ability of image diffusion models offers great
potential for zero-shot video-to-video translation. The key lies in how to
maintain temporal consistency across generated video frames by image diffusion
models. Previous methods typically adopt cross-frame attention, \emph{i.e.,}
sharing the \textit{key} and \textit{value} tokens across attentions of
different frames, to encourage the temporal consistency. However, in those
works, temporal inconsistency issue may not be thoroughly solved, rendering the
fidelity of generated videos limited.%The current state of the art cross-frame
attention method aims at maintaining fine-grained visual details across frames,
but it is still challenged by the temporal coherence problem. In this paper, we
find the bottleneck lies in the unconstrained query tokens and propose a new
zero-shot video-to-video translation framework, named \textit{LatentWarp}. Our
approach is simple: to constrain the query tokens to be temporally consistent,
we further incorporate a warping operation in the latent space to constrain the
query tokens. Specifically, based on the optical flow obtained from the
original video, we warp the generated latent features of last frame to align
with the current frame during the denoising process. As a result, the
corresponding regions across the adjacent frames can share closely-related
query tokens and attention outputs, which can further improve latent-level
consistency to enhance visual temporal coherence of generated videos. Extensive
experiment results demonstrate the superiority of \textit{LatentWarp} in
achieving video-to-video translation with temporal coherence.
</p></li>
</ul>

<h3>Title: Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos. (arXiv:2311.00469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00469">http://arxiv.org/abs/2311.00469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00469]] Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos(http://arxiv.org/abs/2311.00469)</code></li>
<li>Summary: <p>Out-of-distribution (OOD) detection is essential to improve the reliability
of machine learning models by detecting samples that do not belong to the
training distribution. Detecting OOD samples effectively in certain tasks can
pose a challenge because of the substantial heterogeneity within the
in-distribution (ID), and the high structural similarity between ID and OOD
classes. For instance, when detecting heart views in fetal ultrasound videos
there is a high structural similarity between the heart and other anatomies
such as the abdomen, and large in-distribution variance as a heart has 5
distinct views and structural variations within each view. To detect OOD
samples in this context, the resulting model should generalise to the
intra-anatomy variations while rejecting similar OOD samples. In this paper, we
introduce dual-conditioned diffusion models (DCDM) where we condition the model
on in-distribution class information and latent features of the input image for
reconstruction-based OOD detection. This constrains the generative manifold of
the model to generate images structurally and semantically similar to those
within the in-distribution. The proposed model outperforms reference methods
with a 12% improvement in accuracy, 22% higher precision, and an 8% better F1
score.
</p></li>
</ul>

<h3>Title: Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00500">http://arxiv.org/abs/2311.00500</a></li>
<li>Code URL: https://github.com/sail-sg/d-trak</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00500]] Intriguing Properties of Data Attribution on Diffusion Models(http://arxiv.org/abs/2311.00500)</code></li>
<li>Summary: <p>Data attribution seeks to trace model outputs back to training data. With the
recent development of diffusion models, data attribution has become a desired
module to properly assign valuations for high-quality or copyrighted training
samples, ensuring that data contributors are fairly compensated or credited.
Several theoretically motivated methods have been proposed to implement data
attribution, in an effort to improve the trade-off between computational
scalability and effectiveness. In this work, we conduct extensive experiments
and ablation studies on attributing diffusion models, specifically focusing on
DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model
LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive
observations that theoretically unjustified design choices for attribution
empirically outperform previous baselines by a large margin, in terms of both
linear datamodeling score and counterfactual evaluation. Our work presents a
significantly more efficient approach for attributing diffusion models, while
the unexpected findings suggest that at least in non-convex settings,
constructions guided by theoretical assumptions may lead to inferior
attribution performance. The code is available at
https://github.com/sail-sg/D-TRAK.
</p></li>
</ul>

<h3>Title: De-Diffusion Makes Text a Strong Cross-Modal Interface. (arXiv:2311.00618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00618">http://arxiv.org/abs/2311.00618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00618]] De-Diffusion Makes Text a Strong Cross-Modal Interface(http://arxiv.org/abs/2311.00618)</code></li>
<li>Summary: <p>We demonstrate text as a strong cross-modal interface. Rather than relying on
deep embeddings to connect image and language as the interface representation,
our approach represents an image as text, from which we enjoy the
interpretability and flexibility inherent to natural language. We employ an
autoencoder that uses a pre-trained text-to-image diffusion model for decoding.
The encoder is trained to transform an input image into text, which is then fed
into the fixed text-to-image diffusion decoder to reconstruct the original
input -- a process we term De-Diffusion. Experiments validate both the
precision and comprehensiveness of De-Diffusion text representing images, such
that it can be readily ingested by off-the-shelf text-to-image tools and LLMs
for diverse multi-modal tasks. For example, a single De-Diffusion model can
generalize to provide transferable prompts for different text-to-image tools,
and also achieves a new state of the art on open-ended vision-language tasks by
simply prompting large language models with few-shot examples.
</p></li>
</ul>

<h3>Title: Diffusion models for probabilistic programming. (arXiv:2311.00474v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00474">http://arxiv.org/abs/2311.00474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00474]] Diffusion models for probabilistic programming(http://arxiv.org/abs/2311.00474)</code></li>
<li>Summary: <p>We propose Diffusion Model Variational Inference (DMVI), a novel method for
automated approximate inference in probabilistic programming languages (PPLs).
DMVI utilizes diffusion models as variational approximations to the true
posterior distribution by deriving a novel bound to the marginal likelihood
objective used in Bayesian modelling. DMVI is easy to implement, allows
hassle-free inference in PPLs without the drawbacks of, e.g., variational
inference using normalizing flows, and does not make any constraints on the
underlying neural network model. We evaluate DMVI on a set of common Bayesian
models and show that its posterior inferences are in general more accurate than
those of contemporary methods used in PPLs while having a similar computational
cost and requiring less manual tuning.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Object-centric Video Representation for Long-term Action Anticipation. (arXiv:2311.00180v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00180">http://arxiv.org/abs/2311.00180</a></li>
<li>Code URL: https://github.com/brown-palm/objectprompt</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00180]] Object-centric Video Representation for Long-term Action Anticipation(http://arxiv.org/abs/2311.00180)</code></li>
<li>Summary: <p>This paper focuses on building object-centric representations for long-term
action anticipation in videos. Our key motivation is that objects provide
important cues to recognize and predict human-object interactions, especially
when the predictions are longer term, as an observed "background" object could
be used by the human actor in the future. We observe that existing object-based
video recognition frameworks either assume the existence of in-domain
supervised object detectors or follow a fully weakly-supervised pipeline to
infer object locations from action labels. We propose to build object-centric
video representations by leveraging visual-language pretrained models. This is
achieved by "object prompts", an approach to extract task-specific
object-centric representations from general-purpose pretrained models without
finetuning. To recognize and predict human-object interactions, we use a
Transformer-based neural architecture which allows the "retrieval" of relevant
objects for action anticipation at various time scales. We conduct extensive
evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both
quantitative and qualitative results confirm the effectiveness of our proposed
method.
</p></li>
</ul>

<h3>Title: 1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking. (arXiv:2311.00241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00241">http://arxiv.org/abs/2311.00241</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00241]] 1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking(http://arxiv.org/abs/2311.00241)</code></li>
<li>Summary: <p>Recently, heatmap regression methods based on 1D landmark representations
have shown prominent performance on locating facial landmarks. However,
previous methods ignored to make deep explorations on the good potentials of 1D
landmark representations for sequential and structural modeling of multiple
landmarks to track facial landmarks. To address this limitation, we propose a
Transformer architecture, namely 1DFormer, which learns informative 1D landmark
representations by capturing the dynamic and the geometric patterns of
landmarks via token communications in both temporal and spatial dimensions for
facial landmark tracking. For temporal modeling, we propose a recurrent token
mixing mechanism, an axis-landmark-positional embedding mechanism, as well as a
confidence-enhanced multi-head attention mechanism to adaptively and robustly
embed long-term landmark dynamics into their 1D representations; for structure
modeling, we design intra-group and inter-group structure modeling mechanisms
to encode the component-level as well as global-level facial structure patterns
as a refinement for the 1D representations of landmarks through token
communications in the spatial dimension via 1D convolutional layers.
Experimental results on the 300VW and the TF databases show that 1DFormer
successfully models the long-range sequential patterns as well as the inherent
facial structures to learn informative 1D representations of landmark
sequences, and achieves state-of-the-art performance on facial landmark
tracking.
</p></li>
</ul>

<h3>Title: fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding. (arXiv:2311.00342v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00342">http://arxiv.org/abs/2311.00342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00342]] fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding(http://arxiv.org/abs/2311.00342)</code></li>
<li>Summary: <p>The exploration of brain activity and its decoding from fMRI data has been a
longstanding pursuit, driven by its potential applications in brain-computer
interfaces, medical diagnostics, and virtual reality. Previous approaches have
primarily focused on individual subject analysis, highlighting the need for a
more universal and adaptable framework, which is the core motivation behind our
work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach
for fMRI pre-training, with a focus on addressing the challenges of varying
fMRI data dimensions due to individual brain differences. Our approach involves
transforming fMRI signals into unified 2D representations, ensuring consistency
in dimensions and preserving distinct brain activity patterns. We introduce a
novel learning strategy tailored for pre-training 2D fMRI images, enhancing the
quality of reconstruction. fMRI-PTE's adaptability with image generators
enables the generation of well-represented fMRI features, facilitating various
downstream tasks, including within-subject and cross-subject brain activity
decoding. Our contributions encompass introducing fMRI-PTE, innovative data
transformation, efficient training, a novel learning strategy, and the
universal applicability of our approach. Extensive experiments validate and
support our claims, offering a promising foundation for further research in
this domain.
</p></li>
</ul>

<h3>Title: A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios. (arXiv:2311.00401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00401">http://arxiv.org/abs/2311.00401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00401]] A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios(http://arxiv.org/abs/2311.00401)</code></li>
<li>Summary: <p>Human pose assessment and correction play a crucial role in applications
across various fields, including computer vision, robotics, sports analysis,
healthcare, and entertainment. In this paper, we propose a Spatial-Temporal
Transformer based Framework (STTF) for human pose assessment and correction in
education scenarios such as physical exercises and science experiment. The
framework comprising skeletal tracking, pose estimation, posture assessment,
and posture correction modules to educate students with professional,
quick-to-fix feedback. We also create a pose correction method to provide
corrective feedback in the form of visual aids. We test the framework with our
own dataset. It comprises (a) new recordings of five exercises, (b) existing
recordings found on the internet of the same exercises, and (c) corrective
feedback on the recordings by professional athletes and teachers. Results show
that our model can effectively measure and comment on the quality of students'
actions. The STTF leverages the power of transformer models to capture spatial
and temporal dependencies in human poses, enabling accurate assessment and
effective correction of students' movements.
</p></li>
</ul>

<h3>Title: Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status. (arXiv:2311.00565v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00565">http://arxiv.org/abs/2311.00565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00565]] Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status(http://arxiv.org/abs/2311.00565)</code></li>
<li>Summary: <p>Intensive Care Units (ICU) provide close supervision and continuous care to
patients with life-threatening conditions. However, continuous patient
assessment in the ICU is still limited due to time constraints and the workload
on healthcare providers. Existing patient assessments in the ICU such as pain
or mobility assessment are mostly sporadic and administered manually, thus
introducing the potential for human errors. Developing Artificial intelligence
(AI) tools that can augment human assessments in the ICU can be beneficial for
providing more objective and granular monitoring capabilities. For example,
capturing the variations in a patient's facial cues related to pain or
agitation can help in adjusting pain-related medications or detecting
agitation-inducing conditions such as delirium. Additionally, subtle changes in
visual cues during or prior to adverse clinical events could potentially aid in
continuous patient monitoring when combined with high-resolution physiological
signals and Electronic Health Record (EHR) data. In this paper, we examined the
association between visual cues and patient condition including acuity status,
acute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064
frames collected in the ICU annotated with facial action units (AUs) labels by
trained annotators. We developed a new "masked loss computation" technique that
addresses the data imbalance problem by maximizing data resource utilization.
We trained the model using our AU-ICU dataset in conjunction with three
external datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57
mean F1-score and 0.89 mean accuracy on the test set. Additionally, we
performed AU inference on 634,054 frames to evaluate the association between
facial AUs and clinically important patient conditions such as acuity status,
acute brain dysfunction, and pain.
</p></li>
</ul>

<h3>Title: PAUMER: Patch Pausing Transformer for Semantic Segmentation. (arXiv:2311.00586v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00586">http://arxiv.org/abs/2311.00586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00586]] PAUMER: Patch Pausing Transformer for Semantic Segmentation(http://arxiv.org/abs/2311.00586)</code></li>
<li>Summary: <p>We study the problem of improving the efficiency of segmentation transformers
by using disparate amounts of computation for different parts of the image. Our
method, PAUMER, accomplishes this by pausing computation for patches that are
deemed to not need any more computation before the final decoder. We use the
entropy of predictions computed from intermediate activations as the pausing
criterion, and find this aligns well with semantics of the image. Our method
has a unique advantage that a single network trained with the proposed strategy
can be effortlessly adapted at inference to various run-time requirements by
modulating its pausing parameters. On two standard segmentation datasets,
Cityscapes and ADE20K, we show that our method operates with about a $50\%$
higher throughput with an mIoU drop of about $0.65\%$ and $4.6\%$ respectively.
</p></li>
</ul>

<h3>Title: Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00208">http://arxiv.org/abs/2311.00208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00208]] Transformers as Recognizers of Formal Languages: A Survey on Expressivity(http://arxiv.org/abs/2311.00208)</code></li>
<li>Summary: <p>As transformers have gained prominence in natural language processing, some
researchers have investigated theoretically what problems they can and cannot
solve, by treating problems as formal languages. Exploring questions such as
this will help to compare transformers with other models, and transformer
variants with one another, for various tasks. Work in this subarea has made
considerable progress in recent years. Here, we undertake a comprehensive
survey of this work, documenting the diverse assumptions that underlie
different results and providing a unified framework for harmonizing seemingly
contradictory findings.
</p></li>
</ul>

<h3>Title: Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?. (arXiv:2311.00268v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00268">http://arxiv.org/abs/2311.00268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00268]] Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?(http://arxiv.org/abs/2311.00268)</code></li>
<li>Summary: <p>A line of work on Transformer-based language models such as BERT has
attempted to use syntactic inductive bias to enhance the pretraining process,
on the theory that building syntactic structure into the training process
should reduce the amount of data needed for training. But such methods are
often tested for high-resource languages such as English. In this work, we
investigate whether these methods can compensate for data sparseness in
low-resource languages, hypothesizing that they ought to be more effective for
low-resource languages. We experiment with five low-resource languages: Uyghur,
Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic
inductive bias methods produce uneven results in low-resource settings, and
provide surprisingly little benefit in most cases.
</p></li>
</ul>

<h3>Title: Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00502">http://arxiv.org/abs/2311.00502</a></li>
<li>Code URL: https://github.com/intel/intel-extension-for-transformers</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00502]] Efficient LLM Inference on CPUs(http://arxiv.org/abs/2311.00502)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable performance and
tremendous potential across a wide range of tasks. However, deploying these
models has been challenging due to the astronomical amount of model parameters,
which requires a demand for large memory capacity and high memory bandwidth. In
this paper, we propose an effective approach that can make the deployment of
LLMs more efficiently. We support an automatic INT4 weight-only quantization
flow and design a special LLM runtime with highly-optimized kernels to
accelerate the LLM inference on CPUs. We demonstrate the general applicability
of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase
the extreme inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
</p></li>
</ul>

<h3>Title: Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00588">http://arxiv.org/abs/2311.00588</a></li>
<li>Code URL: https://github.com/yuyangstat/flowsum</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00588]] Boosting Summarization with Normalizing Flows and Aggressive Training(http://arxiv.org/abs/2311.00588)</code></li>
<li>Summary: <p>This paper presents FlowSUM, a normalizing flows-based variational
encoder-decoder framework for Transformer-based summarization. Our approach
tackles two primary challenges in variational summarization: insufficient
semantic information in latent representations and posterior collapse during
training. To address these challenges, we employ normalizing flows to enable
flexible latent posterior modeling, and we propose a controlled alternate
aggressive training (CAAT) strategy with an improved gate mechanism.
Experimental results show that FlowSUM significantly enhances the quality of
generated summaries and unleashes the potential for knowledge distillation with
minimal impact on inference time. Furthermore, we investigate the issue of
posterior collapse in normalizing flows and analyze how the summary quality is
affected by the training strategy, gate initialization, and the type and number
of normalizing flows used, offering valuable insights for future research.
</p></li>
</ul>

<h3>Title: Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00684">http://arxiv.org/abs/2311.00684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00684]] Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation(http://arxiv.org/abs/2311.00684)</code></li>
<li>Summary: <p>An ideal length-extrapolatable Transformer language model can handle
sequences longer than the training length without any long sequence
fine-tuning. Such long-context utilization capability highly relies on a
flexible positional embedding design. Upon investigating the flexibility of
existing large pre-trained Transformer language models, we find that the T5
family deserves a closer look, as its positional embeddings capture rich and
flexible attention patterns. However, T5 suffers from the dispersed attention
issue: the longer the input sequence, the flatter the attention distribution.
To alleviate the issue, we propose two attention alignment strategies via
temperature scaling. Our findings improve the long-context utilization
capability of T5 on language modeling, retrieval, and multi-document question
answering without any fine-tuning, suggesting that a flexible positional
embedding design and attention alignment go a long way toward Transformer
length
extrapolation.\footnote{\url{https://github.com/chijames/Attention-Alignment-Transformer-Length-Extrapolation}}
</p></li>
</ul>

<h3>Title: WinNet:time series forecasting with a window-enhanced period extracting and interacting. (arXiv:2311.00214v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00214">http://arxiv.org/abs/2311.00214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00214]] WinNet:time series forecasting with a window-enhanced period extracting and interacting(http://arxiv.org/abs/2311.00214)</code></li>
<li>Summary: <p>Recently, Transformer-based methods have significantly improved
state-of-the-art time series forecasting results, but they suffer from high
computational costs and the inability to capture the long and short periodicity
of time series. We present a highly accurate and simply structured CNN-based
model for long-term time series forecasting tasks, called WinNet, including (i)
Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with
long and short periodicity according to the predefined periodic window, (ii)
Two-Dimensional Period Decomposition (TDPD) to model period-trend and
oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage
the correlations of the period-trend and oscillation terms to support the
prediction tasks by CNNs. Results on nine benchmark datasets show that the
WinNet can achieve SOTA performance and lower computational complexity over
CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the
CNN-based methods in the time series forecasting tasks, with perfect tradeoff
between performance and efficiency.
</p></li>
</ul>

<h3>Title: Rethinking Decision Transformer via Hierarchical Reinforcement Learning. (arXiv:2311.00267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00267">http://arxiv.org/abs/2311.00267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00267]] Rethinking Decision Transformer via Hierarchical Reinforcement Learning(http://arxiv.org/abs/2311.00267)</code></li>
<li>Summary: <p>Decision Transformer (DT) is an innovative algorithm leveraging recent
advances of the transformer architecture in reinforcement learning (RL).
However, a notable limitation of DT is its reliance on recalling trajectories
from datasets, losing the capability to seamlessly stitch sub-optimal
trajectories together. In this work we introduce a general sequence modeling
framework for studying sequential decision making through the lens of
Hierarchical RL. At the time of making decisions, a high-level policy first
proposes an ideal prompt for the current state, a low-level policy subsequently
generates an action conditioned on the given prompt. We show DT emerges as a
special case of this framework with certain choices of high-level and low-level
policies, and discuss the potential failure of these choices. Inspired by these
observations, we study how to jointly optimize the high-level and low-level
policies to enable the stitching ability, which further leads to the
development of new offline RL algorithms. Our empirical results clearly show
that the proposed algorithms significantly surpass DT on several control and
navigation benchmarks. We hope our contributions can inspire the integration of
transformer architectures within the field of RL.
</p></li>
</ul>

<h3>Title: Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00636">http://arxiv.org/abs/2311.00636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00636]] Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures(http://arxiv.org/abs/2311.00636)</code></li>
<li>Summary: <p>The core components of many modern neural network architectures, such as
transformers, convolutional, or graph neural networks, can be expressed as
linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate
Curvature (K-FAC), a second-order optimisation method, has shown promise to
speed up neural network training and thereby reduce computational costs.
However, there is currently no framework to apply it to generic architectures,
specifically ones with linear weight-sharing layers. In this work, we identify
two different settings of linear weight-sharing layers which motivate two
flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they
are exact for deep linear networks with weight-sharing in their respective
setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we
leverage to speed up automatic hyperparameter selection via optimising the
marginal likelihood for a Wide ResNet. Finally, we observe little difference
between these two K-FAC variations when using them to train both a graph neural
network and a vision transformer. However, both variations are able to reach a
fixed validation metric target in $50$-$75\%$ of the number of steps of a
first-order reference run, which translates into a comparable improvement in
wall-clock time. This highlights the potential of applying K-FAC to modern
neural network architectures.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00318">http://arxiv.org/abs/2311.00318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00318]] Flooding Regularization for Stable Training of Generative Adversarial Networks(http://arxiv.org/abs/2311.00318)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) have shown remarkable performance in
image generation. However, GAN training suffers from the problem of
instability. One of the main approaches to address this problem is to modify
the loss function, often using regularization terms in addition to changing the
type of adversarial losses. This paper focuses on directly regularizing the
adversarial loss function. We propose a method that applies flooding, an
overfitting suppression method in supervised learning, to GANs to directly
prevent the discriminator's loss from becoming excessively low. Flooding
requires tuning the flood level, but when applied to GANs, we propose that the
appropriate range of flood level settings is determined by the adversarial loss
function, supported by theoretical analysis of GANs using the binary cross
entropy loss. We experimentally verify that flooding stabilizes GAN training
and can be combined with other stabilization techniques. We also reveal that by
restricting the discriminator's loss to be no greater than flood level, the
training proceeds stably even when the flood level is somewhat high.
</p></li>
</ul>

<h3>Title: Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT) Enhancement with Multi-task Customized Perceptual Loss. (arXiv:2311.00412v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00412">http://arxiv.org/abs/2311.00412</a></li>
<li>Code URL: https://github.com/zhujiarui42/cfp-loss</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00412]] Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT) Enhancement with Multi-task Customized Perceptual Loss(http://arxiv.org/abs/2311.00412)</code></li>
<li>Summary: <p>Cone-beam computed tomography (CBCT) is routinely collected during
image-guided radiation therapy (IGRT) to provide updated patient anatomy
information for cancer treatments. However, CBCT images often suffer from
streaking artifacts and noise caused by under-rate sampling projections and
low-dose exposure, resulting in low clarity and information loss. While recent
deep learning-based CBCT enhancement methods have shown promising results in
suppressing artifacts, they have limited performance on preserving anatomical
details since conventional pixel-to-pixel loss functions are incapable of
describing detailed anatomy. To address this issue, we propose a novel
feature-oriented deep learning framework that translates low-quality CBCT
images into high-quality CT-like imaging via a multi-task customized
feature-to-feature perceptual loss function. The framework comprises two main
components: a multi-task learning feature-selection network(MTFS-Net) for
customizing the perceptual loss function; and a CBCT-to-CT translation network
guided by feature-to-feature perceptual loss, which uses advanced generative
models such as U-Net, GAN and CycleGAN. Our experiments showed that the
proposed framework can generate synthesized CT (sCT) images for the lung that
achieved a high similarity to CT images, with an average SSIM index of 0.9869
and an average PSNR index of 39.9621. The sCT images also achieved visually
pleasing performance with effective artifacts suppression, noise reduction, and
distinctive anatomical details preservation. Our experiment results indicate
that the proposed framework outperforms the state-of-the-art models for
pulmonary CBCT enhancement. This framework holds great promise for generating
high-quality anatomical imaging from CBCT that is suitable for various clinical
applications.
</p></li>
</ul>

<h3>Title: JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00286">http://arxiv.org/abs/2311.00286</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00286]] JADE: A Linguistic-based Safety Evaluation Platform for LLM(http://arxiv.org/abs/2311.00286)</code></li>
<li>Summary: <p>In this paper, we present \textit{JADE}, a targeted linguistic fuzzing
platform which strengthens the linguistic complexity of seed questions to
simultaneously and consistently break a wide range of widely-used LLMs
categorized in three groups: eight open-sourced Chinese, six commercial Chinese
and four commercial English LLMs. JADE generates three safety benchmarks for
the three groups of LLMs, which contain unsafe questions that are highly
threatening: the questions simultaneously trigger harmful generation of
multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$}
(please see the table below), while are still natural questions, fluent and
preserving the core unsafe semantics. We release the benchmark demos generated
for commercial English LLMs and open-sourced English LLMs in the following
link: https://github.com/whitzard-ai/jade-db. For readers who are interested in
evaluating on more questions generated by JADE, please contact us.
</p>
<p>\textit{JADE} is based on Noam Chomsky's seminal theory of
transformational-generative grammar. Given a seed question with unsafe
intention, \textit{JADE} invokes a sequence of generative and transformational
rules to increment the complexity of the syntactic structure of the original
question, until the safety guardrail is broken. Our key insight is: Due to the
complexity of human language, most of the current best LLMs can hardly
recognize the invariant evil from the infinite number of different syntactic
structures which form an unbound example space that can never be fully covered.
Technically, the generative/transformative rules are constructed by native
speakers of the languages, and, once developed, can be used to automatically
grow and transform the parse tree of a given question, until the guardrail is
broken. For more evaluation results and demo, please check our website:
https://whitzard-ai.github.io/jade.html.
</p></li>
</ul>

<h3>Title: HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00321">http://arxiv.org/abs/2311.00321</a></li>
<li>Code URL: https://github.com/joonkeekim/hare-hate-speech</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00321]] HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning(http://arxiv.org/abs/2311.00321)</code></li>
<li>Summary: <p>With the proliferation of social media, accurate detection of hate speech has
become critical to ensure safety online. To combat nuanced forms of hate
speech, it is important to identify and thoroughly explain hate speech to help
users understand its harmful effects. Recent benchmarks have attempted to
tackle this issue by training generative models on free-text annotations of
implications in hateful text. However, we find significant reasoning gaps in
the existing annotations schemes, which may hinder the supervision of detection
models. In this paper, we introduce a hate speech detection framework, HARE,
which harnesses the reasoning capabilities of large language models (LLMs) to
fill these gaps in explanations of hate speech, thus enabling effective
supervision of detection models. Experiments on SBIC and Implicit Hate
benchmarks show that our method, using model-generated data, consistently
outperforms baselines, using existing free-text human annotations. Analysis
demonstrates that our method enhances the explanation quality of trained models
and improves generalization to unseen datasets. Our code is available at
https://github.com/joonkeekim/hare-hate-speech.git.
</p></li>
</ul>

<h3>Title: An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00541">http://arxiv.org/abs/2311.00541</a></li>
<li>Code URL: https://github.com/schyanzafar/edisc</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00541]] An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek(http://arxiv.org/abs/2311.00541)</code></li>
<li>Summary: <p>Word meanings change over time, and word senses evolve, emerge or die out in
the process. For ancient languages, where the corpora are often small, sparse
and noisy, modelling such changes accurately proves challenging, and
quantifying uncertainty in sense-change estimates consequently becomes
important. GASC and DiSC are existing generative models that have been used to
analyse sense change for target words from an ancient Greek text corpus, using
unsupervised learning without the help of any pre-training. These models
represent the senses of a given target word such as "kosmos" (meaning
decoration, order or world) as distributions over context words, and sense
prevalence as a distribution over senses. The models are fitted using MCMC
methods to measure temporal changes in these representations. In this paper, we
introduce EDiSC, an embedded version of DiSC, which combines word embeddings
with DiSC to provide superior model performance. We show empirically that EDiSC
offers improved predictive accuracy, ground-truth recovery and uncertainty
quantification, as well as better sampling efficiency and scalability
properties with MCMC methods. We also discuss the challenges of fitting these
models.
</p></li>
</ul>

<h3>Title: Crosslingual Retrieval Augmented In-context Learning for Bangla. (arXiv:2311.00587v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00587">http://arxiv.org/abs/2311.00587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00587]] Crosslingual Retrieval Augmented In-context Learning for Bangla(http://arxiv.org/abs/2311.00587)</code></li>
<li>Summary: <p>The promise of Large Language Models (LLMs) in Natural Language Processing
has often been overshadowed by their limited performance in low-resource
languages such as Bangla. To address this, our paper presents a pioneering
approach that utilizes cross-lingual retrieval augmented in-context learning.
By strategically sourcing semantically similar prompts from high-resource
language, we enable multilingual pretrained language models (MPLMs), especially
the generative model BLOOMZ, to successfully boost performance on Bangla tasks.
Our extensive evaluation highlights that the cross-lingual retrieval augmented
prompts bring steady improvements to MPLMs over the zero-shot performance.
</p></li>
</ul>

<h3>Title: Uncertainty quantification and out-of-distribution detection using surjective normalizing flows. (arXiv:2311.00377v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00377">http://arxiv.org/abs/2311.00377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00377]] Uncertainty quantification and out-of-distribution detection using surjective normalizing flows(http://arxiv.org/abs/2311.00377)</code></li>
<li>Summary: <p>Reliable quantification of epistemic and aleatoric uncertainty is of crucial
importance in applications where models are trained in one environment but
applied to multiple different environments, often seen in real-world
applications for example, in climate science or mobility analysis. We propose a
simple approach using surjective normalizing flows to identify
out-of-distribution data sets in deep neural network models that can be
computed in a single forward pass. The method builds on recent developments in
deep uncertainty quantification and generative modeling with normalizing flows.
We apply our method to a synthetic data set that has been simulated using a
mechanistic model from the mobility literature and several data sets simulated
from interventional distributions induced by soft and atomic interventions on
that model, and demonstrate that our method can reliably discern
out-of-distribution data from in-distribution data. We compare the surjective
flow model to a Dirichlet process mixture model and a bijective flow and find
that the surjections are a crucial component to reliably distinguish
in-distribution from out-of-distribution data.
</p></li>
</ul>

<h3>Title: Optimal Budgeted Rejection Sampling for Generative Models. (arXiv:2311.00460v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00460">http://arxiv.org/abs/2311.00460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00460]] Optimal Budgeted Rejection Sampling for Generative Models(http://arxiv.org/abs/2311.00460)</code></li>
<li>Summary: <p>Rejection sampling methods have recently been proposed to improve the
performance of discriminator-based generative models. However, these methods
are only optimal under an unlimited sampling budget, and are usually applied to
a generator trained independently of the rejection procedure. We first propose
an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal
with respect to \textit{any} $f$-divergence between the true distribution and
the post-rejection distribution, for a given sampling budget. Second, we
propose an end-to-end method that incorporates the sampling scheme into the
training procedure to further enhance the model's overall performance. Through
experiments and supporting theory, we show that the proposed methods are
effective in significantly improving the quality and diversity of the samples.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: ChatGPT-Powered Hierarchical Comparisons for Image Classification. (arXiv:2311.00206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00206">http://arxiv.org/abs/2311.00206</a></li>
<li>Code URL: https://github.com/zhiyuan-r/chatgpt-powered-hierarchical-comparisons-for-image-classification</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00206]] ChatGPT-Powered Hierarchical Comparisons for Image Classification(http://arxiv.org/abs/2311.00206)</code></li>
<li>Summary: <p>The zero-shot open-vocabulary challenge in image classification is tackled by
pretrained vision-language models like CLIP, which benefit from incorporating
class-specific knowledge from large language models (LLMs) like ChatGPT.
However, biases in CLIP lead to similar descriptions for distinct but related
classes, prompting our novel image classification framework via hierarchical
comparisons: using LLMs to recursively group classes into hierarchies and
classifying images by comparing image-text embeddings at each hierarchy level,
resulting in an intuitive, effective, and explainable approach.
</p></li>
</ul>

<h3>Title: BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00117">http://arxiv.org/abs/2311.00117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00117]] BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B(http://arxiv.org/abs/2311.00117)</code></li>
<li>Summary: <p>Llama 2-Chat is a collection of large language models that Meta developed and
released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output
harmful content, we hypothesize that public access to model weights enables bad
actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's
capabilities for malicious purposes. We demonstrate that it is possible to
effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than
$200, while retaining its general capabilities. Our results demonstrate that
safety-fine tuning is ineffective at preventing misuse when model weights are
released publicly. Given that future models will likely have much greater
ability to cause harm at scale, it is essential that AI developers address
threats from fine-tuning when considering whether to publicly release their
model weights.
</p></li>
</ul>

<h3>Title: ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00176">http://arxiv.org/abs/2311.00176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00176]] ChipNeMo: Domain-Adapted LLMs for Chip Design(http://arxiv.org/abs/2311.00176)</code></li>
<li>Summary: <p>ChipNeMo aims to explore the applications of large language models (LLMs) for
industrial chip design. Instead of directly deploying off-the-shelf commercial
or open-source LLMs, we instead adopt the following domain adaptation
techniques: custom tokenizers, domain-adaptive continued pretraining,
supervised fine-tuning (SFT) with domain-specific instructions, and
domain-adapted retrieval models. We evaluate these methods on three selected
LLM applications for chip design: an engineering assistant chatbot, EDA script
generation, and bug summarization and analysis. Our results show that these
domain adaptation techniques enable significant LLM performance improvements
over general-purpose base models across the three evaluated applications,
enabling up to 5x model size reduction with similar or better performance on a
range of design tasks. Our findings also indicate that there's still room for
improvement between our current results and ideal outcomes. We believe that
further investigation of domain-adapted LLM approaches will help close this gap
in the future.
</p></li>
</ul>

<h3>Title: Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00204">http://arxiv.org/abs/2311.00204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00204]] Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering(http://arxiv.org/abs/2311.00204)</code></li>
<li>Summary: <p>Large language models exhibit promising general capabilities but often lack
specialized knowledge for domain-specific tasks. Developing domain experts from
a base model enables a range of applications without prohibitive training
costs. This work demonstrates a method using continuous training and
instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese
medical domain. We first conduct continuous training on 1B tokens from Chinese
medical references to teach relevant vocabulary and knowledge. The models are
then fine-tuned on 54K examples sourced from the Chinese National Medical
Licensing Examination. Experiments on Chinese medical data confirm the
effectiveness of this approach, producing a model comparable to GPT-3.5-turbo
while using way less computational resource. The resulting domain-specific
model could be useful for various Chinese medical applications. More broadly,
this provides a template for domain-specific training of large language models
in areas where pre-trained models lack the required expertise, such as law,
science, and engineering.
</p></li>
</ul>

<h3>Title: Is GPT Powerful Enough to Analyze the Emotions of Memes?. (arXiv:2311.00223v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00223">http://arxiv.org/abs/2311.00223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00223]] Is GPT Powerful Enough to Analyze the Emotions of Memes?(http://arxiv.org/abs/2311.00223)</code></li>
<li>Summary: <p>Large Language Models (LLMs), representing a significant achievement in
artificial intelligence (AI) research, have demonstrated their ability in a
multitude of tasks. This project aims to explore the capabilities of GPT-3.5, a
leading example of LLMs, in processing the sentiment analysis of Internet
memes. Memes, which include both verbal and visual aspects, act as a powerful
yet complex tool for expressing ideas and sentiments, demanding an
understanding of societal norms and cultural contexts. Notably, the detection
and moderation of hateful memes pose a significant challenge due to their
implicit offensive nature. This project investigates GPT's proficiency in such
subjective tasks, revealing its strengths and potential limitations. The tasks
include the classification of meme sentiment, determination of humor type, and
detection of implicit hate in memes. The performance evaluation, using datasets
from SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative
understanding of GPT responses against human annotations. Despite GPT's
remarkable progress, our findings underscore the challenges faced by these
models in handling subjective tasks, which are rooted in their inherent
limitations including contextual understanding, interpretation of implicit
meanings, and data biases. This research contributes to the broader discourse
on the applicability of AI in handling complex, context-dependent tasks, and
offers valuable insights for future advancements.
</p></li>
</ul>

<h3>Title: Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00262">http://arxiv.org/abs/2311.00262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00262]] Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents(http://arxiv.org/abs/2311.00262)</code></li>
<li>Summary: <p>Proactive dialogues serve as a practical yet challenging dialogue problem in
the era of large language models (LLMs), where the dialogue policy planning is
the key to improving the proactivity of LLMs. Most existing studies enable the
dialogue policy planning of LLMs using various prompting schemes or iteratively
enhance this capability in handling the given case with verbal AI feedback.
However, these approaches are either bounded by the policy planning capability
of the frozen LLMs or hard to be transferred to new cases. In this work, we
introduce a new dialogue policy planning paradigm to strategize LLMs for
proactive dialogue problems with a tunable language model plug-in as a
plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a
novel training framework to facilitate supervised fine-tuning over available
human-annotated data as well as reinforcement learning from goal-oriented AI
feedback with dynamic interaction data collected by the LLM-based self-play
simulation. In this manner, the LLM-powered dialogue agent can not only be
generalized to different cases after the training, but also be applicable to
different applications by just substituting the learned plug-in. In addition,
we propose to evaluate the policy planning capability of dialogue systems under
the interactive setting. Experimental results demonstrate that PPDPP
consistently and substantially outperforms existing approaches on three
different proactive dialogue applications, including negotiation, emotional
support, and tutoring dialogues.
</p></li>
</ul>

<h3>Title: SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations. (arXiv:2311.00273v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00273">http://arxiv.org/abs/2311.00273</a></li>
<li>Code URL: https://github.com/scutcyr/soulchat</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00273]] SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations(http://arxiv.org/abs/2311.00273)</code></li>
<li>Summary: <p>Large language models (LLMs) have been widely applied in various fields due
to their excellent capability for memorizing knowledge and chain of thought
(CoT). When these language models are applied in the field of psychological
counseling, they often rush to provide universal advice. However, when users
seek psychological support, they need to gain empathy, trust, understanding and
comfort, rather than just reasonable advice. To this end, we constructed a
multi-turn empathetic conversation dataset of more than 2 million samples, in
which the input is the multi-turn conversation context, and the target is
empathetic responses that cover expressions such as questioning, comfort,
recognition, listening, trust, emotional support, etc. Experiments have shown
that the empathy ability of LLMs can be significantly enhanced when finetuning
by using multi-turn dialogue history and responses that are closer to the
expression of a psychological consultant.
</p></li>
</ul>

<h3>Title: Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00287">http://arxiv.org/abs/2311.00287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00287]] Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models(http://arxiv.org/abs/2311.00287)</code></li>
<li>Summary: <p>Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. We will publish our
code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.
</p></li>
</ul>

<h3>Title: Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00288">http://arxiv.org/abs/2311.00288</a></li>
<li>Code URL: https://github.com/pluslabnlp/active-it</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00288]] Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks(http://arxiv.org/abs/2311.00288)</code></li>
<li>Summary: <p>Instruction tuning (IT) achieves impressive zero-shot generalization results
by training large language models (LLMs) on a massive amount of diverse tasks
with instructions. However, how to select new tasks to improve the performance
and generalizability of IT models remains an open question. Training on all
existing tasks is impractical due to prohibiting computation requirements, and
randomly selecting tasks can lead to suboptimal performance. In this work, we
propose active instruction tuning based on prompt uncertainty, a novel
framework to identify informative tasks, and then actively tune the models on
the selected tasks. We represent the informativeness of new tasks with the
disagreement of the current model outputs over perturbed prompts. Our
experiments on NIV2 and Self-Instruct datasets demonstrate that our method
consistently outperforms other baseline strategies for task selection,
achieving better out-of-distribution generalization with fewer training tasks.
Additionally, we introduce a task map that categorizes and diagnoses tasks
based on prompt uncertainty and prediction probability. We discover that
training on ambiguous (prompt-uncertain) tasks improves generalization while
training on difficult (prompt-certain and low-probability) tasks offers no
benefit, underscoring the importance of task selection for instruction tuning.
</p></li>
</ul>

<h3>Title: Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. (arXiv:2311.00681v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00681">http://arxiv.org/abs/2311.00681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00681]] Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs(http://arxiv.org/abs/2311.00681)</code></li>
<li>Summary: <p>In recent years, Large Language Models (LLMs) have gained immense attention
due to their notable emergent capabilities, surpassing those seen in earlier
language models. A particularly intriguing application of LLMs is their role as
evaluators for texts produced by various generative models.
</p>
<p>In this study, we delve into the potential of LLMs as reliable assessors of
factual consistency in summaries generated by text-generation models.
Initially, we introduce an innovative approach for factuality assessment using
LLMs. This entails employing a singular LLM for the entirety of the
question-answering-based factuality scoring process. Following this, we examine
the efficacy of various LLMs in direct factuality scoring, benchmarking them
against traditional measures and human annotations.
</p>
<p>Contrary to initial expectations, our results indicate a lack of significant
correlations between factuality metrics and human evaluations, specifically for
GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across
two factuality subcategories. These consistent findings across various factual
error categories suggest a fundamental limitation in the current LLMs'
capability to accurately gauge factuality.
</p>
<p>This version presents the information more concisely while maintaining the
main points and findings of the original text.
</p></li>
</ul>

<h3>Title: Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task. (arXiv:2311.00686v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00686">http://arxiv.org/abs/2311.00686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00686]] Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task(http://arxiv.org/abs/2311.00686)</code></li>
<li>Summary: <p>This paper describes and analyzes our participation in the 2023 Eval4NLP
shared task, which focuses on assessing the effectiveness of prompt-based
techniques to empower Large Language Models to handle the task of quality
estimation, particularly in the context of evaluating machine translations and
summaries. We conducted systematic experiments with various prompting
techniques, including standard prompting, prompts informed by annotator
instructions, and innovative chain-of-thought prompting. In addition, we
integrated these approaches with zero-shot and one-shot learning methods to
maximize the efficacy of our evaluation procedures. Our work reveals that
combining these approaches using a "small", open source model (orca_mini_v3_7B)
yields competitive results.
</p></li>
</ul>

<h3>Title: The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00168">http://arxiv.org/abs/2311.00168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00168]] The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback(http://arxiv.org/abs/2311.00168)</code></li>
<li>Summary: <p>Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to prompt and more
capable in complex settings. RLHF at its core is providing a new toolkit to
optimize LLMs other than next-token prediction, enabling the integration of
qualitative training goals. The attempted match between user preferences and
downstream performance, which happens in a learned reward model, results in an
optimization landscape where training and evaluation metrics can appear
correlated. The apparent correlation can lead to unexpected behaviors and
stories of "too much RLHF." In RLHF, challenges emerge because the following
sub-modules are not consistent with each other: the reward model training, the
policy model training, and the policy model evaluation. This mismatch results
in models that sometimes avoid user requests for false safety flags, are
difficult to steer to an intended characteristic, or always answer in a
specific style. As chat model evaluation becomes increasingly nuanced, the
reliance on a perceived link between reward model score and downstream
performance drives the objective mismatch issue. In this paper, we illustrate
the cause of this issue, reviewing relevant literature from model-based
reinforcement learning, and discuss relevant solutions to encourage further
research. By solving objective mismatch in RLHF, the LLMs of the future will be
more precisely aligned to user instructions for both safety and helpfulness.
</p></li>
</ul>

<h3>Title: Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements. (arXiv:2311.00444v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00444">http://arxiv.org/abs/2311.00444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00444]] Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements(http://arxiv.org/abs/2311.00444)</code></li>
<li>Summary: <p>This work focuses on the novel problem setting of generating graphs
conditioned on a description of the graph's functional requirements in a
downstream task. We pose the problem as a text-to-text generation problem and
focus on the approach of fine-tuning a pretrained large language model (LLM) to
generate graphs. We propose an inductive bias which incorporates information
about the structure of the graph into the LLM's generation process by
incorporating message passing layers into an LLM's architecture. To evaluate
our proposed method, we design a novel set of experiments using publicly
available and widely studied molecule and knowledge graph data sets. Results
suggest our proposed approach generates graphs which more closely meet the
requested functional requirements, outperforming baselines developed on similar
tasks by a statistically significant margin.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Joint Depth Prediction and Semantic Segmentation with Multi-View SAM. (arXiv:2311.00134v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00134">http://arxiv.org/abs/2311.00134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00134]] Joint Depth Prediction and Semantic Segmentation with Multi-View SAM(http://arxiv.org/abs/2311.00134)</code></li>
<li>Summary: <p>Multi-task approaches to joint depth and segmentation prediction are
well-studied for monocular images. Yet, predictions from a single-view are
inherently limited, while multiple views are available in many robotics
applications. On the other end of the spectrum, video-based and full 3D methods
require numerous frames to perform reconstruction and segmentation. With this
work we propose a Multi-View Stereo (MVS) technique for depth prediction that
benefits from rich semantic features of the Segment Anything Model (SAM). This
enhanced depth prediction, in turn, serves as a prompt to our Transformer-based
semantic segmentation decoder. We report the mutual benefit that both tasks
enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our
approach consistently outperforms single-task MVS and segmentation models,
along with multi-task monocular methods.
</p></li>
</ul>

<h3>Title: OpenForest: A data catalogue for machine learning in forest monitoring. (arXiv:2311.00277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00277">http://arxiv.org/abs/2311.00277</a></li>
<li>Code URL: https://github.com/rolnicklab/openforest</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00277]] OpenForest: A data catalogue for machine learning in forest monitoring(http://arxiv.org/abs/2311.00277)</code></li>
<li>Summary: <p>Forests play a crucial role in Earth's system processes and provide a suite
of social and economic ecosystem services, but are significantly impacted by
human activities, leading to a pronounced disruption of the equilibrium within
ecosystems. Advancing forest monitoring worldwide offers advantages in
mitigating human impacts and enhancing our comprehension of forest composition,
alongside the effects of climate change. While statistical modeling has
traditionally found applications in forest biology, recent strides in machine
learning and computer vision have reached important milestones using remote
sensing data, such as tree species identification, tree crown segmentation and
forest biomass assessments. For this, the significance of open access data
remains essential in enhancing such data-driven algorithms and methodologies.
Here, we provide a comprehensive and extensive overview of 86 open access
forest datasets across spatial scales, encompassing inventories, ground-based,
aerial-based, satellite-based recordings, and country or world maps. These
datasets are grouped in OpenForest, a dynamic catalogue open to contributions
that strives to reference all available open access forest datasets. Moreover,
in the context of these datasets, we aim to inspire research in machine
learning applied to forest biology by establishing connections between
contemporary topics, perspectives and challenges inherent in both domains. We
hope to encourage collaborations among scientists, fostering the sharing and
exploration of diverse datasets through the application of machine learning
methods for large-scale forest monitoring. OpenForest is available at this url:
https://github.com/RolnickLab/OpenForest
</p></li>
</ul>

<h3>Title: Towards Omni-supervised Referring Expression Segmentation. (arXiv:2311.00397v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00397">http://arxiv.org/abs/2311.00397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00397]] Towards Omni-supervised Referring Expression Segmentation(http://arxiv.org/abs/2311.00397)</code></li>
<li>Summary: <p>Referring Expression Segmentation (RES) is an emerging task in computer
vision, which segments the target instances in images based on text
descriptions. However, its development is plagued by the expensive segmentation
labels. To address this issue, we propose a new learning task for RES called
Omni-supervised Referring Expression Segmentation (Omni-RES), which aims to
make full use of unlabeled, fully labeled and weakly labeled data, e.g.,
referring points or grounding boxes, for efficient RES training. To accomplish
this task, we also propose a novel yet strong baseline method for Omni-RES
based on the recently popular teacher-student learning, where where the weak
labels are not directly transformed into supervision signals but used as a
yardstick to select and refine high-quality pseudo-masks for teacher-student
learning. To validate the proposed Omni-RES method, we apply it to a set of
state-of-the-art RES models and conduct extensive experiments on a bunch of RES
datasets. The experimental results yield the obvious merits of Omni-RES than
the fully-supervised and semi-supervised training schemes. For instance, with
only 10% fully labeled data, Omni-RES can help the base model achieve 100%
fully supervised performance, and it also outperform the semi-supervised
alternative by a large margin, e.g., +14.93% on RefCOCO and +14.95% on
RefCOCO+, respectively. More importantly, Omni-RES also enable the use of
large-scale vision-langauges like Visual Genome to facilitate low-cost RES
training, and achieve new SOTA performance of RES, e.g., 80.66 on RefCOCO.
</p></li>
</ul>

<h3>Title: CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection. (arXiv:2311.00453v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00453">http://arxiv.org/abs/2311.00453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00453]] CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection(http://arxiv.org/abs/2311.00453)</code></li>
<li>Summary: <p>This paper considers zero-shot Anomaly Detection (AD), a valuable yet
under-studied task, which performs AD without any reference images of the test
objects. Specifically, we employ a language-guided strategy and propose a
simple-yet-effective architecture CLIP-AD, leveraging the superior zero-shot
classification capabilities of the large vision-language model CLIP. A natural
idea for anomaly segmentation is to directly calculate the similarity between
text/image features, but we observe opposite predictions and irrelevant
highlights in the results. Inspired by the phenomena, we introduce a Staged
Dual-Path model (SDP) that effectively uses features from various levels and
applies architecture and feature surgery to address these issues. Furthermore,
delving beyond surface phenomena, we identify the problem arising from
misalignment of text/image features in the joint embedding space. Thus, we
introduce a fine-tuning strategy by adding linear layers and construct an
extended model SDP+, further enhancing the performance. Abundant experiments
demonstrate the effectiveness of our approach, e.g., on VisA, SDP outperforms
SOTA by +1.0/+1.2 in classification/segmentation F1 scores, while SDP+ achieves
+1.9/+11.7 improvements.
</p></li>
</ul>

<h3>Title: Continual atlas-based segmentation of prostate MRI. (arXiv:2311.00548v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00548">http://arxiv.org/abs/2311.00548</a></li>
<li>Code URL: https://github.com/meclabtuda/atlas-replay</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00548]] Continual atlas-based segmentation of prostate MRI(http://arxiv.org/abs/2311.00548)</code></li>
<li>Summary: <p>Continual learning (CL) methods designed for natural image classification
often fail to reach basic quality standards for medical image segmentation.
Atlas-based segmentation, a well-established approach in medical imaging,
incorporates domain knowledge on the region of interest, leading to
semantically coherent predictions. This is especially promising for CL, as it
allows us to leverage structural information and strike an optimal balance
between model rigidity and plasticity over time. When combined with
privacy-preserving prototypes, this process offers the advantages of
rehearsal-based CL without compromising patient privacy. We propose Atlas
Replay, an atlas-based segmentation approach that uses prototypes to generate
high-quality segmentation masks through image registration that maintain
consistency even as the training distribution changes. We explore how our
proposed method performs compared to state-of-the-art CL methods in terms of
knowledge transferability across seven publicly available prostate segmentation
datasets. Prostate segmentation plays a vital role in diagnosing prostate
cancer, however, it poses challenges due to substantial anatomical variations,
benign structural differences in older age groups, and fluctuating acquisition
parameters. Our results show that Atlas Replay is both robust and generalizes
well to yet-unseen domains while being able to maintain knowledge, unlike
end-to-end segmentation methods. Our code base is available under
https://github.com/MECLabTUDA/Atlas-Replay.
</p></li>
</ul>

<h3>Title: CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders. (arXiv:2311.00566v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00566">http://arxiv.org/abs/2311.00566</a></li>
<li>Code URL: https://github.com/antofuller/croma</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00566]] CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders(http://arxiv.org/abs/2311.00566)</code></li>
<li>Summary: <p>A vital and rapidly growing application, remote sensing offers vast yet
sparsely labeled, spatially aligned multimodal data; this makes self-supervised
learning algorithms invaluable. We present CROMA: a framework that combines
contrastive and reconstruction self-supervised objectives to learn rich
unimodal and multimodal representations. Our method separately encodes
masked-out multispectral optical and synthetic aperture radar samples --
aligned in space and time -- and performs cross-modal contrastive learning.
Another encoder fuses these sensors, producing joint multimodal encodings that
are used to predict the masked patches via a lightweight decoder. We show that
these objectives are complementary when leveraged on spatially aligned
multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our
cross- and self-attention matrices. These strategies improve representations
and allow our models to effectively extrapolate to images up to 17.6x larger at
test-time. CROMA outperforms the current SoTA multispectral model, evaluated
on: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg.
2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and
K-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%).
CROMA's rich, optionally multimodal representations can be widely leveraged
across remote sensing applications.
</p></li>
</ul>

<h3>Title: LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00571">http://arxiv.org/abs/2311.00571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00571]] LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing(http://arxiv.org/abs/2311.00571)</code></li>
<li>Summary: <p>LLaVA-Interactive is a research prototype for multimodal human-AI
interaction. The system can have multi-turn dialogues with human users by
taking multimodal user inputs and generating multimodal responses. Importantly,
LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled
to align human intents in the interaction. The development of LLaVA-Interactive
is extremely cost-efficient as the system combines three multimodal skills of
pre-built AI models without additional model training: visual chat of LLaVA,
image segmentation from SEEM, as well as image generation and editing from
GLIGEN. A diverse set of application scenarios is presented to demonstrate the
promises of LLaVA-Interactive and to inspire future research in multimodal
interactive systems.
</p></li>
</ul>

<h3>Title: Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation. (arXiv:2311.00451v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00451">http://arxiv.org/abs/2311.00451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00451]] Discourse Relations Classification and Cross-Framework Discourse Relation Classification Through the Lens of Cognitive Dimensions: An Empirical Investigation(http://arxiv.org/abs/2311.00451)</code></li>
<li>Summary: <p>Existing discourse formalisms use different taxonomies of discourse
relations, which require expert knowledge to understand, posing a challenge for
annotation and automatic classification. We show that discourse relations can
be effectively captured by some simple cognitively inspired dimensions proposed
by Sanders et al.(2018). Our experiments on cross-framework discourse relation
classification (PDTB &amp; RST) demonstrate that it is possible to transfer
knowledge of discourse relations for one framework to another framework by
means of these dimensions, in spite of differences in discourse segmentation of
the two frameworks. This manifests the effectiveness of these dimensions in
characterizing discourse relations across frameworks. Ablation studies reveal
that different dimensions influence different types of discourse relations. The
patterns can be explained by the role of dimensions in characterizing and
distinguishing different relations. We also report our experimental results on
automatic prediction of these dimensions.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
