<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-03</h1>
<h3>Title: Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese</h3>
<ul>
<li><strong>Authors: </strong>William Alberto Cruz-Castañeda, Marcellus Amadeus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00019">https://arxiv.org/abs/2506.00019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00019">https://arxiv.org/pdf/2506.00019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00019]] Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese(https://arxiv.org/abs/2506.00019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This report introduces the experience of developing Amadeus Verbo, a family of large language models for Brazilian Portuguese. To handle diverse use cases, Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main objective is to show how easy it is to fine-tune foundation models to democratize the open-source development of Brazilian Portuguese LLMs when data and resources are available. Amadeus-Verbo family models are all available at HuggingFace at this https URL.</li>
</ul>

<h3>Title: Scaling Physical Reasoning with the PHYSICS Dataset</h3>
<ul>
<li><strong>Authors: </strong>Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, haonan he, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, physics.ed-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00022">https://arxiv.org/abs/2506.00022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00022">https://arxiv.org/pdf/2506.00022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00022]] Scaling Physical Reasoning with the PHYSICS Dataset(https://arxiv.org/abs/2506.00022)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.</li>
</ul>

<h3>Title: From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Zhengyu Chen, Yudong Wang, Teng Xiao, Ruochen Zhou, Xuesheng Yang, Wei Wang, Zhifang Sui, Jingang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00027">https://arxiv.org/abs/2506.00027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00027">https://arxiv.org/pdf/2506.00027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00027]] From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling(https://arxiv.org/abs/2506.00027)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in improving the reasoning capabilities of Large Language Models have underscored the efficacy of Process Reward Models (PRMs) in addressing intermediate errors through structured feedback mechanisms. This study analyzes PRMs from multiple perspectives, including training methodologies, scalability, and generalization capabilities. We investigate the interplay between pre-training and reward model training FLOPs to assess their influence on PRM efficiency and accuracy in complex reasoning tasks. Our analysis reveals a pattern of diminishing returns in performance with increasing PRM scale, highlighting the importance of balancing model size and computational cost. Furthermore, the diversity of training datasets significantly impacts PRM performance, emphasizing the importance of diverse data to enhance both accuracy and efficiency. We further examine test-time scaling strategies, identifying Monte Carlo Tree Search as the most effective method when computational resources are abundant, while Best-of-N Sampling serves as a practical alternative under resource-limited conditions. Notably, our findings indicate that PRMs trained on mathematical datasets exhibit performance comparable to those tailored for code generation, suggesting robust cross-domain generalization. Employing a gradient-based metric, we observe that PRMs exhibit a preference for selecting responses with similar underlying patterns, further informing their optimization.</li>
</ul>

<h3>Title: Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xiang Shi, Rui Zhang, Jiawei Liu, Yinpeng Liu, Qikai Cheng, Wei Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00030">https://arxiv.org/abs/2506.00030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00030">https://arxiv.org/pdf/2506.00030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00030]] Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement(https://arxiv.org/abs/2506.00030)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal fusion is susceptible to modality imbalance, where dominant modalities overshadow weak ones, easily leading to biased learning and suboptimal fusion, especially for incomplete modality conditions. To address this problem, we propose a Shapley-guided alternating training framework that adaptively prioritizes minor modalities to balance and thus enhance the fusion. Our method leverages Shapley Value-based scheduling to improve the training sequence adaptively, ensuring that under-optimized modalities receive sufficient learning. Additionally, we introduce the memory module to refine and inherit modality-specific representations with a cross-modal mapping mechanism to align features at both the feature and sample levels. To further validate the adaptability of the proposed approach, the encoder module empirically adopts both conventional and LLM-based backbones. With building up a novel multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we evaluate the performance in both balance and accuracy across four multimodal benchmark datasets, where our method achieves state-of-the-art (SOTA) results. Meanwhile, robustness analysis under missing modalities highlights its strong generalization capabilities. Accordingly, our findings reveal the untapped potential of alternating training, demonstrating that strategic modality prioritization fundamentally balances and promotes multimodal learning, offering a new paradigm for optimizing multimodal training dynamics.</li>
</ul>

<h3>Title: Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists</h3>
<ul>
<li><strong>Authors: </strong>Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, Xiaofang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00042">https://arxiv.org/abs/2506.00042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00042">https://arxiv.org/pdf/2506.00042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00042]] Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists(https://arxiv.org/abs/2506.00042)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced natural language processing, particularly through the integration of external tools and APIs. However, their effectiveness is frequently hampered by parameter mis-filling during tool calling. In this paper, we propose the Hierarchical Tool Error Checklist (HiTEC) framework to systematically diagnose and mitigate tool-calling errors without relying on extensive real-world interactions. HiTEC introduces a two-tiered approach: a global error checklist that identifies common, cross-tool issues, and a local error checklist that targets tool-specific and contextual failures. Building on this structure, we propose two deployments: HiTEC-In Context Learning (HiTEC-ICL) and HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global checklist in the initial prompts and leverages a two-round conversational interaction to dynamically refine parameter handling, while HiTEC-KTO generates high-quality negative examples to drive fine-tuning via preference-based optimization. Extensive experiments across five public datasets demonstrate that our framework significantly improves parameter-filling accuracy and tool-calling success rates compared to baseline methods.</li>
</ul>

<h3>Title: Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zeng, Yizhe Feng, Mengliang He, Wenhui Lei, Wei Zhang, Zeming Liu, Xiaoming Shi, Aimin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00064">https://arxiv.org/abs/2506.00064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00064">https://arxiv.org/pdf/2506.00064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00064]] Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling(https://arxiv.org/abs/2506.00064)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.</li>
</ul>

<h3>Title: Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages</h3>
<ul>
<li><strong>Authors: </strong>Afrozah Nadeem, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00068">https://arxiv.org/abs/2506.00068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00068">https://arxiv.org/pdf/2506.00068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00068]] Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages(https://arxiv.org/abs/2506.00068)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly shaping public discourse, yet their politico-economic biases remain underexamined in non-Western and low-resource multilingual contexts. This paper presents a systematic analysis of political bias in 13 state-of-the-art LLMs across five low-resource languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We propose a novel framework that integrates an adapted Political Compass Test (PCT) with a multi-level framing analysis. Our method combines quantitative assessment of political orientation across economic (left-right) and social (libertarian-authoritarian) axes with qualitative analysis of framing through content, style, and emphasis. We further contextualize this analysis by aligning prompts with 11 key socio-political themes relevant to Pakistani society. Our results reveal that LLMs predominantly align with liberal-left values, echoing Western training data influences, but exhibit notable shifts toward authoritarian framing in regional languages, suggesting strong cultural modulation effects. We also identify consistent model-specific bias signatures and language-conditioned variations in ideological expression. These findings show the urgent need for culturally grounded, multilingual bias auditing frameworks.</li>
</ul>

<h3>Title: Evaluating the Sensitivity of LLMs to Prior Context</h3>
<ul>
<li><strong>Authors: </strong>Robert Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, Greig A. Cowan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00069">https://arxiv.org/abs/2506.00069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00069">https://arxiv.org/pdf/2506.00069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00069]] Evaluating the Sensitivity of LLMs to Prior Context(https://arxiv.org/abs/2506.00069)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.</li>
</ul>

<h3>Title: Gaussian mixture models as a proxy for interacting language models</h3>
<ul>
<li><strong>Authors: </strong>Edward Wang, Tianyu Wang, Avanti Athreya, Vince Lyzinski, Carey E. Priebe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00077">https://arxiv.org/abs/2506.00077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00077">https://arxiv.org/pdf/2506.00077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00077]] Gaussian mixture models as a proxy for interacting language models(https://arxiv.org/abs/2506.00077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are a powerful tool with the ability to match human capabilities and behavior in many settings. Retrieval-augmented generation (RAG) further allows LLMs to generate diverse output depending on the contents of their RAG database. This motivates their use in the social sciences to study human behavior between individuals when large-scale experiments are infeasible. However, LLMs depend on complex, computationally expensive algorithms. In this paper, we introduce interacting Gaussian mixture models (GMMs) as an alternative to similar frameworks using LLMs. We compare a simplified model of GMMs to select experimental simulations of LLMs whose updating and response depend on feedback from other LLMs. We find that interacting GMMs capture important features of the dynamics in interacting LLMs, and we investigate key similarities and differences between interacting LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture models, potential modifications, and future research directions.</li>
</ul>

<h3>Title: COSMIC: Generalized Refusal Direction Identification in LLM Activations</h3>
<ul>
<li><strong>Authors: </strong>Vincent Siu, Nicholas Crispino, Zihao Yu, Sam Pan, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00085">https://arxiv.org/abs/2506.00085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00085">https://arxiv.org/pdf/2506.00085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00085]] COSMIC: Generalized Refusal Direction Identification in LLM Activations(https://arxiv.org/abs/2506.00085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.</li>
</ul>

<h3>Title: HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Qing Li, Jiahui Geng, Zongxiong Chen, Derui Zhu, Yuxia Wang, Congbo Ma, Chenyang Lyu, Fakhri Karray</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00088">https://arxiv.org/abs/2506.00088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00088">https://arxiv.org/pdf/2506.00088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00088]] HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs(https://arxiv.org/abs/2506.00088)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.</li>
</ul>

<h3>Title: EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00101">https://arxiv.org/abs/2506.00101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00101">https://arxiv.org/pdf/2506.00101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00101]] EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning(https://arxiv.org/abs/2506.00101)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Understanding a procedural activity requires modeling both how action steps transform the scene, and how evolving scene transformations can influence the sequence of action steps, even those that are accidental or erroneous. Yet, existing work on procedure-aware video representations fails to explicitly learned the state changes (scene transformations). In this work, we study procedure-aware video representation learning by incorporating state-change descriptions generated by LLMs as supervision signals for video encoders. Moreover, we generate state-change counterfactuals that simulate hypothesized failure outcomes, allowing models to learn by imagining the unseen ``What if'' scenarios. This counterfactual reasoning facilitates the model's ability to understand the cause and effect of each step in an activity. To verify the procedure awareness of our model, we conduct extensive experiments on procedure-aware tasks, including temporal action segmentation, error detection, and more. Our results demonstrate the effectiveness of the proposed state-change descriptions and their counterfactuals, and achieve significant improvements on multiple tasks.</li>
</ul>

<h3>Title: Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Xun Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00103">https://arxiv.org/abs/2506.00103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00103">https://arxiv.org/pdf/2506.00103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00103]] Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards(https://arxiv.org/abs/2506.00103)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has enabled large language models (LLMs) to achieve remarkable breakthroughs in reasoning tasks with objective ground-truth answers, such as mathematics and code generation. However, a significant gap remains for non-verifiable tasks, like creative writing and open-ended dialogue, where quality assessment is inherently subjective and lacks definitive references. Existing approaches for these domains often rely on scalar reward models trained with human preferences, which suffer from limited generalization and are prone to reward hacking, such as over-explanation and length bias. In this work, we propose a unified RLVR-based training paradigm that bridges the gap between non-verifiable tasks and verifiable rewards. We introduce a writing-principle-based pairwise Generative Reward Model (GenRM) and a novel Bootstrapped Relative Policy Optimization (BRPO) algorithm. The pairwise writing GenRM leverages self-principled critique to transform subjective assessments into reliable, verifiable rewards, while BRPO enables dynamic, reference-free pairwise comparison by leveraging a bootstrapped response as temporary reference from within group rollouts during RL training. Our approach empowers LLMs to develop robust writing capabilities without supervised fine-tuning, as demonstrated by Writing-Zero, which shows consistent improvement and strong resistance to reward hacking compared to scalar reward baselines. Furthermore, our method achieves competitive results on both in-house and open-source writing benchmarks. Our findings suggest the potential to unify rule-based, reference-based, and reference-free reward modeling under the RLVR framework, thus paving the way for a comprehensive and scalable RL training paradigm applicable across all language tasks.</li>
</ul>

<h3>Title: Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces</h3>
<ul>
<li><strong>Authors: </strong>Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, Shenglong Ye, Lewei Lu, Jingbo Wang, Wenhai Wang, Jifeng Dai, Yu Qiao, Rongrong Ji, Xizhou Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00123">https://arxiv.org/abs/2506.00123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00123">https://arxiv.org/pdf/2506.00123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00123]] Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces(https://arxiv.org/abs/2506.00123)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental this http URL this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.</li>
</ul>

<h3>Title: Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Edward Fish, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00129">https://arxiv.org/abs/2506.00129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00129">https://arxiv.org/pdf/2506.00129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00129]] Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation(https://arxiv.org/abs/2506.00129)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in Sign Language Translation (SLT) has focussed primarily on improving the representational capacity of large language models to incorporate Sign Language features. This work explores an alternative direction: enhancing the geometric properties of skeletal representations themselves. We propose Geo-Sign, a method that leverages the properties of hyperbolic geometry to model the hierarchical structure inherent in sign language kinematics. By projecting skeletal features derived from Spatio-Temporal Graph Convolutional Networks (ST-GCNs) into the Poincaré ball model, we aim to create more discriminative embeddings, particularly for fine-grained motions like finger articulations. We introduce a hyperbolic projection layer, a weighted Fréchet mean aggregation scheme, and a geometric contrastive loss operating directly in hyperbolic space. These components are integrated into an end-to-end translation framework as a regularisation function, to enhance the representations within the language model. This work demonstrates the potential of hyperbolic geometry to improve skeletal representations for Sign Language Translation, improving on SOTA RGB methods while preserving privacy and improving computational efficiency. Code available here: this https URL.</li>
</ul>

<h3>Title: Adapting Offline Reinforcement Learning with Online Delays</h3>
<ul>
<li><strong>Authors: </strong>Simon Sinong Zhan, Qingyuan Wu, Frank Yang, Xiangyu Shi, Chao Huang, Qi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00131">https://arxiv.org/abs/2506.00131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00131">https://arxiv.org/pdf/2506.00131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00131]] Adapting Offline Reinforcement Learning with Online Delays(https://arxiv.org/abs/2506.00131)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than naïve history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.</li>
</ul>

<h3>Title: Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fardin Ahsan Sakib, Ziwei Zhu, Karen Trister Grace, Meliha Yetisgen, Ozlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00134">https://arxiv.org/abs/2506.00134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00134">https://arxiv.org/pdf/2506.00134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00134]] Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models(https://arxiv.org/abs/2506.00134)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.</li>
</ul>

<h3>Title: On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Magdalena Proszewska, Nikolay Malkin, N. Siddharth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00136">https://arxiv.org/abs/2506.00136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00136">https://arxiv.org/pdf/2506.00136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00136]] On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning(https://arxiv.org/abs/2506.00136)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion autoencoders (DAs) are variants of diffusion generative models that use an input-dependent latent variable to capture representations alongside the diffusion process. These representations, to varying extents, can be used for tasks such as downstream classification, controllable generation, and interpolation. However, the generative performance of DAs relies heavily on how well the latent variables can be modelled and subsequently sampled from. Better generative modelling is also the primary goal of another class of diffusion models -- those that learn their forward (noising) process. While effective at adjusting the noise process in an input-dependent manner, they must satisfy additional constraints derived from the terminal conditions of the diffusion process. Here, we draw a connection between these two classes of models and show that certain design decisions (latent variable choice, conditioning method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us to obtain the best of both worlds: effective representations as evaluated on downstream tasks, including domain transfer, as well as more efficient modelling and generation with fewer denoising steps compared to standard DMs.</li>
</ul>

<h3>Title: LaMP-QA: A Benchmark for Personalized Long-form Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00137">https://arxiv.org/abs/2506.00137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00137">https://arxiv.org/pdf/2506.00137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00137]] LaMP-QA: A Benchmark for Personalized Long-form Question Answering(https://arxiv.org/abs/2506.00137)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models (LLMs). Our results show that incorporating the personalized context provided leads to performance improvements of up to 39%. The benchmark is publicly released to support future research in this area.</li>
</ul>

<h3>Title: Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Erfan Loghmani</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.EM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00152">https://arxiv.org/abs/2506.00152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00152">https://arxiv.org/pdf/2506.00152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00152]] Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective(https://arxiv.org/abs/2506.00152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are being widely used across industries to generate content that contributes directly to key performance metrics, such as conversion rates. Pretrained models, however, often fall short when it comes to aligning with human preferences or optimizing for business objectives. As a result, fine-tuning with good-quality labeled data is essential to guide models to generate content that achieves better results. Controlled experiments, like A/B tests, can provide such data, but they are often expensive and come with significant engineering and logistical challenges. Meanwhile, companies have access to a vast amount of historical (observational) data that remains underutilized. In this work, we study the challenges and opportunities of fine-tuning LLMs using observational data. We show that while observational outcomes can provide valuable supervision, directly fine-tuning models on such data can lead them to learn spurious correlations. We present empirical evidence of this issue using various real-world datasets and propose DeconfoundLM, a method that explicitly removes the effect of known confounders from reward signals. Using simulation experiments, we demonstrate that DeconfoundLM improves the recovery of causal relationships and mitigates failure modes found in fine-tuning methods that ignore or naively incorporate confounding variables. Our findings highlight that while observational data presents risks, with the right causal corrections, it can be a powerful source of signal for LLM alignment. Please refer to the project page for code and related resources.</li>
</ul>

<h3>Title: Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches</h3>
<ul>
<li><strong>Authors: </strong>Agustín Roca, Gastón Castro, Gabriel Torre, Leonardo J. Colombo, Ignacio Mas, Javier Pereira, Juan I. Giribet</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00154">https://arxiv.org/abs/2506.00154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00154">https://arxiv.org/pdf/2506.00154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00154]] Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches(https://arxiv.org/abs/2506.00154)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study compares the performance of state-of-the-art neural networks including variants of the YOLOv11 and RT-DETR models for detecting marsh deer in UAV imagery, in scenarios where specimens occupy a very small portion of the image and are occluded by vegetation. We extend previous analysis adding precise segmentation masks for our datasets enabling a fine-grained training of a YOLO model with a segmentation head included. Experimental results show the effectiveness of incorporating the segmentation head achieving superior detection performance. This work contributes valuable insights for improving UAV-based wildlife monitoring and conservation strategies through scalable and accurate AI-driven detection systems.</li>
</ul>

<h3>Title: Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States</h3>
<ul>
<li><strong>Authors: </strong>Eli Chien, Wei-Ning Chen, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00158">https://arxiv.org/abs/2506.00158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00158">https://arxiv.org/pdf/2506.00158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00158]] Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States(https://arxiv.org/abs/2506.00158)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Zeroth-order optimization has emerged as a promising approach for fine-tuning large language models on domain-specific data, particularly under differential privacy (DP) and memory constraints. While first-order methods have been extensively studied from a privacy perspective, the privacy analysis and algorithmic design for zeroth-order methods remain significantly underexplored. A critical open question concerns hidden-state DP analysis: although convergent privacy bounds are known for first-order methods, it has remained unclear whether similar guarantees can be established for zeroth-order methods. In this work, we provide an affirmative answer by proving a convergent DP bound for zeroth-order optimization. Our analysis generalizes the celebrated privacy amplification-by-iteration framework to the setting of smooth loss functions in zeroth-order optimization. Furthermore, it induces better DP zeroth-order algorithmic designs that are previously unknown to the literature.</li>
</ul>

<h3>Title: Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement</h3>
<ul>
<li><strong>Authors: </strong>Qihui Fan, Enfu Nan, Wenbo Li, Lei Lu, Pu Zhao, Yanzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00160">https://arxiv.org/abs/2506.00160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00160">https://arxiv.org/pdf/2506.00160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00160]] Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement(https://arxiv.org/abs/2506.00160)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf.</li>
</ul>

<h3>Title: Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents</h3>
<ul>
<li><strong>Authors: </strong>Kaivalya Hariharan, Uzay Girit, Atticus Wang, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00172">https://arxiv.org/abs/2506.00172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00172">https://arxiv.org/pdf/2506.00172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00172]] Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents(https://arxiv.org/abs/2506.00172)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Benchmarks for large language models (LLMs) have predominantly assessed short-horizon, localized reasoning. Existing long-horizon suites (e.g. SWE-bench) rely on manually curated issues, so expanding or tuning difficulty demands expensive human effort and evaluations quickly saturate. However, many real-world tasks, such as software engineering or scientific research, require agents to rapidly comprehend and manipulate novel, complex structures dynamically; evaluating these capabilities requires the ability to construct large and varied sets of problems for agents to solve. We introduce Breakpoint, a benchmarking methodology that automatically generates code-repair tasks by adversarially corrupting functions within real-world software repositories. Breakpoint systematically controls task difficulty along two clear dimensions: local reasoning (characterized by code complexity metrics such as cyclomatic complexity) and system-level reasoning (characterized by call-graph centrality and the number of simultaneously corrupted interdependent functions). In experiments across more than 900 generated tasks we demonstrate that our methodology can scale to arbitrary difficulty, with state-of-the-art models' success rates ranging from 55% on the easiest tasks down to 0% on the hardest.</li>
</ul>

<h3>Title: Heterogeneous Graph Backdoor Attack</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Chen, Lusi Li, Daniel Takabi, Masha Sosonkina, Rui Ning</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00191">https://arxiv.org/abs/2506.00191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00191">https://arxiv.org/pdf/2506.00191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00191]] Heterogeneous Graph Backdoor Attack(https://arxiv.org/abs/2506.00191)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex, multi-typed relationships across diverse domains, yet their vulnerability to backdoor attacks remains unexplored. To address this gap, we conduct the first investigation into the susceptibility of HGNNs to existing graph backdoor attacks, revealing three critical issues: (1) high attack budget required for effective backdoor injection, (2) inefficient and unreliable backdoor activation, and (3) inaccurate attack effectiveness evaluation. To tackle these issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first backdoor attack specifically designed for HGNNs, introducing a novel relation-based trigger mechanism that establishes specific connections between a strategically selected trigger node and poisoned nodes via the backdoor metapath. HGBA achieves efficient and stealthy backdoor injection with minimal structural modifications and supports easy backdoor activation through two flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally, we improve the ASR measurement protocol, enabling a more accurate assessment of attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses multiple state-of-the-art graph backdoor attacks in black-box settings, efficiently attacking HGNNs with low attack budgets. Ablation studies show that the strength of HBGA benefits from our trigger node selection method and backdoor metapath selection strategy. In addition, HGBA shows superior robustness against node feature perturbations and multiple types of existing graph backdoor defense mechanisms. Finally, extension experiments demonstrate that the relation-based trigger mechanism can effectively extend to tasks in homogeneous graph scenarios, thereby posing severe threats to broader security-critical domains.</li>
</ul>

<h3>Title: When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Shen, Yun Shen, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00197">https://arxiv.org/abs/2506.00197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00197">https://arxiv.org/pdf/2506.00197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00197]] When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs(https://arxiv.org/abs/2506.00197)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge files have been widely used in large language model (LLM) agents, such as GPTs, to improve response quality. However, concerns about the potential leakage of knowledge files have grown significantly. Existing studies demonstrate that adversarial prompts can induce GPTs to leak knowledge file content. Yet, it remains uncertain whether additional leakage vectors exist, particularly given the complex data flows across clients, servers, and databases in GPTs. In this paper, we present a comprehensive risk assessment of knowledge file leakage, leveraging a novel workflow inspired by Data Security Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820 flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT initialization, retrieval, sandboxed execution environments, and prompts. These vectors enable adversaries to extract sensitive knowledge file data such as titles, content, types, and sizes. Notably, the activation of the built-in tool Code Interpreter leads to a privilege escalation vulnerability, enabling adversaries to directly download original knowledge files with a 95.95% success rate. Further analysis reveals that 28.80% of leaked files are copyrighted, including digital copies from major publishers and internal materials from a listed company. In the end, we provide actionable solutions for GPT builders and platform providers to secure the GPT data supply chain.</li>
</ul>

<h3>Title: MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models</h3>
<ul>
<li><strong>Authors: </strong>Srivathsan Badrinarayanan, Rishikesh Magar, Akshay Antony, Radheesh Sharma Meda, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00198">https://arxiv.org/abs/2506.00198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00198">https://arxiv.org/pdf/2506.00198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00198]] MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models(https://arxiv.org/abs/2506.00198)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.</li>
</ul>

<h3>Title: Structuring Radiology Reports: Challenging LLMs with Lightweight Models</h3>
<ul>
<li><strong>Authors: </strong>Johannes Moll, Louisa Fay, Asfandyar Azhar, Sophie Ostmeier, Tim Lueth, Sergios Gatidis, Curtis Langlotz, Jean-Benoit Delbrouck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00200">https://arxiv.org/abs/2506.00200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00200">https://arxiv.org/pdf/2506.00200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00200]] Structuring Radiology Reports: Challenging LLMs with Lightweight Models(https://arxiv.org/abs/2506.00200)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)-specifically T5 and BERT2BERT-for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B-70B), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings.</li>
</ul>

<h3>Title: Hush! Protecting Secrets During Model Training: An Indistinguishability Approach</h3>
<ul>
<li><strong>Authors: </strong>Arun Ganesh, Brendan McMahan, Milad Nasr, Thomas Steinke, Abhradeep Thakurta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00201">https://arxiv.org/abs/2506.00201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00201">https://arxiv.org/pdf/2506.00201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00201]] Hush! Protecting Secrets During Model Training: An Indistinguishability Approach(https://arxiv.org/abs/2506.00201)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>We consider the problem of secret protection, in which a business or organization wishes to train a model on their own data, while attempting to not leak secrets potentially contained in that data via the model. The standard method for training models to avoid memorization of secret information is via differential privacy (DP). However, DP requires a large loss in utility or a large dataset to achieve its strict privacy definition, which may be unnecessary in our setting where the data curator and data owner are the same entity. We propose an alternate definition of secret protection that instead of targeting DP, instead targets a bound on the posterior probability of secret reconstruction. We then propose and empirically evaluate an algorithm for model training with this secret protection definition. Our algorithm solves a linear program to assign weights to examples based on the desired per-secret protections, and then performs Poisson sampling using these weights. We show our algorithm significantly outperforms the baseline of running DP-SGD on the whole dataset.</li>
</ul>

<h3>Title: Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Liwen Sun, Hao-Ren Yao, Gary Gao, Ophir Frieder, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00209">https://arxiv.org/abs/2506.00209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00209">https://arxiv.org/pdf/2506.00209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00209]] Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models(https://arxiv.org/abs/2506.00209)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Cancer screening, leading to early detection, saves lives. Unfortunately, existing screening techniques require expensive and intrusive medical procedures, not globally available, resulting in too many lost would-be-saved lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation Models, a cancer pre-screening methodology that identifies high-risk patients for further screening solely based on their historical medical records. With millions of electronic healthcare records (EHR), we establish the scaling law of EHR foundation models pretrained on medical code sequences, pretrain compute-optimal foundation models of up to 2.4 billion parameters, and finetune them on clinician-curated cancer risk prediction cohorts. In our retrospective evaluation comprising of thirty thousand patients, CATCH-FM achieved strong efficacy (60% sensitivity) with low risk (99% specificity and Negative Predictive Value), outperforming feature-based tree models as well as general and medical large language models by large margins. Despite significant demographic, healthcare system, and EHR coding differences, CATCH-FM achieves state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot leaderboard, outperforming EHR foundation models pretrained using on-site patient data. Our analysis demonstrates the robustness of CATCH-FM in various patient distributions, the benefits of operating in the ICD code space, and its ability to capture non-trivial cancer risk factors. Our code will be open-sourced.</li>
</ul>

<h3>Title: Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</h3>
<ul>
<li><strong>Authors: </strong>Anthony Gosselin, Ge Ya Luo, Luis Lara, Florian Golemo, Derek Nowrouzezahrai, Liam Paull, Alexia Jolicoeur-Martineau, Christopher Pal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00227">https://arxiv.org/abs/2506.00227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00227">https://arxiv.org/pdf/2506.00227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00227]] Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes(https://arxiv.org/abs/2506.00227)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.</li>
</ul>

<h3>Title: ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung-won Hwang, Zhewei Yao, Yuxiong He, Feng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00232">https://arxiv.org/abs/2506.00232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00232">https://arxiv.org/pdf/2506.00232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00232]] ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering(https://arxiv.org/abs/2506.00232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet many suffer from monolithic designs that tightly couple core functions like query reformulation, retrieval, reasoning, and verification. This limits their interpretability, systematic evaluation, and targeted improvement, especially for complex multi-hop question answering. We introduce ComposeRAG, a novel modular abstraction that decomposes RAG pipelines into atomic, composable modules. Each module, such as Question Decomposition, Query Rewriting, Retrieval Decision, and Answer Verification, acts as a parameterized transformation on structured inputs/outputs, allowing independent implementation, upgrade, and analysis. To enhance robustness against errors in multi-step reasoning, ComposeRAG incorporates a self-reflection mechanism that iteratively revisits and refines earlier steps upon verification failure. Evaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently outperforms strong baselines in both accuracy and grounding fidelity. Specifically, it achieves up to a 15% accuracy improvement over fine-tuning-based methods and up to a 5% gain over reasoning-specialized pipelines under identical retrieval conditions. Crucially, ComposeRAG significantly enhances grounding: its verification-first design reduces ungrounded answers by over 10% in low-quality retrieval settings, and by approximately 3% even with strong corpora. Comprehensive ablation studies validate the modular architecture, demonstrating distinct and additive contributions from each component. These findings underscore ComposeRAG's capacity to deliver flexible, transparent, scalable, and high-performing multi-hop reasoning with improved grounding and interpretability.</li>
</ul>

<h3>Title: DeGLIF for Label Noise Robust Node Classification using GNNs</h3>
<ul>
<li><strong>Authors: </strong>Pintu Kumar, Nandyala Hemachandra</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00244">https://arxiv.org/abs/2506.00244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00244">https://arxiv.org/pdf/2506.00244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00244]] DeGLIF for Label Noise Robust Node Classification using GNNs(https://arxiv.org/abs/2506.00244)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Noisy labelled datasets are generally inexpensive compared to clean labelled datasets, and the same is true for graph data. In this paper, we propose a denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence Function. DeGLIF uses a small set of clean data and the leave-one- out influence function to make label noise robust node-level prediction on graph data. Leave-one-out influence function approximates the change in the model parameters if a training point is removed from the training dataset. Recent advances propose a way to calculate the leave-one-out influence function for Graph Neural Networks (GNNs). We extend that recent work to estimate the change in validation loss, if a training node is removed from the training dataset. We use this estimate and a new theoretically motivated relabelling function to denoise the training dataset. We propose two DeGLIF variants to identify noisy nodes. Both these variants do not require any information about the noise model or the noise level in the dataset; DeGLIF also does not estimate these quantities. For one of these variants, we prove that the noisy points detected can indeed increase risk. We carry out detailed computational experiments on different datasets to show the effectiveness of DeGLIF. It achieves better accuracy than other baseline algorithms</li>
</ul>

<h3>Title: Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Ali Payani, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00245">https://arxiv.org/abs/2506.00245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00245">https://arxiv.org/pdf/2506.00245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00245]] Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity(https://arxiv.org/abs/2506.00245)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address these limitations, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at this https URL.</li>
</ul>

<h3>Title: PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00250">https://arxiv.org/abs/2506.00250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00250">https://arxiv.org/pdf/2506.00250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00250]] PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain(https://arxiv.org/abs/2506.00250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale, expert-validated dataset of multiple-choice Persian medical questions, designed to evaluate LLMs across both Persian and English. We benchmark over 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 35.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, Persian responses are sometimes more accurate due to cultural and clinical contextual cues. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating multilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be accessed at: this https URL](this https URL</li>
</ul>

<h3>Title: Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</h3>
<ul>
<li><strong>Authors: </strong>Lihao Sun, Chengzhi Mao, Valentin Hofmann, Xuechunzi Bai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00253">https://arxiv.org/abs/2506.00253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00253">https://arxiv.org/pdf/2506.00253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00253]] Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race(https://arxiv.org/abs/2506.00253)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.</li>
</ul>

<h3>Title: The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection</h3>
<ul>
<li><strong>Authors: </strong>Mahammed Kamruzzaman, Gene Louis Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00256">https://arxiv.org/abs/2506.00256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00256">https://arxiv.org/pdf/2506.00256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00256]] The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection(https://arxiv.org/abs/2506.00256)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into hiring processes, concerns about fairness have gained prominence. When applying for jobs, companies often request/require demographic information, including gender, race, and disability or veteran status. This data is collected to support diversity and inclusion initiatives, but when provided to LLMs, especially disability-related information, it raises concerns about potential biases in candidate selection outcomes. Many studies have highlighted how disability can impact CV screening, yet little research has explored the specific effect of voluntarily disclosed information on LLM-driven candidate selection. This study seeks to bridge that gap. When candidates shared identical gender, race, qualifications, experience, and backgrounds, and sought jobs with minimal employment rate gaps between individuals with and without disabilities (e.g., Cashier, Software Developer), LLMs consistently favored candidates who disclosed that they had no disability. Even in cases where candidates chose not to disclose their disability status, the LLMs were less likely to select them compared to those who explicitly stated they did not have a disability.</li>
</ul>

<h3>Title: PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Fan, Wanru Li, Kuo-chu Chang, Ting Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00259">https://arxiv.org/abs/2506.00259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00259">https://arxiv.org/pdf/2506.00259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00259]] PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction(https://arxiv.org/abs/2506.00259)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications.</li>
</ul>

<h3>Title: Compact and Selective Disclosure for Verifiable Credentials</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Buldini, Carlo Mazzocca, Rebecca Montanari, Selcuk Uluagac</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00262">https://arxiv.org/abs/2506.00262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00262">https://arxiv.org/pdf/2506.00262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00262]] Compact and Selective Disclosure for Verifiable Credentials(https://arxiv.org/abs/2506.00262)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Self-Sovereign Identity (SSI) is a novel identity model that empowers individuals with full control over their data, enabling them to choose what information to disclose, with whom, and when. This paradigm is rapidly gaining traction worldwide, supported by numerous initiatives such as the European Digital Identity (EUDI) Regulation or Singapore's National Digital Identity (NDI). For instance, by 2026, the EUDI Regulation will enable all European citizens to seamlessly access services across Europe using Verifiable Credentials (VCs). A key feature of SSI is the ability to selectively disclose only specific claims within a credential, enhancing privacy protection of the identity owner. This paper proposes a novel mechanism designed to achieve Compact and Selective Disclosure for VCs (CSD-JWT). Our method leverages a cryptographic accumulator to encode claims within a credential to a unique, compact representation. We implemented CSD-JWT as an open-source solution and extensively evaluated its performance under various conditions. CSD-JWT provides significant memory savings, reducing usage by up to 46% compared to the state-of-the-art. It also minimizes network overhead by producing remarkably smaller Verifiable Presentations (VPs), reduced in size by 27% to 93%. Such features make CSD-JWT especially well-suited for resource-constrained devices, including hardware wallets designed for managing credentials.</li>
</ul>

<h3>Title: MultiHoax: A Dataset of Multi-hop False-Premise Questions</h3>
<ul>
<li><strong>Authors: </strong>Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00264">https://arxiv.org/abs/2506.00264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00264">https://arxiv.org/pdf/2506.00264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00264]] MultiHoax: A Dataset of Multi-hop False-Premise Questions(https://arxiv.org/abs/2506.00264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models are increasingly deployed in high-stakes domains, their ability to detect false assumptions and reason critically is crucial for ensuring reliable outputs. False-premise questions (FPQs) serve as an important evaluation method by exposing cases where flawed assumptions lead to incorrect responses. While existing benchmarks focus on single-hop FPQs, real-world reasoning often requires multi-hop inference, where models must verify consistency across multiple reasoning steps rather than relying on surface-level cues. To address this gap, we introduce MultiHoax, a benchmark for evaluating LLMs' ability to handle false premises in complex, multi-step reasoning tasks. Our dataset spans seven countries and ten diverse knowledge categories, using Wikipedia as the primary knowledge source to enable factual reasoning across regions. Experiments reveal that state-of-the-art LLMs struggle to detect false premises across different countries, knowledge categories, and multi-hop reasoning types, highlighting the need for improved false premise detection and more robust multi-hop reasoning capabilities in LLMs.</li>
</ul>

<h3>Title: CASPER: A Large Scale Spontaneous Speech Dataset</h3>
<ul>
<li><strong>Authors: </strong>Cihan Xiao, Ruixing Liang, Xiangyu Zhang, Mehmet Emre Tiryaki, Veronica Bae, Lavanya Shankar, Rong Yang, Ethan Poon, Emmanuel Dupoux, Sanjeev Khudanpur, Leibny Paola Garcia Perera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00267">https://arxiv.org/abs/2506.00267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00267">https://arxiv.org/pdf/2506.00267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00267]] CASPER: A Large Scale Spontaneous Speech Dataset(https://arxiv.org/abs/2506.00267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models has driven interest in developing similar speech processing capabilities. However, a key challenge is the scarcity of high-quality spontaneous speech data, as most existing datasets contain scripted dialogues. To address this, we present a novel pipeline for eliciting and recording natural dialogues and release our Stage 1 dataset with 200+ hours of spontaneous speech. Our approach fosters fluid, natural conversations while encouraging a diverse range of topics and interactive exchanges. Unlike traditional methods, it facilitates genuine interactions, providing a reproducible framework for future data collection. This paper introduces our dataset and methodology, laying the groundwork for addressing the shortage of spontaneous speech data. We plan to expand this dataset in future stages, offering a growing resource for the research community.</li>
</ul>

<h3>Title: Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response</h3>
<ul>
<li><strong>Authors: </strong>Jan-Niclas Hilgert, Carlo Jakobs, Michael Külper, Martin Lambertz, Axel Mahr, Elmar Padilla</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00274">https://arxiv.org/abs/2506.00274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00274">https://arxiv.org/pdf/2506.00274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00274]] Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response(https://arxiv.org/abs/2506.00274)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models hold considerable promise for supporting forensic investigations, but their widespread adoption is hindered by a lack of transparency, explainability, and reproducibility. This paper explores how the emerging Model Context Protocol can address these challenges and support the meaningful use of LLMs in digital forensics. Through a theoretical analysis, we examine how MCP can be integrated across various forensic scenarios - ranging from artifact analysis to the generation of interpretable reports. We also outline both technical and conceptual considerations for deploying an MCP server in forensic environments. Our analysis reveals a wide range of use cases in which MCP not only strengthens existing forensic workflows but also facilitates the application of LLMs to areas of forensics where their use was previously limited. Furthermore, we introduce the concept of the inference constraint level - a way of characterizing how specific MCP design choices can deliberately constrain model behavior, thereby enhancing both auditability and traceability. Our insights demonstrate that MCP has significant potential as a foundational component for developing LLM-assisted forensic workflows that are not only more transparent, reproducible, and legally defensible, but also represent a step toward increased automation in digital forensic analysis. However, we also highlight potential challenges that the adoption of MCP may pose for digital forensics in the future.</li>
</ul>

<h3>Title: Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Hans W. A. Hanley, Zakir Durumeric</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00277">https://arxiv.org/abs/2506.00277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00277">https://arxiv.org/pdf/2506.00277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00277]] Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings(https://arxiv.org/abs/2506.00277)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.</li>
</ul>

<h3>Title: 3D Gaussian Splat Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haoran Wang, Matthew Lau, Wenke Lee, Willian T. Lunardi, Martin Andreoni, Polo Chau</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00280">https://arxiv.org/abs/2506.00280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00280">https://arxiv.org/pdf/2506.00280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00280]] 3D Gaussian Splat Vulnerabilities(https://arxiv.org/abs/2506.00280)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications.</li>
</ul>

<h3>Title: Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Chris M. Ward, Josh Harguess</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00281">https://arxiv.org/abs/2506.00281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00281">https://arxiv.org/pdf/2506.00281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00281]] Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2506.00281)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.</li>
</ul>

<h3>Title: Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Oliver Mortensen, Mohammad Sadegh Talebi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00286">https://arxiv.org/abs/2506.00286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00286">https://arxiv.org/pdf/2506.00286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00286]] Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model(https://arxiv.org/abs/2506.00286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and $\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the strength of this dependence grows with the learners risk-sensitivity $|\beta|$. We also provide two lower bounds which shows that exponential dependence on $|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and that the PAC-bound on policy-learning is nearly tight in $A$.</li>
</ul>

<h3>Title: Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Elhady, Eneko Agirre, Mikel Artetxe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00288">https://arxiv.org/abs/2506.00288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00288">https://arxiv.org/pdf/2506.00288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00288]] Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation(https://arxiv.org/abs/2506.00288)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.</li>
</ul>

<h3>Title: DLM-One: Diffusion Language Models for One-Step Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianqi Chen, Shujian Zhang, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00290">https://arxiv.org/abs/2506.00290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00290">https://arxiv.org/pdf/2506.00290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00290]] DLM-One: Diffusion Language Models for One-Step Sequence Generation(https://arxiv.org/abs/2506.00290)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces DLM-One, a score-distillation-based framework for one-step sequence generation with continuous diffusion language models (DLMs). DLM-One eliminates the need for iterative refinement by aligning the scores of a student model's outputs in the continuous token embedding space with the score function of a pretrained teacher DLM. We investigate whether DLM-One can achieve substantial gains in sampling efficiency for language modeling. Through comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we show that DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks used to evaluate the teacher models. We further analyze the method's empirical behavior across multiple datasets, providing initial insights into its generality and practical applicability. Our findings position one-step diffusion as a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models operating in embedding space for natural language processing.</li>
</ul>

<h3>Title: Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiruvathukal, James C. Davis, Yung-Hsiang Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00299">https://arxiv.org/abs/2506.00299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00299">https://arxiv.org/pdf/2506.00299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00299]] Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms(https://arxiv.org/abs/2506.00299)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are state-of-the-art generative models in various domains, yet their samples often fail to satisfy downstream objectives such as safety constraints or domain-specific validity. Existing techniques for alignment require gradients, internal model access, or large computational budgets. We introduce an inference-time alignment framework based on evolutionary algorithms. We treat diffusion models as black-boxes and search their latent space to maximize alignment objectives. Our method enables efficient inference-time alignment for both differentiable and non-differentiable alignment objectives across a range of diffusion models. On the DrawBench and Open Image Preferences benchmark, our EA methods outperform state-of-the-art gradient-based and gradient-free inference-time methods. In terms of memory consumption, we require 55% to 76% lower GPU memory than gradient-based methods. In terms of running-time, we are 72% to 80% faster than gradient-based methods. We achieve higher alignment scores over 50 optimization steps on Open Image Preferences than gradient-based and gradient-free methods.</li>
</ul>

<h3>Title: Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Payal Mohapatra, Akash Pandey, Xiaoyuan Zhang, Qi Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00304">https://arxiv.org/abs/2506.00304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00304">https://arxiv.org/pdf/2506.00304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00304]] Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs(https://arxiv.org/abs/2506.00304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unvoiced electromyography (EMG) is an effective communication tool for individuals unable to produce vocal speech. However, most prior methods rely on paired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text conversion, which is not practical for such individuals. Given the rise of large language models (LLMs) in speech recognition, we explore their potential to understand unvoiced speech. To this end, we address the challenge of learning from unvoiced EMG alone and propose a novel EMG adaptor module that maps EMG features into an LLM's input space, achieving an average word error rate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with a conservative data availability of just six minutes, our approach improves performance over specialized models by nearly 20%. While LLMs have been shown to be extendable to new language modalities -- such as audio -- understanding articulatory biosignals like unvoiced EMG remains more challenging. This work takes a crucial first step toward enabling LLMs to comprehend unvoiced speech using surface EMG.</li>
</ul>

<h3>Title: Lossless Token Sequence Compression via Meta-Tokens</h3>
<ul>
<li><strong>Authors: </strong>John Harvill, Ziwei Fan, Hao Wang, Yizhou Sun, Hao Ding, Luke Huan, Anoop Deoras</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00307">https://arxiv.org/abs/2506.00307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00307">https://arxiv.org/pdf/2506.00307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00307]] Lossless Token Sequence Compression via Meta-Tokens(https://arxiv.org/abs/2506.00307)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.</li>
</ul>

<h3>Title: An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3</h3>
<ul>
<li><strong>Authors: </strong>Brendan Sands, Yining Wang, Chenhao Xu, Yuxuan Zhou, Lai Wei, Rohitash Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00312">https://arxiv.org/abs/2506.00312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00312">https://arxiv.org/pdf/2506.00312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00312]] An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3(https://arxiv.org/abs/2506.00312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.</li>
</ul>

<h3>Title: Data Flows in You: Benchmarking and Improving Static Data-flow Analysis on Binary Executables</h3>
<ul>
<li><strong>Authors: </strong>Nicolaas Weideman, Sima Arasteh, Mukund Raghothaman, Jelena Mirkovic, Christophe Hauser</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00313">https://arxiv.org/abs/2506.00313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00313">https://arxiv.org/pdf/2506.00313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00313]] Data Flows in You: Benchmarking and Improving Static Data-flow Analysis on Binary Executables(https://arxiv.org/abs/2506.00313)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Data-flow analysis is a critical component of security research. Theoretically, accurate data-flow analysis in binary executables is an undecidable problem, due to complexities of binary code. Practically, many binary analysis engines offer some data-flow analysis capability, but we lack understanding of the accuracy of these analyses, and their limitations. We address this problem by introducing a labeled benchmark data set, including 215,072 microbenchmark test cases, mapping to 277,072 binary executables, created specifically to evaluate data- flow analysis implementations. Additionally, we augment our benchmark set with dynamically-discovered data flows from 6 real-world executables. Using our benchmark data set, we evaluate three state of the art data-flow analysis implementations, in angr, Ghidra and Miasm and discuss their very low accuracy and reasons behind it. We further propose three model extensions to static data-flow analysis that significantly improve accuracy, achieving almost perfect recall (0.99) and increasing precision from 0.13 to 0.32. Finally, we show that leveraging these model extensions in a vulnerability-discovery context leads to a tangible improvement in vulnerable instruction identification.</li>
</ul>

<h3>Title: Local Frames: Exploiting Inherited Origins to Bypass Content Blockers</h3>
<ul>
<li><strong>Authors: </strong>Alisha Ukani, Hamed Haddadi, Alex C. Snoeren, Peter Snyder</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00317">https://arxiv.org/abs/2506.00317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00317">https://arxiv.org/pdf/2506.00317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00317]] Local Frames: Exploiting Inherited Origins to Bypass Content Blockers(https://arxiv.org/abs/2506.00317)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>We present a study of how local frames (i.e., iframes with non-URL sources like "about:blank") are mishandled by a wide range of popular Web security and privacy tools. As a result, users of these tools remain vulnerable to the very attack techniques they seek to protect against, including browser fingerprinting, cookie-based tracking, and data exfiltration. The tools we study are vulnerable in different ways, but all share a root cause: legacy Web functionality interacting with browser privacy boundaries in unexpected ways, leading to systemic vulnerabilities in tools developed, maintained, and recommended by privacy experts and activists. We consider four core capabilities supported by most privacy tools and develop tests to determine whether each can be evaded through the use of local frames. We apply our tests to six popular Web privacy and security tools, identifying at least one vulnerability in each for a total of 19, and extract common patterns regarding their mishandling of local frames. Our measurement of popular websites finds that 56% employ local frames and that 73.7% of the requests made by these local frames should be blocked by popular filter lists but instead trigger the vulnerabilities we identify; from another perspective, 14.3% of all sites that we crawl make requests that should be blocked inside of local frames. We disclosed the vulnerabilities to the tool authors and discuss both our experiences working with them to patch their products and the implications of our findings for other privacy and security research.</li>
</ul>

<h3>Title: Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sara Ghazanfari, Francesco Croce, Nicolas Flammarion, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00318">https://arxiv.org/abs/2506.00318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00318">https://arxiv.org/pdf/2506.00318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00318]] Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning(https://arxiv.org/abs/2506.00318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent work has shown that eliciting Large Language Models (LLMs) to generate reasoning traces in natural language before answering the user's request can significantly improve their performance across tasks. This approach has been extended to multimodal LLMs, where the models can produce chain-of-thoughts (CoT) about the content of input images and videos. In this work, we propose to obtain video LLMs whose reasoning steps are grounded in, and explicitly refer to, the relevant video frames. For this, we first create CoF-Data, a large dataset of diverse questions, answers, and corresponding frame-grounded reasoning traces about both natural and synthetic videos, spanning various topics and tasks. Then, we fine-tune existing video LLMs on this chain-of-frames (CoF) data. Our approach is simple and self-contained, and, unlike existing approaches for video CoT, does not require auxiliary networks to select or caption relevant frames. We show that our models based on CoF are able to generate chain-of-thoughts that accurately refer to the key frames to answer the given question. This, in turn, leads to improved performance across multiple video understanding benchmarks, for example, surpassing leading video LLMs on Video-MME, MVBench, and VSI-Bench, and notably reducing the hallucination rate. Code available at this https URL}{this http URL.</li>
</ul>

<h3>Title: dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Mahiou, Amir Dizche, Reza Nazari, Xinmin Wu, Ralph Abbey, Jorge Silva, Georgi Ganev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00322">https://arxiv.org/abs/2506.00322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00322">https://arxiv.org/pdf/2506.00322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00322]] dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation(https://arxiv.org/abs/2506.00322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose dpmm, an open-source library for synthetic data generation with Differentially Private (DP) guarantees. It includes three popular marginal models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer richer functionality compared to alternative implementations. Additionally, we adopt best practices to provide end-to-end DP guarantees and address well-known DP-related vulnerabilities. Our goal is to accommodate a wide audience with easy-to-install, highly customizable, and robust model implementations. Our codebase is available from this https URL.</li>
</ul>

<h3>Title: Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking</h3>
<ul>
<li><strong>Authors: </strong>Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, Ru-Yue Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00325">https://arxiv.org/abs/2506.00325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00325">https://arxiv.org/pdf/2506.00325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00325]] Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking(https://arxiv.org/abs/2506.00325)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Although deep learning-based visual tracking methods have made significant progress, they exhibit vulnerabilities when facing carefully designed adversarial attacks, which can lead to a sharp decline in tracking performance. To address this issue, this paper proposes for the first time a novel adversarial defense method based on denoise diffusion probabilistic models, termed DiffDf, aimed at effectively improving the robustness of existing visual tracking methods against adversarial attacks. DiffDf establishes a multi-scale defense mechanism by combining pixel-level reconstruction loss, semantic consistency loss, and structural similarity loss, effectively suppressing adversarial perturbations through a gradual denoising process. Extensive experimental results on several mainstream datasets show that the DiffDf method demonstrates excellent generalization performance for trackers with different architectures, significantly improving various evaluation metrics while achieving real-time inference speeds of over 30 FPS, showcasing outstanding defense performance and efficiency. Codes are available at this https URL.</li>
</ul>

<h3>Title: Latent Guidance in Diffusion Models for Perceptual Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Shreshth Saini, Ru-Ling Liao, Yan Ye, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00327">https://arxiv.org/abs/2506.00327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00327">https://arxiv.org/pdf/2506.00327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00327]] Latent Guidance in Diffusion Models for Perceptual Evaluations(https://arxiv.org/abs/2506.00327)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in latent diffusion models that generate high-dimensional image data and perform various downstream tasks, there has been little exploration into perceptual consistency within these models on the task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within the data manifold. We leverage this insight to guide on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality features to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work on guiding diffusion model with perceptual features for NR-IQA. Extensive experiments on IQA datasets show that our method, LGDM, achieves state-of-the-art performance, underscoring the superior generalization capabilities of diffusion models for NR-IQA tasks.</li>
</ul>

<h3>Title: Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, Prashant J. Nair</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00329">https://arxiv.org/abs/2506.00329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00329">https://arxiv.org/pdf/2506.00329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00329]] Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation(https://arxiv.org/abs/2506.00329)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality. We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \texttt{this https URL}.</li>
</ul>

<h3>Title: TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Boyi Zhang, Zhuo Liu, Hangfeng He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00331">https://arxiv.org/abs/2506.00331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00331">https://arxiv.org/pdf/2506.00331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00331]] TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering(https://arxiv.org/abs/2506.00331)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In real practice, questions are typically complex and knowledge-intensive, requiring Large Language Models (LLMs) to recognize the multifaceted nature of the question and reason across multiple information sources. Iterative and adaptive retrieval, where LLMs decide when and what to retrieve based on their reasoning, has been shown to be a promising approach to resolve complex, knowledge-intensive questions. However, the performance of such retrieval frameworks is limited by the accumulation of reasoning errors and misaligned retrieval results. To overcome these limitations, we propose TreeRare (Syntax Tree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to guide information retrieval and reasoning for question answering. Following the principle of compositionality, TreeRare traverses the syntax tree in a bottom-up fashion, and in each node, it generates subcomponent-based queries and retrieves relevant passages to resolve localized uncertainty. A subcomponent question answering module then synthesizes these passages into concise, context-aware evidence. Finally, TreeRare aggregates the evidence across the tree to form a final answer. Experiments across five question answering datasets involving ambiguous or multi-hop reasoning demonstrate that TreeRare achieves substantial improvements over existing state-of-the-art methods.</li>
</ul>

<h3>Title: Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus</h3>
<ul>
<li><strong>Authors: </strong>Svetlana Churina, Akshat Gupta, Insyirah Mujtahid, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00332">https://arxiv.org/abs/2506.00332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00332">https://arxiv.org/pdf/2506.00332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00332]] Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus(https://arxiv.org/abs/2506.00332)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Code-mixing involves the seamless integration of linguistic elements from multiple languages within a single discourse, reflecting natural multilingual communication patterns. Despite its prominence in informal interactions such as social media, chat messages and instant-messaging exchanges, there has been a lack of publicly available corpora that are author-labeled and suitable for modeling human conversations and relationships. This study introduces the first labeled and general-purpose corpus for understanding code-mixing in context while maintaining rigorous privacy and ethical standards. Our live project will continuously gather, verify, and integrate code-mixed messages into a structured dataset released in JSON format, accompanied by detailed metadata and linguistic statistics. To date, it includes over 355,641 messages spanning various code-mixing patterns, with a primary focus on English, Mandarin, and other languages. We expect the Codemix Corpus to serve as a foundational dataset for research in computational linguistics, sociolinguistics, and NLP applications.</li>
</ul>

<h3>Title: Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gerard Christopher Yeo, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00334">https://arxiv.org/abs/2506.00334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00334">https://arxiv.org/pdf/2506.00334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00334]] Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models(https://arxiv.org/abs/2506.00334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Datasets used for emotion recognition tasks typically contain overt cues that can be used in predicting the emotions expressed in a text. However, one challenge is that texts sometimes contain covert contextual cues that are rich in affective semantics, which warrant higher-order reasoning abilities to infer emotional states, not simply the emotions conveyed. This study advances beyond surface-level perceptual features to investigate how large language models (LLMs) reason about others' emotional states using contextual information, within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal Theory, we curate a specialized ToM evaluation dataset1 to assess both forward reasoning - from context to emotion- and backward reasoning - from emotion to inferred context. We showed that LLMs can reason to a certain extent, although they are poor at associating situational outcomes and appraisals with specific emotions. Our work highlights the need for psychological theories in the training and evaluation of LLMs in the context of emotion reasoning.</li>
</ul>

<h3>Title: Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Ming Hu, Jianfu Yin, Mingyu Dou, Yuqi Wang, Ruochen Dang, Siyi Liang, Cong Hu, Yao Wang, Bingliang Hu, Quan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00337">https://arxiv.org/abs/2506.00337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00337">https://arxiv.org/pdf/2506.00337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00337]] Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification(https://arxiv.org/abs/2506.00337)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The automatic classification of medical time series signals, such as electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in clinical decision support and early detection of diseases. Although Transformer based models have achieved notable performance by implicitly modeling temporal dependencies through self-attention mechanisms, their inherently complex architectures and opaque reasoning processes undermine their trustworthiness in high stakes clinical settings. In response to these limitations, this study shifts focus toward a modeling paradigm that emphasizes structural transparency, aligning more closely with the intrinsic characteristics of medical data. We propose a novel method, Channel Imposed Fusion (CIF), which enhances the signal-to-noise ratio through cross-channel information fusion, effectively reduces redundancy, and improves classification performance. Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN), known for its structural simplicity and controllable receptive field, to construct an efficient and explicit classification framework. Experimental results on multiple publicly available EEG and ECG datasets demonstrate that the proposed method not only outperforms existing state-of-the-art (SOTA) approaches in terms of various classification metrics, but also significantly enhances the transparency of the classification process, offering a novel perspective for medical time series classification.</li>
</ul>

<h3>Title: Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sungjae Lee, Hoyoung Kim, Jeongyeon Hwang, Eunhyeok Park, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00344">https://arxiv.org/abs/2506.00344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00344">https://arxiv.org/pdf/2506.00344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00344]] Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs(https://arxiv.org/abs/2506.00344)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling test-time computation--generating and analyzing multiple or sequential outputs for a single input--has become a promising strategy for improving the reliability and quality of large language models (LLMs), as evidenced by advances in uncertainty quantification and multi-step reasoning. A key shared component is semantic clustering, which groups outputs that differ in form but convey the same meaning. Semantic clustering enables estimation of the distribution over the semantics of outputs and helps avoid redundant exploration of reasoning paths. However, existing approaches typically rely on external models, which introduce substantial computational overhead and often fail to capture context-aware semantics. We propose Latent Semantic Clustering (LSC), a lightweight and context-sensitive method that leverages the generator LLM's internal hidden states for clustering, eliminating the need for external models. Our extensive experiment across various LLMs and datasets shows that LSC significantly improves the computational efficiency of test-time scaling while maintaining or exceeding the performance of existing methods.</li>
</ul>

<h3>Title: Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy</h3>
<ul>
<li><strong>Authors: </strong>Jie Ren, Zhenwei Dai, Xianfeng Tang, Yue Xing, Shenglai Zeng, Hui Liu, Jingying Zeng, Qiankun Peng, Samarth Varshney, Suhang Wang, Qi He, Charu C. Aggarwal, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00359">https://arxiv.org/abs/2506.00359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00359">https://arxiv.org/pdf/2506.00359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00359]] Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy(https://arxiv.org/abs/2506.00359)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks, growing concerns have emerged over the misuse of sensitive, copyrighted, or harmful data during training. To address these concerns, unlearning techniques have been developed to remove the influence of specific data without retraining from scratch. However, this paper reveals a critical vulnerability in fine-tuning-based unlearning: a malicious user can craft a manipulated forgetting request that stealthily degrades the model's utility for benign users. We demonstrate this risk through a red-teaming Stealthy Attack (SA), which is inspired by two key limitations of existing unlearning (the inability to constrain the scope of unlearning effect and the failure to distinguish benign tokens from unlearning signals). Prior work has shown that unlearned models tend to memorize forgetting data as unlearning signals, and respond with hallucinations or feigned ignorance when unlearning signals appear in the input. By subtly increasing the presence of common benign tokens in the forgetting data, SA enhances the connection between benign tokens and unlearning signals. As a result, when normal users include such tokens in their prompts, the model exhibits unlearning behaviors, leading to unintended utility degradation. To address this vulnerability, we propose Scope-aware Unlearning (SU), a lightweight enhancement that introduces a scope term into the unlearning objective, encouraging the model to localize the forgetting effect. Our method requires no additional data processing, integrates seamlessly with existing fine-tuning frameworks, and significantly improves robustness against SA. Extensive experiments validate the effectiveness of both SA and SU.</li>
</ul>

<h3>Title: Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Ngoc Tuyen Do, Tri Nhu Do</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00365">https://arxiv.org/abs/2506.00365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00365">https://arxiv.org/pdf/2506.00365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00365]] Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection(https://arxiv.org/abs/2506.00365)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>In the surveillance and defense domain, multi-target detection and classification (MTD) is considered essential yet challenging due to heterogeneous inputs from diverse data sources and the computational complexity of algorithms designed for resource-constrained embedded devices, particularly for Al-based solutions. To address these challenges, we propose a feature fusion and knowledge-distilled framework for multi-modal MTD that leverages data fusion to enhance accuracy and employs knowledge distillation for improved domain adaptation. Specifically, our approach utilizes both RGB and thermal image inputs within a novel fusion-based multi-modal model, coupled with a distillation training pipeline. We formulate the problem as a posterior probability optimization task, which is solved through a multi-stage training pipeline supported by a composite loss function. This loss function effectively transfers knowledge from a teacher model to a student model. Experimental results demonstrate that our student model achieves approximately 95% of the teacher model's mean Average Precision while reducing inference time by approximately 50%, underscoring its suitability for practical MTD deployment scenarios.</li>
</ul>

<h3>Title: Adversarial Machine Learning for Robust Password Strength Estimation</h3>
<ul>
<li><strong>Authors: </strong>Pappu Jha, Hanzla Hamid, Oluseyi Olukola, Ashim Dahal, Nick Rahimi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00373">https://arxiv.org/abs/2506.00373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00373">https://arxiv.org/pdf/2506.00373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00373]] Adversarial Machine Learning for Robust Password Strength Estimation(https://arxiv.org/abs/2506.00373)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Passwords remain one of the most common methods for securing sensitive data in the digital age. However, weak password choices continue to pose significant risks to data security and privacy. This study aims to solve the problem by focusing on developing robust password strength estimation models using adversarial machine learning, a technique that trains models on intentionally crafted deceptive passwords to expose and address vulnerabilities posed by such passwords. We apply five classification algorithms and use a dataset with more than 670,000 samples of adversarial passwords to train the models. Results demonstrate that adversarial training improves password strength classification accuracy by up to 20% compared to traditional machine learning models. It highlights the importance of integrating adversarial machine learning into security systems to enhance their robustness against modern adaptive threats. Keywords: adversarial attack, password strength, classification, machine learning</li>
</ul>

<h3>Title: A Systematic Review of Metaheuristics-Based and Machine Learning-Driven Intrusion Detection Systems in IoT</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Shamim Ahsan, Salekul Islam, Swakkhar Shatabda</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00377">https://arxiv.org/abs/2506.00377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00377">https://arxiv.org/pdf/2506.00377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00377]] A Systematic Review of Metaheuristics-Based and Machine Learning-Driven Intrusion Detection Systems in IoT(https://arxiv.org/abs/2506.00377)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The widespread adoption of the Internet of Things (IoT) has raised a new challenge for developers since it is prone to known and unknown cyberattacks due to its heterogeneity, flexibility, and close connectivity. To defend against such security breaches, researchers have focused on building sophisticated intrusion detection systems (IDSs) using machine learning (ML) techniques. Although these algorithms notably improve detection performance, they require excessive computing power and resources, which are crucial issues in IoT networks considering the recent trends of decentralized data processing and computing systems. Consequently, many optimization techniques have been incorporated with these ML models. Specifically, a special category of optimizer adopted from the behavior of living creatures and different aspects of natural phenomena, known as metaheuristic algorithms, has been a central focus in recent years and brought about remarkable results. Considering this vital significance, we present a comprehensive and systematic review of various applications of metaheuristics algorithms in developing a machine learning-based IDS, especially for IoT. A significant contribution of this study is the discovery of hidden correlations between these optimization techniques and machine learning models integrated with state-of-the-art IoT-IDSs. In addition, the effectiveness of these metaheuristic algorithms in different applications, such as feature selection, parameter or hyperparameter tuning, and hybrid usages are separately analyzed. Moreover, a taxonomy of existing IoT-IDSs is proposed. Furthermore, we investigate several critical issues related to such integration. Our extensive exploration ends with a discussion of promising optimization algorithms and technologies that can enhance the efficiency of IoT-IDSs.</li>
</ul>

<h3>Title: Spectral Insights into Data-Oblivious Critical Layers in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuyuan Liu, Lei Hsiung, Yaoqing Yang, Yujun Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00382">https://arxiv.org/abs/2506.00382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00382">https://arxiv.org/pdf/2506.00382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00382]] Spectral Insights into Data-Oblivious Critical Layers in Large Language Models(https://arxiv.org/abs/2506.00382)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a data-oblivious approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment(CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning--a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions. We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.</li>
</ul>

<h3>Title: Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training</h3>
<ul>
<li><strong>Authors: </strong>Keyeun Lee, Seolhee Lee, Esther Hehsun Kim, Yena Ko, Jinsu Eun, Dahee Kim, Hyewon Cho, Haiyi Zhu, Robert E. Kraut, Eunyoung Suh, Eun-mee Kim, Hajin Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00386">https://arxiv.org/abs/2506.00386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00386">https://arxiv.org/pdf/2506.00386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00386]] Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training(https://arxiv.org/abs/2506.00386)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective communication training is essential to preparing nurses for high-quality patient care. While standardized patient (SP) simulations provide valuable experiential learning, they are often costly and inflexible. Virtual patient (VP) systems offer a scalable alternative, but most fail to adapt to the varying communication skills of trainees. In particular, when trainees respond ineffectively, VPs should escalate in hostility or become uncooperative--yet this level of adaptive interaction remains largely unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue generation framework that leverages large language models (LLMs) to dynamically adapt VP behavior based on trainee input. The framework features a pipeline for constructing clinically grounded yet flexible VP scenarios and a modular system for assessing trainee communication and adjusting VP responses in real time, while ensuring learner safety. We validated Adaptive-VP by simulating challenging patient conversations. Automated evaluation using a corpus from practicing nurses showed that our communication skill evaluation mechanism reflected real-world proficiency levels. Expert nurses further confirmed that Adaptive-VP produced more natural and realistic interactions than existing approaches, demonstrating its potential as a scalable and effective tool for nursing communication training.</li>
</ul>

<h3>Title: SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Ge Qu, Jinyang Li, Bowen Qin, Xiaolong Li, Nan Huo, Chenhao Ma, Reynold Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00391">https://arxiv.org/abs/2506.00391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00391">https://arxiv.org/pdf/2506.00391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00391]] SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL(https://arxiv.org/abs/2506.00391)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.</li>
</ul>

<h3>Title: Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Gu, Shangsong Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00396">https://arxiv.org/abs/2506.00396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00396">https://arxiv.org/pdf/2506.00396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00396]] Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively(https://arxiv.org/abs/2506.00396)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective decision-making in Large Language Models (LLMs) is essential for handling intricate tasks. However, existing approaches prioritize performance but often overlook the balance between effectiveness and computational cost. To address this, we first introduce the 3E Criteria to systematically assess the cost-effectiveness of search strategies, revealing that existing methods often trade significant efficiency for marginal performance gains. To improve LLM decision-making while maintaining efficiency, we propose the Speculative Reward Model (SRM), a plug-and-play framework that seamlessly integrates with existing search strategies. Specifically, SRM employs an external reward assigner to predict optimal actions, reducing reliance on LLMs' internal self-evaluation. And a speculative verification mechanism is used to prune suboptimal choices and guide the search toward more promising steps. We evaluate SRM on several complex decision-making tasks including mathematical reasoning, planning and numerical reasoning in specialized domains. Experimental results show that SRM reduces costs to 1/10 of the original search framework on average while maintaining effectiveness.</li>
</ul>

<h3>Title: Scaling Textual Gradients via Sampling-Based Momentum</h3>
<ul>
<li><strong>Authors: </strong>Zixin Ding, Junyuan Hong, Jiachen T. Wang, Zinan Lin, Zhangyang Wang, Yuxin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00400">https://arxiv.org/abs/2506.00400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00400">https://arxiv.org/pdf/2506.00400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00400]] Scaling Textual Gradients via Sampling-Based Momentum(https://arxiv.org/abs/2506.00400)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM - suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD's performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains - including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks - TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks.</li>
</ul>

<h3>Title: Bias as a Virtue: Rethinking Generalization under Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Chen, Wentao Li, Jiahui Xiao, Yuchen Li, Yimin Tang, Xiaonan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00407">https://arxiv.org/abs/2506.00407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00407">https://arxiv.org/pdf/2506.00407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00407]] Bias as a Virtue: Rethinking Generalization under Distribution Shifts(https://arxiv.org/abs/2506.00407)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.</li>
</ul>

<h3>Title: JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering</h3>
<ul>
<li><strong>Authors: </strong>Ziwen Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00410">https://arxiv.org/abs/2506.00410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00410">https://arxiv.org/pdf/2506.00410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00410]] JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering(https://arxiv.org/abs/2506.00410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular processes by enabling gene expression analysis at the individual cell level. Clustering allows for the identification of cell types and the further discovery of intrinsic patterns in single-cell data. However, the high dimensionality and sparsity of scRNA-seq data continue to challenge existing clustering models. In this paper, we introduce JojoSCL, a novel self-supervised contrastive learning framework for scRNA-seq clustering. By incorporating a shrinkage estimator based on hierarchical Bayesian estimation, which adjusts gene expression estimates towards more reliable cluster centroids to reduce intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate (SURE), JojoSCL refines both instance-level and cluster-level contrastive learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL consistently outperforms prevalent clustering methods, with further validation of its practicality through robustness analysis and ablation studies. JojoSCL's code is available at: this https URL.</li>
</ul>

<h3>Title: Accelerating Diffusion LLMs via Adaptive Parallel Decoding</h3>
<ul>
<li><strong>Authors: </strong>Daniel Israel, Guy Van den Broeck, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00413">https://arxiv.org/abs/2506.00413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00413">https://arxiv.org/pdf/2506.00413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00413]] Accelerating Diffusion LLMs via Adaptive Parallel Decoding(https://arxiv.org/abs/2506.00413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.</li>
</ul>

<h3>Title: Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Anum Nawaz, Muhammad Irfan, Xianjia Yu, Zhuo Zou, Tomi Westerlund</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00416">https://arxiv.org/abs/2506.00416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00416">https://arxiv.org/pdf/2506.00416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00416]] Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare(https://arxiv.org/abs/2506.00416)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has attracted increasing attention to mitigate security and privacy challenges in traditional cloud-centric machine learning models specifically in healthcare ecosystems. FL methodologies enable the training of global models through localized policies, allowing independent operations at the edge clients' level. Conventional first-order FL approaches face several challenges in personalized model training due to heterogeneous non-independent and identically distributed (non-iid) data of each edge client. Recently, second-order FL approaches maintain the stability and consistency of non-iid datasets while improving personalized model training. This study proposes and develops a verifiable and auditable optimized second-order FL framework BFEL (blockchain-enhanced federated edge learning) based on optimized FedCurv for personalized healthcare systems. FedCurv incorporates information about the importance of each parameter to each client's task (through Fisher Information Matrix) which helps to preserve client-specific knowledge and reduce model drift during aggregation. Moreover, it minimizes communication rounds required to achieve a target precision convergence for each edge client while effectively managing personalized training on non-iid and heterogeneous data. The incorporation of Ethereum-based model aggregation ensures trust, verifiability, and auditability while public key encryption enhances privacy and security. Experimental results of federated CNNs and MLPs utilizing Mnist, Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the proposed framework.</li>
</ul>

<h3>Title: Dual Debiasing for Noisy In-Context Learning for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Siqi Liang, Sumyeong Ahn, Paramveer S. Dhillon, Jiayu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00418">https://arxiv.org/abs/2506.00418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00418">https://arxiv.org/pdf/2506.00418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00418]] Dual Debiasing for Noisy In-Context Learning for Text Generation(https://arxiv.org/abs/2506.00418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.</li>
</ul>

<h3>Title: Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saqib, Saikat Chakraborty, Santu Karmaker, Niranjan Balasubramanian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00419">https://arxiv.org/abs/2506.00419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00419">https://arxiv.org/pdf/2506.00419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00419]] Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences(https://arxiv.org/abs/2506.00419)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>LLM generated code often contains security issues. We address two key challenges in improving secure code generation. First, obtaining high quality training data covering a broad set of security issues is critical. To address this, we introduce a method for distilling a preference dataset of insecure and secure code pairs from frontier LLMs, along with a security reasoning that explains the issues and the fix. The key idea here is to make use of security knowledge sources to devise a systematic prompting strategy that ensures broad coverage. Second, aligning models to secure code requires focusing on localized regions of code. Direct preference optimization methods, like SimPO, are not designed to handle these localized differences and turn out to be ineffective. We address this with a new localized preference optimization algorithm that masks the security related tokens in both the winning (secure) and losing (insecure) responses. To prevent loss in code quality, we also add a regularizer. Evaluations show that both training on our dataset, DiSCo, and the new preference optimization algorithm, LPO, yield substantial reductions in code insecurity while also improving overall code quality. Code and dataset are available at this https URL.</li>
</ul>

<h3>Title: A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Miao Ye, Suxiao Wang, Jiaguang Han, Yong Wang, Xiaoli Wang, Jingxuan Wei, Peng Wen, Jing Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00420">https://arxiv.org/abs/2506.00420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00420">https://arxiv.org/pdf/2506.00420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00420]] A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks(https://arxiv.org/abs/2506.00420)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in the data collected by WSNs can provide crucial evidence for assessing the reliability and stability of WSNs. Existing methods for WSN anomaly detection often face challenges such as the limited extraction of spatiotemporal correlation features, the absence of sample labels, few anomaly samples, and an imbalanced sample distribution. To address these issues, a spatiotemporal correlation detection model (MTAD-RD) considering both model architecture and a two-stage training strategy perspective is proposed. In terms of model structure design, the proposed MTAD-RD backbone network includes a retentive network (RetNet) enhanced by a cross-retention (CR) module, a multigranular feature fusion module, and a graph attention network module to extract internode correlation information. This proposed model can integrate the intermodal correlation features and spatial features of WSN neighbor nodes while extracting global information from time series data. Moreover, its serialized inference characteristic can remarkably reduce inference overhead. For model training, a two-stage training approach was designed. First, a contrastive learning proxy task was designed for time series data with graph structure information in WSNs, enabling the backbone network to learn transferable features from unlabeled data using unsupervised contrastive learning methods, thereby addressing the issue of missing sample labels in the dataset. Then, a caching-based sample sampler was designed to divide samples into few-shot and contrastive learning data. A specific joint loss function was developed to jointly train the dual-graph discriminator network to address the problem of sample imbalance effectively. In experiments carried out on real public datasets, the designed MTAD-RD anomaly detection method achieved an F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods.</li>
</ul>

<h3>Title: Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments</h3>
<ul>
<li><strong>Authors: </strong>Anjani kumar Polinati</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00426">https://arxiv.org/abs/2506.00426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00426">https://arxiv.org/pdf/2506.00426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00426]] Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments(https://arxiv.org/abs/2506.00426)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The pervasive use of hybrid cloud computing models has changed enterprise as well as Information Technology services infrastructure by giving businesses simple and cost-effective options of combining on-premise IT equipment with public cloud services. hybrid cloud solutions deploy multifaceted models of security, performance optimization, and cost efficiency, conventionally fragmented in the cloud computing milieu. This paper examines how organizations manage these parameters in hybrid cloud ecosystems while providing solutions to the challenges they face in operationalizing hybrid cloud adoptions. The study captures the challenges of achieving a balance in resource distribution between on-premise and cloud resources (herein referred to as the "resource allocation challenge"), the complexity of pricing models from cloud providers like AWS, Microsoft Azure, Google Cloud (herein called the 'pricing complexity problem'), and the urgency for strong security infrastructure to safeguard sensitive information (known as 'the information security problem'). This study demonstrates the security and performance management solutions proposed were validated in a detailed case study of adoption of AWS and Azure based hybrid cloud and provides useful guidance. Also, a hybrid cloud security and cost optimization framework based on zero trust architecture, encryption, hybrid cloud policies, and others, is proposed. The conclusion includes recommendations for research on automation of hybrid cloud service management, integration of multi-clouds, and the ever-present question of data privacy, stressing how those matters affect contemporary enterprises.</li>
</ul>

<h3>Title: TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jie Peng, Zhewei Wei, Yuhang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00431">https://arxiv.org/abs/2506.00431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00431">https://arxiv.org/pdf/2506.00431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00431]] TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer(https://arxiv.org/abs/2506.00431)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Due to the proficiency of self-attention mechanisms (SAMs) in capturing dependencies in sequence modeling, several existing dynamic graph neural networks (DGNNs) utilize Transformer architectures with various encoding designs to capture sequential evolutions of dynamic graphs. However, the effectiveness and efficiency of these Transformer-based DGNNs vary significantly, highlighting the importance of properly defining the SAM on dynamic graphs and comprehensively encoding temporal and interactive dynamics without extra complex modules. In this work, we propose TIDFormer, a dynamic graph TransFormer that fully exploits Temporal and Interactive Dynamics in an efficient manner. We clarify and verify the interpretability of our proposed SAM, addressing the open problem of its uninterpretable definitions on dynamic graphs in previous works. To model the temporal and interactive dynamics, respectively, we utilize the calendar-based time partitioning information and extract informative interaction embeddings for both bipartite and non-bipartite graphs using merely the sampled first-order neighbors. In addition, we jointly model temporal and interactive features by capturing potential changes in historical interaction patterns through a simple decomposition. We conduct extensive experiments on several dynamic graph datasets to verify the effectiveness and efficiency of TIDFormer. The experimental results demonstrate that TIDFormer excels, outperforming state-of-the-art models across most datasets and experimental settings. Furthermore, TIDFormer exhibits significant efficiency advantages compared to previous Transformer-based methods.</li>
</ul>

<h3>Title: Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free</h3>
<ul>
<li><strong>Authors: </strong>Luigi Sigillo, Shengfeng He, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00433">https://arxiv.org/abs/2506.00433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00433">https://arxiv.org/pdf/2506.00433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00433]] Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free(https://arxiv.org/abs/2506.00433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>High-resolution image synthesis remains a core challenge in generative modeling, particularly in balancing computational efficiency with the preservation of fine-grained visual detail. We present Latent Wavelet Diffusion (LWD), a lightweight framework that enables any latent diffusion model to scale to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces three key components: (1) a scale-consistent variational autoencoder objective that enhances the spectral fidelity of latent representations; (2) wavelet energy maps that identify and localize detail-rich spatial regions within the latent space; and (3) a time-dependent masking strategy that focuses denoising supervision on high-frequency components during training. LWD requires no architectural modifications and incurs no additional computational overhead. Despite its simplicity, it consistently improves perceptual quality and reduces FID in ultra-high-resolution image synthesis, outperforming strong baseline models. These results highlight the effectiveness of frequency-aware, signal-driven supervision as a principled and efficient approach for high-resolution generative modeling.</li>
</ul>

<h3>Title: Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal Embedding</h3>
<ul>
<li><strong>Authors: </strong>Tuan-Luc Huynh, Thanh-Danh Le, Tam V. Nguyen, Trung-Nghia Le, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00434">https://arxiv.org/abs/2506.00434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00434">https://arxiv.org/pdf/2506.00434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00434]] Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal Embedding(https://arxiv.org/abs/2506.00434)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we address the crucial task of brain tumor segmentation in medical imaging and propose innovative approaches to enhance its performance. The current state-of-the-art nnU-Net has shown promising results but suffers from extensive training requirements and underutilization of pre-trained weights. To overcome these limitations, we integrate Axial-Coronal-Sagittal convolutions and pre-trained weights from ImageNet into the nnU-Net framework, resulting in reduced training epochs, reduced trainable parameters, and improved efficiency. Two strategies for transferring 2D pre-trained weights to the 3D domain are presented, ensuring the preservation of learned relationships and feature representations critical for effective information propagation. Furthermore, we explore a joint classification and segmentation model that leverages pre-trained encoders from a brain glioma grade classification proxy task, leading to enhanced segmentation performance, especially for challenging tumor labels. Experimental results demonstrate that our proposed methods in the fast training settings achieve comparable or even outperform the ensemble of cross-validation models, a common practice in the brain tumor segmentation literature.</li>
</ul>

<h3>Title: Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Zhang, Xiaoou Liu, Dongsheng Luo, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00437">https://arxiv.org/abs/2506.00437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00437">https://arxiv.org/pdf/2506.00437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00437]] Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks(https://arxiv.org/abs/2506.00437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Explaining Graph Neural Networks (GNNs) has garnered significant attention due to the need for interpretability, enabling users to understand the behavior of these black-box models better and extract valuable insights from their predictions. While numerous post-hoc instance-level explanation methods have been proposed to interpret GNN predictions, the reliability of these explanations remains uncertain, particularly in the out-of-distribution or unknown test datasets. In this paper, we address this challenge by introducing an explainer framework with the confidence scoring module ( ConfExplainer), grounded in theoretical principle, which is generalized graph information bottleneck with confidence constraint (GIB-CC), that quantifies the reliability of generated explanations. Experimental results demonstrate the superiority of our approach, highlighting the effectiveness of the confidence score in enhancing the trustworthiness and robustness of GNN explanations.</li>
</ul>

<h3>Title: PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge</h3>
<ul>
<li><strong>Authors: </strong>Keisuke Sugiura, Mizuki Yasuda, Hiroki Matsutani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00438">https://arxiv.org/abs/2506.00438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00438">https://arxiv.org/pdf/2506.00438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00438]] PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge(https://arxiv.org/abs/2506.00438)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Embedded edge devices are often used as a computing platform to run real-world point cloud applications, but recent deep learning-based methods may not fit on such devices due to limited resources. In this paper, we aim to fill this gap by introducing PointODE, a parameter-efficient ResNet-like architecture for point cloud feature extraction based on a stack of MLP blocks with residual connections. We leverage Neural ODE (Ordinary Differential Equation), a continuous-depth version of ResNet originally developed for modeling the dynamics of continuous-time systems, to compress PointODE by reusing the same parameters across MLP blocks. The point-wise normalization is proposed for PointODE to handle the non-uniform distribution of feature points. We introduce PointODE-Elite as a lightweight version with 0.58M trainable parameters and design its dedicated accelerator for embedded FPGAs. The accelerator consists of a four-stage pipeline to parallelize the feature extraction for multiple points and stores the entire parameters on-chip to eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53 CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature extraction by 4.9x, leading to 3.7x faster inference and 3.5x better energy-efficiency. Despite the simple architecture, PointODE-Elite shows competitive accuracy to the state-of-the-art models on both synthetic and real-world classification datasets, greatly improving the trade-off between accuracy and inference cost.</li>
</ul>

<h3>Title: RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuqian Fu, Yuanheng Zhu, Jiajun Chai, Guojun Yin, Wei Lin, Qichao Zhang, Dongbin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00439">https://arxiv.org/abs/2506.00439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00439">https://arxiv.org/pdf/2506.00439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00439]] RLAE: Reinforcement Learning-Assisted Ensemble for LLMs(https://arxiv.org/abs/2506.00439)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.</li>
</ul>

<h3>Title: PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel-M. Jimenez-Gutierrez, David Solans, Mohammed Elbamby, Nicolas Kourtellis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00440">https://arxiv.org/abs/2506.00440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00440">https://arxiv.org/pdf/2506.00440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00440]] PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning(https://arxiv.org/abs/2506.00440)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables decentralized machine learning (ML) model training while preserving data privacy by keeping data localized across clients. However, non-independent and identically distributed (non-IID) data across clients poses a significant challenge, leading to skewed model updates and performance degradation. Addressing this, we propose PSI-PFL, a novel client selection framework for Personalized Federated Learning (PFL) that leverages the Population Stability Index (PSI) to quantify and mitigate data heterogeneity (so-called non-IIDness). Our approach selects more homogeneous clients based on PSI, reducing the impact of label skew, one of the most detrimental factors in FL performance. Experimental results over multiple data modalities (tabular, image, text) demonstrate that PSI-PFL significantly improves global model accuracy, outperforming state-of-the-art baselines by up to 10\% under non-IID scenarios while ensuring fairer local performance. PSI-PFL enhances FL performance and offers practical benefits in applications where data privacy and heterogeneity are critical.</li>
</ul>

<h3>Title: G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Long Bai, Zixuan Li, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00445">https://arxiv.org/abs/2506.00445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00445">https://arxiv.org/pdf/2506.00445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00445]] G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models(https://arxiv.org/abs/2506.00445)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts based on historical ones has received much attention. Recent studies have introduced Large Language Models (LLMs) for this task to enhance the models' generalization abilities. However, these models perform forecasting via simultaneously learning two kinds of entangled knowledge in the TKG: (1) general patterns, i.e., invariant temporal structures shared across different scenarios; and (2) scenario information, i.e., factual knowledge engaged in specific scenario, such as entities and relations. As a result, the learning processes of these two kinds of knowledge may interfere with each other, which potentially impact the generalization abilities of the models. To enhance the generalization ability of LLMs on this task, in this paper, we propose a General-to-Specific learning framework (G2S) that disentangles the learning processes of the above two kinds of knowledge. In the general learning stage, we mask the scenario information in different TKGs and convert it into anonymous temporal structures. After training on these structures, the model is able to capture the general patterns across different TKGs. In the specific learning stage, we inject the scenario information into the structures via either in-context learning or fine-tuning modes. Experimental results show that G2S effectively improves the generalization abilities of LLMs.</li>
</ul>

<h3>Title: Performance Analysis of Few-Shot Learning Approaches for Bangla Handwritten Character and Digit Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mehedi Ahamed, Radib Bin Kabir, Tawsif Tashwar Dipto, Mueeze Al Mushabbir, Sabbir Ahmed, Md. Hasanul Kabir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00447">https://arxiv.org/abs/2506.00447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00447">https://arxiv.org/pdf/2506.00447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00447]] Performance Analysis of Few-Shot Learning Approaches for Bangla Handwritten Character and Digit Recognition(https://arxiv.org/abs/2506.00447)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This study investigates the performance of few-shot learning (FSL) approaches in recognizing Bangla handwritten characters and numerals using limited labeled data. It demonstrates the applicability of these methods to scripts with intricate and complex structures, where dataset scarcity is a common challenge. Given the complexity of Bangla script, we hypothesize that models performing well on these characters can generalize effectively to languages of similar or lower structural complexity. To this end, we introduce SynergiProtoNet, a hybrid network designed to improve the recognition accuracy of handwritten characters and digits. The model integrates advanced clustering techniques with a robust embedding framework to capture fine-grained details and contextual nuances. It leverages multi-level (both high- and low-level) feature extraction within a prototypical learning framework. We rigorously benchmark SynergiProtoNet against several state-of-the-art few-shot learning models: BD-CSPN, Prototypical Network, Relation Network, Matching Network, and SimpleShot, across diverse evaluation settings including Monolingual Intra-Dataset Evaluation, Monolingual Inter-Dataset Evaluation, Cross-Lingual Transfer, and Split Digit Testing. Experimental results show that SynergiProtoNet consistently outperforms existing methods, establishing a new benchmark in few-shot learning for handwritten character and digit recognition. The code is available on GitHub: this https URL.</li>
</ul>

<h3>Title: Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Suhas BN, Han-Chin Shing, Lei Xu, Mitch Strong, Jon Burnsky, Jessica Ofor, Jordan R. Mason, Susan Chen, Sundararajan Srinivasan, Chaitanya Shivade, Jack Moriarty, Joseph Paul Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00448">https://arxiv.org/abs/2506.00448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00448">https://arxiv.org/pdf/2506.00448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00448]] Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization(https://arxiv.org/abs/2506.00448)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) during summarization of patient-clinician dialogues pose significant risks to patient care and clinical decision-making. However, the phenomenon remains understudied in the clinical domain, with uncertainty surrounding the applicability of general-domain hallucination detectors. The rarity and randomness of hallucinations further complicate their investigation. In this paper, we conduct an evaluation of hallucination detection methods in the medical domain, and construct two datasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by systematically removing facts from source dialogues to induce hallucinated content in summaries; and a natural hallucination dataset -- arising organically during LLM-based medical summarization. We show that general-domain detectors struggle to detect clinical hallucinations, and that performance on fact-controlled hallucinations does not reliably predict effectiveness on natural hallucinations. We then develop fact-based approaches that count hallucinations, offering explainability not available with existing methods. Notably, our LLM-based detectors, which we developed using fact-controlled hallucinations, generalize well to detecting real-world clinical hallucinations. This research contributes a suite of specialized metrics supported by expert-annotated datasets to advance faithful clinical summarization systems.</li>
</ul>

<h3>Title: Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models</h3>
<ul>
<li><strong>Authors: </strong>Junwoo Park, Hyuck Lee, Dohyun Lee, Daehoon Gwak, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00457">https://arxiv.org/abs/2506.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00457">https://arxiv.org/pdf/2506.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00457]] Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models(https://arxiv.org/abs/2506.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable performance across diverse tasks without domain-specific training, fueling interest in their potential for time-series forecasting. While LLMs have shown potential in zero-shot forecasting through prompting alone, recent studies suggest that LLMs lack inherent effectiveness in forecasting. Given these conflicting findings, a rigorous validation is essential for drawing reliable conclusions. In this paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared to state-of-the-art domain-specific models. Our experiments show that LLM-based zero-shot forecasters often struggle to achieve high accuracy due to their sensitivity to noise, underperforming even simple domain-specific models. We have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot setting, but improving their robustness remains a significant challenge. Our findings suggest that rather than emphasizing zero-shot forecasting, a more promising direction would be to focus on fine-tuning LLMs to better process numerical sequences. Our experimental code is available at this https URL.</li>
</ul>

<h3>Title: Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control</h3>
<ul>
<li><strong>Authors: </strong>Elinor Ginzburg, Itay Segev, Yoash Levron, Sarah Keren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00459">https://arxiv.org/abs/2506.00459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00459">https://arxiv.org/pdf/2506.00459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00459]] Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control(https://arxiv.org/abs/2506.00459)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.</li>
</ul>

<h3>Title: Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data</h3>
<ul>
<li><strong>Authors: </strong>Shaoxiong Ji, Zihao Li, Jaakko Paavola, Indraneil Paul, Hengyu Luo, Jörg Tiedemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00469">https://arxiv.org/abs/2506.00469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00469">https://arxiv.org/pdf/2506.00469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00469]] Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data(https://arxiv.org/abs/2506.00469)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.</li>
</ul>

<h3>Title: BAGNet: A Boundary-Aware Graph Attention Network for 3D Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Wei Tao, Xiaoyang Qu, Kai Lu, Jiguang Wan, Shenglin He, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00475">https://arxiv.org/abs/2506.00475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00475">https://arxiv.org/pdf/2506.00475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00475]] BAGNet: A Boundary-Aware Graph Attention Network for 3D Point Cloud Semantic Segmentation(https://arxiv.org/abs/2506.00475)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Since the point cloud data is inherently irregular and unstructured, point cloud semantic segmentation has always been a challenging task. The graph-based method attempts to model the irregular point cloud by representing it as a graph; however, this approach incurs substantial computational cost due to the necessity of constructing a graph for every point within a large-scale point cloud. In this paper, we observe that boundary points possess more intricate spatial structural information and develop a novel graph attention network known as the Boundary-Aware Graph attention Network (BAGNet). On one hand, BAGNet contains a boundary-aware graph attention layer (BAGLayer), which employs edge vertex fusion and attention coefficients to capture features of boundary points, reducing the computation time. On the other hand, BAGNet employs a lightweight attention pooling layer to extract the global feature of the point cloud to maintain model accuracy. Extensive experiments on standard datasets demonstrate that BAGNet outperforms state-of-the-art methods in point cloud semantic segmentation with higher accuracy and less inference time.</li>
</ul>

<h3>Title: Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet - A ResNet-based Model Classification Dataset</h3>
<ul>
<li><strong>Authors: </strong>Abhisek Ray, Lukas Esterle</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00476">https://arxiv.org/abs/2506.00476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00476">https://arxiv.org/pdf/2506.00476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00476]] Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet - A ResNet-based Model Classification Dataset(https://arxiv.org/abs/2506.00476)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a powerful paradigm for training machine learning models across distributed data sources while preserving data locality. However, the privacy of local data is always a pivotal concern and has received a lot of attention in recent research on the FL regime. Moreover, the lack of domain heterogeneity and client-specific segregation in the benchmarks remains a critical bottleneck for rigorous evaluation. In this paper, we introduce ModelNet, a novel image classification dataset constructed from the embeddings extracted from a pre-trained ResNet50 model. First, we modify the CIFAR100 dataset into three client-specific variants, considering three domain heterogeneities (homogeneous, heterogeneous, and random). Subsequently, we train each client-specific subset of all three variants on the pre-trained ResNet50 model to save model parameters. In addition to multi-domain image data, we propose a new hypothesis to define the FL algorithm that can access the anonymized model parameters to preserve the local privacy in a more effective manner compared to existing ones. ModelNet is designed to simulate realistic FL settings by incorporating non-IID data distributions and client diversity design principles in the mainframe for both conventional and futuristic graph-driven FL algorithms. The three variants are ModelNet-S, ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and random data settings, respectively. To the best of our knowledge, we are the first to propose a cross-environment client-specific FL dataset along with the graph-based variant. Extensive experiments based on domain shifts and aggregation strategies show the effectiveness of the above variants, making it a practical benchmark for classical and graph-based FL research. The dataset and related code are available online.</li>
</ul>

<h3>Title: BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Eunsu Kim, Haneul Yoo, Guijin Son, Hitesh Patel, Amit Agarwal, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00482">https://arxiv.org/abs/2506.00482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00482">https://arxiv.org/pdf/2506.00482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00482]] BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation(https://arxiv.org/abs/2506.00482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.</li>
</ul>

<h3>Title: Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aviv Jan, Dean Tahory, Omer Talmi, Omar Abo Mokh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00483">https://arxiv.org/abs/2506.00483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00483">https://arxiv.org/pdf/2506.00483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00483]] Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models(https://arxiv.org/abs/2506.00483)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-hop questions still stump large language models (LLMs), which struggle to link information across multiple reasoning steps. We introduce Auto-Patch, a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch selectively modifies internal representations using a learned classifier. Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from 18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to Chain-of-Thought prompting (27.44\%). Our results highlight the potential of dynamic hidden state interventions for advancing complex reasoning in LLMs.</li>
</ul>

<h3>Title: It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jun Wu, Yirong Xiong, Jiangtao Wen, Yuxing Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00486">https://arxiv.org/abs/2506.00486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00486">https://arxiv.org/pdf/2506.00486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00486]] It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs(https://arxiv.org/abs/2506.00486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: this https URL.</li>
</ul>

<h3>Title: Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Shuguo Hu, Jun Hu, Huaiwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00488">https://arxiv.org/abs/2506.00488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00488">https://arxiv.org/pdf/2506.00488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00488]] Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection(https://arxiv.org/abs/2506.00488)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can assist multimodal fake news detection by predicting pseudo labels. However, LLM-generated pseudo labels alone demonstrate poor performance compared to traditional detection methods, making their effective integration non-trivial. In this paper, we propose Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal fake news detection, which integrates LLM capabilities via label propagation techniques. The global label propagation can utilize LLM-generated pseudo labels, enhancing prediction accuracy by propagating label information among all samples. For label propagation, a mask-based mechanism is designed to prevent label leakage during training by ensuring that training nodes do not propagate their own labels back to themselves. Experimental results on benchmark datasets show that by synergizing LLMs with label propagation, our model achieves superior performance over state-of-the-art baselines.</li>
</ul>

<h3>Title: FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wang, Lirong Gao, Haobo Wang, Yiming Zhang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00495">https://arxiv.org/abs/2506.00495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00495">https://arxiv.org/pdf/2506.00495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00495]] FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts(https://arxiv.org/abs/2506.00495)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely adopted strategy for adapting pre-trained Large Language Models (LLMs) to downstream tasks, significantly reducing memory and computational costs. However, most existing PEFT techniques uniformly deploy LoRA adapters across all layers, disregarding the intrinsic heterogeneity of layer contributions and task-specific rank requirements. This uniform paradigm leads to redundant parameter allocation and suboptimal adaptation efficiency. To address these limitations, we propose FLoE, a novel PEFT framework that introduces two key innovations: (i) a Fisher information-guided importance scoring mechanism to dynamically identify task-critical transformer layers for MoE-based low-rank adaptation, enabling sparse adapter deployment; and (ii) a Bayesian optimization-driven rank allocator that automatically determines optimal LoRA ranks on specific datasets without exhaustive grid search. Extensive experiments across diverse LLMs and benchmarks reveal that FLoE achieves impressive efficiency-accuracy trade-offs, making FLoE particularly advantageous in resource-constrained environments that necessitate rapid adaptation.</li>
</ul>

<h3>Title: Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study</h3>
<ul>
<li><strong>Authors: </strong>Diogo Landau, Ingeborg de Pater, Mihaela Mitici, Nishant Saurabh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.ET, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00499">https://arxiv.org/abs/2506.00499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00499">https://arxiv.org/pdf/2506.00499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00499]] Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study(https://arxiv.org/abs/2506.00499)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Complex systems such as aircraft engines are continuously monitored by sensors. In predictive aircraft maintenance, the collected sensor measurements are used to estimate the health condition and the Remaining Useful Life (RUL) of such systems. However, a major challenge when developing prognostics is the limited number of run-to-failure data samples. This challenge could be overcome if multiple airlines would share their run-to-failure data samples such that sufficient learning can be achieved. Due to privacy concerns, however, airlines are reluctant to share their data in a centralized setting. In this paper, a collaborative federated learning framework is therefore developed instead. Here, several airlines cooperate to train a collective RUL prognostic machine learning model, without the need to centrally share their data. For this, a decentralized validation procedure is proposed to validate the prognostics model without sharing any data. Moreover, sensor data is often noisy and of low quality. This paper therefore proposes four novel methods to aggregate the parameters of the global prognostic model. These methods enhance the robustness of the FL framework against noisy data. The proposed framework is illustrated for training a collaborative RUL prognostic model for aircraft engines, using the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in the FL framework to train a collective RUL prognostic model for their aircraft's engines. When comparing the proposed FL framework with the case where each airline independently develops their own prognostic model, the results show that FL leads to more accurate RUL prognostics for five out of the six airlines. Moreover, the novel robust aggregation methods render the FL framework robust to noisy data samples.</li>
</ul>

<h3>Title: Scaling DeFi with ZK Rollups: Design, Deployment, and Evaluation of a Real-Time Proof-of-Concept</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Gogol, Szczepan Gurgul, Faizan Nehal Siddiqui, David Branes, Claudio Tessone</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00500">https://arxiv.org/abs/2506.00500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00500">https://arxiv.org/pdf/2506.00500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00500]] Scaling DeFi with ZK Rollups: Design, Deployment, and Evaluation of a Real-Time Proof-of-Concept(https://arxiv.org/abs/2506.00500)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Ethereum's scalability limitations pose significant challenges for the adoption of decentralized applications (dApps). Zero-Knowledge Rollups (ZK Rollups) present a promising solution, bundling transactions off-chain and submitting validity proofs on-chain to enhance throughput and efficiency. In this work, we examine the technical underpinnings of ZK Rollups and stress test their performance in real-world applications in decentralized finance (DeFi). We set up a proof-of-concept (PoC) consisting of ZK rollup and decentralized exchange, and implement load balancer generating token swaps. Our results show that the rollup can process up to 71 swap transactions per second, compared to 12 general transaction by Ethereum. We further analyze transaction finality trade-offs with related security concerns, and discuss the future directions for integrating ZK Rollups into Ethereum's broader ecosystem.</li>
</ul>

<h3>Title: Exploring In-context Example Generation for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Dohyun Lee, Seungil Chad Lee, Chanwoo Yang, Yujin Baek, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00507">https://arxiv.org/abs/2506.00507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00507">https://arxiv.org/pdf/2506.00507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00507]] Exploring In-context Example Generation for Machine Translation(https://arxiv.org/abs/2506.00507)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong performance across various tasks, leveraging their exceptional in-context learning ability with only a few examples. Accordingly, the selection of optimal in-context examples has been actively studied in the field of machine translation. However, these studies presuppose the presence of a demonstration pool with human-annotated pairs, making them less applicable to low-resource languages where such an assumption is challenging to meet. To overcome this limitation, this paper explores the research direction of in-context example generation for machine translation. Specifically, we propose Demonstration Augmentation for Translation (DAT), a simple yet effective approach that generates example pairs without relying on any external resources. This method builds upon two prior criteria, relevance and diversity, which have been highlighted in previous work as key factors for in-context example selection. Through experiments and analysis on low-resource languages where human-annotated pairs are scarce, we show that DAT achieves superior translation quality compared to the baselines. Furthermore, we investigate the potential of progressively accumulating generated pairs during test time to build and reuse a demonstration pool. Our implementation is publicly available at this https URL.</li>
</ul>

<h3>Title: Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Zherui Li, Yan Mi, Zhenhong Zhou, Houcheng Jiang, Guibin Zhang, Kun Wang, Junfeng Fang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00509">https://arxiv.org/abs/2506.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00509">https://arxiv.org/pdf/2506.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00509]] Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems(https://arxiv.org/abs/2506.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model-based Multi-Agent Systems (MASs) have demonstrated strong advantages in addressing complex real-world tasks. However, due to the introduction of additional attack surfaces, MASs are particularly vulnerable to misinformation injection. To facilitate a deeper understanding of misinformation propagation dynamics within these systems, we introduce MisinfoTask, a novel dataset featuring complex, realistic tasks designed to evaluate MAS robustness against such threats. Building upon this, we propose ARGUS, a two-stage, training-free defense framework leveraging goal-aware reasoning for precise misinformation rectification within information flows. Our experiments demonstrate that in challenging misinformation scenarios, ARGUS exhibits significant efficacy across various injection attacks, achieving an average reduction in misinformation toxicity of approximately 28.17% and improving task success rates under attack by approximately 10.33%. Our code and dataset is available at: this https URL.</li>
</ul>

<h3>Title: Evaluating the Evaluation of Diversity in Commonsense Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianhui Zhang, Bei Peng, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00514">https://arxiv.org/abs/2506.00514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00514">https://arxiv.org/pdf/2506.00514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00514]] Evaluating the Evaluation of Diversity in Commonsense Generation(https://arxiv.org/abs/2506.00514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In commonsense generation, given a set of input concepts, a model must generate a response that is not only commonsense bearing, but also capturing multiple diverse viewpoints. Numerous evaluation metrics based on form- and content-level overlap have been proposed in prior work for evaluating the diversity of a commonsense generation model. However, it remains unclear as to which metrics are best suited for evaluating the diversity in commonsense generation. To address this gap, we conduct a systematic meta-evaluation of diversity metrics for commonsense generation. We find that form-based diversity metrics tend to consistently overestimate the diversity in sentence sets, where even randomly generated sentences are assigned overly high diversity scores. We then use an Large Language Model (LLM) to create a novel dataset annotated for the diversity of sentences generated for a commonsense generation task, and use it to conduct a meta-evaluation of the existing diversity evaluation metrics. Our experimental results show that content-based diversity evaluation metrics consistently outperform the form-based counterparts, showing high correlations with the LLM-based ratings. We recommend that future work on commonsense generation should use content-based metrics for evaluating the diversity of their outputs.</li>
</ul>

<h3>Title: Robust and Verifiable MPC with Applications to Linear Machine Learning Inference</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Shen Wang, Jimmy Dani, Juan Garay, Soamar Homsi, Nitesh Saxena</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00518">https://arxiv.org/abs/2506.00518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00518">https://arxiv.org/pdf/2506.00518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00518]] Robust and Verifiable MPC with Applications to Linear Machine Learning Inference(https://arxiv.org/abs/2506.00518)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>In this work, we present an efficient secure multi-party computation MPC protocol that provides strong security guarantees in settings with dishonest majority of participants who may behave arbitrarily. Unlike the popular MPC implementation known as SPDZ [Crypto '12], which only ensures security with abort, our protocol achieves both complete identifiability and robustness. With complete identifiability, honest parties can detect and unanimously agree on the identity of any malicious party. Robustness allows the protocol to continue with the computation without requiring a restart, even when malicious behavior is detected. Additionally, our approach addresses the performance limitations observed in the protocol by Cunningham et al. [ICITS '17], which, while achieving complete identifiability, is hindered by the costly exponentiation operations required by the choice of commitment scheme. Our protocol is based on the approach by Rivinius et al. [S&P '22], utilizing lattice-based commitment for better efficiency. We achieved robustness with the help of a semi-honest trusted third party. We benchmark our robust protocol, showing the efficient recovery from parties' malicious behavior. Finally, we benchmark our protocol on a ML-as-a-service scenario, wherein clients off-load the desired computation to the servers, and verify the computation result. We benchmark on linear ML inference, running on various datasets. While our efficiency is slightly lower compared to SPDZ's, we offer stronger security properties that provide distinct advantages.</li>
</ul>

<h3>Title: CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00519">https://arxiv.org/abs/2506.00519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00519">https://arxiv.org/pdf/2506.00519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00519]] CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention(https://arxiv.org/abs/2506.00519)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at this https URL.</li>
</ul>

<h3>Title: SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xingtong Ge, Xin Zhang, Tongda Xu, Yi Zhang, Xinjie Zhang, Yan Wang, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00523">https://arxiv.org/abs/2506.00523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00523">https://arxiv.org/pdf/2506.00523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00523]] SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation(https://arxiv.org/abs/2506.00523)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed \textbf{SenseFlow}, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at this https URL.</li>
</ul>

<h3>Title: Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Runtao Ren, Jian Ma, Jianxi Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00527">https://arxiv.org/abs/2506.00527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00527">https://arxiv.org/pdf/2506.00527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00527]] Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning(https://arxiv.org/abs/2506.00527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at this https URL.</li>
</ul>

<h3>Title: M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Hang Fana, Mingxuan Lib, Zuhan Zhanga, Long Chengc, Yujian Ye, Dunnan Liua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00531">https://arxiv.org/abs/2506.00531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00531">https://arxiv.org/pdf/2506.00531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00531]] M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model(https://arxiv.org/abs/2506.00531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.</li>
</ul>

<h3>Title: The Security Threat of Compressed Projectors in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yudong Zhang, Ruobing Xie, Xingwu Sun, Jiansheng Chen, Zhanhui Kang, Di Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00534">https://arxiv.org/abs/2506.00534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00534">https://arxiv.org/pdf/2506.00534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00534]] The Security Threat of Compressed Projectors in Large Vision-Language Models(https://arxiv.org/abs/2506.00534)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The choice of a suitable visual language projector (VLP) is critical to the successful training of large visual language models (LVLMs). Mainstream VLPs can be broadly categorized into compressed and uncompressed projectors, and each offering distinct advantages in performance and computational efficiency. However, their security implications have not been thoroughly examined. Our comprehensive evaluation reveals significant differences in their security profiles: compressed projectors exhibit substantial vulnerabilities, allowing adversaries to successfully compromise LVLMs even with minimal knowledge of structural information. In stark contrast, uncompressed projectors demonstrate robust security properties and do not introduce additional vulnerabilities. These findings provide critical guidance for researchers in selecting optimal VLPs that enhance the security and reliability of visual language models. The code will be released.</li>
</ul>

<h3>Title: Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Changyue Wang, Weihang Su, Qingyao Ai, Yujia Zhou, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00536">https://arxiv.org/abs/2506.00536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00536">https://arxiv.org/pdf/2506.00536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00536]] Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing(https://arxiv.org/abs/2506.00536)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning this http URL this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: this https URL .</li>
</ul>

<h3>Title: ARIA: Training Language Agents with Intention-Driven Reward Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Yang, Yikai Zhang, Aili Chen, Xintao Wang, Siyu Yuan, Jiangjie Chen, Deqing Yang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00539">https://arxiv.org/abs/2506.00539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00539">https://arxiv.org/pdf/2506.00539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00539]] ARIA: Training Language Agents with Intention-Driven Reward Aggregation(https://arxiv.org/abs/2506.00539)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.</li>
</ul>

<h3>Title: Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Bejani, Guillermo Perez-de-Arenaza-Pozo, Julián D. Arias-Londoño, Juan I. Godino-LLorente</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00545">https://arxiv.org/abs/2506.00545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00545">https://arxiv.org/pdf/2506.00545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00545]] Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach(https://arxiv.org/abs/2506.00545)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>Missing data is a relevant issue in time series, especially in biomedical sequences such as those corresponding to smooth pursuit eye movements, which often contain gaps due to eye blinks and track losses, complicating the analysis and extraction of meaningful biomarkers. In this paper, a novel imputation framework is proposed using Self-Attention-based Imputation networks for time series, which leverages the power of deep learning and self-attention mechanisms to impute missing data. We further refine the imputed data using a custom made autoencoder, tailored to represent smooth pursuit eye movement sequences. The proposed approach was implemented using 5,504 sequences from 172 Parkinsonian patients and healthy controls. Results show a significant improvement in the accuracy of reconstructed eye movement sequences with respect to other state of the art techniques, substantially reducing the values for common time domain error metrics such as the mean absolute error, mean relative error, and root mean square error, while also preserving the signal's frequency domain characteristics. Moreover, it demonstrates robustness when large intervals of data are missing. This method offers an alternative solution for robustly handling missing data in time series, enhancing the reliability of smooth pursuit analysis for the screening and monitoring of neurodegenerative disorders.</li>
</ul>

<h3>Title: Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Geng, Thy Thy Tran, Preslav Nakov, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00548">https://arxiv.org/abs/2506.00548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00548">https://arxiv.org/pdf/2506.00548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00548]] Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities(https://arxiv.org/abs/2506.00548)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Existing attacks against multimodal language models (MLLMs) primarily communicate instructions through text accompanied by adversarial images. In contrast, we exploit the capabilities of MLLMs to interpret non-textual instructions, specifically, adversarial images or audio generated by our novel method, Con Instruction. We optimize these adversarial examples to align closely with target instructions in the embedding space, revealing the detrimental implications of MLLMs' sophisticated understanding. Unlike prior work, our method does not require training data or preprocessing of textual instructions. While these non-textual adversarial examples can effectively bypass MLLM safety mechanisms, their combination with various text inputs substantially amplifies attack success. We further introduce a new Attack Response Categorization (ARC) framework, which evaluates both the quality of the model's response and its relevance to the malicious instructions. Experimental results demonstrate that Con Instruction effectively bypasses safety mechanisms in multiple vision- and audio-language models, including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard benchmarks: AdvBench and SafeBench. Specifically, our method achieves the highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On the defense side, we explore various countermeasures against our attacks and uncover a substantial performance gap among existing techniques. Our implementation is made publicly available.</li>
</ul>

<h3>Title: Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages</h3>
<ul>
<li><strong>Authors: </strong>Hyangsuk Min, Yuho Lee, Minjeong Ban, Jiaqi Deng, Nicole Hee-Yeon Kim, Taewon Yun, Hang Su, Jason Cai, Hwanjun Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00549">https://arxiv.org/abs/2506.00549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00549">https://arxiv.org/pdf/2506.00549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00549]] Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages(https://arxiv.org/abs/2506.00549)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yule Zhu, Ping Liu, Zhedong Zheng, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00562">https://arxiv.org/abs/2506.00562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00562">https://arxiv.org/pdf/2506.00562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00562]] SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models(https://arxiv.org/abs/2506.00562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently enabled precise and photorealistic facial editing across a wide range of semantic attributes. Beyond single-step modifications, a growing class of applications now demands the ability to analyze and track sequences of progressive edits, such as stepwise changes to hair, makeup, or accessories. However, sequential editing introduces significant challenges in edit attribution and detection robustness, further complicated by the lack of large-scale, finely annotated benchmarks tailored explicitly for this task. We introduce SEED, a large-scale Sequentially Edited facE Dataset constructed via state-of-the-art diffusion models. SEED contains over 90,000 facial images with one to four sequential attribute modifications, generated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3). Each image is annotated with detailed edit sequences, attribute masks, and prompts, facilitating research on sequential edit tracking, visual provenance analysis, and manipulation robustness assessment. To benchmark this task, we propose FAITH, a frequency-aware transformer-based model that incorporates high-frequency cues to enhance sensitivity to subtle sequential changes. Comprehensive experiments, including systematic comparisons of multiple frequency-domain methods, demonstrate the effectiveness of FAITH and the unique challenges posed by SEED. SEED offers a challenging and flexible resource for studying progressive diffusion-based edits at scale. Dataset and code will be publicly released at: this https URL.</li>
</ul>

<h3>Title: Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Luo, Tianwei Ni, Pierre-Luc Bacon, Doina Precup, Xujie Si</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00563">https://arxiv.org/abs/2506.00563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00563">https://arxiv.org/pdf/2506.00563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00563]] Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments(https://arxiv.org/abs/2506.00563)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.</li>
</ul>

<h3>Title: Communication Efficient Multiparty Private Set Intersection from Multi-Point Sequential OPRF</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Feng, Yukun Wang, Cong Li, Wu Xin, Ming Yao, Dian Zhang, Wanwan Wang, Hao He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00566">https://arxiv.org/abs/2506.00566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00566">https://arxiv.org/pdf/2506.00566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00566]] Communication Efficient Multiparty Private Set Intersection from Multi-Point Sequential OPRF(https://arxiv.org/abs/2506.00566)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Multiparty private set intersection (MPSI) allows multiple participants to compute the intersection of their locally owned data sets without revealing them. MPSI protocols can be categorized based on the network topology of nodes, with the star, mesh, and ring topologies being the primary types, respectively. Given that star and mesh topologies dominate current implementations, most existing MPSI protocols are based on these two topologies. However, star-topology MPSI protocols suffer from high leader node load, while mesh topology protocols suffer from high communication complexity and overhead. In this paper, we first propose a multi-point sequential oblivious pseudorandom function (MP-SOPRF) in a multi-party setting. Based on MP-SOPRF, we then develop an MPSI protocol with a ring topology, addressing the challenges of communication and computational overhead in existing protocols. We prove that our MPSI protocol is semi-honest secure under the Hamming correlation robustness assumption. Our experiments demonstrate that our MPSI protocol outperforms state-of-the-art protocols, achieving a reduction of 74.8% in communication and a 6% to 287% improvement in computational efficiency.</li>
</ul>

<h3>Title: CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ke Niu, Zhuofan Chen, Haiyang Yu, Yuwen Chen, Teng Fu, Mengyang Zhao, Bin Li, Xiangyang Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00568">https://arxiv.org/abs/2506.00568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00568">https://arxiv.org/pdf/2506.00568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00568]] CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning(https://arxiv.org/abs/2506.00568)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing. Orthographic projection reasoning underpins the entire CAD workflow, encompassing design, manufacturing, and simulation. However, prevailing deep-learning approaches employ standard 3D reconstruction pipelines as an alternative, which often introduce imprecise dimensions and limit the parametric editability required for CAD workflows. Recently, some researchers adopt vision-language models (VLMs), particularly supervised fine-tuning (SFT), to tackle CAD-related challenges. SFT shows promise but often devolves into pattern memorization, yielding poor out-of-distribution performance on complex reasoning tasks. To address these gaps, we introduce CReFT-CAD, a two-stage fine-tuning paradigm that first employs a curriculum-driven reinforcement learning stage with difficulty-aware rewards to build reasoning ability steadily, and then applies supervised post-tuning to hone instruction following and semantic extraction. Complementing this, we release TriView2CAD, the first large-scale, open-source benchmark for orthographic projection reasoning, comprising 200,000 synthetic and 3,000 real-world orthographic projections with precise dimension annotations and six interoperable data modalities. We benchmark leading VLMs on orthographic projection reasoning and demonstrate that CReFT-CAD substantially improves reasoning accuracy and out-of-distribution generalizability in real-world scenarios, offering valuable insights for advancing CAD reasoning research.</li>
</ul>

<h3>Title: AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nicholas E. Corrado, Julian Katz-Samuels, Adithya Devraj, Hyokun Yun, Chao Zhang, Yi Xu, Yi Pan, Bing Yin, Trishul Chilimbi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00569">https://arxiv.org/abs/2506.00569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00569">https://arxiv.org/pdf/2506.00569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00569]] AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs(https://arxiv.org/abs/2506.00569)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When aligning large language models (LLMs), their performance on various tasks (such as being helpful, harmless, and honest) depends heavily on the composition of their training data. However, selecting a data mixture that achieves strong performance across all tasks is challenging. Existing approaches rely on large ablation studies, heuristics, or human intuition, but these can be prohibitively expensive and suboptimal. We study this problem in the setting of preference optimization via DPO and introduce AutoMixAlign (AMA), a theoretically-grounded algorithm that adaptively mixes datasets during training to balance performance across tasks. AMA first trains \textit{specialist models} for each task to determine losses that correspond to strong task performance. Then, it trains a generalist model using a novel minimax optimization that prioritizes tasks for which generalist model losses deviate most from specialist model losses. To optimize this problem, we propose two algorithms: (1) AMA-R, which adaptively reweights the objective to prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is sampled from each task to prioritize tasks. Both algorithms achieve a convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence result follows from Sagawa et al. (2019), and we provide a convergence proof for AMA-S using online learning techniques such as EXP3. We evaluate AMA on several multitask alignment setups and find that AMA outperforms the standard alignment approach -- which simply optimizes the total loss across all tasks -- and also outperforms model merging methods.</li>
</ul>

<h3>Title: Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00574">https://arxiv.org/abs/2506.00574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00574">https://arxiv.org/pdf/2506.00574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00574]] Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing(https://arxiv.org/abs/2506.00574)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.</li>
</ul>

<h3>Title: Decoding the Stressed Brain with Geometric Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Sonia Koszut, Sam Nallaperuma-Herzberg, Pietro Lio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00587">https://arxiv.org/abs/2506.00587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00587">https://arxiv.org/pdf/2506.00587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00587]] Decoding the Stressed Brain with Geometric Machine Learning(https://arxiv.org/abs/2506.00587)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Stress significantly contributes to both mental and physical disorders, yet traditional self-reported questionnaires are inherently subjective. In this study, we introduce a novel framework that employs geometric machine learning to detect stress from raw EEG recordings. Our approach constructs graphs by integrating structural connectivity (derived from electrode spatial arrangement) with functional connectivity from pairwise signal correlations. A spatio-temporal graph convolutional network (ST-GCN) processes these graphs to capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show that the ST-GCN outperforms standard machine learning models on all key classification metrics and enhances interpretability, explored through ablation analyses of key channels and brain regions. These results pave the way for more objective and accurate stress detection methods.</li>
</ul>

<h3>Title: MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xudong Ma, Nantheera Anantrasirichai, Stefanos Bolomytis, Alin Achim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00591">https://arxiv.org/abs/2506.00591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00591">https://arxiv.org/pdf/2506.00591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00591]] MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models(https://arxiv.org/abs/2506.00591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diagnosis of prostate cancer increasingly depends on multimodal imaging, particularly magnetic resonance imaging (MRI) and transrectal ultrasound (TRUS). However, accurate registration between these modalities remains a fundamental challenge due to the differences in dimensionality and anatomical representations. In this work, we present a novel framework that addresses these challenges through a two-stage process: TRUS 3D reconstruction followed by cross-modal registration. Unlike existing TRUS 3D reconstruction methods that rely heavily on external probe tracking information, we propose a totally probe-location-independent approach that leverages the natural correlation between sagittal and transverse TRUS views. With the help of our clustering-based feature matching method, we enable the spatial localization of 2D frames without any additional probe tracking information. For the registration stage, we introduce an unsupervised diffusion-based framework guided by modality translation. Unlike existing methods that translate one modality into another, we map both MR and US into a pseudo intermediate modality. This design enables us to customize it to retain only registration-critical features, greatly easing registration. To further enhance anatomical alignment, we incorporate an anatomy-aware registration strategy that prioritizes internal structural coherence while adaptively reducing the influence of boundary inconsistencies. Extensive validation demonstrates that our approach outperforms state-of-the-art methods by achieving superior registration accuracy with physically realistic deformations in a completely unsupervised fashion.</li>
</ul>

<h3>Title: Graph Evidential Learning for Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Chunyu Wei, Wenji Hu, Xingjia Hao, Yunhai Wang, Yueguo Chen, Bing Bai, Fei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00594">https://arxiv.org/abs/2506.00594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00594">https://arxiv.org/pdf/2506.00594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00594]] Graph Evidential Learning for Anomaly Detection(https://arxiv.org/abs/2506.00594)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection faces significant challenges due to the scarcity of reliable anomaly-labeled datasets, driving the development of unsupervised methods. Graph autoencoders (GAEs) have emerged as a dominant approach by reconstructing graph structures and node features while deriving anomaly scores from reconstruction errors. However, relying solely on reconstruction error for anomaly detection has limitations, as it increases the sensitivity to noise and overfitting. To address these issues, we propose Graph Evidential Learning (GEL), a probabilistic framework that redefines the reconstruction process through evidential learning. By modeling node features and graph topology using evidential distributions, GEL quantifies two types of uncertainty: graph uncertainty and reconstruction uncertainty, incorporating them into the anomaly scoring mechanism. Extensive experiments demonstrate that GEL achieves state-of-the-art performance while maintaining high robustness against noise and structural perturbations.</li>
</ul>

<h3>Title: Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control</h3>
<ul>
<li><strong>Authors: </strong>Danfeng li, Hui Zhang, Sheng Wang, Jiacheng Li, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00596">https://arxiv.org/abs/2506.00596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00596">https://arxiv.org/pdf/2506.00596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00596]] Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control(https://arxiv.org/abs/2506.00596)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entity's image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities.</li>
</ul>

<h3>Title: ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education</h3>
<ul>
<li><strong>Authors: </strong>Ruiming Min, Minghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00605">https://arxiv.org/abs/2506.00605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00605">https://arxiv.org/pdf/2506.00605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00605]] ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education(https://arxiv.org/abs/2506.00605)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>With the advancement of modern medicine and the development of technologies such as MRI, CT, and cellular analysis, it has become increasingly critical for clinicians to accurately interpret various diagnostic images. However, modern medical education often faces challenges due to limited access to high-quality teaching materials, stemming from privacy concerns and a shortage of educational resources (Balogh et al., 2015). In this context, image data generated by machine learning models, particularly generative models, presents a promising solution. These models can create diverse and comparable imaging datasets without compromising patient privacy, thereby supporting modern medical education. In this study, we explore the use of convolutional neural networks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic medical images. The source code is available at this https URL.</li>
</ul>

<h3>Title: Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>JungWoo Chae, Jiyoon Kim, Sangheum Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00607">https://arxiv.org/abs/2506.00607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00607">https://arxiv.org/pdf/2506.00607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00607]] Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models(https://arxiv.org/abs/2506.00607)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Personalizing diffusion models to specific users or concepts remains challenging, particularly when only a few reference images are available. Existing methods such as DreamBooth and Textual Inversion often overfit to limited data, causing misalignment between generated images and text prompts when attempting to balance identity fidelity with prompt adherence. While Direct Consistency Optimization (DCO) with its consistency-guided sampling partially alleviates this issue, it still struggles with complex or stylized prompts. In this paper, we propose a parallel rescaling technique for personalized diffusion models. Our approach explicitly decomposes the consistency guidance signal into parallel and orthogonal components relative to classifier free guidance (CFG). By rescaling the parallel component, we minimize disruptive interference with CFG while preserving the subject's identity. Unlike prior personalization methods, our technique does not require additional training data or expensive annotations. Extensive experiments show improved prompt alignment and visual fidelity compared to baseline methods, even on challenging stylized prompts. These findings highlight the potential of parallel rescaled guidance to yield more stable and accurate personalization for diverse user inputs.</li>
</ul>

<h3>Title: PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements</h3>
<ul>
<li><strong>Authors: </strong>Petros Raptopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00608">https://arxiv.org/abs/2506.00608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00608">https://arxiv.org/pdf/2506.00608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00608]] PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements(https://arxiv.org/abs/2506.00608)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, explainability</a></li>
<li><strong>Abstract: </strong>Contract review is a complex and time-intensive task that typically demands specialized legal expertise, rendering it largely inaccessible to non-experts. Moreover, legal interpretation is rarely straightforward-ambiguity is pervasive, and judgments often hinge on subjective assessments. Compounding these challenges, contracts are usually confidential, restricting their use with proprietary models and necessitating reliance on open-source alternatives. To address these challenges, we introduce PAKTON: a fully open-source, end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is designed to handle the complexities of contract analysis through collaborative agent workflows and a novel retrieval-augmented generation (RAG) component, enabling automated legal document review that is more accessible, adaptable, and privacy-preserving. Experiments demonstrate that PAKTON outperforms both general-purpose and pretrained models in predictive accuracy, retrieval performance, explainability, completeness, and grounded justifications as evaluated through a human study and validated with automated metrics.</li>
</ul>

<h3>Title: Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation</h3>
<ul>
<li><strong>Authors: </strong>Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00612">https://arxiv.org/abs/2506.00612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00612">https://arxiv.org/pdf/2506.00612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00612]] Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation(https://arxiv.org/abs/2506.00612)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Clinical tasks such as diagnosis and treatment require strong decision-making abilities, highlighting the importance of rigorous evaluation benchmarks to assess the reliability of large language models (LLMs). In this work, we introduce a knowledge-guided data augmentation framework that enhances the difficulty of clinical multiple-choice question (MCQ) datasets by generating distractors (i.e., incorrect choices that are similar to the correct one and may confuse existing LLMs). Using our KG-based pipeline, the generated choices are both clinically plausible and deliberately misleading. Our approach involves multi-step, semantically informed walks on a medical knowledge graph to identify distractor paths-associations that are medically relevant but factually incorrect-which then guide the LLM in crafting more deceptive distractors. We apply the designed knowledge graph guided distractor generation (KGGDG) pipline, to six widely used medical QA benchmarks and show that it consistently reduces the accuracy of state-of-the-art LLMs. These findings establish KGGDG as a powerful tool for enabling more robust and diagnostic evaluations of medical LLMs.</li>
</ul>

<h3>Title: LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech</h3>
<ul>
<li><strong>Authors: </strong>Niyati Bafna, Matthew Wiesner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00628">https://arxiv.org/abs/2506.00628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00628">https://arxiv.org/pdf/2506.00628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00628]] LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech(https://arxiv.org/abs/2506.00628)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speaker's native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID.</li>
</ul>

<h3>Title: Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Young Jin Park, Francois Germain, Jing Liu, Ye Wang, Toshiaki Koike-Akino, Gordon Wichern, Navid Azizan, Christopher R. Laughman, Ankush Chakrabarty</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00630">https://arxiv.org/abs/2506.00630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00630">https://arxiv.org/pdf/2506.00630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00630]] Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models(https://arxiv.org/abs/2506.00630)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Decision-making in building energy systems critically depends on the predictive accuracy of relevant time-series models. In scenarios lacking extensive data from a target building, foundation models (FMs) represent a promising technology that can leverage prior knowledge from vast and diverse pre-training datasets to construct accurate probabilistic predictors for use in decision-making tools. This paper investigates the applicability and fine-tuning strategies of time-series foundation models (TSFMs) in building energy forecasting. We analyze both full fine-tuning and parameter-efficient fine-tuning approaches, particularly low-rank adaptation (LoRA), by using real-world data from a commercial net-zero energy building to capture signals such as room occupancy, carbon emissions, plug loads, and HVAC energy consumption. Our analysis reveals that the zero-shot predictive performance of TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that employing either full fine-tuning or parameter-efficient fine-tuning significantly enhances forecasting accuracy, even with limited historical data. Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs consistently outperform state-of-the-art deep forecasting models (e.g., temporal fusion transformers) in accuracy, robustness, and generalization across varying building zones and seasonal conditions. These results underline the efficacy of TSFMs for practical, data-constrained building energy management systems, enabling improved decision-making in pursuit of energy efficiency and sustainability.</li>
</ul>

<h3>Title: Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Daniele Molino, Camillo Maria Caruso, Filippo Ruffini, Paolo Soda, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00633">https://arxiv.org/abs/2506.00633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00633">https://arxiv.org/pdf/2506.00633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00633]] Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining(https://arxiv.org/abs/2506.00633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation to volumetric Computed Tomography (CT) remains a significant challenge, due to its high dimensionality, anatomical complexity, and the absence of robust frameworks that align vision-language data in 3D medical imaging. Methods: We introduce a novel architecture for Text-to-CT generation that combines a latent diffusion model with a 3D contrastive vision-language pretraining scheme. Our approach leverages a dual-encoder CLIP-style model trained on paired CT volumes and radiology reports to establish a shared embedding space, which serves as the conditioning input for generation. CT volumes are compressed into a low-dimensional latent space via a pretrained volumetric VAE, enabling efficient 3D denoising diffusion without requiring external super-resolution stages. Results: We evaluate our method on the CT-RATE dataset and conduct a comprehensive assessment of image fidelity, clinical relevance, and semantic alignment. Our model achieves competitive performance across all tasks, significantly outperforming prior baselines for text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by our framework can effectively augment real data, improving downstream diagnostic performance. Conclusion: Our results show that modality-specific vision-language alignment is a key component for high-quality 3D medical image generation. By integrating contrastive pretraining and volumetric diffusion, our method offers a scalable and controllable solution for synthesizing clinically meaningful CT volumes from text, paving the way for new applications in data augmentation, medical education, and automated clinical simulation.</li>
</ul>

<h3>Title: Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings</h3>
<ul>
<li><strong>Authors: </strong>Adam Visokay, Ruth Bagley, Ian Kennedy, Chris Hess, Kyle Crowder, Rob Voigt, Denis Peskoff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00634">https://arxiv.org/abs/2506.00634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00634">https://arxiv.org/pdf/2506.00634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00634]] Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings(https://arxiv.org/abs/2506.00634)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Rental listings offer a unique window into how urban space is socially constructed through language. We analyze Chicago Craigslist rental advertisements from 2018 to 2024 to examine how listing agents characterize neighborhoods, identifying mismatches between institutional boundaries and neighborhood claims. Through manual and large language model annotation, we classify unstructured listings from Craigslist according to their neighborhood. Geospatial analysis reveals three distinct patterns: properties with conflicting neighborhood designations due to competing spatial definitions, border properties with valid claims to adjacent neighborhoods, and ``reputation laundering" where listings claim association with distant, desirable neighborhoods. Through topic modeling, we identify patterns that correlate with spatial positioning: listings further from neighborhood centers emphasize different amenities than centrally-located units. Our findings demonstrate that natural language processing techniques can reveal how definitions of urban spaces are contested in ways that traditional methods overlook.</li>
</ul>

<h3>Title: Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00635">https://arxiv.org/abs/2506.00635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00635">https://arxiv.org/pdf/2506.00635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00635]] Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting(https://arxiv.org/abs/2506.00635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.</li>
</ul>

<h3>Title: SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions</h3>
<ul>
<li><strong>Authors: </strong>Weijie Xu, Shixian Cui, Xi Fang, Chi Xue, Stephanie Eckman, Chandan Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00643">https://arxiv.org/abs/2506.00643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00643">https://arxiv.org/pdf/2506.00643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00643]] SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions(https://arxiv.org/abs/2506.00643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.</li>
</ul>

<h3>Title: GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Neil De La Fuente, Oscar Sainz, Iker García-Ferrero, Eneko Agirre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00649">https://arxiv.org/abs/2506.00649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00649">https://arxiv.org/pdf/2506.00649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00649]] GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction(https://arxiv.org/abs/2506.00649)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at this http URL</li>
</ul>

<h3>Title: Video Signature: In-generation Watermarking for Latent Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Huang, Junhao Chen, Qi Zheng, Hanqian Li, Shuliang Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00652">https://arxiv.org/abs/2506.00652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00652">https://arxiv.org/pdf/2506.00652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00652]] Video Signature: In-generation Watermarking for Latent Video Diffusion Models(https://arxiv.org/abs/2506.00652)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>The rapid development of Artificial Intelligence Generated Content (AIGC) has led to significant progress in video generation but also raises serious concerns about intellectual property protection and reliable content tracing. Watermarking is a widely adopted solution to this issue, but existing methods for video generation mainly follow a post-generation paradigm, which introduces additional computational overhead and often fails to effectively balance the trade-off between video quality and watermark extraction. To address these issues, we propose Video Signature (VIDSIG), an in-generation watermarking method for latent video diffusion models, which enables implicit and adaptive watermark integration during generation. Specifically, we achieve this by partially fine-tuning the latent decoder, where Perturbation-Aware Suppression (PAS) pre-identifies and freezes perceptually sensitive layers to preserve visual quality. Beyond spatial fidelity, we further enhance temporal consistency by introducing a lightweight Temporal Alignment module that guides the decoder to generate coherent frame sequences during fine-tuning. Experimental results show that VIDSIG achieves the best overall performance in watermark extraction, visual quality, and generation efficiency. It also demonstrates strong robustness against both spatial and temporal tampering, highlighting its practicality in real-world scenarios.</li>
</ul>

<h3>Title: Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</h3>
<ul>
<li><strong>Authors: </strong>Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Leqi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00653">https://arxiv.org/abs/2506.00653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00653">https://arxiv.org/pdf/2506.00653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00653]] Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models(https://arxiv.org/abs/2506.00653)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.</li>
</ul>

<h3>Title: Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection</h3>
<ul>
<li><strong>Authors: </strong>Marco Di Gennaro, Francesco Panebianco, Marco Pianta, Stefano Zanero, Michele Carminati</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00654">https://arxiv.org/abs/2506.00654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00654">https://arxiv.org/pdf/2506.00654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00654]] Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection(https://arxiv.org/abs/2506.00654)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Money laundering is a financial crime that poses a serious threat to financial integrity and social security. The growing number of transactions makes it necessary to use automatic tools that help law enforcement agencies detect such criminal activity. In this work, we present Amatriciana, a novel approach based on Graph Neural Networks to detect money launderers inside a graph of transactions by considering temporal information. Amatriciana uses the whole graph of transactions without splitting it into several time-based subgraphs, exploiting all relational information in the dataset. Our experiments on a public dataset reveal that the model can learn from a limited amount of data. Furthermore, when more data is available, the model outperforms other State-of-the-art approaches; in particular, Amatriciana decreases the number of False Positives (FPs) while detecting many launderers. In summary, Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55% with respect to other State-of-the-art models.</li>
</ul>

<h3>Title: Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Aris J. Aristorenas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00656">https://arxiv.org/abs/2506.00656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00656">https://arxiv.org/pdf/2506.00656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00656]] Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings(https://arxiv.org/abs/2506.00656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We propose a permutation-invariant neural architecture for indoor localization using RSSI scans from Wi-Fi access points. Each scan is modeled as an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned embeddings and concatenated with signal strength. These are processed by a Set Transformer, enabling the model to handle variable-length, sparse inputs while learning attention- based representations over access point relationships. We evaluate the model on a dataset collected across a campus environment consisting of six buildings. Results show that the model accurately recovers fine-grained spatial structure and maintains performance across physically distinct domains. In our experiments, a simple LSTM consistently outperformed all other models, achieving the lowest mean localization error across three tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer performed competitively, ranking second in every experiment and outperforming the MLP, RNN, and basic attention models, particularly in scenarios involving multiple buildings (E2) and multiple floors (E3). Performance degraded most in E2, where signal conditions varied substantially across buildings, highlighting the importance of architectural robustness to domain diversity. This work demonstrates that set-based neural models are a natural fit for signal-based localization, offering a principled approach to handling sparse, unordered inputs in real-world positioning tasks.</li>
</ul>

<h3>Title: Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques</h3>
<ul>
<li><strong>Authors: </strong>Lang Xiong, Raina Gao, Alyssa Jeong, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00658">https://arxiv.org/abs/2506.00658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00658">https://arxiv.org/pdf/2506.00658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00658]] Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques(https://arxiv.org/abs/2506.00658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.</li>
</ul>

<h3>Title: Differential Privacy for Deep Learning in Medicine</h3>
<ul>
<li><strong>Authors: </strong>Marziyeh Mohammadi, Mohsen Vejdanihemmat, Mahshad Lotfinia, Mirabela Rusu, Daniel Truhn, Andreas Maier, Soroosh Tayebi Arasteh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00660">https://arxiv.org/abs/2506.00660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00660">https://arxiv.org/pdf/2506.00660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00660]] Differential Privacy for Deep Learning in Medicine(https://arxiv.org/abs/2506.00660)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate, fair, generative</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.</li>
</ul>

<h3>Title: Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors</h3>
<ul>
<li><strong>Authors: </strong>Richard E. Neddo, Sean Willis, Zander Blasingame, Chen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00661">https://arxiv.org/abs/2506.00661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00661">https://arxiv.org/pdf/2506.00661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00661]] Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors(https://arxiv.org/abs/2506.00661)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Image classifiers, such as those used for autonomous vehicle navigation, are largely known to be susceptible to adversarial attacks that target the input image set. There is extensive discussion on adversarial attacks including perturbations that alter the input images to cause malicious misclassifications without perceivable modification. This work proposes a countermeasure for such attacks by adjusting the weights and classes of pretrained vision transformers with a low-rank adaptation to become more robust against adversarial attacks and allow for scalable fine-tuning without retraining.</li>
</ul>

<h3>Title: Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis</h3>
<ul>
<li><strong>Authors: </strong>Vasilii Korolkov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00667">https://arxiv.org/abs/2506.00667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00667">https://arxiv.org/pdf/2506.00667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00667]] Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis(https://arxiv.org/abs/2506.00667)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Robust scene segmentation and keyframe extraction are essential preprocessing steps in video understanding pipelines, supporting tasks such as indexing, summarization, and semantic retrieval. However, existing methods often lack generalizability across diverse video types and durations. We present a unified, adaptive framework for automatic scene detection and keyframe selection that handles formats ranging from short-form media to long-form films, archival content, and surveillance footage. Our system dynamically selects segmentation policies based on video length: adaptive thresholding for short videos, hybrid strategies for mid-length ones, and interval-based splitting for extended recordings. This ensures consistent granularity and efficient processing across domains. For keyframe selection, we employ a lightweight module that scores sampled frames using a composite metric of sharpness, luminance, and temporal spread, avoiding complex saliency models while ensuring visual relevance. Designed for high-throughput workflows, the system is deployed in a commercial video analysis platform and has processed content from media, education, research, and security domains. It offers a scalable and interpretable solution suitable for downstream applications such as UI previews, embedding pipelines, and content filtering. We discuss practical implementation details and outline future enhancements, including audio-aware segmentation and reinforcement-learned frame scoring.</li>
</ul>

<h3>Title: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00668">https://arxiv.org/abs/2506.00668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00668">https://arxiv.org/pdf/2506.00668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00668]] SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues(https://arxiv.org/abs/2506.00668)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant safety risks to society. To address this challenge, we propose a novel defense mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM). STREAM defends LLMs against multi-turn attacks while preserving their functional capabilities. Our approach involves constructing a human-annotated dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to fine-tune a plug-and-play safety reasoning moderator. This model is designed to identify malicious intent hidden within multi-turn conversations and alert the target LLM of potential risks. We evaluate STREAM across multiple LLMs against prevalent multi-turn attack strategies. Experimental results demonstrate that our method significantly outperforms existing defense techniques, reducing the Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM capability.</li>
</ul>

<h3>Title: SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Saad Hossain, Samanvay Vajpayee, Sirisha Rambhatla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00676">https://arxiv.org/abs/2506.00676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00676">https://arxiv.org/pdf/2506.00676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00676]] SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning(https://arxiv.org/abs/2506.00676)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: this https URL</li>
</ul>

<h3>Title: CineMA: A Foundation Model for Cine Cardiac MRI</h3>
<ul>
<li><strong>Authors: </strong>Yunguan Fu, Weixi Yi, Charlotte Manisty, Anish N Bhuva, Thomas A Treibel, James C Moon, Matthew J Clarkson, Rhodri Huw Davies, Yipeng Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00679">https://arxiv.org/abs/2506.00679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00679">https://arxiv.org/pdf/2506.00679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00679]] CineMA: A Foundation Model for Cine Cardiac MRI(https://arxiv.org/abs/2506.00679)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cardiac magnetic resonance (CMR) is a key investigation in clinical cardiovascular medicine and has been used extensively in population research. However, extracting clinically important measurements such as ejection fraction for diagnosing cardiovascular diseases remains time-consuming and subjective. We developed CineMA, a foundation AI model automating these tasks with limited labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine CMR studies to reconstruct images from masked inputs. After fine-tuning, it was evaluated across eight datasets on 23 tasks from four categories: ventricle and myocardium segmentation, left and right ventricle ejection fraction calculation, disease detection and classification, and landmark localisation. CineMA is the first foundation model for cine CMR to match or outperform convolutional neural networks (CNNs). CineMA demonstrated greater label efficiency than CNNs, achieving comparable or better performance with fewer annotations. This reduces the burden of clinician labelling and supports replacing task-specific training with fine-tuning foundation models in future cardiac imaging applications. Models and code for pre-training and fine-tuning are available at this https URL, democratising access to high-performance models that otherwise require substantial computational resources, promoting reproducibility and accelerating clinical translation.</li>
</ul>

<h3>Title: Existing Large Language Model Unlearning Evaluations Are Inconclusive</h3>
<ul>
<li><strong>Authors: </strong>Zhili Feng, Yixuan Even Xu, Alexander Robey, Robert Kirk, Xander Davies, Yarin Gal, Avi Schwarzschild, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00688">https://arxiv.org/abs/2506.00688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00688">https://arxiv.org/pdf/2506.00688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00688]] Existing Large Language Model Unlearning Evaluations Are Inconclusive(https://arxiv.org/abs/2506.00688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.</li>
</ul>

<h3>Title: Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments</h3>
<ul>
<li><strong>Authors: </strong>Li Zhang, Morgan Gray, Jaromir Savelka, Kevin D. Ashley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00694">https://arxiv.org/abs/2506.00694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00694">https://arxiv.org/pdf/2506.00694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00694]] Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments(https://arxiv.org/abs/2506.00694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Project page: this https URL.</li>
</ul>

<h3>Title: Concept-Centric Token Interpretation for Vector-Quantized Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Tianze Yang, Yucheng Shi, Mengnan Du, Xuansheng Wu, Qiaoyu Tan, Jin Sun, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00698">https://arxiv.org/abs/2506.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00698">https://arxiv.org/pdf/2506.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00698]] Concept-Centric Token Interpretation for Vector-Quantized Generative Models(https://arxiv.org/abs/2506.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for image generation. However, the key component of VQGMs -- the codebook of discrete tokens -- is still not well understood, e.g., which tokens are critical to generate an image of a certain concept? This paper introduces Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting VQGMs by identifying concept-specific token combinations. Our framework employs two methods: (1) a sample-level explanation method that analyzes token importance scores in individual images, and (2) a codebook-level explanation method that explores the entire codebook to find globally relevant tokens. Experimental results demonstrate CORTEX's efficacy in providing clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX is useful in applications such as targeted image editing and shortcut feature detection. Our code is available at this https URL.</li>
</ul>

<h3>Title: Bayesian Inference of Training Dataset Membership</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00701">https://arxiv.org/abs/2506.00701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00701">https://arxiv.org/pdf/2506.00701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00701]] Bayesian Inference of Training Dataset Membership(https://arxiv.org/abs/2506.00701)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, a challenge often addressed through membership inference attacks (MIAs). Traditional MIAs typically require access to model internals or rely on computationally intensive shadow models. This paper proposes an efficient, interpretable and principled Bayesian inference method for membership inference. By analyzing post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model, our approach computes posterior probabilities of membership without requiring extensive model training. Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. Beyond membership inference, this method can also detect distribution shifts, offering a practical and interpretable alternative to existing approaches.</li>
</ul>

<h3>Title: RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Valter Hudovernik, Minkai Xu, Juntong Shi, Lovro Šubelj, Stefano Ermon, Erik Štrumbelj, Jure Leskovec</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00710">https://arxiv.org/abs/2506.00710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00710">https://arxiv.org/pdf/2506.00710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00710]] RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models(https://arxiv.org/abs/2506.00710)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Real-world databases are predominantly relational, comprising multiple interlinked tables that contain complex structural and statistical dependencies. Learning generative models on relational data has shown great promise in generating synthetic data and imputing missing values. However, existing methods often struggle to capture this complexity, typically reducing relational data to conditionally generated flat tables and imposing limiting structural assumptions. To address these limitations, we introduce RelDiff, a novel diffusion generative model that synthesizes complete relational databases by explicitly modeling their foreign key graph structure. RelDiff combines a joint graph-conditioned diffusion process across all tables for attribute synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model for structure generation. The decomposition of graph structure and relational attributes ensures both high fidelity and referential integrity, both of which are crucial aspects of synthetic relational database generation. Experiments on 11 benchmark datasets demonstrate that RelDiff consistently outperforms prior methods in producing realistic and coherent synthetic relational databases. Code is available at this https URL.</li>
</ul>

<h3>Title: QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</h3>
<ul>
<li><strong>Authors: </strong>Wei Dai, Peilin Chen, Chanakya Ekbote, Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00711">https://arxiv.org/abs/2506.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00711">https://arxiv.org/pdf/2506.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00711]] QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training(https://arxiv.org/abs/2506.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at this https URL.</li>
</ul>

<h3>Title: From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</h3>
<ul>
<li><strong>Authors: </strong>Debarati Bhattacharjee, Ashish Anand</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00713">https://arxiv.org/abs/2506.00713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00713">https://arxiv.org/pdf/2506.00713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00713]] From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation(https://arxiv.org/abs/2506.00713)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.</li>
</ul>

<h3>Title: From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Tianqin Li, Ziqi Wen, Leiran Song, Jun Liu, Zhi Jing, Tai Sing Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00718">https://arxiv.org/abs/2506.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00718">https://arxiv.org/pdf/2506.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00718]] From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models(https://arxiv.org/abs/2506.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human vision organizes local cues into coherent global forms using Gestalt principles like closure, proximity, and figure-ground assignment -- functions reliant on global spatial structure. We investigate whether modern vision models show similar behaviors, and under what training conditions these emerge. We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE) exhibit activation patterns consistent with Gestalt laws, including illusory contour completion, convexity preference, and dynamic figure-ground segregation. To probe the computational basis, we hypothesize that modeling global dependencies is necessary for Gestalt-like organization. We introduce the Distorted Spatial Relationship Testbench (DiSRT), which evaluates sensitivity to global spatial perturbations while preserving local textures. Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform supervised baselines and sometimes even exceed human performance. ConvNeXt models trained with MAE also exhibit Gestalt-compatible representations, suggesting such sensitivity can arise without attention architectures. However, classification finetuning degrades this ability. Inspired by biological vision, we show that a Top-K activation sparsity mechanism can restore global sensitivity. Our findings identify training conditions that promote or suppress Gestalt-like perception and establish DiSRT as a diagnostic for global structure sensitivity across models.</li>
</ul>

<h3>Title: Browser Fingerprinting Using WebAssembly</h3>
<ul>
<li><strong>Authors: </strong>Mordechai Guri, Dor Fibert</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00719">https://arxiv.org/abs/2506.00719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00719">https://arxiv.org/pdf/2506.00719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00719]] Browser Fingerprinting Using WebAssembly(https://arxiv.org/abs/2506.00719)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Web client fingerprinting has become a widely used technique for uniquely identifying users, browsers, operating systems, and devices with high accuracy. While it is beneficial for applications such as fraud detection and personalized experiences, it also raises privacy concerns by enabling persistent tracking and detailed user profiling. This paper introduces an advanced fingerprinting method using WebAssembly (Wasm) - a low-level programming language that offers near-native execution speed in modern web browsers. With broad support across major browsers and growing adoption, WebAssembly provides a strong foundation for developing more effective fingerprinting methods. In this work, we present a new approach that leverages WebAssembly's computational capabilities to identify returning devices-such as smartphones, tablets, laptops, and desktops across different browsing sessions. Our method uses subtle differences in the WebAssembly JavaScript API implementation to distinguish between Chromium-based browsers like Google Chrome and Microsoft Edge, even when identifiers such as the User-Agent are completely spoofed, achieving a false-positive rate of less than 1%. The fingerprint is generated using a combination of CPU-bound operations, memory tasks, and I/O activities to capture unique browser behaviors. We validate this approach on a variety of platforms, including Intel, AMD, and ARM CPUs, operating systems such as Windows, macOS, Android, and iOS, and in environments like VMWare, KVM, and VirtualBox. Extensive evaluation shows that WebAssembly-based fingerprinting significantly improves identification accuracy. We also propose mitigation strategies to reduce the privacy risks associated with this method, which could be integrated into future browser designs to better protect user privacy.</li>
</ul>

<h3>Title: Common Inpainted Objects In-N-Out of Context</h3>
<ul>
<li><strong>Authors: </strong>Tianze Yang, Tyson Jordan, Ninghao Liu, Jin Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00721">https://arxiv.org/abs/2506.00721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00721">https://arxiv.org/pdf/2506.00721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00721]] Common Inpainted Objects In-N-Out of Context(https://arxiv.org/abs/2506.00721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel dataset addressing the scarcity of out-of-context examples in existing vision datasets. By systematically replacing objects in COCO images through diffusion-based inpainting, we create 97,722 unique images featuring both contextually coherent and inconsistent scenes, enabling effective context learning. Each inpainted object is meticulously verified and categorized as in- or out-of-context through a multimodal large language model assessment. Our analysis reveals significant patterns in semantic priors that influence inpainting success across object categories. We demonstrate three key tasks enabled by COinCO: (1) training context classifiers that effectively determine whether existing objects belong in their context; (2) a novel Objects-from-Context prediction task that determines which new objects naturally belong in given scenes at both instance and clique levels, and (3) context-enhanced fake detection on state-of-the-art methods without fine-tuning. COinCO provides a controlled testbed with contextual variations, establishing a foundation for advancing context-aware visual understanding in computer vision and image forensics. Our code and data are at: this https URL.</li>
</ul>

<h3>Title: Pitfalls in Evaluating Language Model Forecasters</h3>
<ul>
<li><strong>Authors: </strong>Daniel Paleka, Shashwat Goel, Jonas Geiping, Florian Tramèr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00723">https://arxiv.org/abs/2506.00723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00723">https://arxiv.org/pdf/2506.00723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00723]] Pitfalls in Evaluating Language Model Forecasters(https://arxiv.org/abs/2506.00723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.</li>
</ul>

<h3>Title: Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongye Zheng, Yichen Wang, Ray Pan, Guiran Liu, Binrong Zhu, Hanlu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00726">https://arxiv.org/abs/2506.00726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00726">https://arxiv.org/pdf/2506.00726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00726]] Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models(https://arxiv.org/abs/2506.00726)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a gradient-informed fine-tuning method for large language models under few-shot conditions. The goal is to enhance task adaptability and training stability when data is limited. The method builds on a base loss function and introduces two gradient-related regularization terms. The first enforces gradient direction consistency to guide parameter updates along task-relevant directions and prevent drift. The second controls gradient magnitude to avoid abnormal updates. Together, these components support a more efficient and stable optimization path. To further improve cross-task generalization, the method incorporates a gradient alignment mechanism. This mechanism measures the consistency between optimization directions of the source and target tasks. It enhances fine-tuning performance in multi-task and cross-domain scenarios. Across various natural language understanding tasks, the method outperforms existing fine-tuning strategies in average accuracy, gradient stability, and directional alignment. Empirical evaluations under different sample sizes and domain-specific tasks confirm the method's robustness and broad applicability in low-resource environments. In particular, the method shows clear advantages in controlling parameter update paths. The results demonstrate that a gradient-based fine-tuning framework can effectively leverage the representational power of large language models. It ensures training stability while reducing dependence on large volumes of labeled data.</li>
</ul>

<h3>Title: Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Javier Bisbal, Julio Sotelo, Maria I Valdés, Pablo Irarrazaval, Marcelo E Andia, Julio García, José Rodriguez-Palomarez, Francesca Raimondi, Cristián Tejos, Sergio Uribe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00727">https://arxiv.org/abs/2506.00727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00727">https://arxiv.org/pdf/2506.00727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00727]] Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning(https://arxiv.org/abs/2506.00727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (DRL) algorithms have shown robust results in plane reformatting tasks. In these methods, an agent sequentially adjusts the position and orientation of an initial plane towards an objective location. This process allows accurate plane reformatting, without the need for detailed landmarks, which makes it suitable for images with limited contrast and resolution, such as 4D flow MRI. However, current DRL methods require the test dataset to be in the same position and orientation as the training dataset. In this paper, we present a novel technique that utilizes a flexible coordinate system based on the current state, enabling navigation in volumes at any position or orientation. We adopted the Asynchronous Advantage Actor Critic (A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN). Experimental results in 4D flow MRI demonstrate improved accuracy in plane reformatting angular and distance errors (6.32 +- 4.15 ° and 3.40 +- 2.75 mm), as well as statistically equivalent flow measurements determined by a plane reformatting process done by an expert (p=0.21). The method's flexibility and adaptability make it a promising candidate for other medical imaging applications beyond 4D flow MRI.</li>
</ul>

<h3>Title: MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter</h3>
<ul>
<li><strong>Authors: </strong>Binghang Lu, Changhong Mou, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00731">https://arxiv.org/abs/2506.00731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00731">https://arxiv.org/pdf/2506.00731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00731]] MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter(https://arxiv.org/abs/2506.00731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.</li>
</ul>

<h3>Title: Involution-Infused DenseNet with Two-Step Compression for Resource-Efficient Plant Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>T. Ahmed, S. Jannat, Md. F. Islam, J. Noor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00735">https://arxiv.org/abs/2506.00735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00735">https://arxiv.org/pdf/2506.00735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00735]] Involution-Infused DenseNet with Two-Step Compression for Resource-Efficient Plant Disease Classification(https://arxiv.org/abs/2506.00735)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Agriculture is vital for global food security, but crops are vulnerable to diseases that impact yield and quality. While Convolutional Neural Networks (CNNs) accurately classify plant diseases using leaf images, their high computational demands hinder their deployment in resource-constrained settings such as smartphones, edge devices, and real-time monitoring systems. This study proposes a two-step model compression approach integrating Weight Pruning and Knowledge Distillation, along with the hybridization of DenseNet with Involutional Layers. Pruning reduces model size and computational load, while distillation improves the smaller student models performance by transferring knowledge from a larger teacher network. The hybridization enhances the models ability to capture spatial features efficiently. These compressed models are suitable for real-time applications, promoting precision agriculture through rapid disease identification and crop management. The results demonstrate ResNet50s superior performance post-compression, achieving 99.55% and 98.99% accuracy on the PlantVillage and PaddyLeaf datasets, respectively. The DenseNet-based model, optimized for efficiency, recorded 99.21% and 93.96% accuracy with a minimal parameter count. Furthermore, the hybrid model achieved 98.87% and 97.10% accuracy, supporting the practical deployment of energy-efficient devices for timely disease intervention and sustainable farming practices.</li>
</ul>

<h3>Title: DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments</h3>
<ul>
<li><strong>Authors: </strong>Chiyu Zhang, Marc-Alexandre Cote, Michael Albada, Anush Sankaran, Jack W. Stokes, Tong Wang, Amir Abdi, William Blum, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00739">https://arxiv.org/abs/2506.00739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00739">https://arxiv.org/pdf/2506.00739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00739]] DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments(https://arxiv.org/abs/2506.00739)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench's modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at this https URL.</li>
</ul>

<h3>Title: Data Swarms: Optimizable Generation of Synthetic Evaluation Data</h3>
<ul>
<li><strong>Authors: </strong>Shangbin Feng, Yike Wang, Weijia Shi, Yulia Tsvetkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00741">https://arxiv.org/abs/2506.00741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00741">https://arxiv.org/pdf/2506.00741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00741]] Data Swarms: Optimizable Generation of Synthetic Evaluation Data(https://arxiv.org/abs/2506.00741)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose Data Swarms, an algorithm to optimize the generation of synthetic evaluation data and advance quantitative desiderata of LLM evaluation. We first train a swarm of initial data generators using existing data, and define various evaluation objectives to reflect the desired properties of evaluation (e.g., generate more difficult problems for the evaluated models) and quantitatively evaluate data generators. We then employ particle swarm optimization to optimize the swarm of data generators, where they collaboratively search through the model parameter space to find new generators that advance these objectives. We further extend it to Adversarial Swarms, where the data generator swarm generates harder data while the test taker model swarm learns from such data, co-evolving dynamically for better data and models simultaneously. Extensive experiments demonstrate that Data Swarms outperforms eight data generation baselines across five evaluation objectives, while Adversarial Swarms produce more robust learning of synthetic data and stronger generalization. Further analysis reveals that Data Swarms successfully optimizes compositions of multiple evaluation objectives and generalizes to new off-the-shelf LLMs, unseen at optimization time.</li>
</ul>

<h3>Title: Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection</h3>
<ul>
<li><strong>Authors: </strong>Yeshwanth Venkatesha, Souvik Kundu, Priyadarshini Panda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00743">https://arxiv.org/abs/2506.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00743">https://arxiv.org/pdf/2506.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00743]] Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection(https://arxiv.org/abs/2506.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.</li>
</ul>

<h3>Title: Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Irie, Morris Yau, Samuel J. Gershman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00744">https://arxiv.org/abs/2506.00744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00744">https://arxiv.org/pdf/2506.00744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00744]] Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers(https://arxiv.org/abs/2506.00744)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with dynamic synaptic memory through fast-weight programming (FW-memory) -- the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems.</li>
</ul>

<h3>Title: Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations</h3>
<ul>
<li><strong>Authors: </strong>Pardis Sadat Zahraei, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00748">https://arxiv.org/abs/2506.00748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00748">https://arxiv.org/pdf/2506.00748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00748]] Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations(https://arxiv.org/abs/2506.00748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Addressing gender bias and maintaining logical coherence in machine translation remains challenging, particularly when translating between natural gender languages, like English, and genderless languages, such as Persian, Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset, comprising 3,950 challenging scenarios across six low- to mid-resource languages, to assess translation systems' performance. Our analysis of diverse technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate, reveals a universal struggle in translating genderless content, resulting in gender stereotyping and reasoning errors. All models preferred masculine pronouns when gender stereotypes could influence choices. Google Translate and GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts. Fine-tuning mBART-50 on TWC substantially resolved these biases and errors, led to strong generalization, and surpassed proprietary LLMs while remaining open-source. This work emphasizes the need for targeted approaches to gender and semantic coherence in machine translation, particularly for genderless languages, contributing to more equitable and accurate translation systems.</li>
</ul>

<h3>Title: Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Dong, Qingsong Yang, Shu Yang, Lijie Hu, Meng Ding, Wanyu Lin, Tianhang Zheng, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00759">https://arxiv.org/abs/2506.00759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00759">https://arxiv.org/pdf/2506.00759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00759]] Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons(https://arxiv.org/abs/2506.00759)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) trained on massive data capture rich information embedded in the training data. However, this also introduces the risk of privacy leakage, particularly involving personally identifiable information (PII). Although previous studies have shown that this risk can be mitigated through methods such as privacy neurons, they all assume that both the (sensitive) training data and user queries are in English. We show that they cannot defend against the privacy leakage in cross-lingual contexts: even if the training data is exclusively in one language, these (private) models may still reveal private information when queried in another language. In this work, we first investigate the information flow of cross-lingual privacy leakage to give a better understanding. We find that LLMs process private information in the middle layers, where representations are largely shared across languages. The risk of leakage peaks when converted to a language-specific space in later layers. Based on this, we identify privacy-universal neurons and language-specific privacy neurons. Privacy-universal neurons influence privacy leakage across all languages, while language-specific privacy neurons are only related to specific languages. By deactivating these neurons, the cross-lingual privacy leakage risk is reduced by 23.3%-31.6%.</li>
</ul>

<h3>Title: Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies</h3>
<ul>
<li><strong>Authors: </strong>Sai Vamsi Alisetti, Vikas Kalagi, Sanjukta Krishnagopal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00770">https://arxiv.org/abs/2506.00770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00770">https://arxiv.org/pdf/2506.00770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00770]] Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies(https://arxiv.org/abs/2506.00770)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability. We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline. Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.</li>
</ul>

<h3>Title: Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Zitao Chen, Yinjun Jia, Zitong Tian, Wei-Ying Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00771">https://arxiv.org/abs/2506.00771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00771">https://arxiv.org/pdf/2506.00771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00771]] Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space(https://arxiv.org/abs/2506.00771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medicinal chemists often optimize drugs considering their 3D structures and designing structurally distinct molecules that retain key features, such as shapes, pharmacophores, or chemical properties. Previous deep learning approaches address this through supervised tasks like molecule inpainting or property-guided optimization. In this work, we propose a flexible zero-shot molecule manipulation method by navigating in a shared latent space of 3D molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space independent of atom counts. MolFLAE encodes 3D molecules using an SE(3)-equivariant neural network into fixed number of latent nodes, distinguished by learned embeddings. The latent space is regularized, and molecular structures are reconstructed via a Bayesian Flow Network (BFN) conditioned on the encoder's latent output. MolFLAE achieves competitive performance on standard unconditional 3D molecule generation benchmarks. Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation, including atom number editing, structure reconstruction, and coordinated latent interpolation for both structure and properties. We further demonstrate our approach on a drug optimization task for the human glucocorticoid receptor, generating molecules with improved hydrophilicity while preserving key interactions, under computational evaluations. These results highlight the flexibility, robustness, and real-world utility of our method, opening new avenues for molecule editing and optimization.</li>
</ul>

<h3>Title: Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00773">https://arxiv.org/abs/2506.00773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00773">https://arxiv.org/pdf/2506.00773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00773]] Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models(https://arxiv.org/abs/2506.00773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle to accurately read and comprehend extremely long texts. Current methods for improvement typically rely on splitting long contexts into fixed-length chunks. However, fixed truncation risks separating semantically relevant content, leading to ambiguity and compromising accurate understanding. To overcome this limitation, we propose a straightforward approach for dynamically separating and selecting chunks of long context, facilitating a more streamlined input for LLMs. In particular, we compute semantic similarities between adjacent sentences, using lower similarities to adaptively divide long contexts into variable-length chunks. We further train a question-aware classifier to select sensitive chunks that are critical for answering specific questions. Experimental results on both single-hop and multi-hop question-answering benchmarks show that the proposed approach consistently outperforms strong baselines. Notably, it maintains robustness across a wide range of input lengths, handling sequences of up to 256k tokens. Our datasets and code are available at the following link: this https URL</li>
</ul>

<h3>Title: Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge</h3>
<ul>
<li><strong>Authors: </strong>Md Tahmid Rahman Laskar, Israt Jahan, Elham Dolatabadi, Chun Peng, Enamul Hoque, Jimmy Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00777">https://arxiv.org/abs/2506.00777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00777">https://arxiv.org/pdf/2506.00777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00777]] Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge(https://arxiv.org/abs/2506.00777)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios. However, evaluating LLMs in this task remains challenging due to their ability to generate human-like text, often producing synonyms or abbreviations of gold-standard answers, making traditional automatic evaluation metrics unreliable. On the other hand, while human evaluation is more reliable, it is costly and time-consuming, making it impractical for real-world applications. This paper investigates the use of LLMs-as-the-Judge as an alternative evaluation method for biomedical relation extraction. We benchmark 8 LLMs as judges to evaluate the responses generated by 5 other LLMs across 3 biomedical relation extraction datasets. Unlike other text-generation tasks, we observe that LLM-based judges perform quite poorly (usually below 50% accuracy) in the biomedical relation extraction task. Our findings reveal that it happens mainly because relations extracted by LLMs do not adhere to any standard format. To address this, we propose structured output formatting for LLM-generated responses that helps LLM-Judges to improve their performance by about 15% (on average). We also introduce a domain adaptation technique to further enhance LLM-Judge performance by effectively transferring knowledge between datasets. We release both our human-annotated and LLM-annotated judgment data (36k samples in total) for public use here: this https URL.</li>
</ul>

<h3>Title: KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision</h3>
<ul>
<li><strong>Authors: </strong>Rong Wu, Pinlong Cai, Jianbiao Mei, Licheng Wen, Tao Hu, Xuemeng Yang, Daocheng Fu, Botian Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00783">https://arxiv.org/abs/2506.00783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00783">https://arxiv.org/pdf/2506.00783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00783]] KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision(https://arxiv.org/abs/2506.00783)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made remarkable strides in various natural language processing tasks, but their performance on complex reasoning problems remains hindered by a lack of explainability and trustworthiness. This issue, often manifesting as hallucinations or unattributable reasoning processes, limits their applicability in complex reasoning scenarios. To address this, we propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain Explanation Supervision (KG-TRACES), a novel framework that enhances the reasoning ability of LLMs through explicit supervision over reasoning paths and processes. KG-TRACES jointly supervises the model to: (1) predict symbolic relation paths, (2) predict full triple-level reasoning paths, and (3) generate attribution-aware reasoning processes grounded in the reasoning paths. At inference phase, the model adapts to both KG-available and KG-unavailable scenarios, retrieving reasoning paths from a KG when possible or predicting plausible reasoning paths with only intrinsic knowledge when not. This design enables the model to reason in an explainable and source-attributable pattern. Through extensive experiments on complex reasoning tasks, we demonstrate that KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1% in F1 on CWQ. Moreover, we show its transferability to specialized domains such as medicine. By visualizing the intermediate steps of reasoning processes, we further show that the explicit supervision introduced by KG-TRACES leads to more stable and goal-directed reasoning processes, aligning closely with correct answers. Code is available at this https URL.</li>
</ul>

<h3>Title: Aiding Medical Diagnosis through Image Synthesis and Classification</h3>
<ul>
<li><strong>Authors: </strong>Kanishk Choudhary</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00786">https://arxiv.org/abs/2506.00786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00786">https://arxiv.org/pdf/2506.00786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00786]] Aiding Medical Diagnosis through Image Synthesis and Classification(https://arxiv.org/abs/2506.00786)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Medical professionals, especially those in training, often depend on visual reference materials to support an accurate diagnosis and develop pattern recognition skills. However, existing resources may lack the diversity and accessibility needed for broad and effective clinical learning. This paper presents a system designed to generate realistic medical images from textual descriptions and validate their accuracy through a classification model. A pretrained stable diffusion model was fine-tuned using Low-Rank Adaptation (LoRA) on the PathMNIST dataset, consisting of nine colorectal histopathology tissue types. The generative model was trained multiple times using different training parameter configurations, guided by domain-specific prompts to capture meaningful features. To ensure quality control, a ResNet-18 classification model was trained on the same dataset, achieving 99.76% accuracy in detecting the correct label of a colorectal histopathological medical image. Generated images were then filtered using the trained classifier and an iterative process, where inaccurate outputs were discarded and regenerated until they were correctly classified. The highest performing version of the generative model from experimentation achieved an F1 score of 0.6727, with precision and recall scores of 0.6817 and 0.7111, respectively. Some types of tissue, such as adipose tissue and lymphocytes, reached perfect classification scores, while others proved more challenging due to structural complexity. The self-validating approach created demonstrates a reliable method for synthesizing domain-specific medical images because of high accuracy in both the generation and classification portions of the system, with potential applications in both diagnostic support and clinical education. Future work includes improving prompt-specific accuracy and extending the system to other areas of medical imaging.</li>
</ul>

<h3>Title: RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Zeng, Tianyu Cao, Danqing Wang, Xinran Zhao, Zimeng Qiu, Morteza Ziyadi, Tongshuang Wu, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00789">https://arxiv.org/abs/2506.00789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00789">https://arxiv.org/pdf/2506.00789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00789]] RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2506.00789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.</li>
</ul>

<h3>Title: Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jianglin Ding, Jingcheng Tang, Gangshan Jing</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00797">https://arxiv.org/abs/2506.00797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00797">https://arxiv.org/pdf/2506.00797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00797]] Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning(https://arxiv.org/abs/2506.00797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.</li>
</ul>

<h3>Title: Uni-LoRA: One Vector is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Kaiyang Li, Shaobo Han, Qing Su, Wei Li, Zhipeng Cai, Shihao Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00799">https://arxiv.org/abs/2506.00799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00799">https://arxiv.org/pdf/2506.00799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00799]] Uni-LoRA: One Vector is All You Need(https://arxiv.org/abs/2506.00799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space $R^D$, can be reconstructed through a projection from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM - making Uni-LoRA both a unified framework and a "one-vector-only" solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance.</li>
</ul>

<h3>Title: Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Songtao Jiang, Chenyi Zhou, Yan Zhang, Yeying Jin, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00806">https://arxiv.org/abs/2506.00806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00806">https://arxiv.org/pdf/2506.00806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00806]] Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering(https://arxiv.org/abs/2506.00806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) still struggle with complex reasoning tasks in Visual Question Answering (VQA). While current methods have advanced by incorporating visual prompts, our study uncovers critical limitations: these approaches indiscriminately annotate all detected objects for every visual question, generating excessive visual markers that degrade task performance. This issue stems primarily from a lack of focus on key visual elements, raising two important questions: Are all objects equally important, and do all questions require visual prompts? Motivated by Dual Process Theory, which distinguishes between instinctive and deliberate cognitive modes in human reasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts to the complexity of questions, combining fast intuitive judgments with deliberate analytical reasoning to enhance the vision-language reasoning capability of the MLLM. For straightforward questions, FOCUS supports efficient zero-shot reasoning. For more complex tasks, it employs the conceptualizing before observation strategy to highlight critical elements. Extensive experiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate that FOCUS consistently improves the performance of both open-source and black-box MLLMs, achieving significant gains across all datasets. Ablation studies further validate the importance of combining diverse cognitive strategies with refined visual information for superior performance. Code will be released.</li>
</ul>

<h3>Title: Unlearning Inversion Attacks for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhang, Yilong Wang, Zhiwei Zhang, Xiaorui Liu, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00808">https://arxiv.org/abs/2506.00808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00808">https://arxiv.org/pdf/2506.00808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00808]] Unlearning Inversion Attacks for Graph Neural Networks(https://arxiv.org/abs/2506.00808)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.</li>
</ul>

<h3>Title: TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Luo, Yuan Yuan, Shixin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00813">https://arxiv.org/abs/2506.00813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00813">https://arxiv.org/pdf/2506.00813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00813]] TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning(https://arxiv.org/abs/2506.00813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tabular-image multimodal learning, which integrates structured tabular data with imaging data, holds great promise for a variety of tasks, especially in medical applications. Yet, two key challenges remain: (1) the lack of a standardized, pretrained representation for tabular data, as is commonly available in vision and language domains; and (2) the difficulty of handling missing values in the tabular modality, which are common in real-world medical datasets. To address these issues, we propose the TabPFN-Integrated Multimodal Engine (TIME), a novel multimodal framework that builds on the recently introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen tabular encoder to generate robust, strong embeddings that are naturally resilient to missing data, and combines them with image features from pretrained vision backbones. We explore a range of fusion strategies and tabular encoders, and evaluate our approach on both natural and medical datasets. Extensive experiments demonstrate that TIME consistently outperforms competitive baselines across both complete and incomplete tabular inputs, underscoring its practical value in real-world multimodal learning scenarios.</li>
</ul>

<h3>Title: From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses</h3>
<ul>
<li><strong>Authors: </strong>Manoj Balaji Jagadeeshan, Samarth Bhatia, Pretam Ray, Harshul Raj Surana, Akhil Rajeev P, Priya Mishra, Annarao Kulkarni, Ganesh Ramakrishnan, Prathosh AP, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00815">https://arxiv.org/abs/2506.00815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00815">https://arxiv.org/pdf/2506.00815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00815]] From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses(https://arxiv.org/abs/2506.00815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have significantly improved natural language generation, including creative tasks like poetry composition. However, most progress remains concentrated in high-resource languages. This raises an important question: Can LLMs be adapted for structured poetic generation in a low-resource, morphologically rich language such as Sanskrit? In this work, we introduce a dataset designed for translating English prose into structured Sanskrit verse, with strict adherence to classical metrical patterns, particularly the Anushtub meter. We evaluate a range of generative models-both open-source and proprietary-under multiple settings. Specifically, we explore constrained decoding strategies and instruction-based fine-tuning tailored to metrical and semantic fidelity. Our decoding approach achieves over 99% accuracy in producing syntactically valid poetic forms, substantially outperforming general-purpose models in meter conformity. Meanwhile, instruction-tuned variants show improved alignment with source meaning and poetic style, as supported by human assessments, albeit with marginal trade-offs in metrical precision.</li>
</ul>

<h3>Title: One for All: Update Parameterized Knowledge Across Multiple Models</h3>
<ul>
<li><strong>Authors: </strong>Weitao Ma, Xiyuan Du, Xiaocheng Feng, Lei Huang, Yichong Huang, Huiyi Zhang, Xiaoliang Yang, Baohang Li, Xiachong Feng, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00817">https://arxiv.org/abs/2506.00817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00817">https://arxiv.org/pdf/2506.00817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00817]] One for All: Update Parameterized Knowledge Across Multiple Models(https://arxiv.org/abs/2506.00817)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode vast world knowledge but struggle to stay up-to-date, often leading to errors and hallucinations. Knowledge editing offers an efficient alternative to retraining, enabling targeted modifications by updating specific model parameters. However, existing methods primarily focus on individual models, posing challenges in efficiently updating multiple models and adapting to new models. To address this, we propose OnceEdit, a novel ensemble-based approach that employs a plug-in model as the editing module, enabling stable knowledge updates across multiple models. Building on the model ensemble, OnceEdit introduces two key mechanisms to enhance its effectiveness. First, we introduce a dynamic weight mechanism through a \weight token for distinguishing between edit-related and non-edit-related instances, ensuring the appropriate utilization of knowledge from integrated models. Second, we incorporate an ensemble enhancement mechanism to mitigate the excessive reliance on the central model inherent in the model ensemble technique, making it more suitable for knowledge editing. Extensive experiments on diverse LLMs demonstrate that OnceEdit consistently outperforms existing methods while achieving superior editing efficiency. Further analysis confirms its adaptability and stability in multi-model editing scenarios. Our code will be available.</li>
</ul>

<h3>Title: QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Jiatong Li, Libo Zhu, Haotong Qin, Jingkai Wang, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00820">https://arxiv.org/abs/2506.00820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00820">https://arxiv.org/pdf/2506.00820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00820]] QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration(https://arxiv.org/abs/2506.00820)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have been achieving remarkable performance in face restoration. However, the heavy computations of diffusion models make it difficult to deploy them on devices like smartphones. In this work, we propose QuantFace, a novel low-bit quantization for one-step diffusion face restoration models, where the full-precision (\ie, 32-bit) weights and activations are quantized to 4$\sim$6-bit. We first analyze the data distribution within activations and find that they are highly variant. To preserve the original data information, we employ rotation-scaling channel balancing. Furthermore, we propose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly optimizes for quantization and distillation performance. Finally, we propose an adaptive bit-width allocation strategy. We formulate such a strategy as an integer programming problem, which combines quantization error and perceptual metrics to find a satisfactory resource allocation. Extensive experiments on the synthetic and real-world datasets demonstrate the effectiveness of QuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over recent leading low-bit quantization methods for face restoration. The code is available at this https URL.</li>
</ul>

<h3>Title: SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Huixin Zhan, Jason H. Moore</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00821">https://arxiv.org/abs/2506.00821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00821">https://arxiv.org/pdf/2506.00821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00821]] SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models(https://arxiv.org/abs/2506.00821)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks led to substantial performance degradation, even in large models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.</li>
</ul>

<h3>Title: Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Zhengwen Feng, Hao Peng, Jianwei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00823">https://arxiv.org/abs/2506.00823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00823">https://arxiv.org/pdf/2506.00823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00823]] Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks(https://arxiv.org/abs/2506.00823)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the "truth direction", which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts. Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation. Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs. These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs. Our code is public at this https URL</li>
</ul>

<h3>Title: HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Xiao, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00826">https://arxiv.org/abs/2506.00826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00826">https://arxiv.org/pdf/2506.00826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00826]] HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs(https://arxiv.org/abs/2506.00826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: COMPKE: Complex Question Answering under Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Keyuan Cheng, Zijian Kan, Zhixian He, Zhuoran Zhang, Muhammad Asif Ali, Ke Xu, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00829">https://arxiv.org/abs/2506.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00829">https://arxiv.org/pdf/2506.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00829]] COMPKE: Complex Question Answering under Knowledge Editing(https://arxiv.org/abs/2506.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at this https URL.</li>
</ul>

<h3>Title: SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhengcong Fei, Hao Jiang, Di Qiu, Baoxuan Gu, Youqiang Zhang, Jiahua Wang, Jialin Bai, Debang Li, Mingyuan Fan, Guibin Chen, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00830">https://arxiv.org/abs/2506.00830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00830">https://arxiv.org/pdf/2506.00830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00830]] SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers(https://arxiv.org/abs/2506.00830)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.</li>
</ul>

<h3>Title: A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>M Sabbir Salek, Mashrur Chowdhury, Muhaimin Bin Munir, Yuchen Cai, Mohammad Imtiaz Hasan, Jean-Michel Tine, Latifur Khan, Mizanur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00831">https://arxiv.org/abs/2506.00831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00831">https://arxiv.org/pdf/2506.00831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00831]] A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems(https://arxiv.org/abs/2506.00831)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Modern transportation systems rely on cyber-physical systems (CPS), where cyber systems interact seamlessly with physical systems like transportation-related sensors and actuators to enhance safety, mobility, and energy efficiency. However, growing automation and connectivity increase exposure to cyber vulnerabilities. Existing threat modeling frameworks for transportation CPS are often limited in scope, resource-intensive, and dependent on significant cybersecurity expertise. To address these gaps, we present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling Framework), a large language model (LLM)-based framework that minimizes expert intervention. TraCR-TMF identifies threats, potential attack techniques, and corresponding countermeasures by leveraging the MITRE ATT&CK matrix through three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method requiring no expert input, (ii) an in-context learning approach requiring low expert input, and (iii) a supervised fine-tuning method requiring moderate expert input. TraCR-TMF also maps attack paths to critical assets by analyzing vulnerabilities using a customized LLM. The framework was evaluated in two scenarios. First, it identified relevant attack techniques across transportation CPS applications, with 90% precision as validated by experts. Second, using a fine-tuned LLM, it successfully predicted multiple exploitations including lateral movement, data exfiltration, and ransomware-related encryption that occurred during a major real-world cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in CPS threat modeling, its reduced reliance on cybersecurity expertise, and its adaptability across CPS domains.</li>
</ul>

<h3>Title: Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Baolu Li, Hongkai Yu, Huiming Sun, Jin Ma, Yuewei Lin, Lu Ma, Yonghua Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00836">https://arxiv.org/abs/2506.00836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00836">https://arxiv.org/pdf/2506.00836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00836]] Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision(https://arxiv.org/abs/2506.00836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The synchrotron light source, a cutting-edge large-scale user facility, requires autonomous synchrotron beamline operations, a crucial technique that should enable experiments to be conducted automatically, reliably, and safely with minimum human intervention. However, current state-of-the-art synchrotron beamlines still heavily rely on human safety oversight. To bridge the gap between automated and autonomous operation, a computer vision-based system is proposed, integrating deep learning and multiview cameras for real-time collision detection. The system utilizes equipment segmentation, tracking, and geometric analysis to assess potential collisions with transfer learning that enhances robustness. In addition, an interactive annotation module has been developed to improve the adaptability to new object classes. Experiments on a real beamline dataset demonstrate high accuracy, real-time performance, and strong potential for autonomous synchrotron beamline operations.</li>
</ul>

<h3>Title: Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Gu, Ziting Xian, Yuanzhen Xie, Ye Liu, Enjie Liu, Ruichao Zhong, Mochi Gao, Yunzhi Tan, Bo Hu, Zang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00842">https://arxiv.org/abs/2506.00842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00842">https://arxiv.org/pdf/2506.00842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00842]] Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience(https://arxiv.org/abs/2506.00842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.</li>
</ul>

<h3>Title: Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs</h3>
<ul>
<li><strong>Authors: </strong>Mana Sakai, Ryo Karakida, Masaaki Imaizumi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00846">https://arxiv.org/abs/2506.00846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00846">https://arxiv.org/pdf/2506.00846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00846]] Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs(https://arxiv.org/abs/2506.00846)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In modern theoretical analyses of neural networks, the infinite-width limit is often invoked to justify Gaussian approximations of neuron preactivations (e.g., via neural network Gaussian processes or Tensor Programs). However, these Gaussian-based asymptotic theories have so far been unable to capture the behavior of attention layers, except under special regimes such as infinitely many heads or tailored scaling schemes. In this paper, leveraging the Tensor Programs framework, we rigorously identify the infinite-width limit distribution of variables within a single attention layer under realistic architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$ dimensionality. We derive the exact form of this limit law without resorting to infinite-head approximations or tailored scalings, demonstrating that it departs fundamentally from Gaussianity. This limiting distribution exhibits non-Gaussianity from a hierarchical structure, being Gaussian conditional on the random similarity scores. Numerical experiments validate our theoretical predictions, confirming the effectiveness of our theory at finite width and accurate description of finite-head attentions. Beyond characterizing a standalone attention layer, our findings lay the groundwork for developing a unified theory of deep Transformer architectures in the infinite-width regime.</li>
</ul>

<h3>Title: Speech Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Jiali Cheng, Hadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00848">https://arxiv.org/abs/2506.00848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00848">https://arxiv.org/pdf/2506.00848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00848]] Speech Unlearning(https://arxiv.org/abs/2506.00848)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.</li>
</ul>

<h3>Title: Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Qi Chen, Jierui Zhu, Florian Shkurti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00849">https://arxiv.org/abs/2506.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00849">https://arxiv.org/pdf/2506.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00849]] Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis(https://arxiv.org/abs/2506.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that provides guarantees for the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing computable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.</li>
</ul>

<h3>Title: EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</h3>
<ul>
<li><strong>Authors: </strong>Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MM, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00854">https://arxiv.org/abs/2506.00854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00854">https://arxiv.org/pdf/2506.00854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00854]] EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG(https://arxiv.org/abs/2506.00854)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.</li>
</ul>

<h3>Title: ARIANNA: An Automatic Design Flow for Fabric Customization and eFPGA Redaction</h3>
<ul>
<li><strong>Authors: </strong>Luca Collini, Jitendra Bhandari, Chiara Muscari Tomajoli, Abdul Khader Thalakkattu Moosa, Benjamin Tan, Xifan Tang, Pierre-Emmanuel Gaillardon, Ramesh Karri, Christian Pilato</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00857">https://arxiv.org/abs/2506.00857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00857">https://arxiv.org/pdf/2506.00857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00857]] ARIANNA: An Automatic Design Flow for Fabric Customization and eFPGA Redaction(https://arxiv.org/abs/2506.00857)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>In the modern global Integrated Circuit (IC) supply chain, protecting intellectual property (IP) is a complex challenge, and balancing IP loss risk and added cost for theft countermeasures is hard to achieve. Using embedded configurable logic allows designers to completely hide the functionality of selected design portions from parties that do not have access to the configuration string (bitstream). However, the design space of redacted solutions is huge, with trade-offs between the portions selected for redaction and the configuration of the configurable embedded logic. We propose ARIANNA, a complete flow that aids the designer in all the stages, from selecting the logic to be hidden to tailoring the bespoke fabrics for the configurable logic used to hide it. We present a security evaluation of the considered fabrics and introduce two heuristics for the novel bespoke fabric flow. We evaluate the heuristics against an exhaustive approach. We also evaluate the complete flow using a selection of benchmarks. Results show that using ARIANNA to customize the redaction fabrics yields up to 3.3x lower overheads and 4x higher eFPGA fabric utilization than a one-fits-all fabric as proposed in prior works.</li>
</ul>

<h3>Title: FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Haixin Wang, Jiashu Pan, Hao Wu, Fan Zhang, Tailin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00862">https://arxiv.org/abs/2506.00862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00862">https://arxiv.org/pdf/2506.00862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00862]] FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling(https://arxiv.org/abs/2506.00862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modeling complex fluid systems, especially turbulence governed by partial differential equations (PDEs), remains a fundamental challenge in science and engineering. Recently, diffusion-based generative models have gained attention as a powerful approach for these tasks, owing to their capacity to capture long-range dependencies and recover hierarchical structures. However, we present both empirical and theoretical evidence showing that generative models struggle with significant spectral bias and common-mode noise when generating high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative turbulence modeling framework that enhances the frequency-aware learning by both implicitly and explicitly mitigating spectral bias and common-mode noise. FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch backbone architecture, consisting of a salient flow attention branch with local-global awareness to focus on sensitive turbulence areas. Secondly, we introduce a frequency-guided Fourier mixing branch, which is integrated via an adaptive fusion strategy to explicitly mitigate spectral bias in the generative model. Thirdly, we leverage the high-frequency modeling capabilities of the masked auto-encoder pre-training and implicitly align the features of the generative model toward high-frequency components. We validate the effectiveness of FourierFlow on three canonical turbulent flow scenarios, demonstrating superior performance compared to state-of-the-art methods. Furthermore, we show that our model exhibits strong generalization capabilities in challenging settings such as out-of-distribution domains, long-term temporal extrapolation, and robustness to noisy inputs. The code can be found at this https URL.</li>
</ul>

<h3>Title: L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nidhi Kowtal, Raviraj Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00863">https://arxiv.org/abs/2506.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00863">https://arxiv.org/pdf/2506.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00863]] L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models(https://arxiv.org/abs/2506.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotion recognition in low-resource languages like Marathi remains challenging due to limited annotated data. We present L3Cube-MahaEmotions, a high-quality Marathi emotion recognition dataset with 11 fine-grained emotion labels. The training data is synthetically annotated using large language models (LLMs), while the validation and test sets are manually labeled to serve as a reliable gold-standard benchmark. Building on the MahaSent dataset, we apply the Chain-of-Translation (CoTR) prompting technique, where Marathi sentences are translated into English and emotion labeled via a single prompt. GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data annotation due to superior label quality. We evaluate model performance using standard metrics and explore label aggregation strategies (e.g., Union, Intersection). While GPT-4 predictions outperform fine-tuned BERT models, BERT-based models trained on synthetic labels fail to surpass GPT-4. This highlights both the importance of high-quality human-labeled data and the inherent complexity of emotion recognition. An important finding of this work is that generic LLMs like GPT-4 and Llama3-405B generalize better than fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset and model are shared publicly at this https URL</li>
</ul>

<h3>Title: Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning</h3>
<ul>
<li><strong>Authors: </strong>Kyowoon Lee, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00867">https://arxiv.org/abs/2506.00867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00867">https://arxiv.org/pdf/2506.00867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00867]] Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning(https://arxiv.org/abs/2506.00867)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose Local Manifold Approximation and Projection (LoMAP), a training-free method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.</li>
</ul>

<h3>Title: What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhaotian Weng, Haoxuan Li, Kuan-Hao Huang, Jieyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00869">https://arxiv.org/abs/2506.00869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00869">https://arxiv.org/pdf/2506.00869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00869]] What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning(https://arxiv.org/abs/2506.00869)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of vision-language models (VLMs) on downstream tasks, their ability to understand and reason about causal relationships in visual inputs remains unclear. Robust causal reasoning is fundamental to solving complex high-level reasoning tasks, yet existing benchmarks often include a mixture of reasoning questions, and VLMs can frequently exploit object recognition and activity identification as shortcuts to arrive at the correct answers, making it challenging to truly assess their causal reasoning abilities. To bridge this gap, we introduce VQA-Causal and VCR-Causal, two new benchmarks specifically designed to isolate and rigorously evaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs excel in object and activity recognition, they perform poorly on causal reasoning tasks, often only marginally surpassing random guessing. Further analysis suggests that this limitation stems from a severe lack of causal expressions in widely used training datasets, where causal relationships are rarely explicitly conveyed. We additionally explore fine-tuning strategies with hard negative cases, showing that targeted fine-tuning can improve model's causal reasoning while maintaining generalization and downstream performance. Our study highlights a key gap in current VLMs and lays the groundwork for future work on causal understanding.</li>
</ul>

<h3>Title: Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Xinan He, KaiQing Lin, Bin Fan, Feng Ding, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00874">https://arxiv.org/abs/2506.00874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00874">https://arxiv.org/pdf/2506.00874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00874]] Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection(https://arxiv.org/abs/2506.00874)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current AIGC detectors often achieve near-perfect accuracy on images produced by the same generator used for training but struggle to generalize to outputs from unseen generators. We trace this failure in part to latent prior bias: detectors learn shortcuts tied to patterns stemming from the initial noise vector rather than learning robust generative artifacts. To address this, we propose On-Manifold Adversarial Training (OMAT): by optimizing the initial latent noise of diffusion models under fixed conditioning, we generate on-manifold adversarial examples that remain on the generator's output manifold-unlike pixel-space attacks, which introduce off-manifold perturbations that the generator itself cannot reproduce and that can obscure the true discriminative artifacts. To test against state-of-the-art generative models, we introduce GenImage++, a test-only benchmark of outputs from advanced generators (Flux.1, SD3) with extended prompts and diverse styles. We apply our adversarial-training paradigm to ResNet50 and CLIP baselines and evaluate across existing AIGC forensic benchmarks and recent challenge datasets. Extensive experiments show that adversarially trained detectors significantly improve cross-generator performance without any network redesign. Our findings on latent-prior bias offer valuable insights for future dataset construction and detector evaluation, guiding the development of more robust and generalizable AIGC forensic methodologies.</li>
</ul>

<h3>Title: CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yangfan Ye, Xiaocheng Feng, Zekun Yuan, Xiachong Feng, Libo Qin, Lei Huang, Weitao Ma, Yichong Huang, Zhirui Zhang, Yunfei Lu, Xiaohui Yan, Duyu Tang, Dandan Tu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00875">https://arxiv.org/abs/2506.00875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00875">https://arxiv.org/pdf/2506.00875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00875]] CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning(https://arxiv.org/abs/2506.00875)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current large language models (LLMs) often exhibit imbalanced multilingual capabilities due to their English-centric training corpora. To address this, existing fine-tuning approaches operating at the data-level (e.g., through data augmentation or distillation) typically introduce implicit cross-lingual alignment, overlooking the potential for more profound, latent-level cross-lingual interactions. In this work, we propose CC-Tuning, a novel multilingual fine-tuning paradigm that explicitly establishes a cross-lingual connection mechanism at the latent level. During training, CC-Tuning fuses the feed forward activations from both English and non-English inputs, enabling the model to benefit from both linguistic resources. This process is facilitated with a trainable Decision Maker that identifies beneficial activations. Furthermore, during inference, a Transform Matrix is utilized to simulate the cross-lingual connection under monolingual setting through representation transformation. Our experiments on six benchmarks covering 22 languages show that CC-Tuning outperforms vanilla SFT and offers a strong latent-level alternative to data-level augmentation methods. Further analysis also highlights the practicality of CC-Tuning and the potential of latent-level cross-lingual interactions in advancing the multilingual performance of LLMs.</li>
</ul>

<h3>Title: Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Rahul Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00876">https://arxiv.org/abs/2506.00876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00876">https://arxiv.org/pdf/2506.00876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00876]] Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning(https://arxiv.org/abs/2506.00876)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) unlearning has recently gained significant attention, driven by the need to remove unwanted information, such as private, sensitive, or copyrighted content, from LLMs. However, conventional unlearning approaches indiscriminately update model parameters to forget all tokens in a target document, including common tokens (e.g., pronouns, prepositions, general nouns) that carry general knowledge. In this paper, we highlight that not every token needs forgetting. We propose Selective Unlearning (SU), which identifies a critical subset of tokens within the forgetting set that is relevant to the unwanted information, and unlearns only those tokens. Experiments on two benchmarks and six baseline unlearning algorithms demonstrate that SU not only achieves effective unlearning on the targeted forget data, but also significantly preserves the model's utility in the retaining set.</li>
</ul>

<h3>Title: ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Yizhen Zheng, Huan Yee Koh, Hongxin Xiang, Linjiang Chen, Wenjie Du, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00880">https://arxiv.org/abs/2506.00880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00880">https://arxiv.org/pdf/2506.00880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00880]] ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models(https://arxiv.org/abs/2506.00880)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.</li>
</ul>

<h3>Title: Improve MLLM Benchmark Efficiency through Interview</h3>
<ul>
<li><strong>Authors: </strong>Farong Wen, Yijin Guo, Junying Wang, Jiaohao Xiao, Yingjie Zhou, Chunyi Li, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00883">https://arxiv.org/abs/2506.00883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00883">https://arxiv.org/pdf/2506.00883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00883]] Improve MLLM Benchmark Efficiency through Interview(https://arxiv.org/abs/2506.00883)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of Multimodal Large Language Models (MLLM) has led to a wide range of MLLM applications, and a number of benchmark datasets have sprung up in order to assess MLLM abilities. However, full-coverage Q&A testing on large-scale data is resource-intensive and time-consuming. To address this issue, we propose the MLLM Interview (MITV) strategy, which aims to quickly obtain MLLM performance metrics by quizzing fewer question. First, First, we constructed the interview dataset, which was built on an existing MLLM assessment dataset, by adding difficulty labels based on the performance of some typical MLLMs in this dataset. Second, we propose an MLLM Interview strategy, which obtains an initial performance situation of the large model by quizzing a small number of topics and then continuously tries to test the model's limits. Through extensive experiments, the result shows that the MITV strategy proposed in this paper performs well on MLLM benchmark datasets, and it is able to obtain the model evaluation capability faster through a small number of questions and answers.</li>
</ul>

<h3>Title: Uneven Event Modeling for Partially Relevant Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Sa Zhu, Huashan Chen, Wanqian Zhang, Jinchao Zhang, Zexian Yang, Xiaoshuai Hao, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00891">https://arxiv.org/abs/2506.00891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00891">https://arxiv.org/pdf/2506.00891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00891]] Uneven Event Modeling for Partially Relevant Video Retrieval(https://arxiv.org/abs/2506.00891)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Given a text query, partially relevant video retrieval (PRVR) aims to retrieve untrimmed videos containing relevant moments, wherein event modeling is crucial for partitioning the video into smaller temporal events that partially correspond to the text. Previous methods typically segment videos into a fixed number of equal-length clips, resulting in ambiguous event boundaries. Additionally, they rely on mean pooling to compute event representations, inevitably introducing undesired misalignment. To address these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first introduce the Progressive-Grouped Video Segmentation (PGVS) module, to iteratively formulate events in light of both temporal dependencies and semantic similarity between consecutive frames, enabling clear event boundaries. Furthermore, we also propose the Context-Aware Event Refinement (CAER) module to refine the event representation conditioned the text's cross-attention. This enables event representations to focus on the most relevant frames for a given text, facilitating more precise text-video alignment. Extensive experiments demonstrate that our method achieves state-of-the-art performance on two PRVR benchmarks.</li>
</ul>

<h3>Title: Affordance Benchmark for MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Junying Wang, Wenzhe Li, Yalun Wu, Yingji Liang, Yijin Guo, Chunyi Li, Haodong Duan, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00893">https://arxiv.org/abs/2506.00893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00893">https://arxiv.org/pdf/2506.00893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00893]] Affordance Benchmark for MLLMs(https://arxiv.org/abs/2506.00893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Affordance theory posits that environments inherently offer action possibilities that shape perception and behavior. While Multimodal Large Language Models (MLLMs) excel in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce A4Bench, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance}, assessing understanding of inherent object properties through 1,282 question-answer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. Evaluating 17 MLLMs (nine proprietary and eight open-source) against human performance, we find that proprietary models generally outperform open-source counterparts, but all exhibit limited capabilities, particularly in transformative affordance perception. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions. The dataset is available in this https URL.</li>
</ul>

<h3>Title: State-Covering Trajectory Stitching for Diffusion Planners</h3>
<ul>
<li><strong>Authors: </strong>Kyowoon Lee, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00895">https://arxiv.org/abs/2506.00895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00895">https://arxiv.org/pdf/2506.00895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00895]] State-Covering Trajectory Stitching for Diffusion Planners(https://arxiv.org/abs/2506.00895)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models are emerging as powerful tools for long-horizon planning in reinforcement learning (RL), particularly with offline datasets. However, their performance is fundamentally limited by the quality and diversity of training data. This often restricts their generalization to tasks outside their training distribution or longer planning horizons. To overcome this challenge, we propose State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory augmentation method that incrementally stitches together short trajectory segments, systematically generating diverse and extended trajectories. SCoTS first learns a temporal distance-preserving latent representation that captures the underlying temporal structure of the environment, then iteratively stitches trajectory segments guided by directional exploration and novelty to effectively cover and expand this latent space. We demonstrate that SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Furthermore, augmented trajectories generated by SCoTS significantly improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.</li>
</ul>

<h3>Title: SocialEval: Evaluating Social Intelligence of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinfeng Zhou, Yuxuan Chen, Yihan Shi, Xuanming Zhang, Leqi Lei, Yi Feng, Zexuan Xiong, Miao Yan, Xunzhi Wang, Yaru Cao, Jianing Yin, Shuai Wang, Quanyu Dai, Zhenhua Dong, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00900">https://arxiv.org/abs/2506.00900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00900">https://arxiv.org/pdf/2506.00900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00900]] SocialEval: Evaluating Social Intelligence of Large Language Models(https://arxiv.org/abs/2506.00900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLMs exhibit promising Social Intelligence (SI) in modeling human behavior, raising the need to evaluate LLMs' SI and their discrepancy with humans. SI equips humans with interpersonal abilities to behave wisely in navigating social interactions to achieve social goals. This presents an operational evaluation paradigm: outcome-oriented goal achievement evaluation and process-oriented interpersonal ability evaluation, which existing work fails to address. To this end, we propose SocialEval, a script-based bilingual SI benchmark, integrating outcome- and process-oriented evaluation by manually crafting narrative scripts. Each script is structured as a world tree that contains plot lines driven by interpersonal ability, providing a comprehensive view of how LLMs navigate social interactions. Experiments show that LLMs fall behind humans on both SI evaluations, exhibit prosociality, and prefer more positive social behaviors, even if they lead to goal failure. Analysis of LLMs' formed representation space and neuronal activations reveals that LLMs have developed ability-specific functional partitions akin to the human brain.</li>
</ul>

<h3>Title: Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras</h3>
<ul>
<li><strong>Authors: </strong>Xander Küpers, Jeroen Klein Brinke, Rob Bemthuis, Ozlem Durmaz Incel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00904">https://arxiv.org/abs/2506.00904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00904">https://arxiv.org/pdf/2506.00904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00904]] Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras(https://arxiv.org/abs/2506.00904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The construction industry faces significant challenges in optimizing equipment utilization, as underused machinery leads to increased operational costs and project delays. Accurate and timely monitoring of equipment activity is therefore key to identifying idle periods and improving overall efficiency. This paper presents the Edge-IMI framework for detecting idle construction machinery, specifically designed for integration with surveillance camera systems. The proposed solution consists of three components: object detection, tracking, and idle state identification, which are tailored for execution on resource-constrained, CPU-based edge computing devices. The performance of Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS benchmarks. Experimental results confirm that the object detector achieves an F1 score of 71.75%, indicating robust real-world detection capabilities. The logistic regression-based idle identification module reliably distinguishes between active and idle machinery with minimal false positives. Integrating all three modules, Edge-IMI enables efficient on-site inference, reducing reliance on high-bandwidth cloud services and costly hardware accelerators. We also evaluate the performance of object detection models on Raspberry Pi 5 and an Intel NUC platforms, as example edge computing platforms. We assess the feasibility of real-time processing and the impact of model optimization techniques.</li>
</ul>

<h3>Title: DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation</h3>
<ul>
<li><strong>Authors: </strong>Xianbing Sun, Yan Hong, Jiahui Zhan, Jun Lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00908">https://arxiv.org/abs/2506.00908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00908">https://arxiv.org/pdf/2506.00908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00908]] DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation(https://arxiv.org/abs/2506.00908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Despite recent progress, most existing virtual try-on methods still struggle to simultaneously address two core challenges: accurately aligning the garment image with the target human body, and preserving fine-grained garment textures and patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on framework that explicitly disentangles these objectives for more effective modeling. DS-VTON consists of two stages: the first stage generates a low-resolution try-on result to capture the semantic correspondence between garment and body, where reduced detail facilitates robust structural alignment. The second stage introduces a residual-guided diffusion process that reconstructs high-resolution outputs by refining the residual between the two scales, focusing on texture fidelity. In addition, our method adopts a fully mask-free generation paradigm, eliminating reliance on human parsing maps or segmentation masks. By leveraging the semantic priors embedded in pretrained diffusion models, this design more effectively preserves the person's appearance and geometric consistency. Extensive experiments demonstrate that DS-VTON achieves state-of-the-art performance in both structural alignment and texture preservation across multiple standard virtual try-on benchmarks.</li>
</ul>

<h3>Title: How do Transformer Embeddings Represent Compositions? A Functional Analysis</h3>
<ul>
<li><strong>Authors: </strong>Aishik Nagar, Ishaan Singh Rawal, Mansi Dhanania, Cheston Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00914">https://arxiv.org/abs/2506.00914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00914">https://arxiv.org/pdf/2506.00914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00914]] How do Transformer Embeddings Represent Compositions? A Functional Analysis(https://arxiv.org/abs/2506.00914)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.</li>
</ul>

<h3>Title: 3D Skeleton-Based Action Recognition: A Review</h3>
<ul>
<li><strong>Authors: </strong>Mengyuan Liu, Hong Liu, Qianshuo Hu, Bin Ren, Junsong Yuan, Jiaying Lin, Jiajun Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00915">https://arxiv.org/abs/2506.00915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00915">https://arxiv.org/pdf/2506.00915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00915]] 3D Skeleton-Based Action Recognition: A Review(https://arxiv.org/abs/2506.00915)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>With the inherent advantages of skeleton representation, 3D skeleton-based action recognition has become a prominent topic in the field of computer vision. However, previous reviews have predominantly adopted a model-oriented perspective, often neglecting the fundamental steps involved in skeleton-based action recognition. This oversight tends to ignore key components of skeleton-based action recognition beyond model design and has hindered deeper, more intrinsic understanding of the task. To bridge this gap, our review aims to address these limitations by presenting a comprehensive, task-oriented framework for understanding skeleton-based action recognition. We begin by decomposing the task into a series of sub-tasks, placing particular emphasis on preprocessing steps such as modality derivation and data augmentation. The subsequent discussion delves into critical sub-tasks, including feature extraction and spatio-temporal modeling techniques. Beyond foundational action recognition networks, recently advanced frameworks such as hybrid architectures, Mamba models, large language models (LLMs), and generative models have also been highlighted. Finally, a comprehensive overview of public 3D skeleton datasets is presented, accompanied by an analysis of state-of-the-art algorithms evaluated on these benchmarks. By integrating task-oriented discussions, comprehensive examinations of sub-tasks, and an emphasis on the latest advancements, our review provides a fundamental and accessible structured roadmap for understanding and advancing the field of 3D skeleton-based action recognition.</li>
</ul>

<h3>Title: Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Philip Heejun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00920">https://arxiv.org/abs/2506.00920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00920">https://arxiv.org/pdf/2506.00920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00920]] Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation(https://arxiv.org/abs/2506.00920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.</li>
</ul>

<h3>Title: Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Qiao Xiao, Boqian Wu, Andrey Poddubnyy, Elena Mocanu, Phuong H. Nguyen, Mykola Pechenizkiy, Decebal Constantin Mocanu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00932">https://arxiv.org/abs/2506.00932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00932">https://arxiv.org/pdf/2506.00932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00932]] Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity(https://arxiv.org/abs/2506.00932)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy, leveraging aggregated updates to build robust global models. However, this training paradigm faces significant challenges due to data heterogeneity and limited local datasets, which often impede effective collaboration. In such scenarios, we identify the Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model undergo minimal updates after early communication rounds, ultimately limiting the effectiveness of global aggregation. We demonstrate the presence of this phenomenon across a wide range of federated settings, spanning diverse datasets and architectures. To address this issue, we propose LIPS (Layer-wise Inertia Phenomenon with Sparsity), a simple yet effective method that periodically introduces transient sparsity to stimulate meaningful updates and empower global aggregation. Experiments demonstrate that LIPS effectively mitigates layer-wise inertia, enhances aggregation effectiveness, and improves overall performance in various FL scenarios. This work not only deepens the understanding of layer-wise learning dynamics in FL but also paves the way for more effective collaboration strategies in resource-constrained environments. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Peijin Guo, Minghui Li, Hewen Pan, Bowen Chen, Yang Wu, Zikang Guo, Leo Yu Zhang, Shengshan Hu, Shengqing Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00936">https://arxiv.org/abs/2506.00936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00936">https://arxiv.org/pdf/2506.00936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00936]] Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning(https://arxiv.org/abs/2506.00936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate prediction of molecular metabolic stability (MS) is critical for drug research and development but remains challenging due to the complex interplay of molecular interactions. Despite recent advances in graph neural networks (GNNs) for MS prediction, current approaches face two critical limitations: (1) incomplete molecular modeling due to atom-centric message-passing mechanisms that disregard bond-level topological features, and (2) prediction frameworks that lack reliable uncertainty quantification. To address these challenges, we propose TrustworthyMS, a novel contrastive learning framework designed for uncertainty-aware metabolic stability prediction. First, a molecular graph topology remapping mechanism synchronizes atom-bond interactions through edge-induced feature propagation, capturing both localized electronic effects and global conformational constraints. Second, contrastive topology-bond alignment enforces consistency between molecular topology views and bond patterns via feature alignment, enhancing representation robustness. Third, uncertainty modeling through Beta-Binomial uncertainty quantification enables simultaneous prediction and confidence calibration under epistemic uncertainty. Through extensive experiments, our results demonstrate that TrustworthyMS outperforms current state-of-the-art methods in terms of predictive performance.</li>
</ul>

<h3>Title: anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, Zhengxing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00942">https://arxiv.org/abs/2506.00942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00942">https://arxiv.org/pdf/2506.00942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00942]] anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding(https://arxiv.org/abs/2506.00942)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs.</li>
</ul>

<h3>Title: Deformable registration and generative modelling of aortic anatomies by auto-decoders and neural ODEs</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Tenderini, Luca Pegolotti, Fanwei Kong, Stefano Pagani, Francesco Regazzoni, Alison L. Marsden, Simone Deparis</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00947">https://arxiv.org/abs/2506.00947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00947">https://arxiv.org/pdf/2506.00947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00947]] Deformable registration and generative modelling of aortic anatomies by auto-decoders and neural ODEs(https://arxiv.org/abs/2506.00947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work introduces AD-SVFD, a deep learning model for the deformable registration of vascular shapes to a pre-defined reference and for the generation of synthetic anatomies. AD-SVFD operates by representing each geometry as a weighted point cloud and models ambient space deformations as solutions at unit time of ODEs, whose time-independent right-hand sides are expressed through artificial neural networks. The model parameters are optimized by minimizing the Chamfer Distance between the deformed and reference point clouds, while backward integration of the ODE defines the inverse transformation. A distinctive feature of AD-SVFD is its auto-decoder structure, that enables generalization across shape cohorts and favors efficient weight sharing. In particular, each anatomy is associated with a low-dimensional code that acts as a self-conditioning field and that is jointly optimized with the network parameters during training. At inference, only the latent codes are fine-tuned, substantially reducing computational overheads. Furthermore, the use of implicit shape representations enables generative applications: new anatomies can be synthesized by suitably sampling from the latent space and applying the corresponding inverse transformations to the reference geometry. Numerical experiments, conducted on healthy aortic anatomies, showcase the high-quality results of AD-SVFD, which yields extremely accurate approximations at competitive computational costs.</li>
</ul>

<h3>Title: TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yiyao Huang, Zhedong Zheng, Yu Ziwei, Yaxiong Wang, Tze Ho Elden Tse, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00953">https://arxiv.org/abs/2506.00953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00953">https://arxiv.org/pdf/2506.00953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00953]] TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction(https://arxiv.org/abs/2506.00953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-defined 3D object templates are widely used in 3D reconstruction of hand-object interactions. However, they often require substantial manual efforts to capture or source, and inherently restrict the adaptability of models to unconstrained interaction scenarios, e.g., heavily-occluded objects. To overcome this bottleneck, we propose a new Text-Instructed Generation and Refinement (TIGeR) framework, harnessing the power of intuitive text-driven priors to steer the object shape refinement and pose estimation. We use a two-stage framework: a text-instructed prior generation and vision-guided refinement. As the name implies, we first leverage off-the-shelf models to generate shape priors according to the text description without tedious 3D crafting. Considering the geometric gap between the synthesized prototype and the real object interacted with the hand, we further calibrate the synthesized prototype via 2D-3D collaborative attention. TIGeR achieves competitive performance, i.e., 1.979 and 5.468 object Chamfer distance on the widely-used Dex-YCB and Obman datasets, respectively, surpassing existing template-free methods. Notably, the proposed framework shows robustness to occlusion, while maintaining compatibility with heterogeneous prior sources, e.g., retrieved hand-crafted prototypes, in practical deployment scenarios.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhu Li, Yuqing Zhang, Xiyuan Gao, Shekhar Nayak, Matt Coler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00955">https://arxiv.org/abs/2506.00955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00955">https://arxiv.org/pdf/2506.00955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00955]] Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection(https://arxiv.org/abs/2506.00955)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sarcasm fundamentally alters meaning through tone and context, yet detecting it in speech remains a challenge due to data scarcity. In addition, existing detection systems often rely on multimodal data, limiting their applicability in contexts where only speech is available. To address this, we propose an annotation pipeline that leverages large language models (LLMs) to generate a sarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ GPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human verification to resolve disagreements. We validate this approach by comparing annotation quality and detection performance on a publicly available sarcasm dataset using a collaborative gating architecture. Finally, we introduce PodSarc, a large-scale sarcastic speech dataset created through this pipeline. The detection model achieves a 73.63% F1 score, demonstrating the dataset's potential as a benchmark for sarcasm detection research.</li>
</ul>

<h3>Title: Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Geonu Lee, Yujeong Oh, Geonhui Jang, Soyoung Lee, Jeonghyo Song, Sungmin Cha, YoungJoon Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00956">https://arxiv.org/abs/2506.00956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00956">https://arxiv.org/pdf/2506.00956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00956]] Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection(https://arxiv.org/abs/2506.00956)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a new benchmark for continual learning in anomaly detection, aimed at better reflecting real-world deployment scenarios. Our benchmark, Continual-MEGA, includes a large and diverse dataset that significantly expands existing evaluation settings by combining carefully curated existing datasets with our newly proposed dataset, ContinualAD. In addition to standard continual learning with expanded quantity, we propose a novel scenario that measures zero-shot generalization to unseen classes, those not observed during continual adaptation. This setting poses a new problem setting that continual adaptation also enhances zero-shot performance. We also present a unified baseline algorithm that improves robustness in few-shot detection and maintains strong generalization. Through extensive evaluations, we report three key findings: (1) existing methods show substantial room for improvement, particularly in pixel-level defect localization; (2) our proposed method consistently outperforms prior approaches; and (3) the newly introduced ContinualAD dataset enhances the performance of strong anomaly detection models. We release the benchmark and code in this https URL.</li>
</ul>

<h3>Title: Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Wang, Yu Zhang, Guibin Jiang, Bing Cheng, Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00959">https://arxiv.org/abs/2506.00959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00959">https://arxiv.org/pdf/2506.00959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00959]] Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation(https://arxiv.org/abs/2506.00959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Marketing optimization, commonly formulated as an online budget allocation problem, has emerged as a pivotal factor in driving user growth. Most existing research addresses this problem by following the principle of 'first predict then optimize' for each individual, which presents challenges related to large-scale counterfactual prediction and solving complexity trade-offs. Note that the practical data quality is uncontrollable, and the solving scale tends to be tens of millions. Therefore, the existing approaches make the robust budget allocation non-trivial, especially in industrial scenarios with considerable data noise. To this end, this paper proposes a novel approach that solves the problem from the cluster perspective. Specifically, we propose a multi-task representation network to learn the inherent attributes of individuals and project the original features into high-dimension hidden representations through the first two layers of the trained network. Then, we divide these hidden representations into $K$ groups through partitioning-based clustering, thus reformulating the problem as an integer stochastic programming problem under different total budgets. Finally, we distill the representation module and clustering model into a multi-category model to facilitate online deployment. Offline experiments validate the effectiveness and superiority of our approach compared to six state-of-the-art marketing optimization algorithms. Online A/B tests on the Meituan platform indicate that the approach outperforms the online algorithm by 0.53% and 0.65%, considering order volume (OV) and gross merchandise volume (GMV), respectively.</li>
</ul>

<h3>Title: From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Cheng Cheng, Zhenya Huang, Guanhao Zhao, Yuxiang Guo, Xin Lin, Jinze Wu, Xin Li, Shijin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00963">https://arxiv.org/abs/2506.00963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00963">https://arxiv.org/pdf/2506.00963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00963]] From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation(https://arxiv.org/abs/2506.00963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Automatically generating high-quality mathematical problems that align with educational objectives is a crucial task in NLP-based educational technology. Traditional generation methods focus primarily on textual quality, but they often overlook educational objectives. Moreover, these methods address only single-dimensional, simple question generation, failing to meet complex, multifaceted educational requirements. To address these challenges, we constructed and annotated EduMath, a dataset of 16k mathematical questions with multi-dimensional educational objectives. Based on this dataset, we developed EQGEVAL, which incorporates three evaluation dimensions and is designed to assess the ability of models to generate educational questions. Drawing inspiration from teachers' problem design processes, we propose the Educational Question Planning with self-Reflection (EQPR) method for educational mathematical question generation, following a "plan-evaluate-optimize" approach. Specifically, by combining planning algorithm based on Monte Carlo Tree Search with the generative capabilities of Large Language Models, we continuously optimize questions through iterative feedback. This self-optimization mechanism ensures that the generated questions both fit the educational context and strategically achieve specific basic educational objectives. Through extensive experiments based on EQGEVAL, we have demonstrated that EQPR achieves significant improvements in generating questions that meet multi-dimensional educational objectives.</li>
</ul>

<h3>Title: ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness</h3>
<ul>
<li><strong>Authors: </strong>Dren Fazlija, Arkadij Orlov, Sandipan Sikdar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00964">https://arxiv.org/abs/2506.00964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00964">https://arxiv.org/pdf/2506.00964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00964]] ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness(https://arxiv.org/abs/2506.00964)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly becoming valuable to corporate data management due to their ability to process text from various document formats and facilitate user interactions through natural language queries. However, LLMs must consider the sensitivity of information when communicating with employees, especially given access restrictions. Simple filtering based on user clearance levels can pose both performance and privacy challenges. To address this, we propose the concept of sensitivity awareness (SA), which enables LLMs to adhere to predefined access rights rules. In addition, we developed a benchmarking environment called ACCESS DENIED INC to evaluate SA. Our experimental findings reveal significant variations in model behavior, particularly in managing unauthorized data requests while effectively addressing legitimate queries. This work establishes a foundation for benchmarking sensitivity-aware language models and provides insights to enhance privacy-centric AI systems in corporate environments.</li>
</ul>

<h3>Title: Data Heterogeneity Modeling for Trustworthy Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiashuo Liu, Peng Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00969">https://arxiv.org/abs/2506.00969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00969">https://arxiv.org/pdf/2506.00969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00969]] Data Heterogeneity Modeling for Trustworthy Machine Learning(https://arxiv.org/abs/2506.00969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.</li>
</ul>

<h3>Title: XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content</h3>
<ul>
<li><strong>Authors: </strong>Vadivel Abishethvarman, Bhavik Chandna, Pratik Jalan, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00973">https://arxiv.org/abs/2506.00973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00973">https://arxiv.org/pdf/2506.00973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00973]] XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content(https://arxiv.org/abs/2506.00973)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can generate content spanning ideological rhetoric to explicit instructions for violence. However, existing safety evaluations often rely on simplistic binary labels (safe and unsafe), overlooking the nuanced spectrum of risk these outputs pose. To address this, we present XGUARD, a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. XGUARD includes 3,840 red teaming prompts sourced from real world data such as social media and news, covering a broad range of ideologically charged scenarios. Our framework categorizes model responses into five danger levels (0 to 4), enabling a more nuanced analysis of both the frequency and severity of failures. We introduce the interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and compare defense mechanisms across threat intensities. Using XGUARD, we evaluate six popular LLMs and two lightweight defense strategies, revealing key insights into current safety gaps and trade-offs between robustness and expressive freedom. Our work underscores the value of graded safety metrics for building trustworthy LLMs.</li>
</ul>

<h3>Title: NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction</h3>
<ul>
<li><strong>Authors: </strong>Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00975">https://arxiv.org/abs/2506.00975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00975">https://arxiv.org/pdf/2506.00975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00975]] NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction(https://arxiv.org/abs/2506.00975)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.</li>
</ul>

<h3>Title: Quantization-based Bounds on the Wasserstein Metric</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Bobrutsky, Amit Moscovich</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00976">https://arxiv.org/abs/2506.00976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00976">https://arxiv.org/pdf/2506.00976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00976]] Quantization-based Bounds on the Wasserstein Metric(https://arxiv.org/abs/2506.00976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Wasserstein metric has become increasingly important in many machine learning applications such as generative modeling, image retrieval and domain adaptation. Despite its appeal, it is often too costly to compute. This has motivated approximation methods like entropy-regularized optimal transport, downsampling, and subsampling, which trade accuracy for computational efficiency. In this paper, we consider the challenge of computing efficient approximations to the Wasserstein metric that also serve as strict upper or lower bounds. Focusing on discrete measures on regular grids, our approach involves formulating and exactly solving a Kantorovich problem on a coarse grid using a quantized measure and specially designed cost matrix, followed by an upscaling and correction stage. This is done either in the primal or dual space to obtain valid upper and lower bounds on the Wasserstein metric of the full-resolution inputs. We evaluate our methods on the DOTmark optimal transport images benchmark, demonstrating a 10x-100x speedup compared to entropy-regularized OT while keeping the approximation error below 2%.</li>
</ul>

<h3>Title: CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Zhan Li, Mingyu Zhao, Xin Dong, Haibin Ling, Bingyao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00978">https://arxiv.org/abs/2506.00978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00978">https://arxiv.org/pdf/2506.00978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00978]] CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack(https://arxiv.org/abs/2506.00978)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Projector-based adversarial attack aims to project carefully designed light patterns (i.e., adversarial projections) onto scenes to deceive deep image classifiers. It has potential applications in privacy protection and the development of more robust classifiers. However, existing approaches primarily focus on individual classifiers and fixed camera poses, often neglecting the complexities of multi-classifier systems and scenarios with varying camera poses. This limitation reduces their effectiveness when introducing new classifiers or camera poses. In this paper, we introduce Classifier-Agnostic Projector-Based Adversarial Attack (CAPAA) to address these issues. First, we develop a novel classifier-agnostic adversarial loss and optimization framework that aggregates adversarial and stealthiness loss gradients from multiple classifiers. Then, we propose an attention-based gradient weighting mechanism that concentrates perturbations on regions of high classification activation, thereby improving the robustness of adversarial projections when applied to scenes with varying camera poses. Our extensive experimental evaluations demonstrate that CAPAA achieves both a higher attack success rate and greater stealthiness compared to existing baselines. Codes are available at: this https URL.</li>
</ul>

<h3>Title: IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</h3>
<ul>
<li><strong>Authors: </strong>Wayne Zhang, Changjiang Jiang, Zhonghao Zhang, Chenyang Si, Fengchang Yu, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00979">https://arxiv.org/abs/2506.00979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00979">https://arxiv.org/pdf/2506.00979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00979]] IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection(https://arxiv.org/abs/2506.00979)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at this https URL.</li>
</ul>

<h3>Title: LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World</h3>
<ul>
<li><strong>Authors: </strong>Sina J. Semnani, Pingyue Zhang, Wanyue Zhai, Haozhuo Li, Ryan Beauchamp, Trey Billing, Katayoun Kishi, Manling Li, Monica S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00980">https://arxiv.org/abs/2506.00980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00980">https://arxiv.org/pdf/2506.00980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00980]] LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World(https://arxiv.org/abs/2506.00980)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents LEMONADE, a large-scale conflict event dataset comprising 39,786 events across 20 languages and 171 countries, with extensive coverage of region-specific entities. LEMONADE is based on a partially reannotated subset of the Armed Conflict Location & Event Data (ACLED), which has documented global conflict events for over a decade. To address the challenge of aggregating multilingual sources for global event analysis, we introduce abstractive event extraction (AEE) and its subtask, abstractive entity linking (AEL). Unlike conventional span-based event extraction, our approach detects event arguments and entities through holistic document understanding and normalizes them across the multilingual dataset. We evaluate various large language models (LLMs) on these tasks, adapt existing zero-shot event extraction systems, and benchmark supervised models. Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for AEL. Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs outperforming specialized event extraction models such as GoLLIE. For entity linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a state-of-the-art zero-shot baseline that achieves only 23.7%. However, these zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in the end-to-end and AEL tasks, respectively, highlighting the need for further research.</li>
</ul>

<h3>Title: Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering</h3>
<ul>
<li><strong>Authors: </strong>Valeriya Goloviznina, Alexander Sergeev, Mikhail Melnichenko, Evgeny Kotelnikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00985">https://arxiv.org/abs/2506.00985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00985">https://arxiv.org/pdf/2506.00985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00985]] Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering(https://arxiv.org/abs/2506.00985)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Diary analysis presents challenges, particularly in extracting meaningful information from large corpora, where traditional methods often fail to deliver satisfactory results. This study introduces a novel method based on Large Language Models (LLMs) to identify and cluster the various purposes of diary writing. By "purposes," we refer to the intentions behind diary writing, such as documenting life events, self-reflection, or practicing language skills. Our approach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital archive, a rich collection of personal narratives. We evaluate different proprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the best performance, while a template-based baseline is significantly less effective. Additionally, we analyze the retrieved purposes based on gender, age of the authors, and the year of writing. Furthermore, we examine the types of errors made by the models, providing a deeper understanding of their limitations and potential areas for improvement in future research.</li>
</ul>

<h3>Title: Talking to Data: Designing Smart Assistants for Humanities Databases</h3>
<ul>
<li><strong>Authors: </strong>Alexander Sergeev, Valeriya Goloviznina, Mikhail Melnichenko, Evgeny Kotelnikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00986">https://arxiv.org/abs/2506.00986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00986">https://arxiv.org/pdf/2506.00986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00986]] Talking to Data: Designing Smart Assistants for Humanities Databases(https://arxiv.org/abs/2506.00986)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: this https URL.</li>
</ul>

<h3>Title: GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaorong Zhu, Ziheng Jia, Jiarui Wang, Xiangyu Zhao, Haodong Duan, Xiongkuo Min, Jia Wang, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00991">https://arxiv.org/abs/2506.00991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00991">https://arxiv.org/pdf/2506.00991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00991]] GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs(https://arxiv.org/abs/2506.00991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Multi-modality Large Language Models (MLLMs) is driving significant advancements in visual understanding and generation. Nevertheless, a comprehensive assessment of their capabilities, concerning the fine-grained physical principles especially in geometric optics, remains underexplored. To address this gap, we introduce GOBench, the first benchmark to systematically evaluate MLLMs' ability across two tasks: 1) Generating Optically Authentic Imagery and 2) Understanding Underlying Optical Phenomena. We curates high-quality prompts of geometric optical scenarios and use MLLMs to construct GOBench-Gen-1k this http URL then organize subjective experiments to assess the generated imagery based on Optical Authenticity, Aesthetic Quality, and Instruction Fidelity, revealing MLLMs' generation flaws that violate optical principles. For the understanding task, we apply crafted evaluation instructions to test optical understanding ability of eleven prominent MLLMs. The experimental results demonstrate that current models face significant challenges in both optical generation and understanding. The top-performing generative model, GPT-4o-Image, cannot perfectly complete all generation tasks, and the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\% accuracy in optical understanding.</li>
</ul>

<h3>Title: FlexSelect: Flexible Token Selection for Efficient Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, Linchao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00993">https://arxiv.org/abs/2506.00993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00993">https://arxiv.org/pdf/2506.00993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00993]] FlexSelect: Flexible Token Selection for Efficient Long Video Understanding(https://arxiv.org/abs/2506.00993)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Long-form video understanding poses a significant challenge for video large language models (VideoLLMs) due to prohibitively high computational and memory demands. In this paper, we propose FlexSelect, a flexible and efficient token selection strategy for processing long videos. FlexSelect identifies and retains the most semantically relevant content by leveraging cross-modal attention patterns from a reference transformer layer. It comprises two key components: (1) a training-free token ranking pipeline that leverages faithful cross-modal attention weights to estimate each video token's importance, and (2) a rank-supervised lightweight selector that is trained to replicate these rankings and filter redundant tokens. This generic approach can be seamlessly integrated into various VideoLLM architectures, such as LLaVA-Video, InternVL and Qwen-VL, serving as a plug-and-play module to extend their temporal context length. Empirically, FlexSelect delivers strong gains across multiple long-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover, it achieves significant speed-ups (for example, up to 9 times on a LLaVA-Video-7B model), highlighting FlexSelect's promise for efficient long-form video understanding. Project page available at: this https URL</li>
</ul>

<h3>Title: Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kinam Kim, Junha Hyung, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00996">https://arxiv.org/abs/2506.00996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00996">https://arxiv.org/pdf/2506.00996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00996]] Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models(https://arxiv.org/abs/2506.00996)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit this https URL</li>
</ul>

<h3>Title: LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers</h3>
<ul>
<li><strong>Authors: </strong>Changshun Wu, Tianyi Duan, Saddek Bensalem, Chih-Hong Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.00998">https://arxiv.org/abs/2506.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.00998">https://arxiv.org/pdf/2506.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.00998]] LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers(https://arxiv.org/abs/2506.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) improves performance on domain-specific tasks but can lead to overfitting, making them unreliable on out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD detection monitors to the LoRA layer using boxed abstraction to filter questions beyond the model's competence. Feature vectors from the fine-tuning data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a question is flagged as OoD if its feature vector falls outside all boxes. To improve interpretability and robustness, we introduce a regularization loss during fine-tuning that encourages paraphrased questions to stay close in the feature space, and the enlargement of the decision boundary is based on the feature variance within a cluster. Our method complements existing defenses by providing lightweight and interpretable OoD detection.</li>
</ul>

<h3>Title: Motion-Aware Concept Alignment for Consistent Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Tong Zhang, Juan C Leon Alcazar, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01004">https://arxiv.org/abs/2506.01004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01004">https://arxiv.org/pdf/2506.01004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01004]] Motion-Aware Concept Alignment for Consistent Video Editing(https://arxiv.org/abs/2506.01004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis.</li>
</ul>

<h3>Title: Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack</h3>
<ul>
<li><strong>Authors: </strong>Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, Jinjun Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01011">https://arxiv.org/abs/2506.01011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01011">https://arxiv.org/pdf/2506.01011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01011]] Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack(https://arxiv.org/abs/2506.01011)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.</li>
</ul>

<h3>Title: AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yuyuan Liu, Yuanhong Chen, Chong Wang, Junlin Han, Junde Wu, Can Peng, Jingkun Chen, Yu Tian, Gustavo Carneiro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01015">https://arxiv.org/abs/2506.01015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01015">https://arxiv.org/pdf/2506.01015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01015]] AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting(https://arxiv.org/abs/2506.01015)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model 2 (SAM2) exhibits strong generalisation for promptable segmentation in video clips; however, its integration with the audio modality remains underexplored. Existing approaches mainly follow two directions: (1) injecting adapters into the image encoder to receive audio signals, which incurs efficiency costs during prompt engineering, and (2) leveraging additional foundation models to generate visual prompts for the sounding objects, which are often imprecisely localised, leading to misguidance in SAM2. Moreover, these methods overlook the rich semantic interplay between hierarchical visual features and other modalities, resulting in suboptimal cross-modal fusion. In this work, we propose AuralSAM2, comprising the novel AuralFuser module, which externally attaches to SAM2 to integrate features from different modalities and generate feature-level prompts, guiding SAM2's decoder in segmenting sounding targets. Such integration is facilitated by a feature pyramid, further refining semantic understanding and enhancing object awareness in multimodal scenarios. Additionally, the audio-guided contrastive learning is introduced to explicitly align audio and visual representations and to also mitigate biases caused by dominant visual patterns. Results on public benchmarks show that our approach achieves remarkable improvements over the previous methods in the field. Code is available at this https URL.</li>
</ul>

<h3>Title: Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xudong Ma, Nantheera Anantrasirichai, Stefanos Bolomytis, Alin Achim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01025">https://arxiv.org/abs/2506.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01025">https://arxiv.org/pdf/2506.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01025]] Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models(https://arxiv.org/abs/2506.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal MR-US registration is critical for prostate cancer diagnosis. However, this task remains challenging due to significant modality discrepancies. Existing methods often fail to align critical boundaries while being overly sensitive to irrelevant details. To address this, we propose an anatomically coherent modality translation (ACMT) network based on a hierarchical feature disentanglement design. We leverage shallow-layer features for texture consistency and deep-layer features for boundary preservation. Unlike conventional modality translation methods that convert one modality into another, our ACMT introduces the customized design of an intermediate pseudo modality. Both MR and US images are translated toward this intermediate domain, effectively addressing the bottlenecks faced by traditional translation methods in the downstream registration task. Experiments demonstrate that our method mitigates modality-specific discrepancies while preserving crucial anatomical boundaries for accurate registration. Quantitative evaluations show superior modality similarity compared to state-of-the-art modality translation methods. Furthermore, downstream registration experiments confirm that our translated images achieve the best alignment performance, highlighting the robustness of our framework for multi-modal prostate image registration.</li>
</ul>

<h3>Title: NavBench: Probing Multimodal Large Language Models for Embodied Navigation</h3>
<ul>
<li><strong>Authors: </strong>Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie, Xinyu Wang, Qi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01031">https://arxiv.org/abs/2506.01031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01031">https://arxiv.org/pdf/2506.01031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01031]] NavBench: Probing Multimodal Large Language Models for Embodied Navigation(https://arxiv.org/abs/2506.01031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.</li>
</ul>

<h3>Title: Less is More: Local Intrinsic Dimensions of Contextual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Matthias Ruppik, Julius von Rohrscheidt, Carel van Niekerk, Michael Heck, Renato Vukovic, Shutong Feng, Hsien-chin Lin, Nurul Lubis, Bastian Rieck, Marcus Zibrowius, Milica Gašić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01034">https://arxiv.org/abs/2506.01034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01034">https://arxiv.org/pdf/2506.01034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01034]] Less is More: Local Intrinsic Dimensions of Contextual Language Models(https://arxiv.org/abs/2506.01034)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.</li>
</ul>

<h3>Title: Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution</h3>
<ul>
<li><strong>Authors: </strong>Shijun Shi, Jing Xu, Lijing Lu, Zhihang Li, Kai Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01037">https://arxiv.org/abs/2506.01037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01037">https://arxiv.org/pdf/2506.01037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01037]] Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution(https://arxiv.org/abs/2506.01037)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Existing diffusion-based video super-resolution (VSR) methods are susceptible to introducing complex degradations and noticeable artifacts into high-resolution videos due to their inherent randomness. In this paper, we propose a noise-robust real-world VSR framework by incorporating self-supervised learning and Mamba into pre-trained latent diffusion models. To ensure content consistency across adjacent frames, we enhance the diffusion model with a global spatio-temporal attention mechanism using the Video State-Space block with a 3D Selective Scan module, which reinforces coherence at an affordable computational cost. To further reduce artifacts in generated details, we introduce a self-supervised ControlNet that leverages HR features as guidance and employs contrastive learning to extract degradation-insensitive features from LR videos. Finally, a three-stage training strategy based on a mixture of HR-LR videos is proposed to stabilize VSR training. The proposed Self-supervised ControlNet with Spatio-Temporal Continuous Mamba based VSR algorithm achieves superior perceptual quality than state-of-the-arts on real-world VSR benchmark datasets, validating the effectiveness of the proposed model design and training strategies.</li>
</ul>

<h3>Title: ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Zuzheng Kuang, Haixia Bi, Chen Xu, Jian Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01040">https://arxiv.org/abs/2506.01040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01040">https://arxiv.org/pdf/2506.01040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01040]] ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification(https://arxiv.org/abs/2506.01040)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, polarimetric synthetic aperture radar (PolSAR) image classification has been greatly promoted by deep neural networks. However,current deep learning-based PolSAR classification methods encounter difficulties due to its dependence on extensive labeled data and the computational inefficiency of architectures like Transformers. This paper presents ECP-Mamba, an efficient framework integrating multi-scale self-supervised contrastive learning with a state space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation scarcity through a multi-scale predictive pretext task based on local-to-global feature correspondences, which uses a simplified self-distillation paradigm without negative sample pairs. To enhance computational efficiency,the Mamba architecture (a selective SSM) is first tailored for pixel-wise PolSAR classification task by designing a spiral scan strategy. This strategy prioritizes causally relevant features near the central pixel, leveraging the localized nature of pixel-wise classification tasks. Additionally, the lightweight Cross Mamba module is proposed to facilitates complementary multi-scale feature interaction with minimal overhead. Extensive experiments across four benchmark datasets demonstrate ECP-Mamba's effectiveness in balancing high accuracy with resource efficiency. On the Flevoland 1989 dataset, ECP-Mamba achieves state-of-the-art performance with an overall accuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of 99.62e-2. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Probing Neural Topology of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Zheng, Yuan Yuan, Yong Li, Paolo Santi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01042">https://arxiv.org/abs/2506.01042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01042">https://arxiv.org/pdf/2506.01042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01042]] Probing Neural Topology of Large Language Models(https://arxiv.org/abs/2506.01042)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural representations to interpretable semantics. However, how neurons functionally co-activate with each other to give rise to emergent capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity topology of LLM neurons and relating it to language generation performance. By analyzing internal neural graphs across diverse LLM families and scales, we discover a universal predictability of next-token prediction performance using only neural topology. This predictability is robust even when retaining just 1% of neuron connections or probing models after only 8 pretraining steps, highlighting the sparsity and early emergence of topological patterns. Further graph matching analysis suggests that, despite significant distinctions in architectures, parameters, and training data, different LLMs develop intricate and consistent neural topological structures that may form the foundation for their language generation abilities. Codes and data for the graph probing toolbox are released at this https URL.</li>
</ul>

<h3>Title: CHEER-Ekman: Fine-grained Embodied Emotion Classification</h3>
<ul>
<li><strong>Authors: </strong>Phan Anh Duong, Cat Luong, Divyesh Bommana, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01047">https://arxiv.org/abs/2506.01047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01047">https://arxiv.org/pdf/2506.01047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01047]] CHEER-Ekman: Fine-grained Embodied Emotion Classification(https://arxiv.org/abs/2506.01047)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotions manifest through physical experiences and bodily reactions, yet identifying such embodied emotions in text remains understudied. We present an embodied emotion classification dataset, CHEER-Ekman, extending the existing binary embodied emotion dataset with Ekman's six basic emotion categories. Using automatic best-worst scaling with large language models, we achieve performance superior to supervised approaches on our new dataset. Our investigation reveals that simplified prompting instructions and chain-of-thought reasoning significantly improve emotion recognition accuracy, enabling smaller models to achieve competitive performance with larger ones.</li>
</ul>

<h3>Title: Taming LLMs by Scaling Learning Rates with Gradient Grouping</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Juanxi Tian, Zedong Wang, Xin Jin, Zicheng Liu, Wentao Zhang, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01049">https://arxiv.org/abs/2506.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01049">https://arxiv.org/pdf/2506.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01049]] Taming LLMs by Scaling Learning Rates with Gradient Grouping(https://arxiv.org/abs/2506.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.</li>
</ul>

<h3>Title: No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Attila Szász, Balázs Bánhelyi, Márk Jelasity</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01054">https://arxiv.org/abs/2506.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01054">https://arxiv.org/pdf/2506.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01054]] No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks(https://arxiv.org/abs/2506.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The ultimate goal of verification is to guarantee the safety of deployed neural networks. Here, we claim that all the state-of-the-art verifiers we are aware of fail to reach this goal. Our key insight is that theoretical soundness (bounding the full-precision output while computing with floating point) does not imply practical soundness (bounding the floating point output in a potentially stochastic environment). We prove this observation for the approaches that are currently used to achieve provable theoretical soundness, such as interval analysis and its variants. We also argue that achieving practical soundness is significantly harder computationally. We support our claims empirically as well by evaluating several well-known verification methods. To mislead the verifiers, we create adversarial networks that detect and exploit features of the deployment environment, such as the order and precision of floating point operations. We demonstrate that all the tested verifiers are vulnerable to our new deployment-specific attacks, which proves that they are not practically sound.</li>
</ul>

<h3>Title: Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution</h3>
<ul>
<li><strong>Authors: </strong>Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, Fabrizio Gilardi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01055">https://arxiv.org/abs/2506.01055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01055">https://arxiv.org/pdf/2506.01055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01055]] Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution(https://arxiv.org/abs/2506.01055)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.</li>
</ul>

<h3>Title: XAI-Units: Benchmarking Explainability Methods with Unit Tests</h3>
<ul>
<li><strong>Authors: </strong>Jun Rui Lee, Sadegh Emami, Michael David Hollins, Timothy C. H. Wong, Carlos Ignacio Villalobos Sánchez, Francesca Toni, Dekai Zhang, Adam Dejl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01059">https://arxiv.org/abs/2506.01059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01059">https://arxiv.org/pdf/2506.01059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01059]] XAI-Units: Benchmarking Explainability Methods with Unit Tests(https://arxiv.org/abs/2506.01059)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.</li>
</ul>

<h3>Title: AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Dahyeon Kye, Changhyun Roh, Sukhun Ko, Chanho Eom, Jihyong Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01061">https://arxiv.org/abs/2506.01061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01061">https://arxiv.org/pdf/2506.01061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01061]] AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation(https://arxiv.org/abs/2506.01061)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task that synthesizes intermediate frames between existing ones while maintaining spatial and temporal coherence. VFI techniques have evolved from classical motion compensation-based approach to deep learning-based approach, including kernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently diffusion model-based approach. We introduce AceVFI, the most comprehensive survey on VFI to date, covering over 250+ papers across these approaches. We systematically organize and describe VFI methodologies, detailing the core principles, design assumptions, and technical characteristics of each approach. We categorize the learning paradigm of VFI methods namely, Center-Time Frame Interpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze key challenges of VFI such as large motion, occlusion, lighting variation, and non-linear motion. In addition, we review standard datasets, loss functions, evaluation metrics. We examine applications of VFI including event-based, cartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by outlining promising future research directions to support continued progress in the field. This survey aims to serve as a unified reference for both newcomers and experts seeking a deep understanding of modern VFI landscapes.</li>
</ul>

<h3>Title: Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01064">https://arxiv.org/abs/2506.01064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01064">https://arxiv.org/pdf/2506.01064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01064]] Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs(https://arxiv.org/abs/2506.01064)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive "fighting fire with fire" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available.</li>
</ul>

<h3>Title: Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety</h3>
<ul>
<li><strong>Authors: </strong>Malik A. Altayar, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon, Wesam T. Almagharbeh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01069">https://arxiv.org/abs/2506.01069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01069">https://arxiv.org/pdf/2506.01069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01069]] Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety(https://arxiv.org/abs/2506.01069)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, biometric</a></li>
<li><strong>Abstract: </strong>Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.</li>
</ul>

<h3>Title: IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Chen, Guopeng Lin, Haoyu Niu, Lushan Song, Chengxun Hong, Weili Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01072">https://arxiv.org/abs/2506.01072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01072">https://arxiv.org/pdf/2506.01072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01072]] IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning(https://arxiv.org/abs/2506.01072)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Vertical privacy-preserving machine learning (vPPML) enables multiple parties to train models on their vertically distributed datasets while keeping datasets private. In vPPML, it is critical to perform the secure dataset join, which aligns features corresponding to intersection IDs across datasets and forms a secret-shared and joint training dataset. However, existing methods for this step could be impractical due to: (1) they are insecure when they expose intersection IDs; or (2) they rely on a strong trust assumption requiring a non-colluding auxiliary server; or (3) they are limited to the two-party setting. This paper proposes IDCloak, the first practical secure multi-party dataset join framework for vPPML that keeps IDs private without a non-colluding auxiliary server. IDCloak consists of two protocols: (1) a circuit-based multi-party private set intersection protocol (cmPSI), which obtains secret-shared flags indicating intersection IDs via an optimized communication structure combining OKVS and OPRF; (2) a secure multi-party feature alignment protocol, which obtains the secret-shared and joint dataset using secret-shared flags, via our proposed efficient secure shuffle protocol. Experiments show that: (1) compared to the state-of-the-art secure two-party dataset join framework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party setting and comparable performance when the party number increases; (2) compared to the state-of-the-art cmPSI protocol under honest majority, our proposed cmPSI protocol provides a stronger security guarantee (dishonest majority) while improving efficiency by up to $7.78\times$ in time and $8.73\times$ in communication sizes; (3) our proposed secure shuffle protocol outperforms the state-of-the-art shuffle protocol by up to $138.34\times$ in time and $132.13\times$ in communication sizes.</li>
</ul>

<h3>Title: A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingzhe Hu, Yuan Gao, Yuheng Li, Ricahrd LJ Qiu, Chih-Wei Chang, Keyur D. Shah, Priyanka Kapoor, Beth Bradshaw, Yuan Shao, Justin Roper, Jill Remick, Zhen Tian, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01073">https://arxiv.org/abs/2506.01073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01073">https://arxiv.org/pdf/2506.01073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01073]] A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning(https://arxiv.org/abs/2506.01073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Purpose: Accurate segmentation of clinical target volumes (CTV) and organs-at-risk is crucial for optimizing gynecologic brachytherapy (GYN-BT) treatment planning. However, anatomical variability, low soft-tissue contrast in CT imaging, and limited annotated datasets pose significant challenges. This study presents GynBTNet, a novel multi-stage learning framework designed to enhance segmentation performance through self-supervised pretraining and hierarchical fine-tuning strategies. Methods: GynBTNet employs a three-stage training strategy: (1) self-supervised pretraining on large-scale CT datasets using sparse submanifold convolution to capture robust anatomical representations, (2) supervised fine-tuning on a comprehensive multi-organ segmentation dataset to refine feature extraction, and (3) task-specific fine-tuning on a dedicated GYN-BT dataset to optimize segmentation performance for clinical applications. The model was evaluated against state-of-the-art methods using the Dice Similarity Coefficient (DSC), 95th percentile Hausdorff Distance (HD95), and Average Surface Distance (ASD). Results: Our GynBTNet achieved superior segmentation performance, significantly outperforming nnU-Net and Swin-UNETR. Notably, it yielded a DSC of 0.837 +/- 0.068 for CTV, 0.940 +/- 0.052 for the bladder, 0.842 +/- 0.070 for the rectum, and 0.871 +/- 0.047 for the uterus, with reduced HD95 and ASD compared to baseline models. Self-supervised pretraining led to consistent performance improvements, particularly for structures with complex boundaries. However, segmentation of the sigmoid colon remained challenging, likely due to anatomical ambiguities and inter-patient variability. Statistical significance analysis confirmed that GynBTNet's improvements were significant compared to baseline models.</li>
</ul>

<h3>Title: How Programming Concepts and Neurons Are Shared in Code Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Kargaran, Yihong Liu, François Yvon, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01074">https://arxiv.org/abs/2506.01074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01074">https://arxiv.org/pdf/2506.01074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01074]] How Programming Concepts and Neurons Are Shared in Code Language Models(https://arxiv.org/abs/2506.01074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at this https URL.</li>
</ul>

<h3>Title: GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking</h3>
<ul>
<li><strong>Authors: </strong>Yufei Zhan, Ziheng Wu, Yousong Zhu, Rongkun Xue, Ruipu Luo, Zhenghao Chen, Can Zhang, Yifan Li, Zhentao He, Zheming Yang, Ming Tang, Minghui Qiu, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01078">https://arxiv.org/abs/2506.01078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01078">https://arxiv.org/pdf/2506.01078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01078]] GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking(https://arxiv.org/abs/2506.01078)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at this https URL.</li>
</ul>

<h3>Title: zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Saibo Geng, Nathan Ranchin, Yunzhen yao, Maxime Peyrard, Chris Wendler, Michael Gastpar, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01084">https://arxiv.org/abs/2506.01084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01084">https://arxiv.org/pdf/2506.01084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01084]] zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression(https://arxiv.org/abs/2506.01084)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\%, with significant improvements in inference latency.</li>
</ul>

<h3>Title: Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements</h3>
<ul>
<li><strong>Authors: </strong>Metehan Oguz, Yavuz Bakman, Duygu Nur Yaldiz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01089">https://arxiv.org/abs/2506.01089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01089">https://arxiv.org/pdf/2506.01089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01089]] Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements(https://arxiv.org/abs/2506.01089)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: this https URL.</li>
</ul>

<h3>Title: Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lei Lei, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Tong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01097">https://arxiv.org/abs/2506.01097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01097">https://arxiv.org/pdf/2506.01097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01097]] Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective(https://arxiv.org/abs/2506.01097)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Existing Multimodal Large Language Models (MLLMs) process a large number of visual tokens, leading to significant computational costs and inefficiency. Previous works generally assume that all visual tokens are necessary in the shallow layers of LLMs, and therefore token compression typically occurs in intermediate layers. In contrast, our study reveals an interesting insight: with proper selection, token compression is feasible at the input stage of LLM with negligible performance loss. Specifically, we reveal that explainability methods can effectively evaluate the importance of each visual token with respect to the given instruction, which can well guide the token compression. Furthermore, we propose to learn a mapping from the attention map of the first LLM layer to the explanation results, thereby avoiding the need for a full inference pass and facilitating practical deployment. Interestingly, this mapping can be learned using a simple and lightweight convolutional network, whose training is efficient and independent of MLLMs. Extensive experiments on 10 image and video benchmarks across three leading MLLMs (Qwen2-VL, LLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach, e.g., pruning 50% visual tokens while retaining more than 96% of the original performance across all benchmarks for all these three MLLMs. It also exhibits strong generalization, even when the number of tokens in inference far exceeds that used in training.</li>
</ul>

<h3>Title: Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Steven Robinson, Antonio Carlos Rivera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01104">https://arxiv.org/abs/2506.01104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01104">https://arxiv.org/pdf/2506.01104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01104]] Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection(https://arxiv.org/abs/2506.01104)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The pervasive deployment of large language models (LLMs) in conversational AI systems has revolutionized information access, yet their propensity for generating factually unsupported or hallucinated responses remains a critical impediment to trustworthiness and widespread adoption. This paper introduces Reinforced Unanswerability Learning (RUL), a novel hybrid training paradigm designed to imbue LLMs with the intrinsic capability to accurately detect unanswerable questions and generate reliably appropriate responses. Unlike conventional approaches that rely on external classifiers or simple prompting, RUL integrates a discriminative unanswerability prediction head with the LLM's generative core, guided by a multi-stage learning strategy. This includes supervised fine-tuning on a novel, richly annotated dataset, Enhanced-CAsT-Answerability (ECA), which features hierarchical answerability labels and ground-truth refusal responses. Crucially, RUL incorporates a subsequent reinforcement learning with human feedback (RLHF) phase to refine the nuance, helpfulness, and informativeness of refusal responses. Extensive experiments demonstrate RUL's superior performance, achieving significantly higher accuracy in unanswerability detection across sentence, paragraph, and ranking levels, and substantially increasing the generation of appropriate refusals for unanswerable queries, alongside strong performance on answerable questions. Human evaluations further corroborate RUL's effectiveness, highlighting a marked improvement in perceived helpfulness and trustworthiness, ultimately paving the way for more reliable and user-centric conversational AI.</li>
</ul>

<h3>Title: CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01109">https://arxiv.org/abs/2506.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01109">https://arxiv.org/pdf/2506.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01109]] CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting(https://arxiv.org/abs/2506.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate fruit counting in real-world agricultural environments is a longstanding challenge due to visual occlusions, semantic ambiguity, and the high computational demands of 3D reconstruction. Existing methods based on neural radiance fields suffer from low inference speed, limited generalization, and lack support for open-set semantic control. This paper presents FruitLangGS, a real-time 3D fruit counting framework that addresses these limitations through spatial reconstruction, semantic embedding, and language-guided instance estimation. FruitLangGS first reconstructs orchard-scale scenes using an adaptive Gaussian splatting pipeline with radius-aware pruning and tile-based rasterization for efficient rendering. To enable semantic control, each Gaussian encodes a compressed CLIP-aligned language embedding, forming a compact and queryable 3D representation. At inference time, prompt-based semantic filtering is applied directly in 3D space, without relying on image-space segmentation or view-level fusion. The selected Gaussians are then converted into dense point clouds via distribution-aware sampling and clustered to estimate fruit counts. Experimental results on real orchard data demonstrate that FruitLangGS achieves higher rendering speed, semantic flexibility, and counting accuracy compared to prior approaches, offering a new perspective for language-driven, real-time neural rendering across open-world scenarios.</li>
</ul>

<h3>Title: Reconsidering LLM Uncertainty Estimation Methods in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yavuz Bakman, Duygu Nur Yaldiz, Sungmin Kang, Tuo Zhang, Baturalp Buyukates, Salman Avestimehr, Sai Praneeth Karimireddy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01114">https://arxiv.org/abs/2506.01114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01114">https://arxiv.org/pdf/2506.01114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01114]] Reconsidering LLM Uncertainty Estimation Methods in the Wild(https://arxiv.org/abs/2506.01114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: this https URL.</li>
</ul>

<h3>Title: Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yihe Dong, Lorenzo Noci, Mikhail Khodak, Mufan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01115">https://arxiv.org/abs/2506.01115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01115">https://arxiv.org/pdf/2506.01115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01115]] Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer(https://arxiv.org/abs/2506.01115)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Transformer architecture is central to the success of modern Large Language Models (LLMs), in part due to its surprising ability to perform a wide range of algorithmic tasks -- including mathematical reasoning, memorization, and retrieval -- using only gradient-based training on next-token prediction. While the core component of a Transformer is the self-attention mechanism, we question how much, and which aspects, of the performance gains can be attributed to it. To this end, we compare standard Transformers to variants in which either the multi-layer perceptron (MLP) layers or the attention projectors (queries and keys) are frozen at initialization. To further isolate the contribution of attention, we introduce MixiT -- the Mixing Transformer -- a simplified, principled model in which the attention coefficients are entirely random and fixed at initialization, eliminating any input-dependent computation or learning in attention. Surprisingly, we find that MixiT matches the performance of fully trained Transformers on various algorithmic tasks, especially those involving basic arithmetic or focusing heavily on memorization. For retrieval-based tasks, we observe that having input-dependent attention coefficients is consistently beneficial, while MixiT underperforms. We attribute this failure to its inability to form specialized circuits such as induction heads -- a specific circuit known to be crucial for learning and exploiting repeating patterns in input sequences. Even more interestingly, we find that attention with frozen key and query projectors is not only able to form induction heads, but can also perform competitively on language modeling. Our results underscore the importance of architectural heterogeneity, where distinct components contribute complementary inductive biases crucial for solving different classes of tasks.</li>
</ul>

<h3>Title: Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Pimchanok Sukjai, Apiradee Boonmee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01118">https://arxiv.org/abs/2506.01118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01118">https://arxiv.org/pdf/2506.01118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01118]] Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation(https://arxiv.org/abs/2506.01118)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The escalating demand for medical image interpretation underscores the critical need for advanced artificial intelligence solutions to enhance the efficiency and accuracy of radiological diagnoses. This paper introduces CXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model specifically engineered for automated chest X-ray (CXR) report generation. We propose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning (CGAFT), which meticulously integrates expert clinical feedback into an adversarial learning framework to mitigate factual inconsistencies and improve diagnostic precision. Complementing this, our Knowledge Graph Augmentation Module (KGAM) acts as an inference-time safeguard, dynamically verifying generated medical statements against authoritative knowledge bases to minimize hallucinations and ensure standardized terminology. Leveraging a comprehensive dataset of millions of paired CXR images and expert reports, our experiments demonstrate that CXR-PathFinder significantly outperforms existing state-of-the-art medical vision-language models across various quantitative metrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14): 59.5). Furthermore, blinded human evaluation by board-certified radiologists confirms CXR-PathFinder's superior clinical utility, completeness, and accuracy, establishing its potential as a reliable and efficient aid for radiological practice. The developed method effectively balances high diagnostic fidelity with computational efficiency, providing a robust solution for automated medical report generation.</li>
</ul>

<h3>Title: MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows</h3>
<ul>
<li><strong>Authors: </strong>Hong Nguyen, Dung Tran, Hieu Hoang, Phong Nguyen, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01119">https://arxiv.org/abs/2506.01119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01119">https://arxiv.org/pdf/2506.01119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01119]] MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows(https://arxiv.org/abs/2506.01119)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Many motion-centric video analysis tasks, such as atomic actions, detecting atypical motor behavior in individuals with autism, or analyzing articulatory motion in real-time MRI of human speech, require efficient and interpretable temporal modeling. Capturing temporal dynamics is a central challenge in video analysis, often requiring significant computational resources and fine-grained annotations that are not widely available. This paper presents MOOSE (Motion Flow Over Spatial Space), a novel temporally-centric video encoder explicitly integrating optical flow with spatial embeddings to model temporal information efficiently, inspired by human perception of motion. Unlike prior models, MOOSE takes advantage of rich, widely available pre-trained visual and optical flow encoders instead of training video models from scratch. This significantly reduces computational complexity while enhancing temporal interpretability. Our primary contributions includes (1) proposing a computationally efficient temporally-centric architecture for video understanding (2) demonstrating enhanced interpretability in modeling temporal dynamics; and (3) achieving state-of-the-art performance on diverse benchmarks, including clinical, medical, and standard action recognition datasets, confirming the broad applicability and effectiveness of our approach.</li>
</ul>

<h3>Title: Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</h3>
<ul>
<li><strong>Authors: </strong>Jacob K. Christopher, Michael Cardei, Jinhao Liang, Ferdinando Fioretto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01121">https://arxiv.org/abs/2506.01121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01121">https://arxiv.org/pdf/2506.01121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01121]] Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation(https://arxiv.org/abs/2506.01121)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.</li>
</ul>

<h3>Title: ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection</h3>
<ul>
<li><strong>Authors: </strong>Yiliang Chen, Zhixi Li, Cheng Xu, Alex Qinyang Liu, Xuemiao Xu, Jeremy Yuen-Chun Teoh, Shengfeng He, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01130">https://arxiv.org/abs/2506.01130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01130">https://arxiv.org/pdf/2506.01130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01130]] ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection(https://arxiv.org/abs/2506.01130)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Surgical triplet detection has emerged as a pivotal task in surgical video analysis, with significant implications for performance assessment and the training of novice surgeons. However, existing datasets such as CholecT50 exhibit critical limitations: they lack precise spatial bounding box annotations, provide inconsistent and clinically ungrounded temporal labels, and rely on a single data source, which limits model this http URL address these shortcomings, we introduce ProstaTD, a large-scale, multi-institutional dataset for surgical triplet detection, developed from the technically demanding domain of robot-assisted prostatectomy. ProstaTD offers clinically defined temporal boundaries and high-precision bounding box annotations for each structured triplet action. The dataset comprises 60,529 video frames and 165,567 annotated triplet instances, collected from 21 surgeries performed across multiple institutions, reflecting a broad range of surgical practices and intraoperative conditions. The annotation process was conducted under rigorous medical supervision and involved more than 50 contributors, including practicing surgeons and medically trained annotators, through multiple iterative phases of labeling and verification. ProstaTD is the largest and most diverse surgical triplet dataset to date, providing a robust foundation for fair benchmarking, the development of reliable surgical AI systems, and scalable tools for procedural training.</li>
</ul>

<h3>Title: From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Asım Ersoy, Basel Mousi, Shammur Chowdhury, Firoj Alam, Fahim Dalvi, Nadir Durrani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01133">https://arxiv.org/abs/2506.01133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01133">https://arxiv.org/pdf/2506.01133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01133]] From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models(https://arxiv.org/abs/2506.01133)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.</li>
</ul>

<h3>Title: FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ariel Shaulov, Itay Hazan, Lior Wolf, Hila Chefer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01144">https://arxiv.org/abs/2506.01144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01144">https://arxiv.org/pdf/2506.01144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01144]] FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation(https://arxiv.org/abs/2506.01144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce \textbf{FlowMo}, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.</li>
</ul>

<h3>Title: A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition</h3>
<ul>
<li><strong>Authors: </strong>Prerak Srivastava, Giulio Corallo, Sergey Rybalko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01147">https://arxiv.org/abs/2506.01147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01147">https://arxiv.org/pdf/2506.01147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01147]] A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition(https://arxiv.org/abs/2506.01147)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>System-generated logs are typically converted into categorical log templates through parsing. These templates are crucial for generating actionable insights in various downstream tasks. However, existing parsers often fail to capture fine-grained template details, leading to suboptimal accuracy and reduced utility in downstream tasks requiring precise pattern identification. We propose a character-level log parser utilizing a novel neural architecture that aggregates character embeddings. Our approach estimates a sequence of binary-coded decimals to achieve highly granular log templates extraction. Our low-resource character-level parser, tested on revised Loghub-2k and a manually annotated industrial dataset, matches LLM-based parsers in accuracy while outperforming semantic parsers in efficiency.</li>
</ul>

<h3>Title: Earley-Driven Dynamic Pruning for Efficient Structured Decoding</h3>
<ul>
<li><strong>Authors: </strong>Xintong Sun, Chi Wei, Minghao Tian, Shiwen Ni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01151">https://arxiv.org/abs/2506.01151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01151">https://arxiv.org/pdf/2506.01151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01151]] Earley-Driven Dynamic Pruning for Efficient Structured Decoding(https://arxiv.org/abs/2506.01151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at this https URL.</li>
</ul>

<h3>Title: FORT: Forward-Only Regression Training of Normalizing Flows</h3>
<ul>
<li><strong>Authors: </strong>Danyal Rehman, Oscar Davis, Jiarui Lu, Jian Tang, Michael Bronstein, Yoshua Bengio, Alexander Tong, Avishek Joey Bose</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01158">https://arxiv.org/abs/2506.01158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01158">https://arxiv.org/pdf/2506.01158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01158]] FORT: Forward-Only Regression Training of Normalizing Flows(https://arxiv.org/abs/2506.01158)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to neural dynamical systems that encompass modern large-scale diffusion and flow matching models. Despite the scalability of training, the generation of high-quality samples and their corresponding likelihood under the model requires expensive numerical simulation -- inhibiting adoption in numerous scientific applications such as equilibrium sampling of molecular systems. In this paper, we revisit classical normalizing flows as one-step generative models with exact likelihoods and propose a novel, scalable training objective that does not require computing the expensive change of variable formula used in conventional maximum likelihood training. We propose Forward-Only Regression Training (FORT), a simple $\ell_2$-regression objective that maps prior samples under our flow to specifically chosen targets. We demonstrate that FORT supports a wide class of targets, such as optimal transport targets and targets from pre-trained continuous-time normalizing flows (CNF). We further demonstrate that by using CNF targets, our one-step flows allow for larger-scale training that exceeds the performance and stability of maximum likelihood training, while unlocking a broader class of architectures that were previously challenging to train. Empirically, we elucidate that our trained flows can perform equilibrium conformation sampling in Cartesian coordinates of alanine dipeptide, alanine tripeptide, and alanine tetrapeptide.</li>
</ul>

<h3>Title: Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Andrew Smith, Erhan Guven</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01177">https://arxiv.org/abs/2506.01177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01177">https://arxiv.org/pdf/2506.01177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01177]] Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation(https://arxiv.org/abs/2506.01177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture for generative adversarial networks (GANs) in molecular discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, using over 60% fewer parameters. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.</li>
</ul>

<h3>Title: Doubly Robust Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, Chengchun Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01183">https://arxiv.org/abs/2506.01183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01183">https://arxiv.org/pdf/2506.01183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01183]] Doubly Robust Alignment for Large Language Models(https://arxiv.org/abs/2506.01183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at this https URL</li>
</ul>

<h3>Title: LAQuer: Localized Attribution Queries in Content-grounded Generation</h3>
<ul>
<li><strong>Authors: </strong>Eran Hirsch, Aviv Slobodkin, David Wan, Elias Stengel-Eskin, Mohit Bansal, Ido Dagan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01187">https://arxiv.org/abs/2506.01187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01187">https://arxiv.org/pdf/2506.01187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01187]] LAQuer: Localized Attribution Queries in Content-grounded Generation(https://arxiv.org/abs/2506.01187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Grounded text generation models often produce content that deviates from their source material, requiring user verification to ensure accuracy. Existing attribution methods associate entire sentences with source documents, which can be overwhelming for users seeking to fact-check specific claims. In contrast, existing sub-sentence attribution methods may be more precise but fail to align with users' interests. In light of these limitations, we introduce Localized Attribution Queries (LAQuer), a new task that localizes selected spans of generated output to their corresponding source spans, allowing fine-grained and user-directed attribution. We compare two approaches for the LAQuer task, including prompting large language models (LLMs) and leveraging LLM internal representations. We then explore a modeling framework that extends existing attributed text generation methods to LAQuer. We evaluate this framework across two grounded text generation tasks: Multi-document Summarization (MDS) and Long-form Question Answering (LFQA). Our findings show that LAQuer methods significantly reduce the length of the attributed text. Our contributions include: (1) proposing the LAQuer task to enhance attribution usability, (2) suggesting a modeling framework and benchmarking multiple baselines, and (3) proposing a new evaluation setting to promote future research on localized attribution in content-grounded generation.</li>
</ul>

<h3>Title: SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data</h3>
<ul>
<li><strong>Authors: </strong>Emmanuel Hartman, Nicolas Charon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, math.DG, math.FA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01189">https://arxiv.org/abs/2506.01189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01189">https://arxiv.org/pdf/2506.01189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01189]] SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data(https://arxiv.org/abs/2506.01189)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite progress in the rapidly developing field of geometric deep learning, performing statistical analysis on geometric data--where each observation is a shape such as a curve, graph, or surface--remains challenging due to the non-Euclidean nature of shape spaces, which are defined as equivalence classes under invariance groups. Building machine learning frameworks that incorporate such invariances, notably to shape parametrization, is often crucial to ensure generalizability of the trained models to new observations. This work proposes SVarM to exploit varifold representations of shapes as measures and their duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$. This method provides a general framework akin to linear support vector machines but operating instead over the infinite-dimensional space of varifolds. We develop classification and regression models on shape datasets by introducing a neural network-based representation of the trainable test function $h$. This approach demonstrates strong performance and robustness across various shape graph and surface datasets, achieving results comparable to state-of-the-art methods while significantly reducing the number of trainable parameters.</li>
</ul>

<h3>Title: Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Madhavendra Thakur</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01190">https://arxiv.org/abs/2506.01190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01190">https://arxiv.org/pdf/2506.01190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01190]] Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages(https://arxiv.org/abs/2506.01190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) struggle with culturally-specific reasoning tasks, particularly in low-resource languages, hindering their global applicability. Addressing this gap is crucial for equitable AI deployment. We introduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting strategy that combines dense vector retrieval of cultural context with explicit reasoning sequences. Our extensive experiments on Yoruba proverb interpretation demonstrate that CG-CoT provides significantly higher culturally-aligned accuracy and depth than traditional prompting methods, validated through both automated metrics and LLM-based evaluations. Notably, we uncover stark disparities between token-level translation metrics like BLEU and human-judged cultural relevance, suggesting a rethinking of evaluation approaches for low-resource NLP.</li>
</ul>

<h3>Title: FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA</h3>
<ul>
<li><strong>Authors: </strong>Divyansh Jhunjhunwala, Arian Raje, Madan Ravi Ganesh, Chaithanya Kumar Mummadi, Chaoqun Dong, Jiawei Zhou, Wan-Yi Lin, Gauri Joshi, Zhenzhen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01194">https://arxiv.org/abs/2506.01194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01194">https://arxiv.org/pdf/2506.01194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01194]] FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA(https://arxiv.org/abs/2506.01194)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines.</li>
</ul>

<h3>Title: Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures</h3>
<ul>
<li><strong>Authors: </strong>Mark Muchane, Sean Richardson, Kiho Park, Victor Veitch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01197">https://arxiv.org/abs/2506.01197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01197">https://arxiv.org/pdf/2506.01197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01197]] Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures(https://arxiv.org/abs/2506.01197)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.</li>
</ul>

<h3>Title: Perceptual Inductive Bias Is What You Need Before Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianqin Li, Junru Zhao, Dunhan Jiang, Shenghao Wu, Alan Ramirez, Tai Sing Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01201">https://arxiv.org/abs/2506.01201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01201">https://arxiv.org/pdf/2506.01201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01201]] Perceptual Inductive Bias Is What You Need Before Contrastive Learning(https://arxiv.org/abs/2506.01201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>David Marr's seminal theory of human perception stipulates that visual processing is a multi-stage process, prioritizing the derivation of boundary and surface properties before forming semantic object representations. In contrast, contrastive representation learning frameworks typically bypass this explicit multi-stage approach, defining their objective as the direct learning of a semantic representation space for objects. While effective in general contexts, this approach sacrifices the inductive biases of vision, leading to slower convergence speed and learning shortcut resulting in texture bias. In this work, we demonstrate that leveraging Marr's multi-stage theory-by first constructing boundary and surface-level representations using perceptual constructs from early visual processing stages and subsequently training for object semantics-leads to 2x faster convergence on ResNet18, improved final representations on semantic segmentation, depth estimation, and object recognition, and enhanced robustness and out-of-distribution capability. Together, we propose a pretraining stage before the general contrastive representation pretraining to further enhance the final representation quality and reduce the overall convergence time via inductive bias from human vision systems.</li>
</ul>

<h3>Title: Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Muzammil Behzad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01203">https://arxiv.org/abs/2506.01203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01203">https://arxiv.org/pdf/2506.01203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01203]] Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition(https://arxiv.org/abs/2506.01203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial expression recognition (FER) is a fundamental task in affective computing with applications in human-computer interaction, mental health analysis, and behavioral understanding. In this paper, we propose SMILE-VLM, a self-supervised vision-language model for 3D/4D FER that unifies multiview visual representation learning with natural language supervision. SMILE-VLM learns robust, semantically aligned, and view-invariant embeddings by proposing three core components: multiview decorrelation via a Barlow Twins-style loss, vision-language contrastive alignment, and cross-modal redundancy minimization. Our framework achieves the state-of-the-art performance on multiple benchmarks. We further extend SMILE-VLM to the task of 4D micro-expression recognition (MER) to recognize the subtle affective cues. The extensive results demonstrate that SMILE-VLM not only surpasses existing unsupervised methods but also matches or exceeds supervised baselines, offering a scalable and annotation-efficient solution for expressive facial behavior understanding.</li>
</ul>

<h3>Title: Trick or Neat: Adversarial Ambiguity and Language Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Antonia Karamolegkou, Oliver Eberle, Phillip Rust, Carina Kauf, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01205">https://arxiv.org/abs/2506.01205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01205">https://arxiv.org/pdf/2506.01205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01205]] Trick or Neat: Adversarial Ambiguity and Language Model Evaluation(https://arxiv.org/abs/2506.01205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting ambiguity is important for language understanding, including uncertainty estimation, humour detection, and processing garden path sentences. We assess language models' sensitivity to ambiguity by introducing an adversarial ambiguity dataset that includes syntactic, lexical, and phonological ambiguities along with adversarial variations (e.g., word-order changes, synonym replacements, and random-based alterations). Our findings show that direct prompting fails to robustly identify ambiguity, while linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90\%. Our results offer insights into the prompting paradigm and how language models encode ambiguity at different layers. We release both our code and data: this https URL.</li>
</ul>

<h3>Title: Mamba Drafters for Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Daewon Choi, Seunghyuk Oh, Saket Dingliwal, Jihoon Tack, Kyuyoung Kim, Woomin Song, Seojin Kim, Insu Han, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01206">https://arxiv.org/abs/2506.01206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01206">https://arxiv.org/pdf/2506.01206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01206]] Mamba Drafters for Speculative Decoding(https://arxiv.org/abs/2506.01206)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding has emerged as a promising approach to accelerating large language model (LLM) generation using a fast drafter while maintaining alignment with the target model's distribution. However, existing approaches face a trade-off: external drafters offer flexibility but can suffer from slower drafting, while self-speculation methods use drafters tailored to the target model but require re-training. In this paper, we introduce novel drafters based on Mamba, a state-of-the-art state space model (SSM), as a solution that combines the best aspects of both approaches. By leveraging the linear structure of SSMs, our approach avoids the quadratic complexity inherent in traditional Transformer-based methods, enabling faster drafting and lower memory usage while maintaining the flexibility to work across different target models. We further enhance efficiency with a novel test-time tree search algorithm for generating high-quality draft candidates. Our empirical evaluation demonstrates that Mamba-based drafters not only outperform existing external drafting methods but are also comparable to state-of-the-art self-speculation approaches while using less memory and maintaining their cross-model adaptability.</li>
</ul>

<h3>Title: Multiresolution Analysis and Statistical Thresholding on Dynamic Networks</h3>
<ul>
<li><strong>Authors: </strong>Raphaël Romero, Tijl De Bie, Nick Heard, Alexander Modell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01208">https://arxiv.org/abs/2506.01208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01208">https://arxiv.org/pdf/2506.01208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01208]] Multiresolution Analysis and Statistical Thresholding on Dynamic Networks(https://arxiv.org/abs/2506.01208)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Detecting structural change in dynamic network data has wide-ranging applications. Existing approaches typically divide the data into time bins, extract network features within each bin, and then compare these features over time. This introduces an inherent tradeoff between temporal resolution and the statistical stability of the extracted features. Despite this tradeoff, reminiscent of time-frequency tradeoffs in signal processing, most methods rely on a fixed temporal resolution. Choosing an appropriate resolution parameter is typically difficult and can be especially problematic in domains like cybersecurity, where anomalous behavior may emerge at multiple time scales. We address this challenge by proposing ANIE (Adaptive Network Intensity Estimation), a multi-resolution framework designed to automatically identify the time scales at which network structure evolves, enabling the joint detection of both rapid and gradual changes. Modeling interactions as Poisson processes, our method proceeds in two steps: (1) estimating a low-dimensional subspace of node behavior, and (2) deriving a set of novel empirical affinity coefficients that quantify change in interaction intensity between latent factors and support statistical testing for structural change across time scales. We provide theoretical guarantees for subspace estimation and the asymptotic behavior of the affinity coefficients, enabling model-based change detection. Experiments on synthetic networks show that ANIE adapts to the appropriate time resolution and is able to capture sharp structural changes while remaining robust to noise. Furthermore, applications to real-world data showcase the practical benefits of ANIE's multiresolution approach to detecting structural change over fixed resolution methods.</li>
</ul>

<h3>Title: On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01213">https://arxiv.org/abs/2506.01213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01213">https://arxiv.org/pdf/2506.01213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01213]] On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective(https://arxiv.org/abs/2506.01213)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis.</li>
</ul>

<h3>Title: Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Woomin Song, Sai Muralidhar Jayanthi, Srikanth Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01215">https://arxiv.org/abs/2506.01215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01215">https://arxiv.org/pdf/2506.01215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01215]] Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers(https://arxiv.org/abs/2506.01215)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.</li>
</ul>

<h3>Title: Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization</h3>
<ul>
<li><strong>Authors: </strong>Naoyuki Shimizu, Masaki Hashimoto</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01220">https://arxiv.org/abs/2506.01220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01220">https://arxiv.org/pdf/2506.01220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01220]] Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization(https://arxiv.org/abs/2506.01220)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cybersecurity teams face an overwhelming vulnerability crisis: with 25,000+ new CVEs disclosed annually, traditional CVSS-based prioritization requires addressing 60% of all vulnerabilities while correctly identifying only 20% of those actually exploited. We propose Vulnerability Management Chaining, an integrated decision tree framework combining historical exploitation evidence (KEV), predictive threat modeling (EPSS), and technical impact assessment (CVSS) to transform vulnerability management from reactive patching to strategic threat-driven prioritization. Experimental validation using 28,377 real-world vulnerabilities demonstrates 14-18 fold efficiency improvements while maintaining 85%+ coverage of actual threats. Organizations can reduce urgent remediation workload by 95% (from ~16,000 to ~850 vulnerabilities). The integration identifies 57 additional exploited vulnerabilities that neither KEV nor EPSS captures individually. Our framework uses exclusively open-source data, democratizing advanced vulnerability management regardless of budget or expertise. This research establishes the first empirically validated methodology for systematic vulnerability management integration, with immediate applicability across diverse organizational contexts.</li>
</ul>

<h3>Title: Dirty and Clean-Label attack detection using GAN discriminators</h3>
<ul>
<li><strong>Authors: </strong>John Smutny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01224">https://arxiv.org/abs/2506.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01224">https://arxiv.org/pdf/2506.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01224]] Dirty and Clean-Label attack detection using GAN discriminators(https://arxiv.org/abs/2506.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Gathering enough images to train a deep computer vision model is a constant challenge. Unfortunately, collecting images from unknown sources can leave your model s behavior at risk of being manipulated by a dirty-label or clean-label attack unless the images are properly inspected. Manually inspecting each image-label pair is impractical and common poison-detection methods that involve re-training your model can be time consuming. This research uses GAN discriminators to protect a single class against mislabeled and different levels of modified images. The effect of said perturbation on a basic convolutional neural network classifier is also included for reference. The results suggest that after training on a single class, GAN discriminator s confidence scores can provide a threshold to identify mislabeled images and identify 100% of the tested poison starting at a perturbation epsilon magnitude of 0.20, after decision threshold calibration using in-class samples. Developers can use this report as a basis to train their own discriminators to protect high valued classes in their CV models.</li>
</ul>

<h3>Title: SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs</h3>
<ul>
<li><strong>Authors: </strong>Rakesh Podder, Turgay Caglar, Shadaab Kawnain Bashir, Sarath Sreedharan, Indrajit Ray, Indrakshi Ray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01227">https://arxiv.org/abs/2506.01227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01227">https://arxiv.org/pdf/2506.01227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01227]] SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs(https://arxiv.org/abs/2506.01227)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Graph-based frameworks are often used in network hardening to help a cyber defender understand how a network can be attacked and how the best defenses can be deployed. However, incorporating network connectivity parameters in the attack graph, reasoning about the attack graph when we do not have access to complete information, providing system administrator suggestions in an understandable format, and allowing them to do what-if analysis on various scenarios and attacker motives is still missing. We fill this gap by presenting SPEAR, a formal framework with tool support for security posture evaluation and analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI planning to model vulnerabilities and configurations in a networked system. It automatically converts network configurations and vulnerability descriptions into planning models expressed in the Planning Domain Definition Language (PDDL). SPEAR identifies a set of diverse security hardening strategies that can be presented in a manner understandable to the domain expert. These allow the administrator to explore the network hardening solution space in a systematic fashion and help evaluate the impact and compare the different solutions.</li>
</ul>

<h3>Title: Stress-Testing ML Pipelines with Adversarial Data Corruption</h3>
<ul>
<li><strong>Authors: </strong>Jiongli Zhu, Geyang Xu, Felipe Lorenzi, Boris Glavic, Babak Salimi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01230">https://arxiv.org/abs/2506.01230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01230">https://arxiv.org/pdf/2506.01230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01230]] Stress-Testing ML Pipelines with Adversarial Data Corruption(https://arxiv.org/abs/2506.01230)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Structured data-quality issues, such as missing values correlated with demographics, culturally biased labels, or systemic selection biases, routinely degrade the reliability of machine-learning pipelines. Regulators now increasingly demand evidence that high-stakes systems can withstand these realistic, interdependent errors, yet current robustness evaluations typically use random or overly simplistic corruptions, leaving worst-case scenarios unexplored. We introduce SAVAGE, a causally inspired framework that (i) formally models realistic data-quality issues through dependency graphs and flexible corruption templates, and (ii) systematically discovers corruption patterns that maximally degrade a target performance metric. SAVAGE employs a bi-level optimization approach to efficiently identify vulnerable data subpopulations and fine-tune corruption severity, treating the full ML pipeline, including preprocessing and potentially non-differentiable models, as a black box. Extensive experiments across multiple datasets and ML tasks (data cleaning, fairness-aware learning, uncertainty quantification) demonstrate that even a small fraction (around 5 %) of structured corruptions identified by SAVAGE severely impacts model performance, far exceeding random or manually crafted errors, and invalidating core assumptions of existing techniques. Thus, SAVAGE provides a practical tool for rigorous pipeline stress-testing, a benchmark for evaluating robustness methods, and actionable guidance for designing more resilient data workflows.</li>
</ul>

<h3>Title: Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Song, Xuan Wu, Bo Yang, You Zhou, Yubin Xiao, Yanchun Liang, Hongwei Ge, Heow Pueh Lee, Chunguo Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01231">https://arxiv.org/abs/2506.01231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01231">https://arxiv.org/pdf/2506.01231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01231]] Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution(https://arxiv.org/abs/2506.01231)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.</li>
</ul>

<h3>Title: ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists</h3>
<ul>
<li><strong>Authors: </strong>Jie Ruan, Inderjeet Nair, Shuyang Cao, Amy Liu, Sheza Munir, Micah Pollens-Dempsey, Tiffany Chiang, Lucy Kates, Nicholas David, Sihan Chen, Ruxin Yang, Yuqian Yang, Jasmine Gump, Tessa Bialek, Vivek Sankaran, Margo Schlanger, Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01241">https://arxiv.org/abs/2506.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01241">https://arxiv.org/pdf/2506.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01241]] ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists(https://arxiv.org/abs/2506.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items for model outputs are then compared with corresponding items for reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 11 large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer achieving only a 26.8% F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, though often not accurately; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable and low-cost usage.</li>
</ul>

<h3>Title: Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He, Yue Xing, Shen Dong, Juanhui Li, Zhenwei Dai, Xianfeng Tang, Hui Liu, Han Xu, Zhen Xiang, Charu C. Aggarwal, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01245">https://arxiv.org/abs/2506.01245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01245">https://arxiv.org/pdf/2506.01245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01245]] Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS(https://arxiv.org/abs/2506.01245)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>This paper argues that a comprehensive vulnerability analysis is essential for building trustworthy Large Language Model-based Multi-Agent Systems (LLM-MAS). These systems, which consist of multiple LLM-powered agents working collaboratively, are increasingly deployed in high-stakes applications but face novel security threats due to their complex structures. While single-agent vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces through inter-agent communication, trust relationships, and tool integration that remain significantly underexplored. We present a systematic framework for vulnerability analysis of LLM-MAS that unifies diverse research. For each type of vulnerability, we define formal threat models grounded in practical attacker capabilities and illustrate them using real-world LLM-MAS applications. This formulation enables rigorous quantification of vulnerability across different architectures and provides a foundation for designing meaningful evaluation benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to compositional effects -- vulnerabilities in individual components can cascade through agent communication, creating threat models not present in single-agent systems. We conclude by identifying critical open challenges: (1) developing benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2) considering new potential attacks specific to multi-agent architectures, and (3) implementing trust management systems that can enforce security in LLM-MAS. This research provides essential groundwork for future efforts to enhance LLM-MAS trustworthiness as these systems continue their expansion into critical applications.</li>
</ul>

<h3>Title: MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine</h3>
<ul>
<li><strong>Authors: </strong>Shufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie Wang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng Wang, Junwen Bai, Zhuangbin Chen, Zibin Zheng, Caihua Liu, Hao Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01252">https://arxiv.org/abs/2506.01252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01252">https://arxiv.org/pdf/2506.01252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01252]] MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine(https://arxiv.org/abs/2506.01252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional Chinese Medicine (TCM) is a holistic medical system with millennia of accumulated clinical experience, playing a vital role in global healthcare-particularly across East Asia. However, the implicit reasoning, diverse textual forms, and lack of standardization in TCM pose major challenges for computational modeling and evaluation. Large Language Models (LLMs) have demonstrated remarkable potential in processing natural language across diverse domains, including general medicine. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual question answering or lack domain-specific tasks and clinical realism. To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. The benchmark integrates real-world case records, national licensing exams, and classical texts, providing an authentic and comprehensive testbed for TCM-capable models. Preliminary results indicate that current LLMs perform well on foundational knowledge but fall short in clinical reasoning, prescription planning, and safety compliance. These findings highlight the urgent need for domain-aligned benchmarks like MTCMB to guide the development of more competent and trustworthy medical AI systems. All datasets, code, and evaluation tools are publicly available at: this https URL.</li>
</ul>

<h3>Title: DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Ye, Sophie Bronstein, Jiarui Hai, Malak Abu Hashish</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01257">https://arxiv.org/abs/2506.01257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01257">https://arxiv.org/pdf/2506.01257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01257]] DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models(https://arxiv.org/abs/2506.01257)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.</li>
</ul>

<h3>Title: Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism</h3>
<ul>
<li><strong>Authors: </strong>Sameera Ramasinghe, Thalaiyasingam Ajanthan, Gil Avraham, Yan Zuo, Alexander Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01260">https://arxiv.org/abs/2506.01260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01260">https://arxiv.org/pdf/2506.01260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01260]] Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism(https://arxiv.org/abs/2506.01260)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Scaling models has led to significant advancements in deep learning, but training these models in decentralized settings remains challenging due to communication bottlenecks. While existing compression techniques are effective in data-parallel, they do not extend to model parallelism. Unlike data-parallel training, where weight gradients are exchanged, model-parallel requires compressing activations and activation gradients as they propagate through layers, accumulating compression errors. We propose a novel compression algorithm that compresses both forward and backward passes, enabling up to 99% compression with no convergence degradation with negligible memory/compute overhead. By leveraging a recursive structure in transformer networks, we predefine a low-dimensional subspace to confine the activations and gradients, allowing full reconstruction in subsequent layers. Our method achieves up to 100x improvement in communication efficiency and enables training billion-parameter-scale models over low-end GPUs connected via consumer-grade internet speeds as low as 80Mbps, matching the convergence of centralized datacenter systems with 100Gbps connections with model parallel.</li>
</ul>

<h3>Title: The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Xie, Shenghui Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01261">https://arxiv.org/abs/2506.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01261">https://arxiv.org/pdf/2506.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01261]] The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning(https://arxiv.org/abs/2506.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In the context of Federated Reinforcement Learning (FRL), applying Proximal Policy Optimization (PPO) faces challenges related to the update order of its actor and critic due to the aggregation step occurring between successive iterations. In particular, when local actors are updated based on local critic estimations, the algorithm becomes vulnerable to data heterogeneity. As a result, the conventional update order in PPO (critic first, then actor) may cause heterogeneous gradient directions among clients, hindering convergence to a globally optimal policy. To address this issue, we propose FedRAC, which reverses the update order (actor first, then critic) to eliminate the divergence of critics from different clients. Theoretical analysis shows that the convergence bound of FedRAC is immune to data heterogeneity under mild conditions, i.e., bounded level of heterogeneity and accurate policy evaluation. Empirical results indicate that the proposed algorithm obtains higher cumulative rewards and converges more rapidly in five experiments, including three classical RL environments and a highly heterogeneous autonomous driving scenario using the SUMO traffic simulator.</li>
</ul>

<h3>Title: Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jisoo Mok, Ik-hwan Kim, Sangkwon Park, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01262">https://arxiv.org/abs/2506.01262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01262">https://arxiv.org/pdf/2506.01262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01262]] Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis(https://arxiv.org/abs/2506.01262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalized AI assistants, a hallmark of the human-like capabilities of Large Language Models (LLMs), are a challenging application that intertwines multiple problems in LLM research. Despite the growing interest in the development of personalized assistants, the lack of an open-source conversational dataset tailored for personalization remains a significant obstacle for researchers in the field. To address this research gap, we introduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs to deliver personalized responses. Alongside a conversational dataset, HiCUPID provides a Llama-3.2-based automated evaluation model whose assessment closely mirrors human preferences. We release our dataset, evaluation model, and code at this https URL.</li>
</ul>

<h3>Title: Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Do Xuan Long, Duong Ngoc Yen, Do Xuan Trong, Luu Anh Tuan, Kenji Kawaguchi, Shafiq Joty, Min-Yen Kan, Nancy F. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01265">https://arxiv.org/abs/2506.01265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01265">https://arxiv.org/pdf/2506.01265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01265]] Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines(https://arxiv.org/abs/2506.01265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.</li>
</ul>

<h3>Title: Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model</h3>
<ul>
<li><strong>Authors: </strong>Yuanhe Tian, Mingjie Deng, Guoqing Jin, Yan Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01266">https://arxiv.org/abs/2506.01266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01266">https://arxiv.org/pdf/2506.01266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01266]] Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model(https://arxiv.org/abs/2506.01266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Existing approaches for Large language model (LLM) detoxification generally rely on training on large-scale non-toxic or human-annotated preference data, designing prompts to instruct the LLM to generate safe content, or modifying the model parameters to remove toxic information, which are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding. In this paper, we propose a simple yet effective approach for LLM detoxification, which leverages a compact, pre-trained calibration model that guides the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. By learning a detoxified embedding space from non-toxic data, the calibration model effectively steers the LLM away from generating harmful content. This approach only requires a one-time training of the calibration model that is able to be seamlessly applied to multiple LLMs without compromising fluency or contextual understanding. Experiment results on the benchmark dataset demonstrate that our approach reduces toxicity while maintaining reasonable content expression.</li>
</ul>

<h3>Title: Schema as Parameterized Tools for Universal Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Sheng Liang, Yongyue Zhang, Yaxiong Wu, Ruiming Tang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01276">https://arxiv.org/abs/2506.01276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01276">https://arxiv.org/pdf/2506.01276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01276]] Schema as Parameterized Tools for Universal Information Extraction(https://arxiv.org/abs/2506.01276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Universal information extraction (UIE) primarily employs an extractive generation approach with large language models (LLMs), typically outputting structured information based on predefined schemas such as JSON or tables. UIE suffers from a lack of adaptability when selecting between predefined schemas and on-the-fly schema generation within the in-context learning paradigm, especially when there are numerous schemas to choose from. In this paper, we propose a unified adaptive text-to-structure generation framework, called Schema as Parameterized Tools (SPT), which reimagines the tool-calling capability of LLMs by treating predefined schemas as parameterized tools for tool selection and parameter filling. Specifically, our SPT method can be applied to unify closed, open, and on-demand IE tasks by adopting Schema Retrieval by fetching the relevant schemas from a predefined pool, Schema Filling by extracting information and filling slots as with tool parameters, or Schema Generation by synthesizing new schemas with uncovered cases. Experiments show that the SPT method can handle four distinct IE tasks adaptively, delivering robust schema retrieval and selection performance. SPT also achieves comparable extraction performance to LoRA baselines and current leading UIE systems with significantly fewer trainable parameters.</li>
</ul>

<h3>Title: Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Min Zhang, Wen Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01293">https://arxiv.org/abs/2506.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01293">https://arxiv.org/pdf/2506.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01293]] Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation(https://arxiv.org/abs/2506.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at this https URL</li>
</ul>

<h3>Title: Latent Structured Hopfield Network for Semantic Association and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Chong Li, Xiangyang Xue, Jianfeng Feng, Taiping Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01303">https://arxiv.org/abs/2506.01303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01303">https://arxiv.org/pdf/2506.01303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01303]] Latent Structured Hopfield Network for Semantic Association and Retrieval(https://arxiv.org/abs/2506.01303)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored. Inspired by hippocampal CA3 dynamics and its role in associative memory, we propose the Latent Structured Hopfield Network (LSHN), a biologically inspired framework that integrates continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input. Unlike traditional Hopfield networks, our model is trained end-to-end with gradient descent, achieving scalable and robust memory retrieval. Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models. Our work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms.</li>
</ul>

<h3>Title: SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Mei, Pengyu Zhang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01304">https://arxiv.org/abs/2506.01304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01304">https://arxiv.org/pdf/2506.01304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01304]] SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost(https://arxiv.org/abs/2506.01304)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V, an effective image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM's static image encoder to enable spatiotemporal video perception, (ii) a memory filtering strategy that selects the most relevant past frames for more effective utilization of historical information, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves over 90% of SAM 2's performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Code and model are available at: this https URL.</li>
</ul>

<h3>Title: Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01307">https://arxiv.org/abs/2506.01307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01307">https://arxiv.org/pdf/2506.01307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01307]] Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models(https://arxiv.org/abs/2506.01307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.</li>
</ul>

<h3>Title: A Platform for Investigating Public Health Content with Efficient Concern Classification</h3>
<ul>
<li><strong>Authors: </strong>Christopher Li, Rickard Stureborg, Bhuwan Dhingra, Jun Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01308">https://arxiv.org/abs/2506.01308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01308">https://arxiv.org/pdf/2506.01308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01308]] A Platform for Investigating Public Health Content with Efficient Concern Classification(https://arxiv.org/abs/2506.01308)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A recent rise in online content expressing concerns with public health initiatives has contributed to already stalled uptake of preemptive measures globally. Future public health efforts must attempt to understand such content, what concerns it may raise among readers, and how to effectively respond to it. To this end, we present ConcernScope, a platform that uses a teacher-student framework for knowledge transfer between large language models and light-weight classifiers to quickly and effectively identify the health concerns raised in a text corpus. The platform allows uploading massive files directly, automatically scraping specific URLs, and direct text editing. ConcernScope is built on top of a taxonomy of public health concerns. Intended for public health officials, we demonstrate several applications of this platform: guided data exploration to find useful examples of common concerns found in online community datasets, identification of trends in concerns through an example time series analysis of 186,000 samples, and finding trends in topic frequency before and after significant events.</li>
</ul>

<h3>Title: Growing Through Experience: Scaling Episodic Grounding in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chunhui Zhang, Sirui (Elsie)Wang, Zhongyu Ouyang, Xiangchi Yuan, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01312">https://arxiv.org/abs/2506.01312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01312">https://arxiv.org/pdf/2506.01312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01312]] Growing Through Experience: Scaling Episodic Grounding in Language Models(https://arxiv.org/abs/2506.01312)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Language models (LMs) require robust episodic grounding-the capacity to learn from and apply past experiences-to excel at physical planning tasks. Current episodic grounding approaches struggle with scalability and integration, limiting their effectiveness, especially for medium-sized LMs (7B parameters). While larger LMs (70-405B parameters) possess superior hierarchical representations and extensive pre-trained knowledge, they encounter a fundamental scale paradox: despite their advanced abstraction capabilities, they lack efficient mechanisms to leverage experience streams. We propose a scalable weak-to-strong episodic learning framework that effectively transfers episodic behaviors from smaller to larger LMs. This framework integrates Monte Carlo tree search for structured experience collection with a novel distillation method, preserving the inherent LM capabilities while embedding episodic memory. Experiments demonstrate our method surpasses state-of-the-art proprietary LMs by 3.45% across diverse planning and question-answering tasks. Layer-wise probing further indicates significant improvements in task alignment, especially within deeper LM layers, highlighting stable generalization even for previously unseen scenarios with increased planning complexity-conditions where baseline methods degrade markedly.</li>
</ul>

<h3>Title: T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yanjun Fu, Faisal Hamman, Sanghamitra Dutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01317">https://arxiv.org/abs/2506.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01317">https://arxiv.org/pdf/2506.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01317]] T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning(https://arxiv.org/abs/2506.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples using 40 minutes on a single GPU.</li>
</ul>

<h3>Title: Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack</h3>
<ul>
<li><strong>Authors: </strong>SeungBum Ha, Saerom Park, Sung Whan Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01318">https://arxiv.org/abs/2506.01318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01318">https://arxiv.org/pdf/2506.01318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01318]] Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack(https://arxiv.org/abs/2506.01318)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: "over-unlearning" that deteriorates retained data near the forget set, and post-hoc "relearning" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@{\epsilon}, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots.</li>
</ul>

<h3>Title: $Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</h3>
<ul>
<li><strong>Authors: </strong>Taehoon Yoon, Yunhong Min, Kyeongmin Yeo, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01320">https://arxiv.org/abs/2506.01320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01320">https://arxiv.org/pdf/2506.01320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01320]] $Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models(https://arxiv.org/abs/2506.01320)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.</li>
</ul>

<h3>Title: Zero-Shot Text-to-Speech for Vietnamese</h3>
<ul>
<li><strong>Authors: </strong>Thi Vu, Linh The Nguyen, Dat Quoc Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01322">https://arxiv.org/abs/2506.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01322">https://arxiv.org/pdf/2506.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01322]] Zero-Shot Text-to-Speech for Vietnamese(https://arxiv.org/abs/2506.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.</li>
</ul>

<h3>Title: Understanding the Identity-Transformation Approach in OIDC-Compatible Privacy-Preserving SSO Services</h3>
<ul>
<li><strong>Authors: </strong>Jingqiang Lin, Baitao Zhang, Wei Wang, Quanwei Cai, Jiwu Jing, Huiyang He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01325">https://arxiv.org/abs/2506.01325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01325">https://arxiv.org/pdf/2506.01325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01325]] Understanding the Identity-Transformation Approach in OIDC-Compatible Privacy-Preserving SSO Services(https://arxiv.org/abs/2506.01325)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers to log into multiple websites, called relying parties (RPs), by her username and credential set up in another trusted web system, called the identity provider (IdP). Identity transformations are proposed in UppreSSO to provide OIDC-compatible SSO services, preventing both IdP-based login tracing and RP-based identity linkage. While security and privacy of SSO services in UppreSSO have been proved, several essential issues of this identity-transformation approach are not well studied. In this paper, we comprehensively investigate the approach as below. Firstly, several suggestions for the efficient integration of identity transformations in OIDC-compatible SSO are explained. Then, we uncover the relationship between identity-transformations in SSO and oblivious pseudo-random functions (OPRFs), and present two variations of the properties required for SSO security as well as the privacy requirements, to analyze existing OPRF protocols. Finally, new identity transformations different from those designed in UppreSSO, are constructed based on OPRFs, satisfying different variations of SSO security requirements. To the best of our knowledge, this is the first time to uncover the relationship between identity transformations in OIDC-compatible privacy-preserving SSO services and OPRFs, and prove the SSO-related properties (i.e., key-identifier freeness, RP designation and user identification) of OPRF protocols, in addition to the basic properties of correctness, obliviousness and pseudo-randomness.</li>
</ul>

<h3>Title: STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Zenghao Guan, Guojun Zhu, Yucan Zhou, Wu Liu, Weiping Wang, Jiebo Luo, Xiaoyan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01327">https://arxiv.org/abs/2506.01327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01327">https://arxiv.org/pdf/2506.01327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01327]] STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation(https://arxiv.org/abs/2506.01327)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency.</li>
</ul>

<h3>Title: Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines</h3>
<ul>
<li><strong>Authors: </strong>Guifeng Deng, Shuyin Rao, Tianyu Lin, Anlu Dai, Pan Wang, Junyi Xie, Haidong Song, Ke Zhao, Dongwu Xu, Zhengdong Cheng, Tao Li, Haiteng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01329">https://arxiv.org/abs/2506.01329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01329">https://arxiv.org/pdf/2506.01329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01329]] Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines(https://arxiv.org/abs/2506.01329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Psychological support hotlines are critical for crisis intervention but face significant challenges due to rising demand. Large language models (LLMs) could support crisis assessments, yet their capabilities in emotionally sensitive contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. We evaluated 64 LLMs across 15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot, few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with statistical comparisons via Welch's t-tests. LLMs performed strongly on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood status recognition was more challenging (max F1=0.709), likely due to lost vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) surpassed larger models on mood and suicidal ideation. Open-source models like QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though closed models retained an edge in mood detection (p=0.007). Performance scaled with size up to a point; quantization (AWQ) reduced GPU memory by 70% with minimal F1 degradation. LLMs show substantial promise in structured psychological crisis assessments, especially with fine-tuning. Mood recognition remains limited due to contextual complexity. The narrowing gap between open- and closed-source models, combined with efficient quantization, suggests feasible integration. PsyCrisisBench offers a robust evaluation framework to guide model development and ethical deployment in mental health.</li>
</ul>

<h3>Title: Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01331">https://arxiv.org/abs/2506.01331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01331">https://arxiv.org/pdf/2506.01331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01331]] Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation(https://arxiv.org/abs/2506.01331)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ultra-high-resolution image synthesis holds significant potential, yet remains an underexplored challenge due to the absence of standardized benchmarks and computational constraints. In this paper, we establish Aesthetic-4K, a meticulously curated dataset containing dedicated training and evaluation subsets specifically designed for comprehensive research on ultra-high-resolution image synthesis. This dataset consists of high-quality 4K images accompanied by descriptive captions generated by GPT-4o. Furthermore, we propose Diffusion-4K, an innovative framework for the direct generation of ultra-high-resolution images. Our approach incorporates the Scale Consistent Variational Auto-Encoder (SC-VAE) and Wavelet-based Latent Fine-tuning (WLF), which are designed for efficient visual token compression and the capture of intricate details in ultra-high-resolution images, thereby facilitating direct training with photorealistic 4K data. This method is applicable to various latent diffusion models and demonstrates its efficacy in synthesizing highly detailed 4K images. Additionally, we propose novel metrics, namely the GLCM Score and Compression Ratio, to assess the texture richness and fine details in local patches, in conjunction with holistic measures such as FID, Aesthetics, and CLIPScore, enabling a thorough and multifaceted evaluation of ultra-high-resolution image synthesis. Consequently, Diffusion-4K achieves impressive performance in ultra-high-resolution image synthesis, particularly when powered by state-of-the-art large-scale diffusion models (eg, Flux-12B). The source code is publicly available at this https URL.</li>
</ul>

<h3>Title: ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control</h3>
<ul>
<li><strong>Authors: </strong>Manish Bhatt, Vineeth Sai Narajala, Idan Habler</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01333">https://arxiv.org/abs/2506.01333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01333">https://arxiv.org/pdf/2506.01333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01333]] ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control(https://arxiv.org/abs/2506.01333)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) plays a crucial role in extending the capabilities of Large Language Models (LLMs) by enabling integration with external tools and data sources. However, the standard MCP specification presents significant security vulnerabilities, notably Tool Poisoning and Rug Pull attacks. This paper introduces the Enhanced Tool Definition Interface (ETDI), a security extension designed to fortify MCP. ETDI incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management, often leveraging OAuth 2.0. We further propose extending MCP with fine-grained, policy-based access control, where tool capabilities are dynamically evaluated against explicit policies using a dedicated policy engine, considering runtime context beyond static OAuth scopes. This layered approach aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.</li>
</ul>

<h3>Title: Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Jiang, Deval Mehta, Wei Feng, Zongyuan Ge</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01334">https://arxiv.org/abs/2506.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01334">https://arxiv.org/pdf/2506.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01334]] Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models(https://arxiv.org/abs/2506.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs' concept scoring mechanisms. It enhances the accuracy of assessing each concept's contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.</li>
</ul>

<h3>Title: NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zeming Li, Xiangyue Liu, Xiangyu Zhang, Ping Tan, Heung-Yeung Shum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01337">https://arxiv.org/abs/2506.01337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01337">https://arxiv.org/pdf/2506.01337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01337]] NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models(https://arxiv.org/abs/2506.01337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks. In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization. Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes and Reinforcement Learning. Our code will be available at this https URL</li>
</ul>

<h3>Title: Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Changsheng Wang, Yihua Zhang, Jinghan Jia, Parikshit Ram, Dennis Wei, Yuguang Yao, Soumyadeep Pal, Nathalie Baracaldo, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01339">https://arxiv.org/abs/2506.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01339">https://arxiv.org/pdf/2506.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01339]] Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning(https://arxiv.org/abs/2506.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Machine unlearning offers a promising solution to privacy and safety concerns in large language models (LLMs) by selectively removing targeted knowledge while preserving utility. However, current methods are highly sensitive to downstream fine-tuning, which can quickly recover forgotten information-even from unrelated tasks. To address this, we introduce invariance into unlearning for the first time, inspired by invariant risk minimization (IRM). Building on this principle, we propose invariant LLM unlearning (ILU), a regularization-based framework that enhances robustness. Notably, ILU generalizes well to diverse fine-tuning tasks, even when trained using a single dataset. A task vector analysis is also provided to further elucidate the rationale behind ILU's effectiveness. Extensive experiments on the WMDP and MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art unlearning methods, including negative preference optimization (NPO) and representation misdirection for unlearning (RMU). Notably, ILU achieves superior unlearning robustness across diverse downstream fine-tuning scenarios (e.g., math, paraphrase detection, and sentiment analysis) while preserving the fine-tuning performance.</li>
</ul>

<h3>Title: The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology</h3>
<ul>
<li><strong>Authors: </strong>Shahad Al-Khalifa, Nadir Durrani, Hend Al-Khalifa, Firoj Alam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01340">https://arxiv.org/abs/2506.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01340">https://arxiv.org/pdf/2506.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01340]] The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology(https://arxiv.org/abs/2506.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of ChatGPT marked a transformative milestone for Artificial Intelligence (AI), showcasing the remarkable potential of Large Language Models (LLMs) to generate human-like text. This wave of innovation has revolutionized how we interact with technology, seamlessly integrating LLMs into everyday tasks such as vacation planning, email drafting, and content creation. While English-speaking users have significantly benefited from these advancements, the Arabic world faces distinct challenges in developing Arabic-specific LLMs. Arabic, one of the languages spoken most widely around the world, serves more than 422 million native speakers in 27 countries and is deeply rooted in a rich linguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an unparalleled opportunity to bridge technological gaps and empower communities. The journey of ALLMs has been both fascinating and complex, evolving from rudimentary text processing systems to sophisticated AI-driven models. This article explores the trajectory of ALLMs, from their inception to the present day, highlighting the efforts to evaluate these models through benchmarks and public leaderboards. We also discuss the challenges and opportunities that ALLMs present for the Arab world.</li>
</ul>

<h3>Title: TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiran Zhang, Mo Wang, Xiaoyang Li, Kaixuan Ren, Chencheng Zhu, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01341">https://arxiv.org/abs/2506.01341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01341">https://arxiv.org/pdf/2506.01341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01341]] TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models(https://arxiv.org/abs/2506.01341)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite impressive advances in large language models (LLMs), existing benchmarks often focus on single-turn or single-step tasks, failing to capture the kind of iterative reasoning required in real-world settings. To address this limitation, we introduce TurnBench, a novel benchmark that evaluates multi-turn, multi-step reasoning through an interactive code-breaking task inspired by a "Turing Machine Board Game." In each episode, a model must uncover hidden logical or arithmetic rules by making sequential guesses, receiving structured feedback, and integrating clues across multiple rounds. This dynamic setup requires models to reason over time, adapt based on past information, and maintain consistency across steps-capabilities underexplored in current benchmarks. TurnBench includes two modes: Classic, which tests standard reasoning, and Nightmare, which introduces increased complexity and requires robust inferential chains. To support fine-grained analysis, we provide ground-truth annotations for intermediate reasoning steps. Our evaluation of state-of-the-art LLMs reveals significant gaps: the best model achieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in Nightmare mode. In contrast, human participants achieve 100% in both, underscoring the challenge TurnBench poses to current models. By incorporating feedback loops and hiding task rules, TurnBench reduces contamination risks and provides a rigorous testbed for diagnosing and advancing multi-step, multi-turn reasoning in LLMs.</li>
</ul>

<h3>Title: Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents</h3>
<ul>
<li><strong>Authors: </strong>Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Vivek Gupta, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01344">https://arxiv.org/abs/2506.01344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01344">https://arxiv.org/pdf/2506.01344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01344]] Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents(https://arxiv.org/abs/2506.01344)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.</li>
</ul>

<h3>Title: Distributionally Robust Learning in Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yeping Jin, Lauren Wise, Ioannis Paschalidis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01348">https://arxiv.org/abs/2506.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01348">https://arxiv.org/pdf/2506.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01348]] Distributionally Robust Learning in Survival Analysis(https://arxiv.org/abs/2506.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce an innovative approach that incorporates a Distributionally Robust Learning (DRL) approach into Cox regression to enhance the robustness and accuracy of survival predictions. By formulating a DRL framework with a Wasserstein distance-based ambiguity set, we develop a variant Cox model that is less sensitive to assumptions about the underlying data distribution and more resilient to model misspecification and data perturbations. By leveraging Wasserstein duality, we reformulate the original min-max DRL problem into a tractable regularized empirical risk minimization problem, which can be computed by exponential conic programming. We provide guarantees on the finite sample behavior of our DRL-Cox model. Moreover, through extensive simulations and real world case studies, we demonstrate that our regression model achieves superior performance in terms of prediction accuracy and robustness compared with traditional methods.</li>
</ul>

<h3>Title: Target Driven Adaptive Loss For Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuho Shoji, Takahiro Toizumi, Atsushi Ito</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01349">https://arxiv.org/abs/2506.01349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01349">https://arxiv.org/pdf/2506.01349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01349]] Target Driven Adaptive Loss For Infrared Small Target Detection(https://arxiv.org/abs/2506.01349)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We propose a target driven adaptive (TDA) loss to enhance the performance of infrared small target detection (IRSTD). Prior works have used loss functions, such as binary cross-entropy loss and IoU loss, to train segmentation models for IRSTD. Minimizing these loss functions guides models to extract pixel-level features or global image context. However, they have two issues: improving detection performance for local regions around the targets and enhancing robustness to small scale and low local contrast. To address these issues, the proposed TDA loss introduces a patch-based mechanism, and an adaptive adjustment strategy to scale and local contrast. The proposed TDA loss leads the model to focus on local regions around the targets and pay particular attention to targets with smaller scales and lower local contrast. We evaluate the proposed method on three datasets for IRSTD. The results demonstrate that the proposed TDA loss achieves better detection performance than existing losses on these datasets.</li>
</ul>

<h3>Title: TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network</h3>
<ul>
<li><strong>Authors: </strong>Guangxin He, Yuan Cao, Yutong He, Tianyi Bai, Kun Yuan, Binhang Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01352">https://arxiv.org/abs/2506.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01352">https://arxiv.org/pdf/2506.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01352]] TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network(https://arxiv.org/abs/2506.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Decentralized training of large language models offers the opportunity to pool computational resources across geographically distributed participants but faces significant network communication bottlenecks, particularly in pipeline-parallel settings. While pipeline parallelism partitions model layers across devices to handle large-scale models, it necessitates frequent communication of intermediate activations, creating challenges when network bandwidth is limited. Existing activation compression methods, such as AQ-SGD, mitigate quantization-induced errors through error compensation but impose prohibitive memory overhead by requiring storage of previous activations. To address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard Quantization), a novel activation quantization framework designed specifically for pipeline parallelism. Our approach integrates fine-grained tile-wise quantization for precise control, entropy-guided token-level adaptive bit allocation for optimal bit usage, and a Hadamard-based transform with pivot element swapping to effectively suppress quantization outliers. We further provide a theoretical analysis, proving that pipeline parallel training equipped with TAH-Quant maintains a convergence rate of $\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant achieves aggressive activation quantization (3-4 bits) ratio, which provides up to 4.3$\times$ end-to-end speedup without compromising training convergence, matches state-of-the-art methods, incurs no extra memory overhead, and generalizes well across different training scenarios.</li>
</ul>

<h3>Title: KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Qi, Takumasa Kaneko, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01357">https://arxiv.org/abs/2506.01357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01357">https://arxiv.org/pdf/2506.01357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01357]] KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors(https://arxiv.org/abs/2506.01357)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at this https URL.</li>
</ul>

<h3>Title: TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Hasan Ferdous, Emam Hossain, Md Osman Gani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01361">https://arxiv.org/abs/2506.01361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01361">https://arxiv.org/pdf/2506.01361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01361]] TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery(https://arxiv.org/abs/2506.01361)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Robust causal discovery in time series datasets depends on reliable benchmark datasets with known ground-truth causal relationships. However, such datasets remain scarce, and existing synthetic alternatives often overlook critical temporal properties inherent in real-world data, including nonstationarity driven by trends and seasonality, irregular sampling intervals, and the presence of unobserved confounders. To address these challenges, we introduce TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets that systematically incorporates both linear and nonlinear dependencies while modeling key temporal characteristics such as trends, seasonal effects, and heterogeneous noise patterns. Each dataset is accompanied by a fully specified causal graph featuring varying densities and diverse noise distributions and is provided in two versions: one including unobserved confounders and one without, thereby offering extensive coverage of real-world complexity while preserving methodological neutrality. We further demonstrate the utility of TimeGraph through systematic evaluations of state-of-the-art causal discovery algorithms including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and metrics. Our experiments reveal significant variations in algorithmic performance under realistic temporal conditions, underscoring the need for robust synthetic benchmarks in the fair and transparent assessment of causal discovery methods. The complete TimeGraph suite, including dataset generation scripts, evaluation metrics, and recommended experimental protocols, is freely available to facilitate reproducible research and foster community-driven advancements in time-series causal discovery.</li>
</ul>

<h3>Title: MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Kensuke Mitsuzawa, Damien Garreau</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01367">https://arxiv.org/abs/2506.01367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01367">https://arxiv.org/pdf/2506.01367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01367]] MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations(https://arxiv.org/abs/2506.01367)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become pervasive in our everyday life. Yet, a fundamental obstacle prevents their use in many critical applications: their propensity to generate fluent, human-quality content that is not grounded in reality. The detection of such hallucinations is thus of the highest importance. In this work, we propose a new method to flag hallucinated content, MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric distance between distributions. On a high-level perspective, MMD-Flagger tracks the MMD between the generated documents and documents generated with various temperature parameters. We show empirically that inspecting the shape of this trajectory is sufficient to detect most hallucinations. This novel method is benchmarked on two machine translation datasets, on which it outperforms natural competitors.</li>
</ul>

<h3>Title: Synthetic Data Augmentation using Pre-trained Diffusion Models for Long-tailed Food Image Classification</h3>
<ul>
<li><strong>Authors: </strong>GaYeon Koh, Hyun-Jic Oh, Jeonghyun Noh, Won-Ki Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01368">https://arxiv.org/abs/2506.01368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01368">https://arxiv.org/pdf/2506.01368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01368]] Synthetic Data Augmentation using Pre-trained Diffusion Models for Long-tailed Food Image Classification(https://arxiv.org/abs/2506.01368)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning-based food image classification enables precise identification of food categories, further facilitating accurate nutritional analysis. However, real-world food images often show a skewed distribution, with some food types being more prevalent than others. This class imbalance can be problematic, causing models to favor the majority (head) classes with overall performance degradation for the less common (tail) classes. Recently, synthetic data augmentation using diffusion-based generative models has emerged as a promising solution to address this issue. By generating high-quality synthetic images, these models can help uniformize the data distribution, potentially improving classification performance. However, existing approaches face challenges: fine-tuning-based methods need a uniformly distributed dataset, while pre-trained model-based approaches often overlook inter-class separation in synthetic data. In this paper, we propose a two-stage synthetic data augmentation framework, leveraging pre-trained diffusion models for long-tailed food classification. We generate a reference set conditioned by a positive prompt on the generation target and then select a class that shares similar features with the generation target as a negative prompt. Subsequently, we generate a synthetic augmentation set using positive and negative prompt conditions by a combined sampling strategy that promotes intra-class diversity and inter-class separation. We demonstrate the efficacy of the proposed method on two long-tailed food benchmark datasets, achieving superior performance compared to previous works in terms of top-1 accuracy.</li>
</ul>

<h3>Title: Incentivizing LLMs to Self-Verify Their Answers</h3>
<ul>
<li><strong>Authors: </strong>Fuxiang Zhang, Jiacheng Xu, Chaojie Wang, Ce Cui, Yang Liu, Bo An</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01369">https://arxiv.org/abs/2506.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01369">https://arxiv.org/pdf/2506.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01369]] Incentivizing LLMs to Self-Verify Their Answers(https://arxiv.org/abs/2506.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance during inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at this https URL.</li>
</ul>

<h3>Title: PointT2I: LLM-based text-to-image generation via keypoints</h3>
<ul>
<li><strong>Authors: </strong>Taekyung Lee, Donggyu Lee, Myungjoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01370">https://arxiv.org/abs/2506.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01370">https://arxiv.org/pdf/2506.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01370]] PointT2I: LLM-based text-to-image generation via keypoints(https://arxiv.org/abs/2506.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generation model has made significant advancements, resulting in high-quality images aligned with an input prompt. However, despite T2I generation's ability to generate fine-grained images, it still faces challenges in accurately generating images when the input prompt contains complex concepts, especially human pose. In this paper, we propose PointT2I, a framework that effectively generates images that accurately correspond to the human pose described in the prompt by using a large language model (LLM). PointT2I consists of three components: Keypoint generation, Image generation, and Feedback system. The keypoint generation uses an LLM to directly generate keypoints corresponding to a human pose, solely based on the input prompt, without external references. Subsequently, the image generation produces images based on both the text prompt and the generated keypoints to accurately reflect the target pose. To refine the outputs of the preceding stages, we incorporate an LLM-based feedback system that assesses the semantic consistency between the generated contents and the given prompts. Our framework is the first approach to leveraging LLM for keypoints-guided image generation without any fine-tuning, producing accurate pose-aligned images based solely on textual prompts.</li>
</ul>

<h3>Title: No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Tomasz Stanczyk, Seongro Yoon, Francois Bremond</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01373">https://arxiv.org/abs/2506.01373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01373">https://arxiv.org/pdf/2506.01373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01373]] No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond(https://arxiv.org/abs/2506.01373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) is essential for sports analytics, enabling performance evaluation and tactical insights. However, tracking in sports is challenging due to fast movements, occlusions, and camera shifts. Traditional tracking-by-detection methods require extensive tuning, while segmentation-based approaches struggle with track processing. We propose McByte, a tracking-by-detection framework that integrates temporally propagated segmentation mask as an association cue to improve robustness without per-video tuning. Unlike many existing methods, McByte does not require training, relying solely on pre-trained models and object detectors commonly used in the community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and MOT17, McByte demonstrates strong performance across sports and general pedestrian tracking. Our results highlight the benefits of mask propagation for a more adaptable and generalizable MOT approach. Code will be made available at this https URL.</li>
</ul>

<h3>Title: Compiler Optimization via LLM Reasoning for Efficient Model Serving</h3>
<ul>
<li><strong>Authors: </strong>Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01374">https://arxiv.org/abs/2506.01374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01374">https://arxiv.org/pdf/2506.01374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01374]] Compiler Optimization via LLM Reasoning for Efficient Model Serving(https://arxiv.org/abs/2506.01374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</li>
</ul>

<h3>Title: RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01379">https://arxiv.org/abs/2506.01379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01379">https://arxiv.org/pdf/2506.01379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01379]] RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes(https://arxiv.org/abs/2506.01379)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>High-Fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR / 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction. A project page is available at this https URL.</li>
</ul>

<h3>Title: Playing with Transformer at 30+ FPS via Next-Frame Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xinle Cheng, Tianyu He, Jiayi Xu, Junliang Guo, Di He, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01380">https://arxiv.org/abs/2506.01380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01380">https://arxiv.org/pdf/2506.01380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01380]] Playing with Transformer at 30+ FPS via Next-Frame Diffusion(https://arxiv.org/abs/2506.01380)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.</li>
</ul>

<h3>Title: Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems</h3>
<ul>
<li><strong>Authors: </strong>Craig Steven Wright</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.GT, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01384">https://arxiv.org/abs/2506.01384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01384">https://arxiv.org/pdf/2506.01384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01384]] Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems(https://arxiv.org/abs/2506.01384)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper presents a mathematically rigorous formal analysis of Simplified Payment Verification (SPV) clients, as specified in Section 8 of the original Bitcoin white paper, versus non-mining full nodes operated by home users. It defines security as resistance to divergence from global consensus and models transaction acceptance, enforcement capability, and divergence probability under adversarial conditions. The results demonstrate that SPV clients, despite omitting script verification, are cryptographically sufficient under honest-majority assumptions and topologically less vulnerable to attack than structurally passive, non-enforcing full nodes. The paper introduces new axioms on behavioral divergence and communication topology, proving that home-based full nodes increase systemic entropy without contributing to consensus integrity. Using a series of formally defined lemmas, propositions, and Monte Carlo simulation results, it is shown that SPV clients represent the rational equilibrium strategy for non-mining participants. This challenges the prevailing narrative that home validators enhance network security, providing formal and operational justifications for the sufficiency of SPV models.</li>
</ul>

<h3>Title: ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Manit Baser, Dinil Mon Divakaran, Mohan Gurusamy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01386">https://arxiv.org/abs/2506.01386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01386">https://arxiv.org/pdf/2506.01386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01386]] ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs(https://arxiv.org/abs/2506.01386)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Model editing has become an important tool for addressing privacy, bias, and misinformation in large language models (LLMs) by enabling updates to knowledge without the need for retraining from scratch. However, existing editing techniques often target isolated facts, ignoring ripple effects on related knowledge, allowing edited facts to remain deducible and compromising broader contextual integrity. For example, changing Harry Potter's school from Hogwarts to Ilvermorny requires reassigning his house from Gryffindor to a suitable alternative while preserving Gryffindor's relationship with Hogwarts. In this work, we present a new model-editing setting, deep editing, to show: (1) how editing techniques fail to handle connected facts, evaluating how original knowledge sneaks through unchanged causal links, and (2) their impact on broader contextual knowledge. We introduce ThinkEval, a framework to systematically evaluate model- editing techniques by building model-specific knowledge graphs to analyze pre- and post-edit effects on fact persistence and catastrophic forgetting. We present KnowGIC, a benchmark created with ThinkEval, consisting of sequentially linked queries to measure these effects. We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE across multiple LLMs. We find that these techniques struggle to balance indirect fact suppression with the preservation of related knowledge. Our dataset is available at: this https URL.</li>
</ul>

<h3>Title: VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yihao Ding, Soyeon Caren Han, Yan Li, Josiah Poon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01388">https://arxiv.org/abs/2506.01388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01388">https://arxiv.org/pdf/2506.01388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01388]] VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding(https://arxiv.org/abs/2506.01388)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Visually Rich Document Understanding (VRDU) has emerged as a critical field in document intelligence, enabling automated extraction of key information from complex documents across domains such as medical, financial, and educational applications. However, form-like documents pose unique challenges due to their complex layouts, multi-stakeholder involvement, and high structural variability. Addressing these issues, the VRD-IU Competition was introduced, focusing on extracting and localizing key information from multi-format forms within the Form-NLU dataset, which includes digital, printed, and handwritten documents. This paper presents insights from the competition, which featured two tracks: Track A, emphasizing entity-based key information retrieval, and Track B, targeting end-to-end key information localization from raw document images. With over 20 participating teams, the competition showcased various state-of-the-art methodologies, including hierarchical decomposition, transformer-based retrieval, multimodal feature fusion, and advanced object detection techniques. The top-performing models set new benchmarks in VRDU, providing valuable insights into document intelligence.</li>
</ul>

<h3>Title: Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping</h3>
<ul>
<li><strong>Authors: </strong>Linzh Zhao (1), Aki Rehn (1), Mikko A. Heikkilä (1), Razane Tajeddine (2), Antti Honkela (1) ((1) Department of Computer Science, University of Helsinki, Finland, (2) Department of Electrical and Computer Engineering, American University of Beirut, Lebanon)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01396">https://arxiv.org/abs/2506.01396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01396">https://arxiv.org/pdf/2506.01396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01396]] Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping(https://arxiv.org/abs/2506.01396)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) has become an essential framework for privacy-preserving machine learning. Existing DP learning methods, however, often have disparate impacts on model predictions, e.g., for minority groups. Gradient clipping, which is often used in DP learning, can suppress larger gradients from challenging samples. We show that this problem is amplified by adaptive clipping, which will often shrink the clipping bound to tiny values to match a well-fitting majority, while significantly reducing the accuracy for others. We propose bounded adaptive clipping, which introduces a tunable lower bound to prevent excessive gradient suppression. Our method improves the accuracy of the worst-performing class on average over 10 percentage points on skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and over 5 percentage points over constant clipping.</li>
</ul>

<h3>Title: Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xue Xian Zheng, Weihang Liu, Xin Lou, Stefan Vlaski, Tareq Al-Naffouri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01404">https://arxiv.org/abs/2506.01404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01404">https://arxiv.org/pdf/2506.01404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01404]] Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs(https://arxiv.org/abs/2506.01404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative error feedback framework designed to mitigate quantization noise in distributed graph filtering, where communications are constrained to quantized messages. It comes from error spectrum shaping techniques from state-space digital filters, and therefore establishes connections between quantized filtering processes over different domains. In contrast to existing error compensation methods, our framework quantitatively feeds back the quantization noise for exact compensation. We examine the framework under three key scenarios: (i) deterministic graph filtering, (ii) graph filtering over random graphs, and (iii) graph filtering with random node-asynchronous updates. Rigorous theoretical analysis demonstrates that the proposed framework significantly reduces the effect of quantization noise, and we provide closed-form solutions for the optimal error feedback coefficients. Moreover, this quantitative error feedback mechanism can be seamlessly integrated into communication-efficient decentralized optimization frameworks, enabling lower error floors. Numerical experiments validate the theoretical results, consistently showing that our method outperforms conventional quantization strategies in terms of both accuracy and robustness.</li>
</ul>

<h3>Title: Comparing LLM-generated and human-authored news text using formal syntactic theory</h3>
<ul>
<li><strong>Authors: </strong>Olga Zamaraeva, Dan Flickinger, Francis Bond, Carlos Gómez-Rodríguez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01407">https://arxiv.org/abs/2506.01407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01407">https://arxiv.org/pdf/2506.01407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01407]] Comparing LLM-generated and human-authored news text using formal syntactic theory(https://arxiv.org/abs/2506.01407)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing. The comparison is based on a formal syntactic theory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structure of the texts. We then investigate and illustrate the differences in the distributions of HPSG grammar types, revealing systematic distinctions between human and LLM-generated writing. These findings contribute to a deeper understanding of the syntactic behavior of LLMs as well as humans, within the NYT genre.</li>
</ul>

<h3>Title: ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition</h3>
<ul>
<li><strong>Authors: </strong>Minjeong Park, Hongbeen Park, Jinkyu Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01411">https://arxiv.org/abs/2506.01411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01411">https://arxiv.org/pdf/2506.01411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01411]] ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition(https://arxiv.org/abs/2506.01411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Pedestrian Attribute Recognition (PAR) task aims to identify various detailed attributes of an individual, such as clothing, accessories, and gender. To enhance PAR performance, a model must capture features ranging from coarse-grained global attributes (e.g., for identifying gender) to fine-grained local details (e.g., for recognizing accessories) that may appear in diverse regions. Recent research suggests that body part representation can enhance the model's robustness and accuracy, but these methods are often restricted to attribute classes within fixed horizontal regions, leading to degraded performance when attributes appear in varying or unexpected body locations. In this paper, we propose Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance attribute recognition through specialized multimodal prompting and vision-language alignment. We introduce visual attribute prompts that capture global-to-local semantics, enabling diverse attribute representations. To enrich textual embeddings, we design a learnable prompt template, termed person and attribute context prompting, to learn person and attributes context. Finally, we align visual and textual attribute features for effective fusion. ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance with efficient inference. We release our code and model at this https URL.</li>
</ul>

<h3>Title: Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01413">https://arxiv.org/abs/2506.01413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01413">https://arxiv.org/pdf/2506.01413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01413]] Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models(https://arxiv.org/abs/2506.01413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at this https URL.</li>
</ul>

<h3>Title: Self-supervised Latent Space Optimization with Nebula Variational Coding</h3>
<ul>
<li><strong>Authors: </strong>Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01414">https://arxiv.org/abs/2506.01414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01414">https://arxiv.org/pdf/2506.01414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01414]] Self-supervised Latent Space Optimization with Nebula Variational Coding(https://arxiv.org/abs/2506.01414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning approaches process data in a layer-by-layer way with intermediate (or latent) features. We aim at designing a general solution to optimize the latent manifolds to improve the performance on classification, segmentation, completion and/or reconstruction through probabilistic models. This paper proposes a variational inference model which leads to a clustered embedding. We introduce additional variables in the latent space, called \textbf{nebula anchors}, that guide the latent variables to form clusters during training. To prevent the anchors from clustering among themselves, we employ the variational constraint that enforces the latent features within an anchor to form a Gaussian distribution, resulting in a generative model we refer as Nebula Variational Coding (NVC). Since each latent feature can be labeled with the closest anchor, we also propose to apply metric learning in a self-supervised way to make the separation between clusters more explicit. As a consequence, the latent variables of our variational coder form clusters which adapt to the generated semantic of the training data, \textit{e.g.} the categorical labels of each sample. We demonstrate experimentally that it can be used within different architectures designed to solve different problems including text sequence, images, 3D point clouds and volumetric data, validating the advantage of our proposed method.</li>
</ul>

<h3>Title: Self-Refining Language Model Anonymizers via Adversarial Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kyuyoung Kim, Hyunjun Jeon, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01420">https://arxiv.org/abs/2506.01420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01420">https://arxiv.org/pdf/2506.01420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01420]] Self-Refining Language Model Anonymizers via Adversarial Distillation(https://arxiv.org/abs/2506.01420)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in sensitive domains, where their ability to infer personal data from seemingly benign text poses emerging privacy risks. While recent LLM-based anonymization methods help mitigate such risks, they often rely on proprietary models (e.g., GPT-4), raising concerns about cost and the potential exposure of sensitive data to untrusted external systems. To address this, we introduce SElf-refining Anonymization with Language model (SEAL), a novel distillation framework for training small language models (SLMs) to perform effective anonymization without relying on external costly models at inference time. We leverage adversarial interactions between an LLM anonymizer and an inference model to collect trajectories of anonymized texts and inferred attributes, which are used to distill anonymization, adversarial inference, and utility evaluation capabilities into SLMs via supervised fine-tuning and preference learning. The resulting models learn to both anonymize text and critique their outputs, enabling iterative improvement of anonymization quality via self-refinement. Experiments on SynthPAI, a dataset of synthetic personal profiles and text comments, demonstrate that SLMs trained with SEAL achieve substantial improvements in anonymization capabilities. Notably, 8B models attain a privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with self-refinement, even surpass it in terms of privacy. These results show the effectiveness of our adversarial distillation framework in training SLMs as efficient anonymizers. To facilitate further research, we release the full dataset used in our experiments.</li>
</ul>

<h3>Title: CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive Shuffling Against Overfitting</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Zhenya Ma, Yan Zhang, Donghua Cai, Ye Zhang, Qiushi Li, Yongheng Deng, Ye Guo, Ju Ren, Xuemin (Sherman)Shen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01425">https://arxiv.org/abs/2506.01425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01425">https://arxiv.org/pdf/2506.01425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01425]] CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive Shuffling Against Overfitting(https://arxiv.org/abs/2506.01425)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, attack, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Although federated learning preserves training data within local privacy domains, the aggregated model parameters may still reveal private characteristics. This vulnerability stems from clients' limited training data, which predisposes models to overfitting. Such overfitting enables models to memorize distinctive patterns from training samples, thereby amplifying the success probability of privacy attacks like membership inference. To enhance visual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image Shuffling with Variance-Guided Adaptive Region Partitioning), a novel image shuffling framework to generate obfuscated images for secure data transmission and each training epoch, addressing both overfitting-induced privacy leaks and raw image transmission risks. CSVAR adopts region-variance as the metric to measure visual privacy sensitivity across image regions. Guided by this, CSVAR adaptively partitions each region into multiple blocks, applying fine-grained partitioning to privacy-sensitive regions with high region-variances for enhancing visual privacy protection and coarse-grained partitioning to privacy-insensitive regions for balancing model utility. In each region, CSVAR then shuffles between blocks in both the spatial domains and chromatic channels to hide visual spatial features and disrupt color distribution. Experimental evaluations conducted on diverse real-world datasets demonstrate that CSVAR is capable of generating visually obfuscated images that exhibit high perceptual ambiguity to human eyes, simultaneously mitigating the effectiveness of adversarial data reconstruction attacks and achieving a good trade-off between visual privacy protection and model utility.</li>
</ul>

<h3>Title: DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Xie, Minghan Li, Shuai Li, Yuhui Wu, Qiaosi Yi, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01430">https://arxiv.org/abs/2506.01430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01430">https://arxiv.org/pdf/2506.01430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01430]] DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing(https://arxiv.org/abs/2506.01430)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Leveraging the powerful generation capability of large-scale pretrained text-to-image models, training-free methods have demonstrated impressive image editing results. Conventional diffusion-based methods, as well as recent rectified flow (RF)-based methods, typically reverse synthesis trajectories by gradually adding noise to clean images, during which the noisy latent at the current timestep is used to approximate that at the next timesteps, introducing accumulated drift and degrading reconstruction accuracy. Considering the fact that in RF the noisy latent is estimated through direct interpolation between Gaussian noises and clean images at each timestep, we propose Direct Noise Alignment (DNA), which directly refines the desired Gaussian noise in the noise domain, significantly reducing the error accumulation in previous methods. Specifically, DNA estimates the velocity field of the interpolated noised latent at each timestep and adjusts the Gaussian noise by computing the difference between the predicted and expected velocity field. We validate the effectiveness of DNA and reveal its relationship with existing RF-based inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG) to control the target prompt-guided generation process, balancing image background preservation and target object editability. DNA and MVG collectively constitute our proposed method, namely DNAEdit. Finally, we introduce DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced image editing models. Experimental results demonstrate that our DNAEdit achieves superior performance to state-of-the-art text-guided editing methods. Codes and benchmark will be available at \href{ this https URL}{this https URL}.</li>
</ul>

<h3>Title: Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data</h3>
<ul>
<li><strong>Authors: </strong>Yosuke Kashiwagi, Hayato Futami, Emiru Tsunoo, Satoshi Asakawa</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01439">https://arxiv.org/abs/2506.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01439">https://arxiv.org/pdf/2506.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01439]] Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data(https://arxiv.org/abs/2506.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper reports on the development of a large-scale speech recognition model, Whale. Similar to models such as Whisper and OWSM, Whale leverages both a large model size and a diverse, extensive dataset. Whale's architecture integrates w2v-BERT self-supervised model, an encoder-decoder backbone built on E-Branchformer, and a joint CTC-attention decoding strategy. The training corpus comprises varied speech data, of not only public corpora but also in-house data, thereby enhancing the model's robustness to different speaking styles and acoustic conditions. Through evaluations on multiple benchmarks, Whale achieved comparable performance to existing models. In particular, it achieves a word error rate of 2.4% on the Librispeech test-clean set and a character error rate of 3.4% on the CSJ eval3 set, outperforming Whisper large-v3 and OWSM v3.1.</li>
</ul>

<h3>Title: Variance-Based Defense Against Blended Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Sujeevan Aseervatham, Achraf Kerzazi, Younès Bennani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01444">https://arxiv.org/abs/2506.01444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01444">https://arxiv.org/pdf/2506.01444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01444]] Variance-Based Defense Against Blended Backdoor Attacks(https://arxiv.org/abs/2506.01444)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal, explainability</a></li>
<li><strong>Abstract: </strong>Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.</li>
</ul>

<h3>Title: A Novel Context-Adaptive Fusion of Shadow and Highlight Regions for Efficient Sonar Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Kamal Basha S, Anukul Kiran B, Athira Nambiar, Suresh Rajendran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01445">https://arxiv.org/abs/2506.01445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01445">https://arxiv.org/pdf/2506.01445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01445]] A Novel Context-Adaptive Fusion of Shadow and Highlight Regions for Efficient Sonar Image Classification(https://arxiv.org/abs/2506.01445)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, interpretability, explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Sonar imaging is fundamental to underwater exploration, with critical applications in defense, navigation, and marine research. Shadow regions, in particular, provide essential cues for object detection and classification, yet existing studies primarily focus on highlight-based analysis, leaving shadow-based classification underexplored. To bridge this gap, we propose a Context-adaptive sonar image classification framework that leverages advanced image processing techniques to extract and integrate discriminative shadow and highlight features. Our framework introduces a novel shadow-specific classifier and adaptive shadow segmentation, enabling effective classification based on the dominant region. This approach ensures optimal feature representation, improving robustness against noise and occlusions. In addition, we introduce a Region-aware denoising model that enhances sonar image quality by preserving critical structural details while suppressing noise. This model incorporates an explainability-driven optimization strategy, ensuring that denoising is guided by feature importance, thereby improving interpretability and classification reliability. Furthermore, we present S3Simulator+, an extended dataset incorporating naval mine scenarios with physics-informed noise specifically tailored for the underwater sonar domain, fostering the development of robust AI models. By combining novel classification strategies with an enhanced dataset, our work addresses key challenges in sonar image analysis, contributing to the advancement of autonomous underwater perception.</li>
</ul>

<h3>Title: ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Manuel Franco de la Peña (1), Ángel Luis Perales Gómez (1), Lorenzo Fernández Maimó (1) ((1) Departamento de Ingeniería y Tecnología de Computadores, University of Murcia, Spain, Murcia)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01450">https://arxiv.org/abs/2506.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01450">https://arxiv.org/pdf/2506.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01450]] ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things(https://arxiv.org/abs/2506.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Industrial Internet of Things environments increasingly rely on advanced Anomaly Detection and explanation techniques to rapidly detect and mitigate cyberincidents, thereby ensuring operational safety. The sequential nature of data collected from these environments has enabled improvements in Anomaly Detection using Machine Learning and Deep Learning models by processing time windows rather than treating the data as tabular. However, conventional explanation methods often neglect this temporal structure, leading to imprecise or less actionable explanations. This work presents ShaTS (Shapley values for Time Series models), which is a model-agnostic explainable Artificial Intelligence method designed to enhance the precision of Shapley value explanations for time series models. ShaTS addresses the shortcomings of traditional approaches by incorporating an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS accurately identifies critical time instants, precisely pinpoints the sensors, actuators, and processes affected by anomalies, and outperforms SHAP in terms of both explainability and resource efficiency, fulfilling the real-time requirements of industrial environments.</li>
</ul>

<h3>Title: Building Entity Association Mining Framework for Knowledge Discovery</h3>
<ul>
<li><strong>Authors: </strong>Anshika Rawal, Abhijeet Kumar, Mridul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01451">https://arxiv.org/abs/2506.01451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01451">https://arxiv.org/pdf/2506.01451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01451]] Building Entity Association Mining Framework for Knowledge Discovery(https://arxiv.org/abs/2506.01451)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Extracting useful signals or pattern to support important business decisions for example analyzing investment product traction and discovering customer preference, risk monitoring etc. from unstructured text is a challenging task. Capturing interaction of entities or concepts and association mining is a crucial component in text mining, enabling information extraction and reasoning over and knowledge discovery from text. Furthermore, it can be used to enrich or filter knowledge graphs to guide exploration processes, descriptive analytics and uncover hidden stories in the text. In this paper, we introduce a domain independent pipeline i.e., generalized framework to enable document filtering, entity extraction using various sources (or techniques) as plug-ins and association mining to build any text mining business use-case and quantitatively define a scoring metric for ranking purpose. The proposed framework has three major components a) Document filtering: filtering documents/text of interest from massive amount of texts b) Configurable entity extraction pipeline: include entity extraction techniques i.e., i) DBpedia Spotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or dictionary) based c) Association Relationship Mining: To generates co-occurrence graph to analyse potential relationships among entities, concepts. Further, co-occurrence count based frequency statistics provide a holistic window to observe association trends or buzz rate in specific business context. The paper demonstrates the usage of framework as fundamental building box in two financial use-cases namely brand product discovery and vendor risk monitoring. We aim that such framework will remove duplicated effort, minimize the development effort, and encourage reusability and rapid prototyping in association mining business applications for institutions.</li>
</ul>

<h3>Title: DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Geunmin Hwang, Hyun-kyu Ko, Younghyun Kim, Seungryong Lee, Eunbyung Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01454">https://arxiv.org/abs/2506.01454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01454">https://arxiv.org/pdf/2506.01454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01454]] DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion(https://arxiv.org/abs/2506.01454)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have revolutionized video generation, enabling the creation of high-quality, temporally consistent videos. However, generating high frame-rate (FPS) videos remains a significant challenge due to issues such as flickering and degradation in long sequences, particularly in fast-motion scenarios. Existing methods often suffer from computational inefficiencies and limitations in maintaining video quality over extended frames. In this paper, we present a novel, training-free approach for high FPS video generation using pre-trained diffusion models. Our method, DiffuseSlide, introduces a new pipeline that leverages key frames from low FPS videos and applies innovative techniques, including noise re-injection and sliding window latent denoising, to achieve smooth, consistent video outputs without the need for additional fine-tuning. Through extensive experiments, we demonstrate that our approach significantly improves video quality, offering enhanced temporal coherence and spatial fidelity. The proposed method is not only computationally efficient but also adaptable to various video generation tasks, making it ideal for applications such as virtual reality, video games, and high-quality content creation.</li>
</ul>

<h3>Title: First-Spammed, First-Served: MEV Extraction on Fast-Finality Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Gogol, Manvir Schneider, Claudio Tessone</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01462">https://arxiv.org/abs/2506.01462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01462">https://arxiv.org/pdf/2506.01462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01462]] First-Spammed, First-Served: MEV Extraction on Fast-Finality Blockchains(https://arxiv.org/abs/2506.01462)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This research analyzes the economics of spam-based arbitrage strategies on fast-finality blockchains. We begin by theoretically demonstrating that, splitting a profitable MEV opportunity into multiple small transactions is the optimal strategy for CEX-DEX arbitrageurs. We then empirically validate these findings on major Ethereum rollups. To uncover the structure of reverted transactions, we construct execution graphs from transaction traces and systematically search them to identify DEX or router interactions and targeted liquidity pools. This analysis reveals that 80\% of reverted transactions are swaps with approximately 50\% targeting USDC-WETH pools on Uniswap v3/v4. These patterns intensified following the March 2024 Dencun upgrade, which lowered L2 gas costs and made spam-based arbitrage economically viable. Counterintuitively, we find that these reverted MEV transactions rarely engage with Priority Fee Auctions (PFAs), preferring to submit duplicate transactions rather than bid for inclusion. Moreover, reverted transactions cluster at the very top of blocks on fast rollups like Arbitrum and ZKsync, indicating an intense latency race and revealing the fragility of fee-based ordering under sub-second block times.</li>
</ul>

<h3>Title: Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Yang, Yilun Wang, Yaxiong Wang, Li Zhu, Zhedong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01466">https://arxiv.org/abs/2506.01466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01466">https://arxiv.org/pdf/2506.01466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01466]] Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark(https://arxiv.org/abs/2506.01466)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, steal, generative, large language model</a></li>
<li><strong>Abstract: </strong>Video anomaly retrieval aims to localize anomalous events in videos using natural language queries to facilitate public safety. However, existing datasets suffer from severe limitations: (1) data scarcity due to the long-tail nature of real-world anomalies, and (2) privacy constraints that impede large-scale collection. To address the aforementioned issues in one go, we introduce SVTA (Synthetic Video-Text Anomaly benchmark), the first large-scale dataset for cross-modal anomaly retrieval, leveraging generative models to overcome data availability challenges. Specifically, we collect and generate video descriptions via the off-the-shelf LLM (Large Language Model) covering 68 anomaly categories, e.g., throwing, stealing, and shooting. These descriptions encompass common long-tail events. We adopt these texts to guide the video generative model to produce diverse and high-quality videos. Finally, our SVTA involves 41,315 videos (1.36M frames) with paired captions, covering 30 normal activities, e.g., standing, walking, and sports, and 68 anomalous events, e.g., falling, fighting, theft, explosions, and natural disasters. We adopt three widely-used video-text retrieval baselines to comprehensively test our SVTA, revealing SVTA's challenging nature and its effectiveness in evaluating a robust cross-modal retrieval method. SVTA eliminates privacy risks associated with real-world anomaly collection while maintaining realistic scenarios. The dataset demo is available at: [this https URL].</li>
</ul>

<h3>Title: Feature-aware Hypergraph Generation via Next-Scale Prediction</h3>
<ul>
<li><strong>Authors: </strong>Dorian Gailhard, Enzo Tartaglione, Lirida Naviner, Jhony H. Giraldo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01467">https://arxiv.org/abs/2506.01467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01467">https://arxiv.org/pdf/2506.01467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01467]] Feature-aware Hypergraph Generation via Next-Scale Prediction(https://arxiv.org/abs/2506.01467)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hypergraphs generalize traditional graphs by allowing hyperedges to connect multiple nodes, making them well-suited for modeling complex structures with higher-order relationships, such as 3D meshes, molecular systems, and electronic circuits. While topology is central to hypergraph structure, many real-world applications also require node and hyperedge features. Existing hypergraph generation methods focus solely on topology, often overlooking feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph generation via next-scale prediction), a hierarchical approach that jointly generates hypergraph topology and features. FAHNES builds a multi-scale representation through node coarsening, then learns to reconstruct finer levels via localized expansion and refinement, guided by a new node budget mechanism that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs, 3D meshes, and molecular datasets. FAHNES achieves competitive results in reconstructing topology and features, establishing a foundation for future research in featured hypergraph generative modeling.</li>
</ul>

<h3>Title: SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yiping Li, Ronald de Jong, Sahar Nasirihaghighi, Tim Jaspers, Romy van Jaarsveld, Gino Kuiper, Richard van Hillegersberg, Fons van der Sommen, Jelle Ruurda, Marcel Breeuwer, Yasmina Al Khalil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01471">https://arxiv.org/abs/2506.01471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01471">https://arxiv.org/pdf/2506.01471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01471]] SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition(https://arxiv.org/abs/2506.01471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurate surgical phase recognition is crucial for computer-assisted interventions and surgical video analysis. Annotating long surgical videos is labor-intensive, driving research toward leveraging unlabeled data for strong performance with minimal annotations. Although self-supervised learning has gained popularity by enabling large-scale pretraining followed by fine-tuning on small labeled subsets, semi-supervised approaches remain largely underexplored in the surgical domain. In this work, we propose a video transformer-based model with a robust pseudo-labeling framework. Our method incorporates temporal consistency regularization for unlabeled data and contrastive learning with class prototypes, which leverages both labeled data and pseudo-labels to refine the feature space. Through extensive experiments on the private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and the public Cholec80 dataset, we demonstrate the effectiveness of our approach. By incorporating unlabeled data, we achieve state-of-the-art performance on RAMIE with a 4.9% accuracy increase and obtain comparable results to full supervision while using only 1/4 of the labeled data on Cholec80. Our findings establish a strong benchmark for semi-supervised surgical phase recognition, paving the way for future research in this domain.</li>
</ul>

<h3>Title: Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, Yunfei Li, Siliang Tang, Jun Xiao, Fei Wu, Hang Zhao, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01480">https://arxiv.org/abs/2506.01480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01480">https://arxiv.org/pdf/2506.01480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01480]] Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation(https://arxiv.org/abs/2506.01480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation. However, these two capabilities remain largely independent, as if they are two separate functions encapsulated within the same model. Consequently, visual comprehension does not enhance visual generation, and the reasoning mechanisms of LLMs have not been fully integrated to revolutionize image generation. In this paper, we propose to enable the collaborative co-evolution of visual comprehension and generation, advancing image generation into an iterative introspective process. We introduce a two-stage training approach: supervised fine-tuning teaches the MLLM with the foundational ability to generate genuine CoT for visual generation, while reinforcement learning activates its full potential via an exploration-exploitation trade-off. Ultimately, we unlock the Aha moment in visual generation, advancing MLLMs from text-to-image tasks to unified image generation. Extensive experiments demonstrate that our model not only excels in text-to-image generation and image editing, but also functions as a superior image semantic evaluator with enhanced visual comprehension capabilities. Project Page: this https URL.</li>
</ul>

<h3>Title: Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhao, Dian Jin, Zijing Zhou, Xiaoyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MM, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01482">https://arxiv.org/abs/2506.01482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01482">https://arxiv.org/pdf/2506.01482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01482]] Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?(https://arxiv.org/abs/2506.01482)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Stage lighting plays an essential role in live music performances, influencing the engaging experience of both musicians and audiences. Given the high costs associated with hiring or training professional lighting engineers, Automatic Stage Lighting Control (ASLC) has gained increasing attention. However, most existing approaches only classify music into limited categories and map them to predefined light patterns, resulting in formulaic and monotonous outcomes that lack rationality. To address this issue, this paper presents an end-to-end solution that directly learns from experienced lighting engineers -- Skip-BART. To the best of our knowledge, this is the first work to conceptualize ASLC as a generative task rather than merely a classification problem. Our method modifies the BART model to take audio music as input and produce light hue and value (intensity) as output, incorporating a novel skip connection mechanism to enhance the relationship between music and light within the frame this http URL validate our method through both quantitative analysis and an human evaluation, demonstrating that Skip-BART outperforms conventional rule-based methods across all evaluation metrics and shows only a limited gap compared to real lighting this http URL, our method yields a p-value of 0.72 in a statistical comparison based on human evaluations with human lighting engineers, suggesting that the proposed approach closely matches human lighting engineering performance. To support further research, we have made our self-collected dataset, code, and trained model parameters available at this https URL .</li>
</ul>

<h3>Title: Model-agnostic Mitigation Strategies of Data Imbalance for Regression</h3>
<ul>
<li><strong>Authors: </strong>Jelke Wibbeke, Sebastian Rohjans, Andreas Rauh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01486">https://arxiv.org/abs/2506.01486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01486">https://arxiv.org/pdf/2506.01486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01486]] Model-agnostic Mitigation Strategies of Data Imbalance for Regression(https://arxiv.org/abs/2506.01486)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Data imbalance persists as a pervasive challenge in regression tasks, introducing bias in model performance and undermining predictive reliability. This is particularly detrimental in applications aimed at predicting rare events that fall outside the domain of the bulk of the training data. In this study, we review the current state-of-the-art regarding sampling-based methods and cost-sensitive learning. Additionally, we propose novel approaches to mitigate model bias. To better asses the importance of data, we introduce the density-distance and density-ratio relevance functions, which effectively integrate empirical frequency of data with domain-specific preferences, offering enhanced interpretability for end-users. Furthermore, we present advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and improve existing sampling methods. In a comprehensive quantitative evaluation, we benchmark state-of-the-art methods on 10 synthetic and 42 real-world datasets, using neural networks, XGBoosting trees and Random Forest models. Our analysis reveals that while most strategies improve performance on rare samples, they often degrade it on frequent ones. We demonstrate that constructing an ensemble of models -- one trained with imbalance mitigation and another without -- can significantly reduce these negative effects. The key findings underscore the superior performance of our novel crbSMOGN sampling technique with the density-ratio relevance function for neural networks, outperforming state-of-the-art methods.</li>
</ul>

<h3>Title: Multilingual Definition Modeling</h3>
<ul>
<li><strong>Authors: </strong>Edison Marrese-Taylor, Erica K. Shimomoto, Alfredo Solano, Enrique Reid</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01489">https://arxiv.org/abs/2506.01489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01489">https://arxiv.org/pdf/2506.01489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01489]] Multilingual Definition Modeling(https://arxiv.org/abs/2506.01489)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose the first multilingual study on definition modeling. We use monolingual dictionary data for four new languages (Spanish, French, Portuguese, and German) and perform an in-depth empirical study to test the performance of pre-trained multilingual language models on definition modeling of monosemic words when finetuned on this data. Furthermore, we use a zero-shot approach to test the multilingual capabilities of two popular chat-based Large Language Models (LLMs) in the task. Results show that multilingual language models can perform on-pair with English but cannot leverage potential cross-lingual synergies, with LLMs generally offering better performance overall. A comprehensive human evaluation of the LLM-generated definition highlights the zero and few-shot capabilities of these models in this new task, also showing their shortcomings. Finally, we show that performance on our task via BERTScore strongly correlates to the performance on multilingual LLM benchmarks, suggesting that our task offers a viable compute-constrained, stable and natural alternative to these.</li>
</ul>

<h3>Title: Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities</h3>
<ul>
<li><strong>Authors: </strong>Yanxi Luo, Shijin Wang, Zhongxing Xu, Yulong Li, Feilong Tang, Jionglong Su</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01490">https://arxiv.org/abs/2506.01490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01490">https://arxiv.org/pdf/2506.01490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01490]] Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities(https://arxiv.org/abs/2506.01490)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. In real-world scenarios, practical factors often lead to uncertain modality missingness. Existing methods for handling modality missingness are based on data reconstruction or common subspace projections. However, these methods neglect the confidence in multimodal combinations and impose constraints on intra-class representation, hindering the capture of modality-specific information and resulting in suboptimal performance. To address these challenges, we propose a Confidence-Aware Self-Distillation (CASD) strategy that effectively incorporates multimodal probabilistic embeddings via a mixture of Student's $t$-distributions, enhancing its robustness by incorporating confidence and accommodating heavy-tailed properties. This strategy estimates joint distributions with uncertainty scores and reduces uncertainty in the student network by consistency distillation. Furthermore, we introduce a reparameterization representation module that facilitates CASD in robust multimodal learning by sampling embeddings from the joint distribution for the prediction module to calculate the task loss. As a result, the directional constraint from the loss minimization is alleviated by the sampled representation. Experimental results on three benchmark datasets demonstrate that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity</h3>
<ul>
<li><strong>Authors: </strong>Yuya Kobayashi, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01493">https://arxiv.org/abs/2506.01493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01493">https://arxiv.org/pdf/2506.01493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01493]] Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity(https://arxiv.org/abs/2506.01493)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, Generative Adversarial Networks (GANs) have been successfully scaled to billion-scale large text-to-image datasets. However, training such models entails a high training cost, limiting some applications and research usage. To reduce the cost, one promising direction is the incorporation of pre-trained models. The existing method of utilizing pre-trained models for a generator significantly reduced the training cost compared with the other large-scale GANs, but we found the model loses the diversity of generation for a given prompt by a large margin. To build an efficient and high-fidelity text-to-image GAN without compromise, we propose to use two specialized discriminators with Slicing Adversarial Networks (SANs) adapted for text-to-image tasks. Our proposed model, called SCAD, shows a notable enhancement in diversity for a given prompt with better sample fidelity. We also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID competitive with the latest large-scale GANs at two orders of magnitude less training cost.</li>
</ul>

<h3>Title: CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01495">https://arxiv.org/abs/2506.01495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01495">https://arxiv.org/pdf/2506.01495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01495]] CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models(https://arxiv.org/abs/2506.01495)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring that Large Language Models (LLMs) align with mainstream human values and ethical norms is crucial for the safe and sustainable development of AI. Current value evaluation and alignment are constrained by Western cultural bias and incomplete domestic frameworks reliant on non-native rules; furthermore, the lack of scalable, rule-driven scenario generation methods makes evaluations costly and inadequate across diverse cultural contexts. To address these challenges, we propose a hierarchical value framework grounded in core Chinese values, encompassing three main dimensions, 12 core values, and 50 derived values. Based on this framework, we construct a large-scale Chinese Values Corpus (CVC) containing over 250,000 value rules enhanced and expanded through human annotation. Experimental results show that CVC-guided scenarios outperform direct generation ones in value boundaries and content diversity. In the evaluation across six sensitive themes (e.g., surrogacy, suicide), seven mainstream LLMs preferred CVC-generated options in over 70.5% of cases, while five Chinese human annotators showed an 87.5% alignment with CVC, confirming its universality, cultural relevance, and strong alignment with Chinese values. Additionally, we construct 400,000 rule-based moral dilemma scenarios that objectively capture nuanced distinctions in conflicting value prioritization across 17 LLMs. Our work establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment, representing Chinese characteristics. All data are available at this https URL, and the code is available at this https URL.</li>
</ul>

<h3>Title: Continual Speech Learning with Fused Speech Features</h3>
<ul>
<li><strong>Authors: </strong>Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01496">https://arxiv.org/abs/2506.01496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01496">https://arxiv.org/pdf/2506.01496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01496]] Continual Speech Learning with Fused Speech Features(https://arxiv.org/abs/2506.01496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rapid growth in speech data demands adaptive models, as traditional static methods fail to keep pace with dynamic and diverse speech information. We introduce continuous speech learning, a new set-up targeting at bridging the adaptation gap in current speech models. We use the encoder-decoder Whisper model to standardize speech tasks into a generative format. We integrate a learnable gated-fusion layer on the top of the encoder to dynamically select task-specific features for downstream tasks. Our approach improves accuracy significantly over traditional methods in six speech processing tasks, demonstrating gains in adapting to new speech tasks without full retraining.</li>
</ul>

<h3>Title: Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment</h3>
<ul>
<li><strong>Authors: </strong>Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01511">https://arxiv.org/abs/2506.01511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01511">https://arxiv.org/pdf/2506.01511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01511]] Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment(https://arxiv.org/abs/2506.01511)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion</a></li>
<li><strong>Abstract: </strong>Preference alignment in diffusion models has primarily focused on benign human preferences (e.g., aesthetic). In this paper, we propose a novel perspective: framing unrestricted adversarial example generation as a problem of aligning with adversary preferences. Unlike benign alignment, adversarial alignment involves two inherently conflicting preferences: visual consistency and attack effectiveness, which often lead to unstable optimization and reward hacking (e.g., reducing visual quality to improve attack success). To address this, we propose APA (Adversary Preferences Alignment), a two-stage framework that decouples conflicting preferences and optimizes each with differentiable rewards. In the first stage, APA fine-tunes LoRA to improve visual consistency using rule-based similarity reward. In the second stage, APA updates either the image latent or prompt embedding based on feedback from a substitute classifier, guided by trajectory-level and step-wise rewards. To enhance black-box transferability, we further incorporate a diffusion augmentation strategy. Experiments demonstrate that APA achieves significantly better attack transferability while maintaining high visual consistency, inspiring further research to approach adversarial attacks from an alignment perspective. Code will be available at this https URL.</li>
</ul>

<h3>Title: Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes</h3>
<ul>
<li><strong>Authors: </strong>Meng Li, Michael Vrazitulis, David Schlangen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01512">https://arxiv.org/abs/2506.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01512">https://arxiv.org/pdf/2506.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01512]] Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes(https://arxiv.org/abs/2506.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.</li>
</ul>

<h3>Title: Speed-up of Vision Transformer Models by Attention-aware Token Filtering</h3>
<ul>
<li><strong>Authors: </strong>Takahiro Naruko, Hiroaki Akutsu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01519">https://arxiv.org/abs/2506.01519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01519">https://arxiv.org/pdf/2506.01519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01519]] Speed-up of Vision Transformer Models by Attention-aware Token Filtering(https://arxiv.org/abs/2506.01519)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformer (ViT) models have made breakthroughs in image embedding extraction, which provide state-of-the-art performance in tasks such as zero-shot image classification. However, the models suffer from a high computational burden. In this paper, we propose a novel speed-up method for ViT models called Attention-aware Token Filtering (ATF). ATF consists of two main ideas: a novel token filtering module and a filtering strategy. The token filtering module is introduced between a tokenizer and a transformer encoder of the ViT model, without modifying or fine-tuning of the transformer encoder. The module filters out tokens inputted to the encoder so that it keeps tokens in regions of specific object types dynamically and keeps tokens in regions that statically receive high attention in the transformer encoder. This filtering strategy maintains task accuracy while filtering out tokens inputted to the transformer encoder. Evaluation results on retrieval tasks show that ATF provides $2.8\times$ speed-up to a ViT model, SigLIP, while maintaining the retrieval recall rate.</li>
</ul>

<h3>Title: FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents</h3>
<ul>
<li><strong>Authors: </strong>Bobo Li, Yuheng Wang, Hao Fei, Juncheng Li, Wei Ji, Mong-Li Lee, Wynne Hsu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01520">https://arxiv.org/abs/2506.01520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01520">https://arxiv.org/pdf/2506.01520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01520]] FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents(https://arxiv.org/abs/2506.01520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Online form filling is a common yet labor-intensive task involving extensive keyboard and mouse interactions. Despite the long-standing vision of automating this process with "one click", existing tools remain largely rule-based and lack generalizable, generative capabilities. Recent advances in Multimodal Large Language Models (MLLMs) have enabled promising agents for GUI-related tasks in general-purpose scenarios. However, they struggle with the unique challenges of form filling, such as flexible layouts and the difficulty of aligning textual instructions with on-screen fields. To bridge this gap, we formally define the form-filling task and propose FormFactory, an interactive benchmarking suite comprising a web-based interface, backend evaluation module, and carefully constructed dataset. Our benchmark covers diverse real-world scenarios, incorporates various field formats, and simulates high-fidelity form interactions. We conduct a comprehensive evaluation of state-of-the-art MLLMs and observe that no model surpasses 5% accuracy, underscoring the inherent difficulty of the task. These findings also reveal significant limitations in current models' visual layout reasoning and field-value alignment abilities. We hope our benchmark can serve as a stepping stone for further research into robust, practical form-filling agents.</li>
</ul>

<h3>Title: Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows</h3>
<ul>
<li><strong>Authors: </strong>Peter Sorrenson, Lukas Lührs, Hans Olischläger, Ullrich Köthe</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01522">https://arxiv.org/abs/2506.01522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01522">https://arxiv.org/pdf/2506.01522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01522]] Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows(https://arxiv.org/abs/2506.01522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) are powerful generative models widely used for learning interpretable latent spaces, quantifying uncertainty, and compressing data for downstream generative tasks. VAEs typically rely on diagonal Gaussian posteriors due to computational constraints. Using arguments grounded in differential geometry, we demonstrate inherent limitations in the representational capacity of diagonal covariance VAEs, as illustrated by explicit low-dimensional examples. In response, we show that a regularized variant of the recently introduced Free-form Injective Flow (FIF) can be interpreted as a VAE featuring a highly flexible, implicitly defined posterior. Crucially, this regularization yields a posterior equivalent to a full Gaussian covariance distribution, yet maintains computational costs comparable to standard diagonal covariance VAEs. Experiments on image datasets validate our approach, demonstrating that incorporating full covariance substantially improves model likelihood.</li>
</ul>

<h3>Title: Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jihun Yun, Juno Kim, Jongho Park, Junhyuck Kim, Jongha Jon Ryu, Jaewoong Cho, Kwang-Sung Jun</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01523">https://arxiv.org/abs/2506.01523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01523">https://arxiv.org/pdf/2506.01523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01523]] Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model(https://arxiv.org/abs/2506.01523)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment via reinforcement learning from human feedback (RLHF) has become the dominant paradigm for controlling the quality of outputs from large language models (LLMs). However, when viewed as `loss + regularization,' the standard RLHF objective lacks theoretical justification and incentivizes degenerate, deterministic solutions, an issue that variants such as Direct Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by framing it as \emph{distribution learning} from pairwise preference feedback by explicitly modeling how information about the target language model bleeds through the preference data. This explicit modeling leads us to propose three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization. We theoretically show that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Finally, we empirically demonstrate that our distribution learning framework, especially preference distillation, consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.</li>
</ul>

<h3>Title: V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat</h3>
<ul>
<li><strong>Authors: </strong>Qi Lin, Weikai Xu, Lisi Chen, Bin Dai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01524">https://arxiv.org/abs/2506.01524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01524">https://arxiv.org/pdf/2506.01524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01524]] V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat(https://arxiv.org/abs/2506.01524)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.</li>
</ul>

<h3>Title: STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, Yan Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01531">https://arxiv.org/abs/2506.01531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01531">https://arxiv.org/pdf/2506.01531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01531]] STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework(https://arxiv.org/abs/2506.01531)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation. To address these, we introduce $\textbf{STORM-BORN}$, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues. To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians' evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems. Even most advanced models like GPT-o1 solved fewer than $5\%$ of them. Fine-tuning on STORM-BORN boosts accuracy by $7.84\%$ (LLaMA3-8B) and $9.12\%$ (Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Ma, Jonas Schweisthal, Hengrui Zhang, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01533">https://arxiv.org/abs/2506.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01533">https://arxiv.org/pdf/2506.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01533]] A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments(https://arxiv.org/abs/2506.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In medicine, treatments often influence multiple, interdependent outcomes, such as primary endpoints, complications, adverse events, or other secondary endpoints. Hence, to make optimal treatment decisions, clinicians are interested in learning the distribution of multi-dimensional treatment outcomes. However, the vast majority of machine learning methods for predicting treatment effects focus on single-outcome settings, despite the fact that medical data often include multiple, interdependent outcomes. To address this limitation, we propose a novel diffusion-based method called DIME to learn the joint distribution of multiple outcomes of medical treatments. We addresses three challenges relevant in medical practice: (i)it is tailored to learn the joint interventional distribution of multiple medical outcomes, which enables reliable decision-making with uncertainty quantification rather than relying solely on point estimates; (ii)it explicitly captures the dependence structure between outcomes; (iii)it can handle outcomes of mixed type, including binary, categorical, and continuous variables. In DIME, we take into account the fundamental problem of causal inference through causal masking. For training, our method decomposes the joint distribution into a series of conditional distributions with a customized conditional masking to account for the dependence structure across outcomes. For inference, our method auto-regressively generates predictions. This allows our method to move beyond point estimates of causal quantities and thus learn the joint interventional distribution. To the best of our knowledge, DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments. Across various experiments, we demonstrate that our method effectively learns the joint distribution and captures shared information among multiple outcomes.</li>
</ul>

<h3>Title: G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tianjiao Zhang, Fei Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01539">https://arxiv.org/abs/2506.01539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01539">https://arxiv.org/pdf/2506.01539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01539]] G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models(https://arxiv.org/abs/2506.01539)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>This paper considers the problem of utilizing a large-scale text-to-image diffusion model to tackle the challenging Inexact Segmentation (IS) task. Unlike traditional approaches that rely heavily on discriminative-model-based paradigms or dense visual representations derived from internal attention mechanisms, our method focuses on the intrinsic generative priors in Stable Diffusion~(SD). Specifically, we exploit the pattern discrepancies between original images and mask-conditional generated images to facilitate a coarse-to-fine segmentation refinement by establishing a semantic correspondence alignment and updating the foreground probability. Comprehensive quantitative and qualitative experiments validate the effectiveness and superiority of our plug-and-play design, underscoring the potential of leveraging generation discrepancies to model dense representations and encouraging further exploration of generative approaches for solving discriminative tasks.</li>
</ul>

<h3>Title: Adaptive Destruction Processes for Diffusion Samplers</h3>
<ul>
<li><strong>Authors: </strong>Timofei Gritsaev, Nikita Morozov, Kirill Tamogashev, Daniil Tiapkin, Sergey Samsonov, Alexey Naumov, Dmitry Vetrov, Nikolay Malkin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01541">https://arxiv.org/abs/2506.01541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01541">https://arxiv.org/pdf/2506.01541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01541]] Adaptive Destruction Processes for Diffusion Samplers(https://arxiv.org/abs/2506.01541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper explores the challenges and benefits of a trainable destruction process in diffusion samplers -- diffusion-based generative models trained to sample an unnormalised density without access to data samples. Contrary to the majority of work that views diffusion samplers as approximations to an underlying continuous-time model, we view diffusion models as discrete-time policies trained to produce samples in very few generation steps. We propose to trade some of the elegance of the underlying theory for flexibility in the definition of the generative and destruction policies. In particular, we decouple the generation and destruction variances, enabling both transition kernels to be learned as unconstrained Gaussian densities. We show that, when the number of steps is limited, training both generation and destruction processes results in faster convergence and improved sampling quality on various benchmarks. Through a robust ablation study, we investigate the design choices necessary to facilitate stable training. Finally, we show the scalability of our approach through experiments on GAN latent space sampling for conditional image generation.</li>
</ul>

<h3>Title: LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Wang, Zhirong Wu, Peixi Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01546">https://arxiv.org/abs/2506.01546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01546">https://arxiv.org/pdf/2506.01546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01546]] LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model(https://arxiv.org/abs/2506.01546)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Driving world models are used to simulate futures by video generation based on the condition of the current state and actions. However, current models often suffer serious error accumulations when predicting the long-term future, which limits the practical application. Recent studies utilize the Diffusion Transformer (DiT) as the backbone of driving world models to improve learning flexibility. However, these models are always trained on short video clips (high fps and short duration), and multiple roll-out generations struggle to produce consistent and reasonable long videos due to the training-inference gap. To this end, we propose several solutions to build a simple yet effective long-term driving world model. First, we hierarchically decouple world model learning into large motion learning and bidirectional continuous motion learning. Then, considering the continuity of driving scenes, we propose a simple distillation method where fine-grained video flows are self-supervised signals for coarse-grained flows. The distillation is designed to improve the coherence of infinite video generation. The coarse-grained and fine-grained modules are coordinated to generate long-term and temporally coherent videos. In the public benchmark NuScenes, compared with the state-of-the-art front-view model, our model improves FVD by $27\%$ and reduces inference time by $85\%$ for the video task of generating 110+ frames. More videos (including 90s duration) are available at this https URL.</li>
</ul>

<h3>Title: EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01551">https://arxiv.org/abs/2506.01551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01551">https://arxiv.org/pdf/2506.01551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01551]] EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation(https://arxiv.org/abs/2506.01551)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at this https URL.</li>
</ul>

<h3>Title: SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yuji Wang, Haoran Xu, Yong Liu, Jiaze Li, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01558">https://arxiv.org/abs/2506.01558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01558">https://arxiv.org/pdf/2506.01558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01558]] SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes(https://arxiv.org/abs/2506.01558)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task requires the model to continuously segment objects referred to by text and audio from a video. Previous dual-modality methods always fail due to the lack of a third modality and the existing triple-modality method struggles with spatio-temporal consistency, leading to the target shift of different frames. In this work, we introduce a novel framework, termed SAM2-LOVE, which integrates textual, audio, and visual representations into a learnable token to prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our approach includes a multimodal fusion module aimed at improving multimodal understanding of SAM2, as well as token propagation and accumulation strategies designed to enhance spatio-temporal consistency without forgetting historical information. We conducted extensive experiments to demonstrate that SAM2-LOVE outperforms the SOTA by 8.5\% in $\mathcal{J\&F}$ on the Ref-AVS benchmark and showcase the simplicity and effectiveness of the components. Our code will be available here.</li>
</ul>

<h3>Title: Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization</h3>
<ul>
<li><strong>Authors: </strong>Wojciech Masarczyk, Mateusz Ostaszewski, Tin Sum Cheng, Tomasz Trzciński, Aurelien Lucchi, Razvan Pascanu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01562">https://arxiv.org/abs/2506.01562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01562">https://arxiv.org/pdf/2506.01562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01562]] Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization(https://arxiv.org/abs/2506.01562)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The softmax function is a fundamental building block of deep neural networks, commonly used to define output distributions in classification tasks or attention weights in transformer architectures. Despite its widespread use and proven effectiveness, its influence on learning dynamics and learned representations remains poorly understood, limiting our ability to optimize model behavior. In this paper, we study the pivotal role of the softmax function in shaping the model's representation. We introduce the concept of rank deficit bias - a phenomenon in which softmax-based deep networks find solutions of rank much lower than the number of classes. This bias depends on the softmax function's logits norm, which is implicitly influenced by hyperparameters or directly modified by softmax temperature. Furthermore, we demonstrate how to exploit the softmax dynamics to learn compressed representations or to enhance their performance on out-of-distribution data. We validate our findings across diverse architectures and real-world datasets, highlighting the broad applicability of temperature tuning in improving model performance. Our work provides new insights into the mechanisms of softmax, enabling better control over representation learning in deep neural networks.</li>
</ul>

<h3>Title: Trajectory First: A Curriculum for Discovering Diverse Policies</h3>
<ul>
<li><strong>Authors: </strong>Cornelius V. Braun, Sayantan Auddy, Marc Toussaint</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01568">https://arxiv.org/abs/2506.01568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01568">https://arxiv.org/pdf/2506.01568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01568]] Trajectory First: A Curriculum for Discovering Diverse Policies(https://arxiv.org/abs/2506.01568)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Being able to solve a task in diverse ways makes agents more robust to task variations and less prone to local optima. In this context, constrained diversity optimization has emerged as a powerful reinforcement learning (RL) framework to train a diverse set of agents in parallel. However, existing constrained-diversity RL methods often under-explore in complex tasks such as robotic manipulation, leading to a lack in policy diversity. To improve diversity optimization in RL, we therefore propose a curriculum that first explores at the trajectory level before learning step-based policies. In our empirical evaluation, we provide novel insights into the shortcoming of skill-based diversity optimization, and demonstrate empirically that our curriculum improves the diversity of the learned skills.</li>
</ul>

<h3>Title: Prompt Engineering Large Language Models' Forecasting Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Philipp Schoenegger, Cameron R. Jones, Philip E. Tetlock, Barbara Mellers</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01578">https://arxiv.org/abs/2506.01578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01578">https://arxiv.org/pdf/2506.01578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01578]] Prompt Engineering Large Language Models' Forecasting Capabilities(https://arxiv.org/abs/2506.01578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model performance can be improved in a large number of ways. Many such techniques, like fine-tuning or advanced tool usage, are time-intensive and expensive. Although prompt engineering is significantly cheaper and often works for simpler tasks, it remains unclear whether prompt engineering suffices for more complex domains like forecasting. Here we show that small prompt modifications rarely boost forecasting accuracy beyond a minimal baseline. In our first study, we tested 38 prompts across Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we introduced compound prompts and prompts from external sources, also including the reasoning models o1 and o1-mini. Our results show that most prompts lead to negligible gains, although references to base rates yield slight benefits. Surprisingly, some strategies showed strong negative effects on accuracy: especially encouraging the model to engage in Bayesian reasoning. These results suggest that, in the context of complex tasks like forecasting, basic prompt refinements alone offer limited gains, implying that more robust or specialized techniques may be required for substantial performance improvements in AI forecasting.</li>
</ul>

<h3>Title: HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception</h3>
<ul>
<li><strong>Authors: </strong>Wei Yao, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01579">https://arxiv.org/abs/2506.01579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01579">https://arxiv.org/pdf/2506.01579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01579]] HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception(https://arxiv.org/abs/2506.01579)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity full-body human interactions with dynamic objects and static scenes remains a critical challenge in computer graphics and animation. Existing methods for human-object interaction often neglect scene context, leading to implausible penetrations, while human-scene interaction approaches struggle to coordinate fine-grained manipulations with long-range navigation. To address these limitations, we propose HOSIG, a novel framework for synthesizing full-body interactions through hierarchical scene perception. Our method decouples the task into three key components: 1) a scene-aware grasp pose generator that ensures collision-free whole-body postures with precise hand-object contact by integrating local geometry constraints, 2) a heuristic navigation algorithm that autonomously plans obstacle-avoiding paths in complex indoor environments via compressed 2D floor maps and dual-component spatial reasoning, and 3) a scene-guided motion diffusion model that generates trajectory-controlled, full-body motions with finger-level accuracy by incorporating spatial anchors and dual-space classifier-free guidance. Extensive experiments on the TRUMANS dataset demonstrate superior performance over state-of-the-art methods. Notably, our framework supports unlimited motion length through autoregressive generation and requires minimal manual intervention. This work bridges the critical gap between scene-aware navigation and dexterous object manipulation, advancing the frontier of embodied interaction synthesis. Codes will be available after publication. Project page: this http URL</li>
</ul>

<h3>Title: Bayes optimal learning of attention-indexed models</h3>
<ul>
<li><strong>Authors: </strong>Fabrizio Boncoraglio, Emanuele Troiani, Vittorio Erba, Lenka Zdeborová</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01582">https://arxiv.org/abs/2506.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01582">https://arxiv.org/pdf/2506.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01582]] Bayes optimal learning of attention-indexed models(https://arxiv.org/abs/2506.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce the attention-indexed model (AIM), a theoretical framework for analyzing learning in deep attention layers. Inspired by multi-index models, AIM captures how token-level outputs emerge from layered bilinear interactions over high-dimensional embeddings. Unlike prior tractable attention models, AIM allows full-width key and query matrices, aligning more closely with practical transformers. Using tools from statistical mechanics and random matrix theory, we derive closed-form predictions for Bayes-optimal generalization error and identify sharp phase transitions as a function of sample complexity, model width, and sequence length. We propose a matching approximate message passing algorithm and show that gradient descent can reach optimal performance. AIM offers a solvable playground for understanding learning in modern attention architectures.</li>
</ul>

<h3>Title: Multi-Modal Dataset Distillation in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Zhuohang Dang, Minnan Luo, Chengyou Jia, Hangwei Qian, Xiaojun Chang, Ivor W. Tsang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01586">https://arxiv.org/abs/2506.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01586">https://arxiv.org/pdf/2506.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01586]] Multi-Modal Dataset Distillation in the Wild(https://arxiv.org/abs/2506.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading model performance. To these ends, we propose Multi-modal dataset Distillation in the Wild, i.e., MDW, the first framework to distill noisy multi-modal datasets into compact clean ones for effective and efficient model training. Specifically, MDW introduces learnable fine-grained correspondences during distillation and adaptively optimizes distilled data to emphasize correspondence-discriminative regions, thereby enhancing distilled data's information density and efficacy. Moreover, to capture robust cross-modal correspondence prior knowledge from real data, MDW proposes dual-track collaborative learning to avoid the risky data noise, alleviating information loss with certifiable noise tolerance. Extensive experiments validate MDW's theoretical and empirical efficacy with remarkable scalability, surpassing prior methods by over 15% across various compression ratios, highlighting its appealing practicality for applications with diverse efficacy and resource needs.</li>
</ul>

<h3>Title: Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Islam, Javed Ali Khan, Mohammed Abaker, Ali Daud, Azeem Irshad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01587">https://arxiv.org/abs/2506.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01587">https://arxiv.org/pdf/2506.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01587]] Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings(https://arxiv.org/abs/2506.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of social media platforms has significantly increased the dissemination of forged content and misinformation, making the detection of fake news a critical area of research. Although fact-checking efforts predominantly focus on English-language news, there is a noticeable gap in resources and strategies to detect news in regional languages, such as Urdu. Advanced Fake News Detection (FND) techniques rely heavily on large, accurately labeled datasets. However, FND in under-resourced languages like Urdu faces substantial challenges due to the scarcity of extensive corpora and the lack of validated lexical resources. Current Urdu fake news datasets are often domain-specific and inaccessible to the public. They also lack human verification, relying mainly on unverified English-to-Urdu translations, which compromises their reliability in practical applications. This study highlights the necessity of developing reliable, expert-verified, and domain-independent Urdu-enhanced FND datasets to improve fake news detection in Urdu and other resource-constrained languages. This paper presents the first benchmark large FND dataset for Urdu news, which is publicly available for validation and deep analysis. We also evaluate this dataset using multiple state-of-the-art pre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa, RoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model that outperforms the others with different embedding and feature extraction techniques. The performance of these models is compared based on accuracy, F1 score, precision, recall, and human judgment for vetting the sample results of news.</li>
</ul>

<h3>Title: Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Elshabrawy, Thanh-Nhi Nguyen, Yeeun Kang, Lihan Feng, Annant Jain, Faadil Abdullah Shaikh, Jonibek Mansurov, Mohamed Fazli Mohamed Imam, Jesus-German Ortiz-Barajas, Rendi Chevi, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01592">https://arxiv.org/abs/2506.01592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01592">https://arxiv.org/pdf/2506.01592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01592]] Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models(https://arxiv.org/abs/2506.01592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but achieving similar performance with encoder-only models like BERT and RoBERTa has been challenging due to their architecture. However, encoders offer advantages such as lower computational and memory costs. Recent work adapts them for zero-shot generalization using Statement Tuning, which reformulates tasks into finite templates. We extend this approach to multilingual NLP, exploring whether encoders can achieve zero-shot cross-lingual generalization and serve as efficient alternatives to memory-intensive LLMs for low-resource languages. Our results show that state-of-the-art encoder models generalize well across languages, rivaling multilingual LLMs while being more efficient. We also analyze multilingual Statement Tuning dataset design, efficiency gains, and language-specific generalization, contributing to more inclusive and resource-efficient NLP models. We release our code and models.</li>
</ul>

<h3>Title: Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice</h3>
<ul>
<li><strong>Authors: </strong>Hana Samad, Michael Akinwumi, Jameel Khan, Christoph Mügge-Durum, Emmanuel O. Ogundimu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01594">https://arxiv.org/abs/2506.01594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01594">https://arxiv.org/pdf/2506.01594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01594]] Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice(https://arxiv.org/abs/2506.01594)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>As machine learning models are increasingly embedded into society through high-stakes decision-making, selecting the right algorithm for a given task, audience, and sector presents a critical challenge, particularly in the context of fairness. Traditional assessments of model fairness have often framed fairness as an objective mathematical property, treating model selection as an optimization problem under idealized informational conditions. This overlooks model multiplicity as a consideration--that multiple models can deliver similar performance while exhibiting different fairness characteristics. Legal scholars have engaged this challenge through the concept of Less Discriminatory Algorithms (LDAs), which frames model selection as a civil rights obligation. In real-world deployment, this normative challenge is bounded by constraints on fairness experimentation, e.g., regulatory standards, institutional priorities, and resource capacity. Against these considerations, the paper revisits Lee and Floridi (2021)'s relational fairness approach using updated 2021 Home Mortgage Disclosure Act (HMDA) data, and proposes an expansion of the scope of the LDA search process. We argue that extending the LDA search horizontally, considering fairness across model families themselves, provides a lightweight complement, or alternative, to within-model hyperparameter optimization, when operationalizing fairness in non-experimental, resource constrained settings. Fairness metrics alone offer useful, but insufficient signals to accurately evaluate candidate LDAs. Rather, by using a horizontal LDA search approach with the relational trade-off framework, we demonstrate a responsible minimum viable LDA search on real-world lending outcomes. Organizations can modify this approach to systematically compare, evaluate, and select LDAs that optimize fairness and accuracy in a sector-based contextualized manner.</li>
</ul>

<h3>Title: PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations</h3>
<ul>
<li><strong>Authors: </strong>Jin Song, Kenji Kawaguchi, Zhenya Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01598">https://arxiv.org/abs/2506.01598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01598">https://arxiv.org/pdf/2506.01598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01598]] PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations(https://arxiv.org/abs/2506.01598)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural operators, which aim to approximate mappings between infinite-dimensional function spaces, have been widely applied in the simulation and prediction of physical systems. However, the limited representational capacity of network architectures, combined with their heavy reliance on large-scale data, often hinder effective training and result in poor extrapolation performance. In this paper, inspired by traditional numerical methods, we propose a novel physics guided multi-step neural operator (PMNO) architecture to address these challenges in long-horizon prediction of complex physical systems. Distinct from general operator learning methods, the PMNO framework replaces the single-step input with multi-step historical data in the forward pass and introduces an implicit time-stepping scheme based on the Backward Differentiation Formula (BDF) during backpropagation. This design not only strengthens the model's extrapolation capacity but also facilitates more efficient and stable training with fewer data samples, especially for long-term predictions. Meanwhile, a causal training strategy is employed to circumvent the need for multi-stage training and to ensure efficient end-to-end optimization. The neural operator architecture possesses resolution-invariant properties, enabling the trained model to perform fast extrapolation on arbitrary spatial resolutions. We demonstrate the superior predictive performance of PMNO predictor across a diverse range of physical systems, including 2D linear system, modeling over irregular domain, complex-valued wave dynamics, and reaction-diffusion processes. Depending on the specific problem setting, various neural operator architectures, including FNO, DeepONet, and their variants, can be seamlessly integrated into the PMNO framework.</li>
</ul>

<h3>Title: EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</h3>
<ul>
<li><strong>Authors: </strong>Andy Bonnetto, Haozhe Qi, Franklin Leong, Matea Tashkovska, Mahdi Rad, Solaiman Shokur, Friedhelm Hummel, Silvestro Micera, Marc Pollefeys, Alexander Mathis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, q-bio.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01608">https://arxiv.org/abs/2506.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01608">https://arxiv.org/pdf/2506.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01608]] EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models(https://arxiv.org/abs/2506.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at this https URL</li>
</ul>

<h3>Title: Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01625">https://arxiv.org/abs/2506.01625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01625">https://arxiv.org/pdf/2506.01625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01625]] Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks(https://arxiv.org/abs/2506.01625)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.</li>
</ul>

<h3>Title: Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</h3>
<ul>
<li><strong>Authors: </strong>Zehao Wu, Yanjie Zhao, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01631">https://arxiv.org/abs/2506.01631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01631">https://arxiv.org/pdf/2506.01631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01631]] Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification(https://arxiv.org/abs/2506.01631)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance. To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.</li>
</ul>

<h3>Title: Visual Explanation via Similar Feature Activation for Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Liao, Ugochukwu Ejike Akpudo, Jue Zhang, Yongsheng Gao, Jun Zhou, Wenyi Zeng, Weichuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01636">https://arxiv.org/abs/2506.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01636">https://arxiv.org/pdf/2506.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01636]] Visual Explanation via Similar Feature Activation for Metric Learning(https://arxiv.org/abs/2506.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Visual explanation maps enhance the trustworthiness of decisions made by deep learning models and offer valuable guidance for developing new algorithms in image recognition tasks. Class activation maps (CAM) and their variants (e.g., Grad-CAM and Relevance-CAM) have been extensively employed to explore the interpretability of softmax-based convolutional neural networks, which require a fully connected layer as the classifier for decision-making. However, these methods cannot be directly applied to metric learning models, as such models lack a fully connected layer functioning as a classifier. To address this limitation, we propose a novel visual explanation method termed Similar Feature Activation Map (SFAM). This method introduces the channel-wise contribution importance score (CIS) to measure feature importance, derived from the similarity measurement between two image embeddings. The explanation map is constructed by linearly combining the proposed importance weights with the feature map from a CNN model. Quantitative and qualitative experiments show that SFAM provides highly promising interpretable visual explanations for CNN models using Euclidean distance or cosine similarity as the similarity metric.</li>
</ul>

<h3>Title: ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Chaoyue He, Xin Zhou, Yi Wu, Xinjia Yu, Yan Zhang, Lei Zhang, Di Wang, Shengfei Lyu, Hong Xu, Xiaoqiao Wang, Wei Liu, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01646">https://arxiv.org/abs/2506.01646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01646">https://arxiv.org/pdf/2506.01646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01646]] ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge(https://arxiv.org/abs/2506.01646)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, "DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.</li>
</ul>

<h3>Title: Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xuan Yu, Dayan Guan, Michael Ying Yang, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01663">https://arxiv.org/abs/2506.01663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01663">https://arxiv.org/pdf/2506.01663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01663]] Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement(https://arxiv.org/abs/2506.01663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLM) often struggle to interpret high-resolution images accurately, where fine-grained details are crucial for complex visual understanding. We introduce Zoom-Refine, a novel training-free method that enhances MLLM capabilities to address this issue. Zoom-Refine operates through a synergistic process of \textit{Localized Zoom} and \textit{Self-Refinement}. In the \textit{Localized Zoom} step, Zoom-Refine leverages the MLLM to provide a preliminary response to an input query and identifies the most task-relevant image region by predicting its bounding box coordinates. During the \textit{Self-Refinement} step, Zoom-Refine then integrates fine-grained details from the high-resolution crop (identified by \textit{Localized Zoom}) with its initial reasoning to re-evaluate and refine its preliminary response. Our method harnesses the MLLM's inherent capabilities for spatial localization, contextual reasoning and comparative analysis without requiring additional training or external experts. Comprehensive experiments demonstrate the efficacy of Zoom-Refine on two challenging high-resolution multimodal benchmarks. Code is available at \href{this https URL}{\color{magenta}this http URL}</li>
</ul>

<h3>Title: Minimal Impact ControlNet: Advancing Multi-ControlNet Integration</h3>
<ul>
<li><strong>Authors: </strong>Shikun Sun, Min Zhou, Zixuan Wang, Xubin Li, Tiezheng Ge, Zijie Ye, Xiaoyu Qin, Junliang Xing, Bo Zheng, Jia Jia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01672">https://arxiv.org/abs/2506.01672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01672">https://arxiv.org/pdf/2506.01672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01672]] Minimal Impact ControlNet: Advancing Multi-ControlNet Integration(https://arxiv.org/abs/2506.01672)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the advancement of diffusion models, there is a growing demand for high-quality, controllable image generation, particularly through methods that utilize one or multiple control signals based on ControlNet. However, in current ControlNet training, each control is designed to influence all areas of an image, which can lead to conflicts when different control signals are expected to manage different parts of the image in practical applications. This issue is especially pronounced with edge-type control conditions, where regions lacking boundary information often represent low-frequency signals, referred to as silent control signals. When combining multiple ControlNets, these silent control signals can suppress the generation of textures in related areas, resulting in suboptimal outcomes. To address this problem, we propose Minimal Impact ControlNet. Our approach mitigates conflicts through three key strategies: constructing a balanced dataset, combining and injecting feature signals in a balanced manner, and addressing the asymmetry in the score function's Jacobian matrix induced by ControlNet. These improvements enhance the compatibility of control signals, allowing for freer and more harmonious generation in areas with silent control signals.</li>
</ul>

<h3>Title: MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yipeng Du, Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Xiang Li, Jian Yang, Zhenheng Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01674">https://arxiv.org/abs/2506.01674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01674">https://arxiv.org/pdf/2506.01674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01674]] MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs(https://arxiv.org/abs/2506.01674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\Theta}(40K) video clips and {\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.</li>
</ul>

<h3>Title: Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Zhiyuan Liao, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01675">https://arxiv.org/abs/2506.01675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01675">https://arxiv.org/pdf/2506.01675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01675]] Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon(https://arxiv.org/abs/2506.01675)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite substantial research efforts evaluating how well large language models~(LLMs) handle global cultural diversity, the mechanisms behind their cultural knowledge acquisition, particularly in multilingual settings, remain unclear. We study this question by investigating how cultural knowledge transfers across languages during language adaptation of LLMs. We introduce an interpretable framework for studying this transfer, ensuring training data transparency and controlling transfer effects. Through a study of four non-Anglophonic cultures, we observe bidirectional cultural transfer between English and other high-resource languages, while low-resource languages primarily transfer knowledge to English with limited reverse flow. To explain this asymmetric phenomenon, we propose a frequency-based hypothesis: cultural knowledge appearing more frequently in the pretraining data transfers more easily, which is supported by empirical analysis of the training corpora.</li>
</ul>

<h3>Title: StochasTok: Improving Fine-Grained Subword Understanding in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Anya Sims, Thom Foster, Klara Kaleb, Tuan-Duy H. Nguyen, Joseph Lee, Jakob N. Foerster, Yee Whye Teh, Cong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01687">https://arxiv.org/abs/2506.01687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01687">https://arxiv.org/pdf/2506.01687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01687]] StochasTok: Improving Fine-Grained Subword Understanding in LLMs(https://arxiv.org/abs/2506.01687)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Subword-level understanding is integral to numerous tasks, including understanding multi-digit numbers, spelling mistakes, abbreviations, rhyming, and wordplay. Despite this, current large language models (LLMs) still often struggle with seemingly simple subword-level tasks like How many 'r's in 'strawberry'?. A key factor behind these failures is tokenization which obscures the fine-grained structure of words. Current alternatives, such as character-level and dropout tokenization methods, significantly increase computational costs and provide inconsistent improvements. In this paper we revisit tokenization and introduce StochasTok, a simple, efficient stochastic tokenization scheme that randomly splits tokens during training, allowing LLMs to 'see' their internal structure. Our experiments show that pretraining with StochasTok substantially improves LLMs' downstream performance across multiple subword-level language games, including character counting, substring identification, and math tasks. Furthermore, StochasTok's simplicity allows seamless integration at any stage of the training pipeline; and we demonstrate that post-training with StochasTok can instill improved subword understanding into existing pretrained models, thus avoiding costly pretraining from scratch. These dramatic improvements achieved with a minimal change suggest StochasTok holds exciting potential when applied to larger, more capable models. Code open-sourced at: this https URL.</li>
</ul>

<h3>Title: SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation</h3>
<ul>
<li><strong>Authors: </strong>Sang-Eun Lee, Ko Nishino, Shohei Nobuhara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01691">https://arxiv.org/abs/2506.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01691">https://arxiv.org/pdf/2506.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01691]] SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation(https://arxiv.org/abs/2506.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Can freely moving humans or animals themselves serve as calibration targets for multi-camera systems while simultaneously estimating their correspondences across views? We humans can solve this problem by mentally rotating the observed 2D poses and aligning them with those in the target views. Inspired by this cognitive ability, we propose SteerPose, a neural network that performs this rotation of 2D poses into another view. By integrating differentiable matching, SteerPose simultaneously performs extrinsic camera calibration and correspondence search within a single unified framework. We also introduce a novel geometric consistency loss that explicitly ensures that the estimated rotation and correspondences result in a valid translation estimation. Experimental results on diverse in-the-wild datasets of humans and animals validate the effectiveness and robustness of the proposed method. Furthermore, we demonstrate that our method can reconstruct the 3D poses of novel animals in multi-camera setups by leveraging off-the-shelf 2D pose estimators and our class-agnostic model.</li>
</ul>

<h3>Title: When LLMs Team Up: The Emergence of Collaborative Affective Computing</h3>
<ul>
<li><strong>Authors: </strong>Wenna Lai, Haoran Xie, Guandong Xu, Qing Li, S. Joe Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01698">https://arxiv.org/abs/2506.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01698">https://arxiv.org/pdf/2506.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01698]] When LLMs Team Up: The Emergence of Collaborative Affective Computing(https://arxiv.org/abs/2506.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.</li>
</ul>

<h3>Title: Combining Different Existing Methods for Describing Steganography Hiding Methods</h3>
<ul>
<li><strong>Authors: </strong>Steffen Wendzel, Christian Krätzer, Jana Dittmann, Luca Caviglione, Aleksandra Mileva, Tobias Schmidbauer, Claus Vielhauer, Sebastian Zander</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01700">https://arxiv.org/abs/2506.01700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01700">https://arxiv.org/pdf/2506.01700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01700]] Combining Different Existing Methods for Describing Steganography Hiding Methods(https://arxiv.org/abs/2506.01700)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The proliferation of digital carriers that can be exploited to conceal arbitrary data has greatly increased the number of techniques for implementing network steganography. As a result, the literature overlaps greatly in terms of concepts and terminology. Moreover, from a cybersecurity viewpoint, the same hiding mechanism may be perceived differently, making harder the development of a unique defensive strategy or the definition of practices to mitigate risks arising from the use of steganography. To mitigate these drawbacks, several researchers introduced approaches that aid in the unified description of steganography methods and network covert channels. Understanding and combining all descriptive methods for steganography techniques is a challenging but important task. For instance, researchers might want to explain how malware applies a certain steganography technique or categorize a novel hiding approach. Consequently, this paper aims to provide an introduction to the concept of descriptive methods for steganography. The paper is organized in the form of a tutorial, with the main goal of explaining how existing descriptions and taxonomy objects can be combined to achieve a detailed categorization and description of hiding methods. To show how this can effectively help the research community, the paper also contains various real-world examples.</li>
</ul>

<h3>Title: Data Pruning by Information Maximization</h3>
<ul>
<li><strong>Authors: </strong>Haoru Tan, Sitong Wu, Wei Huang, Shizhen Zhao, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01701">https://arxiv.org/abs/2506.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01701">https://arxiv.org/pdf/2506.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01701]] Data Pruning by Information Maximization(https://arxiv.org/abs/2506.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models.</li>
</ul>

<h3>Title: mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Dominik Macko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01702">https://arxiv.org/abs/2506.01702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01702">https://arxiv.org/pdf/2506.01702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01702]] mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection(https://arxiv.org/abs/2506.01702)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The large language models (LLMs) are able to generate high-quality texts in multiple languages. Such texts are often not recognizable by humans as generated, and therefore present a potential of LLMs for misuse (e.g., plagiarism, spams, disinformation spreading). An automated detection is able to assist humans to indicate the machine-generated texts; however, its robustness to out-of-distribution data is still challenging. This notebook describes our mdok approach in robust detection, based on fine-tuning smaller LLMs for text classification. It is applied to both subtasks of Voight-Kampff Generative AI Detection 2025, providing remarkable performance in binary detection as well as in multiclass (1st rank) classification of various cases of human-AI collaboration.</li>
</ul>

<h3>Title: Fairness Dynamics During Training</h3>
<ul>
<li><strong>Authors: </strong>Krishna Patel, Nivedha Sivakumar, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01709">https://arxiv.org/abs/2506.01709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01709">https://arxiv.org/pdf/2506.01709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01709]] Fairness Dynamics During Training(https://arxiv.org/abs/2506.01709)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>We investigate fairness dynamics during Large Language Model (LLM) training to enable the diagnoses of biases and mitigations through training interventions like early stopping; we find that biases can emerge suddenly and do not always follow common performance metrics. We introduce two new metrics to evaluate fairness dynamics holistically during model pre-training: Average Rank and Jensen-Shannon Divergence by Parts. These metrics provide insights into the Pythia models' progression of biases in gender prediction of occupations on the WinoBias dataset. By monitoring these dynamics, we find that (1) Pythia-6.9b is biased towards men; it becomes more performant and confident predicting "male" than "female" during training, (2) via early-stopping, Pythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in fairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more assumptions about gender than Pythia-160m, even when a subject's gender is not specified.</li>
</ul>

<h3>Title: Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fangyu Lei, Jinxiang Meng, Yiming Huang, Tinghong Chen, Yun Zhang, Shizhu He, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01710">https://arxiv.org/abs/2506.01710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01710">https://arxiv.org/pdf/2506.01710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01710]] Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning(https://arxiv.org/abs/2506.01710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Table reasoning, encompassing tasks such as table question answering, fact verification, and text-to-SQL, requires precise understanding of structured tabular data, coupled with numerical computation and code manipulation for effective inference. Supervised fine-tuning (SFT) approaches have achieved notable success but often struggle with generalization and robustness due to biases inherent in imitative learning. We introduce Reasoning-Table, the first application of reinforcement learning (RL) to table reasoning, achieving state-of-the-art performance. Through rigorous data preprocessing, reward design, and tailored training strategies, our method leverages simple rule-based outcome rewards to outperform SFT across multiple benchmarks. Unified training across diverse tasks enables Reasoning-Table to emerge as a robust table reasoning large language model, surpassing larger proprietary models like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The approach also achieves excellent performance on text-to-SQL tasks, reaching 68.3% performance on the BIRD dev dataset with a 7B model. Further experiments demonstrate that Reasoning-Table enhances the model's generalization capabilities and robustness.</li>
</ul>

<h3>Title: SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Yangfan He, Mi Zhang, Shen Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01713">https://arxiv.org/abs/2506.01713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01713">https://arxiv.org/pdf/2506.01713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01713]] SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning(https://arxiv.org/abs/2506.01713)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.</li>
</ul>

<h3>Title: Tug-of-war between idiom's figurative and literal meanings in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Soyoung Oh, Xinting Huang, Mathis Pink, Michael Hahn, Vera Demberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01723">https://arxiv.org/abs/2506.01723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01723">https://arxiv.org/pdf/2506.01723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01723]] Tug-of-war between idiom's figurative and literal meanings in LLMs(https://arxiv.org/abs/2506.01723)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Idioms present a unique challenge for language models due to their non-compositional figurative meanings, which often strongly diverge from the idiom's literal interpretation. This duality requires a model to learn representing and deciding between the two meanings to interpret an idiom in a figurative sense, or literally. In this paper, we employ tools from mechanistic interpretability to trace how a large pretrained causal transformer (LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom processing: First, the idiom's figurative meaning is retrieved in early attention and MLP sublayers. We identify specific attention heads which boost the figurative meaning of the idiom while suppressing the idiom's literal interpretation. The model subsequently represents the figurative representation through an intermediate path. Meanwhile, a parallel bypass route forwards literal interpretation, ensuring that a both reading remain available. Overall, our findings provide a mechanistic evidence for idiom comprehension in an autoregressive transformer.</li>
</ul>

<h3>Title: VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking</h3>
<ul>
<li><strong>Authors: </strong>Desen Meng, Rui Huang, Zhilin Dai, Xinhao Li, Yifan Xu, Jun Zhang, Zhenpeng Huang, Meng Zhang, Lingshu Zhang, Yi Liu, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01725">https://arxiv.org/abs/2506.01725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01725">https://arxiv.org/pdf/2506.01725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01725]] VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking(https://arxiv.org/abs/2506.01725)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While recent advances in reinforcement learning have significantly enhanced reasoning capabilities in large language models (LLMs), these techniques remain underexplored in multi-modal LLMs for video captioning. This paper presents the first systematic investigation of GRPO-based RL post-training for video MLLMs, with the goal of enhancing video MLLMs' capability of describing actions in videos. Specifically, we develop the VideoCap-R1, which is prompted to first perform structured thinking that analyzes video subjects with their attributes and actions before generating complete captions, supported by two specialized reward mechanisms: a LLM-free think scorer evaluating the structured thinking quality and a LLM-assisted caption scorer assessing the output quality. The RL training framework effectively establishes the connection between structured reasoning and comprehensive description generation, enabling the model to produce captions with more accurate actions. Our experiments demonstrate that VideoCap-R1 achieves substantial improvements over the Qwen2VL-7B baseline using limited samples (1.5k) across multiple video caption benchmarks (DREAM1K: +4.4 event F1, VDC: +4.2 Acc, CAREBENCH: +3.1 action F1, +6.9 object F1) while consistently outperforming the SFT-trained counterparts, confirming GRPO's superiority in enhancing MLLMs' captioning capabilities.</li>
</ul>

<h3>Title: Principled data augmentation for learning to solve quadratic programming problems</h3>
<ul>
<li><strong>Authors: </strong>Chendi Qian, Christopher Morris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01728">https://arxiv.org/abs/2506.01728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01728">https://arxiv.org/pdf/2506.01728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01728]] Principled data augmentation for learning to solve quadratic programming problems(https://arxiv.org/abs/2506.01728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Linear and quadratic optimization are crucial in numerous real-world applications, from training machine learning models to integer-linear optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, requiring solving many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised learning framework based on contrastive learning, thereby pretraining MPNNs for enhanced performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.</li>
</ul>

<h3>Title: Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Pierre-Carl Langlais, Carlos Rosas Hinostroza, Mattia Nee, Catherine Arnett, Pavel Chizhov, Eliot Krzystof Jones, Irène Girard, David Mach, Anastasia Stasenko, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01732">https://arxiv.org/abs/2506.01732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01732">https://arxiv.org/pdf/2506.01732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01732]] Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training(https://arxiv.org/abs/2506.01732)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are pre-trained on large amounts of data from different sources and domains. These data most often contain trillions of tokens with large portions of copyrighted or proprietary content, which hinders the usage of such models under AI legislation. This raises the need for truly open pre-training data that is compliant with the data security regulations. In this paper, we introduce Common Corpus, the largest open dataset for language model pre-training. The data assembled in Common Corpus are either uncopyrighted or under permissible licenses and amount to about two trillion tokens. The dataset contains a wide variety of languages, ranging from the main European languages to low-resource ones rarely present in pre-training datasets; in addition, it includes a large portion of code data. The diversity of data sources in terms of covered domains and time periods opens up the paths for both research and entrepreneurial needs in diverse areas of knowledge. In this technical report, we present the detailed provenance of data assembling and the details of dataset filtering and curation. Being already used by such industry leaders as Anthropic and multiple LLM training projects, we believe that Common Corpus will become a critical infrastructure for open science research in LLMs.</li>
</ul>

<h3>Title: Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiandong Shao, Yao Lu, Jianfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01734">https://arxiv.org/abs/2506.01734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01734">https://arxiv.org/pdf/2506.01734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01734]] Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs(https://arxiv.org/abs/2506.01734)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive performance on complex reasoning tasks, yet they frequently fail on basic numerical problems, producing incorrect outputs. Inspired by Benford's Law -- a statistical pattern where lower digits occur more frequently as leading digits -- we hypothesize that the long-tailed digit distributions in web-collected corpora may be learned by LLMs during pretraining, leading to biased numerical generation. To investigate the hypothesis, we first examine whether digits frequencies in pretraining corpus (OLMo2) follows Benford's law. We then construct an evaluation benchmark with uniformly distributed ground-truth digits across seven numerical reasoning tasks. Our evaluation results demonstrate that leading open-source LLMs show a consistent pattern of digit bias that resembles Benford's law. Through logit-lens tracing and neuron-level dissection, we identify that this bias arises predominantly from a small subset of highly digit-selective feed-forward network (FFN) neurons in the deeper layers. Finally, we demonstrate that pruning these neurons mitigates imbalanced overgeneration and partially corrects erroneous outputs, providing causal evidence that fine-grained pretraining digit bias can propagate into model behavior. Our findings reveal a fundamental connection between corpus-level statistics and symbolic failure modes in LLMs, offering a new lens for diagnosing and mitigating hallucinations in numerical tasks.</li>
</ul>

<h3>Title: STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jinhong Wang, Shuo Tong, Jian liu, Dongqi Tang, Jintai Chen, Haochao Ying, Hongxia Xu, Danny Chen, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01738">https://arxiv.org/abs/2506.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01738">https://arxiv.org/pdf/2506.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01738]] STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset(https://arxiv.org/abs/2506.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual rating is an essential capability of artificial intelligence (AI) for multi-dimensional quantification of visual content, primarily applied in ordinal regression (OR) tasks such as image quality assessment, facial age estimation, and medical image grading. However, current multi-modal large language models (MLLMs) under-perform in such visual rating ability while also suffering the lack of relevant datasets and benchmarks. In this work, we collect and present STORM, a data collection and benchmark for Stimulating Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating. STORM encompasses 14 ordinal regression datasets across five common visual rating domains, comprising 655K image-level pairs and the corresponding carefully curated VQAs. Importantly, we also propose a coarse-to-fine processing pipeline that dynamically considers label candidates and provides interpretable thoughts, providing MLLMs with a general and trustworthy ordinal thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot performance of MLLMs in scenarios requiring understanding of the essential common ordinal relationships of rating labels. Extensive experiments demonstrate the effectiveness of our framework and shed light on better fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models are available on the following webpage to support further research in this area. Datasets and codes are released on the project page: this https URL.</li>
</ul>

<h3>Title: Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yihong Tang, Kehai Chen, Muyun Yang, Zhengyu Niu, Jing Li, Tiejun Zhao, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01748">https://arxiv.org/abs/2506.01748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01748">https://arxiv.org/pdf/2506.01748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01748]] Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning(https://arxiv.org/abs/2506.01748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift.</li>
</ul>

<h3>Title: Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Tao Yang, Ruibin Li, Yangming Shi, Yuqi Zhang, Qide Dong, Haoran Cheng, Weiguo Feng, Shilei Wen, Bingyue Peng, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01758">https://arxiv.org/abs/2506.01758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01758">https://arxiv.org/pdf/2506.01758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01758]] Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks(https://arxiv.org/abs/2506.01758)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at this https URL.</li>
</ul>

<h3>Title: Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Somayeh Sobati-M</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01767">https://arxiv.org/abs/2506.01767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01767">https://arxiv.org/pdf/2506.01767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01767]] Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks(https://arxiv.org/abs/2506.01767)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Fragmentation is a routine part of communication in 6LoWPAN-based IoT networks, designed to accommodate small frame sizes on constrained wireless links. However, this process introduces a critical vulnerability fragments are typically stored and processed before their legitimacy is confirmed, allowing attackers to exploit this gap with minimal effort. In this work, we explore a defense strategy that takes a more adaptive, behavior-aware approach to this problem. Our system, called Predictive-CSM, introduces a combination of two lightweight mechanisms. The first tracks how each node behaves over time, rewarding consistent and successful interactions while quickly penalizing suspicious or failing patterns. The second checks the integrity of packet fragments using a chained hash, allowing incomplete or manipulated sequences to be caught early, before they can occupy memory or waste processing time. We put this system to the test using a set of targeted attack simulations, including early fragment injection, replayed headers, and flooding with fake data. Across all scenarios, Predictive CSM preserved network delivery and maintained energy efficiency, even under pressure. Rather than relying on heavyweight cryptography or rigid filters, this approach allows constrained de vices to adapt their defenses in real time based on what they observe, not just what they're told. In that way, it offers a step forward for securing fragmented communication in real world IoT systems</li>
</ul>

<h3>Title: ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zeming Wei, Chengcan Wu, Meng Sun</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01770">https://arxiv.org/abs/2506.01770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01770">https://arxiv.org/pdf/2506.01770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01770]] ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs(https://arxiv.org/abs/2506.01770)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at this https URL.</li>
</ul>

<h3>Title: MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yile Liu, Ziwei Ma, Xiu Jiang, Jinglu Hu, Jing Chang, Liang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01776">https://arxiv.org/abs/2506.01776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01776">https://arxiv.org/pdf/2506.01776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01776]] MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation(https://arxiv.org/abs/2506.01776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 languages with 1,667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial and open-source LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.</li>
</ul>

<h3>Title: DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems</h3>
<ul>
<li><strong>Authors: </strong>Hithem Lamri, Manaar Alam, Haiyan Jiang, Michail Maniatakos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01777">https://arxiv.org/abs/2506.01777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01777">https://arxiv.org/pdf/2506.01777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01777]] DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems(https://arxiv.org/abs/2506.01777)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Unlearning (FU) enables clients to remove the influence of specific data from a collaboratively trained shared global model, addressing regulatory requirements such as GDPR and CCPA. However, this unlearning process introduces a new privacy risk: A malicious server may exploit unlearning updates to reconstruct the data requested for removal, a form of Data Reconstruction Attack (DRA). While DRAs for machine unlearning have been studied extensively in centralized Machine Learning-as-a-Service (MLaaS) settings, their applicability to FU remains unclear due to the decentralized, client-driven nature of FU. This work presents DRAUN, the first attack framework to reconstruct unlearned data in FU systems. DRAUN targets optimization-based unlearning methods, which are widely adopted for their efficiency. We theoretically demonstrate why existing DRAs targeting machine unlearning in MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate our approach through extensive experiments on four datasets and four model architectures, evaluating its performance against five popular unlearning methods, effectively demonstrating that state-of-the-art FU methods remain vulnerable to DRAs.</li>
</ul>

<h3>Title: unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yafei Yang, Zihui Zhang, Bo Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01778">https://arxiv.org/abs/2506.01778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01778">https://arxiv.org/pdf/2506.01778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01778]] unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning(https://arxiv.org/abs/2506.01778)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse.</li>
</ul>

<h3>Title: Federated Gaussian Mixture Models</h3>
<ul>
<li><strong>Authors: </strong>Sophia Zhang Pettersson, Kuo-Yun Liang, Juan Carlos Andresen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01780">https://arxiv.org/abs/2506.01780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01780">https://arxiv.org/pdf/2506.01780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01780]] Federated Gaussian Mixture Models(https://arxiv.org/abs/2506.01780)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces FedGenGMM, a novel one-shot federated learning approach for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios. In federated learning (FL), where multiple decentralized clients collaboratively train models without sharing raw data, significant challenges include statistical heterogeneity, high communication costs, and privacy concerns. FedGenGMM addresses these issues by allowing local GMM models, trained independently on client devices, to be aggregated through a single communication round. This approach leverages the generative property of GMMs, enabling the creation of a synthetic dataset on the server side to train a global model efficiently. Evaluation across diverse datasets covering image, tabular, and time series data demonstrates that FedGenGMM consistently achieves performance comparable to non-federated and iterative federated methods, even under significant data heterogeneity. Additionally, FedGenGMM significantly reduces communication overhead, maintains robust performance in anomaly detection tasks, and offers flexibility in local model complexities, making it particularly suitable for edge computing environments.</li>
</ul>

<h3>Title: FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Honglu Zhang, Zhiqin Fang, Ningning Zhao, Saihui Hou, Long Ma, Renwang Pei, Zhaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01783">https://arxiv.org/abs/2506.01783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01783">https://arxiv.org/pdf/2506.01783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01783]] FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning(https://arxiv.org/abs/2506.01783)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image-text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision-language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.</li>
</ul>

<h3>Title: iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Shuai Wang, Yinan Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01784">https://arxiv.org/abs/2506.01784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01784">https://arxiv.org/pdf/2506.01784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01784]] iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering(https://arxiv.org/abs/2506.01784)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.</li>
</ul>

<h3>Title: R2SM: Referring and Reasoning for Selective Masks</h3>
<ul>
<li><strong>Authors: </strong>Yu-Lin Shih, Wei-En Tai, Cheng Sun, Yu-Chiang Frank Wang, Hwann-Tzong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01795">https://arxiv.org/abs/2506.01795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01795">https://arxiv.org/pdf/2506.01795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01795]] R2SM: Referring and Reasoning for Selective Masks(https://arxiv.org/abs/2506.01795)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a new task, Referring and Reasoning for Selective Masks (R2SM), which extends text-guided segmentation by incorporating mask-type selection driven by user intent. This task challenges vision-language models to determine whether to generate a modal (visible) or amodal (complete) segmentation mask based solely on natural language prompts. To support the R2SM task, we present the R2SM dataset, constructed by augmenting annotations of COCOA-cls, D2SA, and MUVA. The R2SM dataset consists of both modal and amodal text queries, each paired with the corresponding ground-truth mask, enabling model finetuning and evaluation for the ability to segment images as per user intent. Specifically, the task requires the model to interpret whether a given prompt refers to only the visible part of an object or to its complete shape, including occluded regions, and then produce the appropriate segmentation. For example, if a prompt explicitly requests the whole shape of a partially hidden object, the model is expected to output an amodal mask that completes the occluded parts. In contrast, prompts without explicit mention of hidden regions should generate standard modal masks. The R2SM benchmark provides a challenging and insightful testbed for advancing research in multimodal reasoning and intent-aware segmentation.</li>
</ul>

<h3>Title: Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Jiuheng Lin, Xiao Liu, Zekai Zhang, Yansong Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01796">https://arxiv.org/abs/2506.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01796">https://arxiv.org/pdf/2506.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01796]] Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books(https://arxiv.org/abs/2506.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have shown promise in translating extremely low-resource languages using resources like dictionaries, the effectiveness of grammar books remains debated. This paper investigates the role of grammar books in translating extremely low-resource languages by decomposing it into two key steps: grammar rule retrieval and application. To facilitate the study, we introduce ZhuangRules, a modularized dataset of grammar rules and their corresponding test sentences. Our analysis reveals that rule retrieval constitutes a primary bottleneck in grammar-based translation. Moreover, although LLMs can apply simple rules for translation when explicitly provided, they encounter difficulties in handling more complex rules. To address these challenges, we propose representing grammar rules as code functions, considering their similarities in structure and the benefit of code in facilitating LLM reasoning. Our experiments show that using code rules significantly boosts both rule retrieval and application, ultimately resulting in a 13.1% BLEU improvement in translation.</li>
</ul>

<h3>Title: WorldExplorer: Towards Generating Fully Navigable 3D Scenes</h3>
<ul>
<li><strong>Authors: </strong>Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01799">https://arxiv.org/abs/2506.01799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01799">https://arxiv.org/pdf/2506.01799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01799]] WorldExplorer: Towards Generating Fully Navigable 3D Scenes(https://arxiv.org/abs/2506.01799)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating 3D worlds from text is a highly anticipated goal in computer vision. Existing works are limited by the degree of exploration they allow inside of a scene, i.e., produce streched-out and noisy artifacts when moving beyond central or panoramic perspectives. To this end, we propose WorldExplorer, a novel method based on autoregressive video trajectory generation, which builds fully navigable 3D scenes with consistent visual quality across a wide range of viewpoints. We initialize our scenes by creating multi-view consistent images corresponding to a 360 degree panorama. Then, we expand it by leveraging video diffusion models in an iterative scene generation pipeline. Concretely, we generate multiple videos along short, pre-defined trajectories, that explore the scene in depth, including motion around objects. Our novel scene memory conditions each video on the most relevant prior views, while a collision-detection mechanism prevents degenerate results, like moving into objects. Finally, we fuse all generated views into a unified 3D representation via 3D Gaussian Splatting optimization. Compared to prior approaches, WorldExplorer produces high-quality scenes that remain stable under large camera motion, enabling for the first time realistic and unrestricted exploration. We believe this marks a significant step toward generating immersive and truly explorable virtual 3D environments.</li>
</ul>

<h3>Title: OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, Zhibo Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01801">https://arxiv.org/abs/2506.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01801">https://arxiv.org/pdf/2506.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01801]] OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation(https://arxiv.org/abs/2506.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The emergence of Diffusion Transformers (DiT) has brought significant advancements to video generation, especially in text-to-video and image-to-video tasks. Although video generation is widely applied in various fields, most existing models are limited to single scenarios and cannot perform diverse video generation and editing through dynamic content manipulation. We propose OmniV2V, a video model capable of generating and editing videos across different scenarios based on various operations, including: object movement, object addition, mask-guided video edit, try-on, inpainting, outpainting, human animation, and controllable character video synthesis. We explore a unified dynamic content manipulation injection module, which effectively integrates the requirements of the above tasks. In addition, we design a visual-text instruction module based on LLaVA, enabling the model to effectively understand the correspondence between visual content and instructions. Furthermore, we build a comprehensive multi-task data processing system. Since there is data overlap among various tasks, this system can efficiently provide data augmentation. Using this system, we construct a multi-type, multi-scenario OmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive experiments show that OmniV2V works as well as, and sometimes better than, the best existing open-source and commercial models for many video generation and editing tasks.</li>
</ul>

<h3>Title: UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</h3>
<ul>
<li><strong>Authors: </strong>Heming Zhu, Guoxing Sun, Christian Theobalt, Marc Habermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01802">https://arxiv.org/abs/2506.01802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01802">https://arxiv.org/pdf/2506.01802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01802]] UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment(https://arxiv.org/abs/2506.01802)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning an animatable and clothed human avatar model with vivid dynamics and photorealistic appearance from multi-view videos is an important foundational research problem in computer graphics and vision. Fueled by recent advances in implicit representations, the quality of the animatable avatars has achieved an unprecedented level by attaching the implicit representation to drivable human template meshes. However, they usually fail to preserve the highest level of detail, particularly apparent when the virtual camera is zoomed in and when rendering at 4K resolution and higher. We argue that this limitation stems from inaccurate surface tracking, specifically, depth misalignment and surface drift between character geometry and the ground truth surface, which forces the detailed appearance model to compensate for geometric errors. To address this, we propose a latent deformation model and supervising the 3D deformation of the animatable character using guidance from foundational 2D video point trackers, which offer improved robustness to shading and surface variations, and are less prone to local minima than differentiable rendering. To mitigate the drift over time and lack of 3D awareness of 2D point trackers, we introduce a cascaded training strategy that generates consistent 3D point tracks by anchoring point tracks to the rendered avatar, which ultimately supervises our avatar at the vertex and texel level. To validate the effectiveness of our approach, we introduce a novel dataset comprising five multi-view video sequences, each over 10 minutes in duration, captured using 40 calibrated 6K-resolution cameras, featuring subjects dressed in clothing with challenging texture patterns and wrinkle deformations. Our approach demonstrates significantly improved performance in rendering quality and geometric accuracy over the prior state of the art.</li>
</ul>

<h3>Title: Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01806">https://arxiv.org/abs/2506.01806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01806">https://arxiv.org/pdf/2506.01806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01806]] Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition(https://arxiv.org/abs/2506.01806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, extraction, transformer</a></li>
<li><strong>Abstract: </strong>The increasing demand for hygienic and portable biometric systems has underscored the critical need for advancements in contactless fingerprint recognition. Despite its potential, this technology faces notable challenges, including out-of-focus image acquisition, reduced contrast between fingerprint ridges and valleys, variations in finger positioning, and perspective distortion. These factors significantly hinder the accuracy and reliability of contactless fingerprint matching. To address these issues, we propose a novel multi-stage transformer-based contactless fingerprint matching approach that first captures global spatial features and subsequently refines localized feature alignment across fingerprint samples. By employing a hierarchical feature extraction and matching pipeline, our method ensures fine-grained, cross-sample alignment while maintaining the robustness of global feature representation. We perform extensive evaluations on publicly available datasets such as HKPolyU and RidgeBase under different evaluation protocols, such as contactless-to-contact matching and contactless-to-contactless matching and demonstrate that our proposed approach outperforms existing methods, including COTS solutions.</li>
</ul>

<h3>Title: Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high</h3>
<ul>
<li><strong>Authors: </strong>PeiHsuan Huang, ZihWei Lin, Simon Imbot, WenCheng Fu, Ethan Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01814">https://arxiv.org/abs/2506.01814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01814">https://arxiv.org/pdf/2506.01814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01814]] Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high(https://arxiv.org/abs/2506.01814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an "invisible loudspeaker" effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1.</li>
</ul>

<h3>Title: Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique</h3>
<ul>
<li><strong>Authors: </strong>Stephan Sturm</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01815">https://arxiv.org/abs/2506.01815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01815">https://arxiv.org/pdf/2506.01815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01815]] Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique(https://arxiv.org/abs/2506.01815)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We provide an introduction to the topic of path signatures as means of feature extraction for machine learning from data streams. The article stresses the mathematical theory underlying the signature methodology, highlighting the conceptual character without plunging into the technical details of rigorous proofs. These notes are based on an introductory presentation given to students of the Research Experience for Undergraduates in Industrial Mathematics and Statistics at Worcester Polytechnic Institute in June 2024.</li>
</ul>

<h3>Title: BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses</h3>
<ul>
<li><strong>Authors: </strong>Shadman Rohan, Ishita Sur Apan, Muhtasim Ibteda Shochcho, Md Fahim, Mohammad Ashfaq Ur Rahman, AKM Mahbubur Rahman, Amin Ahsan Ali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01817">https://arxiv.org/abs/2506.01817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01817">https://arxiv.org/pdf/2506.01817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01817]] BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses(https://arxiv.org/abs/2506.01817)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors, under Track 1 (Mistake Identification) and Track 2 (Mistake Location). Both tracks involve three-class classification of tutor responses in educational dialogues - determining if a tutor correctly recognizes a student's mistake (Track 1) and whether the tutor pinpoints the mistake's location (Track 2). Our system is built on MPNet, a Transformer-based language model that combines BERT and XLNet's pre-training advantages. We fine-tuned MPNet on the task data using a class-weighted cross-entropy loss to handle class imbalance, and leveraged grouped cross-validation (10 folds) to maximize the use of limited data while avoiding dialogue overlap between training and validation. We then performed a hard-voting ensemble of the best models from each fold, which improves robustness and generalization by combining multiple classifiers. Our approach achieved strong results on both tracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake Identification and 0.5543 for Mistake Location on the official test set. We include comprehensive analysis of our system's performance, including confusion matrices and t-SNE visualizations to interpret classifier behavior, as well as a taxonomy of common errors with examples. We hope our ensemble-based approach and findings provide useful insights for designing reliable tutor response evaluation systems in educational dialogue settings.</li>
</ul>

<h3>Title: Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor</h3>
<ul>
<li><strong>Authors: </strong>Moahmmadamin Shafiei, Hamidreza Saffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01819">https://arxiv.org/abs/2506.01819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01819">https://arxiv.org/pdf/2506.01819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01819]] Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor(https://arxiv.org/abs/2506.01819)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the recent advances in Artificial Intelligence (AI) and Large Language Models (LLMs), the automation of daily tasks, like automatic writing, is getting more and more attention. Hence, efforts have focused on aligning LLMs with human values, yet humor, particularly professional industrial humor used in workplaces, has been largely neglected. To address this, we develop a dataset of professional humor statements along with features that determine the appropriateness of each statement. Our evaluation of five LLMs shows that LLMs often struggle to judge the appropriateness of humor accurately.</li>
</ul>

<h3>Title: Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Zhou Yang, Yaniv Harel, David Lo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01825">https://arxiv.org/abs/2506.01825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01825">https://arxiv.org/pdf/2506.01825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01825]] Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study(https://arxiv.org/abs/2506.01825)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Code LLMs are increasingly employed in software development. However, studies have shown that they are vulnerable to backdoor attacks: when a trigger (a specific input pattern) appears in the input, the backdoor will be activated and cause the model to generate malicious outputs. Researchers have designed various triggers and demonstrated the feasibility of implanting backdoors by poisoning a fraction of the training data. Some basic conclusions have been made, such as backdoors becoming easier to implant when more training data are modified. However, existing research has not explored other factors influencing backdoor attacks on Code LLMs, such as training batch size, epoch number, and the broader design space for triggers, e.g., trigger length. To bridge this gap, we use code summarization as an example to perform an empirical study that systematically investigates the factors affecting backdoor effectiveness and understands the extent of the threat posed. Three categories of factors are considered: data, model, and inference, revealing previously overlooked findings. We find that the prevailing consensus -- that attacks are ineffective at extremely low poisoning rates -- is incorrect. The absolute number of poisoned samples matters as well. Specifically, poisoning just 20 out of 454K samples (0.004\% poisoning rate -- far below the minimum setting of 0.1\% in prior studies) successfully implants backdoors! Moreover, the common defense is incapable of removing even a single poisoned sample from it. Additionally, small batch sizes increase the risk of backdoor attacks. We also uncover other critical factors such as trigger types, trigger length, and the rarity of tokens in the triggers, leading to valuable insights for assessing Code LLMs' vulnerability to backdoor attacks. Our study highlights the urgent need for defense mechanisms against extremely low poisoning rate settings.</li>
</ul>

<h3>Title: Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts</h3>
<ul>
<li><strong>Authors: </strong>Spencer Banasik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01827">https://arxiv.org/abs/2506.01827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01827">https://arxiv.org/pdf/2506.01827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01827]] Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts(https://arxiv.org/abs/2506.01827)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>As machine learning algorithms are shown to be an increasingly valuable tool, the demand for their access has grown accordingly. Oftentimes, it is infeasible to run inference with larger models without an accelerator, which may be unavailable in environments that have constraints such as energy consumption, security, or cost. To increase the availability of these models, we aim to im- prove the LLM inference speed on a CPU-only environment by modifying the cache architecture. To determine what improvements could be made, we conducted two experiments using this http URL and the QWEN model: running various cache configurations and evaluating their performance, and outputting a trace of the memory footprint. Using these experiments, we investigate the memory access patterns and performance characteristics to identify potential optimizations.</li>
</ul>

<h3>Title: Minimal Pair-Based Evaluation of Code-Switching</h3>
<ul>
<li><strong>Authors: </strong>Igor Sterner, Simone Teufel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01840">https://arxiv.org/abs/2506.01840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01840">https://arxiv.org/pdf/2506.01840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01840]] Minimal Pair-Based Evaluation of Code-Switching(https://arxiv.org/abs/2506.01840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There is a lack of an evaluation methodology that estimates the extent to which large language models (LLMs) use code-switching (CS) in the same way as bilinguals. Existing methods do not have wide language coverage, fail to account for the diverse range of CS phenomena, or do not scale. We propose an intervention based on minimal pairs of CS. Each minimal pair contains one naturally occurring CS sentence and one minimally manipulated variant. We collect up to 1,000 such pairs each for 11 language pairs. Our human experiments show that, for every language pair, bilinguals consistently prefer the naturally occurring CS sentence. Meanwhile our experiments with current LLMs show that the larger the model, the more consistently it assigns higher probability to the naturally occurring CS sentence than to the variant. In accordance with theoretical claims, the largest probability differences arise in those pairs where the manipulated material consisted of closed-class words.</li>
</ul>

<h3>Title: Identifying Key Expert Actors in Cybercrime Forums Based on their Technical Expertise</h3>
<ul>
<li><strong>Authors: </strong>Estelle Ruellan, Francois Labreche, Masarah Paquet-Clouston</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01848">https://arxiv.org/abs/2506.01848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01848">https://arxiv.org/pdf/2506.01848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01848]] Identifying Key Expert Actors in Cybercrime Forums Based on their Technical Expertise(https://arxiv.org/abs/2506.01848)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The advent of Big Data has made the collection and analysis of cyber threat intelligence challenging due to its volume, leading research to focus on identifying key threat actors; yet these studies have failed to consider the technical expertise of these actors. Expertise, especially towards specific attack patterns, is crucial for cybercrime intelligence, as it focuses on targeting actors with the knowledge and skills to attack enterprises. Using CVEs and CAPEC classifications to build a bimodal network, as well as community detection, k-means and a criminological framework, this study addresses the key hacker identification problem by identifying communities interested in specific attack patterns across cybercrime forums and their related key expert actors. The analyses reveal several key contributions. First, the community structure of the CAPEC-actor bimodal network shows that there exists groups of actors interested in similar attack patterns across cybercrime forums. Second, key actors identified in this study account for about 4% of the study population. Third, about half of the study population are amateurs who show little technical expertise. Finally, key actors highlighted in this study represent a promising scarcity for resources allocation in cyber threat intelligence production. Further research should look into how they develop and use their technical expertise in cybercrime forums.</li>
</ul>

<h3>Title: Trojan Horse Hunt in Time Series Forecasting for Space Operations</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Kotowski, Ramez Shendy, Jakub Nalepa, Przemysław Biecek, Piotr Wilczyński, Agata Kaczmarek, Dawid Płudowski, Artur Janicki, Evridiki Ntagiou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01849">https://arxiv.org/abs/2506.01849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01849">https://arxiv.org/pdf/2506.01849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01849]] Trojan Horse Hunt in Time Series Forecasting for Space Operations(https://arxiv.org/abs/2506.01849)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This competition hosted on Kaggle (this https URL) is the first part of a series of follow-up competitions and hackathons related to the "Assurance for Space Domain AI Applications" project funded by the European Space Agency (this https URL). The competition idea is based on one of the real-life AI security threats identified within the project -- the adversarial poisoning of continuously fine-tuned satellite telemetry forecasting models. The task is to develop methods for finding and reconstructing triggers (trojans) in advanced models for satellite telemetry forecasting used in safety-critical space operations. Participants are provided with 1) a large public dataset of real-life multivariate satellite telemetry (without triggers), 2) a reference model trained on the clean data, 3) a set of poisoned neural hierarchical interpolation (N-HiTS) models for time series forecasting trained on the dataset with injected triggers, and 4) Jupyter notebook with the training pipeline and baseline algorithm (the latter will be published in the last month of the competition). The main task of the competition is to reconstruct a set of 45 triggers (i.e., short multivariate time series segments) injected into the training data of the corresponding set of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and duration) of these triggers must be identified by participants. The popular Neural Cleanse method is adopted as a baseline, but it is not designed for time series analysis and new approaches are necessary for the task. The impact of the competition is not limited to the space domain, but also to many other safety-critical applications of advanced time series analysis where model poisoning may lead to serious consequences.</li>
</ul>

<h3>Title: MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Wayner Barrios, Andrés Villa, Juan León Alcázar, SouYoung Jin, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01850">https://arxiv.org/abs/2506.01850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01850">https://arxiv.org/pdf/2506.01850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01850]] MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs(https://arxiv.org/abs/2506.01850)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.</li>
</ul>

<h3>Title: ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01853">https://arxiv.org/abs/2506.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01853">https://arxiv.org/pdf/2506.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01853]] ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding(https://arxiv.org/abs/2506.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: this https URL</li>
</ul>

<h3>Title: Trade-offs in Data Memorization via Strong Data Processing Inequalities</h3>
<ul>
<li><strong>Authors: </strong>Vitaly Feldman, Guy Kornowski, Xin Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01855">https://arxiv.org/abs/2506.01855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01855">https://arxiv.org/pdf/2506.01855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01855]] Trade-offs in Data Memorization via Strong Data Processing Inequalities(https://arxiv.org/abs/2506.01855)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Recent research demonstrated that training large language models involves memorization of a significant fraction of training data. Such memorization can lead to privacy violations when training on sensitive user data and thus motivates the study of data memorization's role in learning. In this work, we develop a general approach for proving lower bounds on excess data memorization, that relies on a new connection between strong data processing inequalities and data memorization. We then demonstrate that several simple and natural binary classification problems exhibit a trade-off between the number of samples available to a learning algorithm, and the amount of information about the training data that a learning algorithm needs to memorize to be accurate. In particular, $\Omega(d)$ bits of information about the training data need to be memorized when $O(1)$ $d$-dimensional examples are available, which then decays as the number of examples grows at a problem-specific rate. Further, our lower bounds are generally matched (up to logarithmic factors) by simple learning algorithms. We also extend our lower bounds to more general mixture-of-clusters models. Our definitions and results build on the work of Brown et al. (2021) and address several limitations of the lower bounds in their work.</li>
</ul>

<h3>Title: Synchronic Web Digital Identity: Speculations on the Art of the Possible</h3>
<ul>
<li><strong>Authors: </strong>Thien-Nam Dinh, Justin Li, Mitch Negus, Ken Goss</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01856">https://arxiv.org/abs/2506.01856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01856">https://arxiv.org/pdf/2506.01856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01856]] Synchronic Web Digital Identity: Speculations on the Art of the Possible(https://arxiv.org/abs/2506.01856)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>As search, social media, and artificial intelligence continue to reshape collective knowledge, the preservation of trust on the public infosphere has become a defining challenge of our time. Given the breadth and versatility of adversarial threats, the best--and perhaps only--defense is an equally broad and versatile infrastructure for digital identity. This document discusses the opportunities and implications of building such an infrastructure from the perspective of a national laboratory. The technical foundation for this discussion is the emergence of the Synchronic Web, a Sandia-developed infrastructure for asserting cryptographic provenance at Internet scale. As of the writing of this document, there is ongoing work to develop the underlying technology and apply it to multiple mission-specific domains within Sandia. The primary objective of this document to extend the body of existing work toward the more public-facing domain of digital identity. Our approach depends on a non-standard, but philosophically defensible notion of identity: digital identity is an unbroken sequence of states in a well-defined digital space. From this foundation, we abstractly describe the infrastructural foundations and applied configurations that we expect to underpin future notions of digital identity.</li>
</ul>

<h3>Title: CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions</h3>
<ul>
<li><strong>Authors: </strong>Tamer Alkhouli, Katerina Margatina, James Gung, Raphael Shu, Claudia Zaghi, Monica Sunkara, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01859">https://arxiv.org/abs/2506.01859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01859">https://arxiv.org/pdf/2506.01859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01859]] CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions(https://arxiv.org/abs/2506.01859)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Conversational Function-Calling Evaluation Through Turn-Level Interactions (CONFETTI), a conversational benchmark1 designed to evaluate the function-calling capabilities and response quality of large language models (LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex conversational scenarios. CONFETTI addresses this gap through 109 human-simulated conversations, comprising 313 user turns and covering 86 APIs. These conversations explicitly target various conversational complexities, such as follow-ups, goal correction and switching, ambiguous and implicit goals. We perform off-policy turn-level evaluation using this benchmark targeting function-calling. Our benchmark also incorporates dialog act annotations to assess agent responses. We evaluate a series of state-of-the-art LLMs and analyze their performance with respect to the number of available APIs, conversation lengths, and chained function calling. Our results reveal that while some models are able to handle long conversations, and leverage more than 20+ APIs successfully, other models struggle with longer context or when increasing the number of APIs. We also report that the performance on chained function-calls is severely limited across the models. Overall, the top performing models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5 (35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and Mistral-Large-2407 (30.07%).</li>
</ul>

<h3>Title: Unified Scaling Laws for Compressed Representations</h3>
<ul>
<li><strong>Authors: </strong>Andrei Panferov, Alexandra Volkova, Ionut-Vlad Modoranu, Vage Egiazarian, Mher Safaryan, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01863">https://arxiv.org/abs/2506.01863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01863">https://arxiv.org/pdf/2506.01863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01863]] Unified Scaling Laws for Compressed Representations(https://arxiv.org/abs/2506.01863)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation's ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.</li>
</ul>

<h3>Title: Learning to Explore: An In-Context Learning Approach for Pure Exploration</h3>
<ul>
<li><strong>Authors: </strong>Alessio Russo, Ryan Welch, Aldo Pacchiano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01876">https://arxiv.org/abs/2506.01876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01876">https://arxiv.org/pdf/2506.01876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01876]] Learning to Explore: An In-Context Learning Approach for Pure Exploration(https://arxiv.org/abs/2506.01876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In this work, we study the active sequential hypothesis testing problem, also known as pure exploration, where the goal is to actively control a data collection process to efficiently identify the correct hypothesis underlying a decision problem. While relevant across multiple domains, devising adaptive exploration strategies remains challenging, particularly due to difficulties in encoding appropriate inductive biases. Existing Reinforcement Learning (RL)-based methods often underperform when relevant information structures are inadequately represented, whereas more complex methods, like Best Arm Identification (BAI) techniques, may be difficult to devise and typically rely on explicit modeling assumptions. To address these limitations, we introduce In-Context Pure Exploration (ICPE), an in-context learning approach that uses Transformers to learn exploration strategies directly from experience. ICPE combines supervised learning and reinforcement learning to identify and exploit latent structure across related tasks, without requiring prior assumptions. Numerical results across diverse synthetic and semi-synthetic benchmarks highlight ICPE's capability to achieve robust performance performance in deterministic, stochastic, and structured settings. These results demonstrate ICPE's ability to match optimal instance-dependent algorithms using only deep learning techniques, making it a practical and general approach to data-efficient exploration.</li>
</ul>

<h3>Title: SoK: Concurrency in Blockchain - A Systematic Literature Review and the Unveiling of a Misconception</h3>
<ul>
<li><strong>Authors: </strong>Atefeh Zareh Chahoki, Maurice Herlihy, Marco Roveri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01885">https://arxiv.org/abs/2506.01885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01885">https://arxiv.org/pdf/2506.01885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01885]] SoK: Concurrency in Blockchain - A Systematic Literature Review and the Unveiling of a Misconception(https://arxiv.org/abs/2506.01885)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Smart contracts, the cornerstone of blockchain technology, enable secure, automated distributed execution. Given their role in handling large transaction volumes across clients, miners, and validators, exploring concurrency is critical. This includes concurrent transaction execution or validation within blocks, block processing across shards, and miner competition to select and persist transactions. Concurrency and parallelism are a double-edged sword: while they improve throughput, they also introduce risks like race conditions, non-determinism, and vulnerabilities such as deadlock and livelock. This paper presents the first survey of concurrency in smart contracts, offering a systematic literature review organized into key dimensions. First, it establishes a taxonomy of concurrency levels in blockchain systems and discusses proposed solutions for future adoption. Second, it examines vulnerabilities, attacks, and countermeasures in concurrent operations, emphasizing the need for correctness and security. Crucially, we reveal a flawed concurrency assumption in a major research category, which has led to widespread misinterpretation. This work aims to correct that and guide future research toward more accurate models. Finally, we identify gaps in each category to outline future research directions and support blockchain's advancement.</li>
</ul>

<h3>Title: MLorc: Momentum Low-rank Compression for Large Language Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Wei Shen, Yaxiang Zhang, Minhui Huang, Mengfan Xu, Jiawei Zhang, Cong Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01897">https://arxiv.org/abs/2506.01897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01897">https://arxiv.org/pdf/2506.01897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01897]] MLorc: Momentum Low-rank Compression for Large Language Model Adaptation(https://arxiv.org/abs/2506.01897)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With increasing size of large language models (LLMs), full-parameter fine-tuning imposes substantial memory demands. To alleviate this, we propose a novel memory-efficient training paradigm called Momentum Low-rank compression (MLorc). By directly compressing and reconstructing momentum rather than gradients, MLorc avoids imposing a fixed-rank constraint on weight update matrices and better preserves the training dynamics of full-parameter fine-tuning, in contrast to existing low-rank approaches such as LoRA and GaLore. Empirically, MLorc consistently outperforms other memory-efficient training methods, matches or even exceeds the performance of full fine-tuning with a small rank (e.g., $r=4$), and generalizes well across different optimizers -- all while not compromising time or memory efficiency. Furthermore, we provide a theoretical guarantee for its convergence under reasonable assumptions.</li>
</ul>

<h3>Title: Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination</h3>
<ul>
<li><strong>Authors: </strong>Xinliu Zhong, Kayhan Batmanghelich, Li Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01902">https://arxiv.org/abs/2506.01902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01902">https://arxiv.org/pdf/2506.01902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01902]] Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination(https://arxiv.org/abs/2506.01902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models pre-trained on large scale of unlabeled biomedical images and associated reports learn generalizable semantic representations. These multi-modal representations can benefit various downstream tasks in the biomedical domain. Contrastive learning is widely used to pre-train vision-language models for general natural images and associated captions. Despite its popularity, we found biomedical texts have complex and domain-specific semantics that are often neglected by common contrastive methods. To address this issue, we propose a novel method, perturbed report discrimination, for pre-train biomedical vision-language models. First, we curate a set of text perturbation methods that keep the same words, but disrupt the semantic structure of the sentence. Next, we apply different types of perturbation to reports, and use the model to distinguish the original report from the perturbed ones given the associated image. Parallel to this, we enhance the sensitivity of our method to higher level of granularity for both modalities by contrasting attention-weighted image sub-regions and sub-words in the image-text pairs. We conduct extensive experiments on multiple downstream tasks, and our method outperforms strong baseline methods. The results demonstrate that our approach learns more semantic meaningful and robust multi-modal representations.</li>
</ul>

<h3>Title: SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yan Zhou, Bradley Malin, Murat Kantarcioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01907">https://arxiv.org/abs/2506.01907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01907">https://arxiv.org/pdf/2506.01907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01907]] SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data(https://arxiv.org/abs/2506.01907)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, generative</a></li>
<li><strong>Abstract: </strong>Privacy-preserving data publication, including synthetic data sharing, often experiences trade-offs between privacy and utility. Synthetic data is generally more effective than data anonymization in balancing this trade-off, however, not without its own challenges. Synthetic data produced by generative models trained on source data may inadvertently reveal information about outliers. Techniques specifically designed for preserving privacy, such as introducing noise to satisfy differential privacy, often incur unpredictable and significant losses in utility. In this work we show that, with the right mechanism of synthetic data generation, we can achieve strong privacy protection without significant utility loss. Synthetic data generators producing contracting data patterns, such as Synthetic Minority Over-sampling Technique (SMOTE), can enhance a differentially private data generator, leveraging the strengths of both. We prove in theory and through empirical demonstration that this SMOTE-DP technique can produce synthetic data that not only ensures robust privacy protection but maintains utility in downstream learning tasks.</li>
</ul>

<h3>Title: Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, Si Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01908">https://arxiv.org/abs/2506.01908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01908">https://arxiv.org/pdf/2506.01908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01908]] Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency(https://arxiv.org/abs/2506.01908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding real-world videos with complex semantics and long temporal dependencies remains a fundamental challenge in computer vision. Recent progress in multimodal large language models (MLLMs) has demonstrated strong capabilities in vision-language tasks, while reinforcement learning tuning (RLT) has further improved their reasoning abilities. In this work, we explore RLT as a post-training strategy to enhance the video-specific reasoning capabilities of MLLMs. Built upon the Group Relative Policy Optimization (GRPO) framework, we propose a dual-reward formulation that supervises both semantic and temporal reasoning through discrete and continuous reward signals. To facilitate effective preference-based optimization, we introduce a variance-aware data selection strategy based on repeated inference to identify samples that provide informative learning signals. We evaluate our approach across eight representative video understanding tasks, including VideoQA, Temporal Video Grounding, and Grounded VideoQA. Our method consistently outperforms supervised fine-tuning and existing RLT baselines, achieving superior performance with significantly less training data. These results underscore the importance of reward design and data selection in advancing reasoning-centric video understanding with MLLMs. Notably, The initial code release (two months ago) has now been expanded with updates, including optimized reward mechanisms and additional datasets. The latest version is available at this https URL .</li>
</ul>

<h3>Title: Elucidating the representation of images within an unconditional diffusion model denoiser</h3>
<ul>
<li><strong>Authors: </strong>Zahra Kadkhodaie, Stéphane Mallat, Eero Simoncelli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01912">https://arxiv.org/abs/2506.01912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01912">https://arxiv.org/pdf/2506.01912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01912]] Elucidating the representation of images within an unconditional diffusion model denoiser(https://arxiv.org/abs/2506.01912)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative diffusion models learn probability densities over diverse image datasets by estimating the score with a neural network trained to remove noise. Despite their remarkable success in generating high-quality images, the internal mechanisms of the underlying score networks are not well understood. Here, we examine a UNet trained for denoising on the ImageNet dataset, to better understand its internal representation and computation of the score. We show that the middle block of the UNet decomposes individual images into sparse subsets of active channels, and that the vector of spatial averages of these channels can provide a nonlinear representation of the underlying clean images. We develop a novel algorithm for stochastic reconstruction of images from this representation and demonstrate that it recovers a sample from a set of images defined by a target image representation. We then study the properties of the representation and demonstrate that Euclidean distances in the latent space correspond to distances between conditional densities induced by representations as well as semantic similarities in the image space. Applying a clustering algorithm in the representation space yields groups of images that share both fine details (e.g., specialized features, textured regions, small objects), as well as global structure, but are only partially aligned with object identities. Thus, we show for the first time that a network trained solely on denoising contains a rich and accessible sparse representation of images.</li>
</ul>

<h3>Title: Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis</h3>
<ul>
<li><strong>Authors: </strong>Chi-Jane Chen, Yuhang Chen, Sukwon Yun, Natalie Stanley, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01918">https://arxiv.org/abs/2506.01918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01918">https://arxiv.org/pdf/2506.01918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01918]] Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis(https://arxiv.org/abs/2506.01918)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry's analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information: they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently: they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates single-cell expression and spatial information into natural language using a multi-sentence approach. Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations enable LLMs to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: this https URL.</li>
</ul>

<h3>Title: Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Hao, Chenlu Ye, Chi Han, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01919">https://arxiv.org/abs/2506.01919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01919">https://arxiv.org/pdf/2506.01919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01919]] Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models(https://arxiv.org/abs/2506.01919)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Despite their empirical success, a comprehensive theoretical understanding of this phenomenon remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model, i.e, Hidden Markov Models, which are fundamental to many language tasks, we observe that: first, lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens; second, on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement. Building on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer's effectiveness and efficiency on sequence learning across diverse tasks.</li>
</ul>

<h3>Title: TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation</h3>
<ul>
<li><strong>Authors: </strong>Amin Karimi Monsefi, Mridul Khurana, Rajiv Ramnath, Anuj Karpatne, Wei-Lun Chao, Cheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01923">https://arxiv.org/abs/2506.01923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01923">https://arxiv.org/pdf/2506.01923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01923]] TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation(https://arxiv.org/abs/2506.01923)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: this https URL</li>
</ul>

<h3>Title: Esoteric Language Models</h3>
<ul>
<li><strong>Authors: </strong>Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, Arash Vahdat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01928">https://arxiv.org/abs/2506.01928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01928">https://arxiv.org/pdf/2506.01928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01928]] Esoteric Language Models(https://arxiv.org/abs/2506.01928)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [this http URL](this http URL)</li>
</ul>

<h3>Title: E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Wenyan Cong, Yiqing Liang, Yancheng Zhang, Ziyi Yang, Yan Wang, Boris Ivanovic, Marco Pavone, Chen Chen, Zhangyang Wang, Zhiwen Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01933">https://arxiv.org/abs/2506.01933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01933">https://arxiv.org/pdf/2506.01933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01933]] E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models(https://arxiv.org/abs/2506.01933)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Spatial intelligence, encompassing 3D reconstruction, perception, and reasoning, is fundamental to applications such as robotics, aerial imaging, and extended reality. A key enabler is the real-time, accurate estimation of core 3D attributes (camera parameters, point clouds, depth maps, and 3D point tracks) from unstructured or streaming imagery. Inspired by the success of large foundation models in language and 2D vision, a new class of end-to-end 3D geometric foundation models (GFMs) has emerged, directly predicting dense 3D representations in a single feed-forward pass, eliminating the need for slow or unavailable precomputed camera parameters. Since late 2023, the field has exploded with diverse variants, but systematic evaluation is lacking. In this work, we present the first comprehensive benchmark for 3D GFMs, covering five core tasks: sparse-view depth estimation, video depth estimation, 3D reconstruction, multi-view pose estimation, novel view synthesis, and spanning both standard and challenging out-of-distribution datasets. Our standardized toolkit automates dataset handling, evaluation protocols, and metric computation to ensure fair, reproducible comparisons. We evaluate 16 state-of-the-art GFMs, revealing their strengths and limitations across tasks and domains, and derive key insights to guide future model scaling and optimization. All code, evaluation scripts, and processed data will be publicly released to accelerate research in 3D spatial intelligence.</li>
</ul>

<h3>Title: Novel Benchmark for NER in the Wastewater and Stormwater Domain</h3>
<ul>
<li><strong>Authors: </strong>Franco Alberto Cardillo, Franca Debole, Francesca Frontini, Mitra Aelami, Nanée Chahinian, Serge Conrad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01938">https://arxiv.org/abs/2506.01938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01938">https://arxiv.org/pdf/2506.01938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01938]] Novel Benchmark for NER in the Wastewater and Stormwater Domain(https://arxiv.org/abs/2506.01938)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, extraction</a></li>
<li><strong>Abstract: </strong>Effective wastewater and stormwater management is essential for urban sustainability and environmental protection. Extracting structured knowledge from reports and regulations is challenging due to domainspecific terminology and multilingual contexts. This work focuses on domain-specific Named Entity Recognition (NER) as a first step towards effective relation and information extraction to support decision making. A multilingual benchmark is crucial for evaluating these methods. This study develops a French-Italian domain-specific text corpus for wastewater management. It evaluates state-of-the-art NER methods, including LLM-based approaches, to provide a reliable baseline for future strategies and explores automated annotation projection in view of an extension of the corpus to new languages.</li>
</ul>

<h3>Title: Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01939">https://arxiv.org/abs/2506.01939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01939">https://arxiv.org/pdf/2506.01939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01939]] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning(https://arxiv.org/abs/2506.01939)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.</li>
</ul>

<h3>Title: Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent</h3>
<ul>
<li><strong>Authors: </strong>Yaroslava Lochman, Carl Olsson, Christopher Zach</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01940">https://arxiv.org/abs/2506.01940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01940">https://arxiv.org/pdf/2506.01940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01940]] Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent(https://arxiv.org/abs/2506.01940)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Anisotropic rotation averaging has recently been explored as a natural extension of respective isotropic methods. In the anisotropic formulation, uncertainties of the estimated relative rotations -- obtained via standard two-view optimization -- are propagated to the optimization of absolute rotations. The resulting semidefinite relaxations are able to recover global minima but scale poorly with the problem size. Local methods are fast and also admit robust estimation but are sensitive to initialization. They usually employ minimum spanning trees and therefore suffer from drift accumulation and can get trapped in poor local minima. In this paper, we attempt to bridge the gap between optimality, robustness and efficiency of anisotropic rotation averaging. We analyze a family of block coordinate descent methods initially proposed to optimize the standard chordal distances, and derive a much simpler formulation and an anisotropic extension obtaining a fast general solver. We integrate this solver into the extended anisotropic large-scale robust rotation averaging pipeline. The resulting algorithm achieves state-of-the-art performance on public structure-from-motion datasets. Project page: this https URL</li>
</ul>

<h3>Title: Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control</h3>
<ul>
<li><strong>Authors: </strong>Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01943">https://arxiv.org/abs/2506.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01943">https://arxiv.org/pdf/2506.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01943]] Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control(https://arxiv.org/abs/2506.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.</li>
</ul>

<h3>Title: MLLMs Need 3D-Aware Representation Supervision for Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaohu Huang, Jingjing Wu, Qunyi Xie, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01946">https://arxiv.org/abs/2506.01946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01946">https://arxiv.org/pdf/2506.01946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01946]] MLLMs Need 3D-Aware Representation Supervision for Scene Understanding(https://arxiv.org/abs/2506.01946)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in scene understanding have leveraged multimodal large language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D pretraining. However, the lack of explicit 3D data during MLLM pretraining limits 3D representation capability. In this paper, we investigate the 3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a strong positive correlation between the quality of 3D-aware representation and downstream task performance. Motivated by this, we propose 3DRS, a framework that enhances MLLM 3D representation learning by introducing supervision from pretrained 3D foundation models. Our approach aligns MLLM visual features with rich 3D knowledge distilled from 3D models, effectively improving scene understanding. Extensive experiments across multiple benchmarks and MLLMs -- including visual grounding, captioning, and question answering -- demonstrate consistent performance gains. Project page: this https URL</li>
</ul>

<h3>Title: IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout</h3>
<ul>
<li><strong>Authors: </strong>Fei Shen, Xiaoyu Du, Yutong Gao, Jian Yu, Yushe Cao, Xing Lei, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01949">https://arxiv.org/abs/2506.01949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01949">https://arxiv.org/pdf/2506.01949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01949]] IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout(https://arxiv.org/abs/2506.01949)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion models have advanced image editing by enhancing visual quality and control, supporting broad applications across creative and personalized domains. However, current image editing largely overlooks multi-object scenarios, where precise control over object categories, counts, and spatial layouts remains a significant challenge. To address this, we introduce a new task, quantity-and-layout consistent image editing (QL-Edit), which aims to enable fine-grained control of object quantity and spatial structure in complex scenes. We further propose IMAGHarmony, a structure-aware framework that incorporates harmony-aware attention (HA) to integrate multimodal semantics, explicitly modeling object counts and layouts to enhance editing accuracy and structural consistency. In addition, we observe that diffusion models are susceptible to initial noise and exhibit strong preferences for specific noise patterns. Motivated by this, we present a preference-guided noise selection (PNS) strategy that chooses semantically aligned initial noise samples based on vision-language matching, thereby improving generation stability and layout consistency in multi-object editing. To support evaluation, we construct HarmonyBench, a comprehensive benchmark covering diverse quantity and layout control scenarios. Extensive experiments demonstrate that IMAGHarmony consistently outperforms state-of-the-art methods in structural alignment and semantic accuracy. The code and model are available at this https URL.</li>
</ul>

<h3>Title: Self-ensemble: Mitigating Confidence Distortion for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zicheng Xu, Guanchu Wang, Guangyao Zheng, Yu-Neng Chuang, Alexander Szalay, Xia Hu, Vladimir Braverman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01951">https://arxiv.org/abs/2506.01951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01951">https://arxiv.org/pdf/2506.01951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01951]] Self-ensemble: Mitigating Confidence Distortion for Large Language Models(https://arxiv.org/abs/2506.01951)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) perform well in general fields, they exhibit a confidence distortion problem on multi-choice question-answering (MCQA), particularly as the number of answer choices increases. Specifically, on MCQA with many choices, LLMs suffer from under-confidence in correct predictions and over-confidence in incorrect ones, leading to a substantially degraded performance. To solve this problem, we propose Self-ensemble in this work. Our method splits the choices into several groups and ensembles LLM predictions across these groups to reach a final decision. The advantage of Self-ensemble is its plug-and-play nature, where it can be integrated into existing LLM architecture based on a designed attention mask and positional encoding, without requiring labeled datasets for parameter tuning. Experimental results on three LLMs and datasets demonstrate that Self-ensemble comprehensively addresses the confidence distortion problem of LLMs, outperforming standard inference as well as baseline methods.</li>
</ul>

<h3>Title: WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks</h3>
<ul>
<li><strong>Authors: </strong>Atsuyuki Miyai, Zaiying Zhao, Kazuki Egashira, Atsuki Sato, Tatsumi Sunada, Shota Onohara, Hiromasa Yamanishi, Mashiro Toyooka, Kunato Nishina, Ryoma Maeda, Kiyoharu Aizawa, Toshihiko Yamasaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01952">https://arxiv.org/abs/2506.01952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01952">https://arxiv.org/pdf/2506.01952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01952]] WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks(https://arxiv.org/abs/2506.01952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.</li>
</ul>

<h3>Title: DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jennifer Chen, Aidar Myrzakhan, Yaxin Luo, Hassaan Muhammad Khan, Sondos Mahmoud Bsharat, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.01954">https://arxiv.org/abs/2506.01954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.01954">https://arxiv.org/pdf/2506.01954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.01954]] DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation(https://arxiv.org/abs/2506.01954)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
