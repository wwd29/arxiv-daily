<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: All Rivers Run to the Sea: Private Learning with Asymmetric Flows. (arXiv:2312.05264v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05264">http://arxiv.org/abs/2312.05264</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05264]] All Rivers Run to the Sea: Private Learning with Asymmetric Flows(http://arxiv.org/abs/2312.05264)</code></li>
<li>Summary: <p>Data privacy is of great concern in cloud machine-learning service platforms,
when sensitive data are exposed to service providers. While private computing
environments (e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protection, their computing
performance still falls short compared to cloud GPUs. To achieve privacy
protection with high computing performance, we propose Delta, a new private
training and inference framework, with comparable model performance as
non-private centralized training. Delta features two asymmetric data flows: the
main information-sensitive flow and the residual flow. The main part flows into
a small model while the residuals are offloaded to a large model. Specifically,
Delta embeds the information-sensitive representations into a low-dimensional
space while pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small model in a private
environment. On the other hand, the residual part is sent to fast cloud GPUs,
and processed by a large model. To further enhance privacy and reduce the
communication cost, Delta applies a random binary quantization technique along
with a DP-based technique to the residuals before sharing them with the public
platform. We theoretically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity in the private
environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet
datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong
privacy protection, fast training, and inference without significantly
compromising the model utility.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Exploring the Limits of ChatGPT in Software Security Applications. (arXiv:2312.05275v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05275">http://arxiv.org/abs/2312.05275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05275]] Exploring the Limits of ChatGPT in Software Security Applications(http://arxiv.org/abs/2312.05275)</code></li>
<li>Summary: <p>Large language models (LLMs) have undergone rapid evolution and achieved
remarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or
GPT-4, has gained instant popularity due to its strong capability across a wide
range of tasks, including natural language tasks, coding, mathematics, and
engaging conversations. However, the impacts and limits of such LLMs in system
security domain are less explored. In this paper, we delve into the limits of
LLMs (i.e., ChatGPT) in seven software security applications including
vulnerability detection/repair, debugging, debloating, decompilation, patching,
root cause analysis, symbolic execution, and fuzzing. Our exploration reveals
that ChatGPT not only excels at generating code, which is the conventional
application of language models, but also demonstrates strong capability in
understanding user-provided commands in natural languages, reasoning about
control and data flows within programs, generating complex data structures, and
even decompiling assembly code. Notably, GPT-4 showcases significant
improvements over GPT-3.5 in most security tasks. Also, certain limitations of
ChatGPT in security-related tasks are identified, such as its constrained
ability to process long code contexts.
</p></li>
</ul>

<h3>Title: Trade-off of Security, Latency, and Throughput of the Nakamoto Consensus. (arXiv:2312.05506v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05506">http://arxiv.org/abs/2312.05506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05506]] Trade-off of Security, Latency, and Throughput of the Nakamoto Consensus(http://arxiv.org/abs/2312.05506)</code></li>
<li>Summary: <p>This paper delves into the fundamental trade-off between security, latency,
and throughput in proof-of-work longest-chain-wins protocols, also known as the
Nakamoto consensus. New upper and lower bounds on the probability of violating
transaction safety are derived as a function of honest and adversarial mining
rates, an upper bound on block propagation delays, and transaction confirmation
latency, both in time and in block depth. The results include a first
closed-form finite-latency bound applicable to all delays and mining rates up
to the ultimate fault tolerance. Notably, for most parameters relevant to
Bitcoin and proof-of-work Ethereum, the gap between the upper and lower bounds
is significantly narrower than the best gaps previously established in the
literature. Furthermore, the paper reveals a fundamental trade-off between
transaction throughput and confirmation latency, ultimately determined by the
desired fault tolerance and the growth of block propagation delay as block size
increases.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: StableQ: Enhancing Data-Scarce Quantization with Text-to-Image Data. (arXiv:2312.05272v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05272">http://arxiv.org/abs/2312.05272</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05272]] StableQ: Enhancing Data-Scarce Quantization with Text-to-Image Data(http://arxiv.org/abs/2312.05272)</code></li>
<li>Summary: <p>Though low-bit quantization enables efficient storage and inference of deep
neural networks, it often requires the use of training data to maintain
resilience against quantization errors. However, training data are frequently
subject to privacy or copyright concerns. In this work, we address the
challenge of Data-Scarce Quantization, where access to training data is
severely limited or non-existent for quantization purposes. Conventional
approaches typically rely on inverting dummy images or jointly training
generative models to produce synthetic input samples. However, these methods
struggle to accurately recreate complex objects in large-scale datasets like
ImageNet. To overcome these limitations, we introduce StableQ, a novel method
that utilizes an advanced text-to-image diffusion model to generate
high-resolution, photo-realistic synthetic data. To verify the quality of the
generated data, we implement two robust filtering mechanisms. These mechanisms
are designed to select images that closely resemble the intrinsic
characteristics of the actual training data. Furthermore, in scenarios where
limited training data are available, we use these data to guide the synthetic
data generation process by inverting a learnable token embedding in the text
encoder. Our extensive experimental results demonstrate that StbaleQ sets a new
benchmark in both zero-shot and few-shot quantization, outperforming existing
methods in terms of accuracy and efficiency.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Model Copyright Protection in Buyer-seller Environment. (arXiv:2312.05262v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05262">http://arxiv.org/abs/2312.05262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05262]] Model Copyright Protection in Buyer-seller Environment(http://arxiv.org/abs/2312.05262)</code></li>
<li>Summary: <p>Training a deep neural network (DNN) requires a high computational cost.
Buying models from sellers with a large number of computing resources has
become prevailing. However, the buyer-seller environment is not always trusted.
To protect the neural network models from leaking in an untrusted environment,
we propose a novel copyright protection scheme for DNN using an input-sensitive
neural network (ISNN). The main idea of ISNN is to make a DNN sensitive to the
key and copyright information. Therefore, only the buyer with a correct key can
utilize the ISNN. During the training phase, we add a specific perturbation to
the clean images and mark them as legal inputs, while the other inputs are
treated as illegal input. We design a loss function to make the outputs of
legal inputs close to the true ones, while the illegal inputs are far away from
true results. Experimental results demonstrate that the proposed scheme is
effective, valid, and secure.
</p></li>
</ul>

<h3>Title: Mitigating Nonlinear Algorithmic Bias in Binary Classification. (arXiv:2312.05429v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05429">http://arxiv.org/abs/2312.05429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05429]] Mitigating Nonlinear Algorithmic Bias in Binary Classification(http://arxiv.org/abs/2312.05429)</code></li>
<li>Summary: <p>This paper proposes the use of causal modeling to detect and mitigate
algorithmic bias that is nonlinear in the protected attribute. We provide a
general overview of our approach. We use the German Credit data set, which is
available for download from the UC Irvine Machine Learning Repository, to
develop (1) a prediction model, which is treated as a black box, and (2) a
causal model for bias mitigation. In this paper, we focus on age bias and the
problem of binary classification. We show that the probability of getting
correctly classified as "low risk" is lowest among young people. The
probability increases with age nonlinearly. To incorporate the nonlinearity
into the causal model, we introduce a higher order polynomial term. Based on
the fitted causal model, the de-biased probability estimates are computed,
showing improved fairness with little impact on overall classification
accuracy. Causal modeling is intuitive and, hence, its use can enhance
explicability and promotes trust among different stakeholders of AI.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Model Extraction Attacks Revisited. (arXiv:2312.05386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05386">http://arxiv.org/abs/2312.05386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05386]] Model Extraction Attacks Revisited(http://arxiv.org/abs/2312.05386)</code></li>
<li>Summary: <p>Model extraction (ME) attacks represent one major threat to
Machine-Learning-as-a-Service (MLaaS) platforms by ``stealing'' the
functionality of confidential machine-learning models through querying
black-box APIs. Over seven years have passed since ME attacks were first
conceptualized in the seminal work. During this period, substantial advances
have been made in both ME attacks and MLaaS platforms, raising the intriguing
question: How has the vulnerability of MLaaS platforms to ME attacks been
evolving? In this work, we conduct an in-depth study to answer this critical
question. Specifically, we characterize the vulnerability of current,
mainstream MLaaS platforms to ME attacks from multiple perspectives including
attack strategies, learning techniques, surrogate-model design, and benchmark
tasks. Many of our findings challenge previously reported results, suggesting
emerging patterns of ME vulnerability. Further, by analyzing the vulnerability
of the same MLaaS platforms using historical datasets from the past four years,
we retrospectively characterize the evolution of ME vulnerability over time,
leading to a set of interesting findings. Finally, we make suggestions about
improving the current practice of MLaaS in terms of attack robustness. Our
study sheds light on the current state of ME vulnerability in the wild and
points to several promising directions for future research.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Nuvo: Neural UV Mapping for Unruly 3D Representations. (arXiv:2312.05283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05283">http://arxiv.org/abs/2312.05283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05283]] Nuvo: Neural UV Mapping for Unruly 3D Representations(http://arxiv.org/abs/2312.05283)</code></li>
<li>Summary: <p>Existing UV mapping algorithms are designed to operate on well-behaved
meshes, instead of the geometry representations produced by state-of-the-art 3D
reconstruction and generation techniques. As such, applying these methods to
the volume densities recovered by neural radiance fields and related techniques
(or meshes triangulated from such fields) results in texture atlases that are
too fragmented to be useful for tasks such as view synthesis or appearance
editing. We present a UV mapping method designed to operate on geometry
produced by 3D reconstruction and generation techniques. Instead of computing a
mapping defined on a mesh's vertices, our method Nuvo uses a neural field to
represent a continuous UV mapping, and optimizes it to be a valid and
well-behaved mapping for just the set of visible points, i.e. only points that
affect the scene's appearance. We show that our model is robust to the
challenges posed by ill-behaved geometry, and that it produces editable UV
mappings that can represent detailed appearance.
</p></li>
</ul>

<h3>Title: Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation. (arXiv:2312.05508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05508">http://arxiv.org/abs/2312.05508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05508]] Improving Adversarial Robust Fairness via Anti-Bias Soft Label Distillation(http://arxiv.org/abs/2312.05508)</code></li>
<li>Summary: <p>Adversarial Training (AT) has been widely proved to be an effective method to
improve the adversarial robustness against adversarial examples for Deep Neural
Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)
has demonstrated its superior performance in improving the robustness of small
student models with the guidance of large teacher models. However, both AT and
ARD encounter the robust fairness problem: these models exhibit strong
robustness when facing part of classes (easy class), but weak robustness when
facing others (hard class). In this paper, we give an in-depth analysis of the
potential factors and argue that the smoothness degree of samples' soft labels
for different classes (i.e., hard class or easy class) will affect the robust
fairness of DNN models from both empirical observation and theoretical
analysis. Based on the above finding, we propose an Anti-Bias Soft Label
Distillation (ABSLD) method to mitigate the adversarial robust fairness problem
within the framework of Knowledge Distillation (KD). Specifically, ABSLD
adaptively reduces the student's error risk gap between different classes to
achieve fairness by adjusting the class-wise smoothness degree of samples' soft
labels during the training process, and the smoothness degree of soft labels is
controlled by assigning different temperatures in KD to different classes.
Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,
ARD, and robust fairness methods in terms of overall performance of robustness
and fairness.
</p></li>
</ul>

<h3>Title: You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception. (arXiv:2312.05525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05525">http://arxiv.org/abs/2312.05525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05525]] You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception(http://arxiv.org/abs/2312.05525)</code></li>
<li>Summary: <p>Human-centric perception (e.g. pedetrian detection, segmentation, pose
estimation, and attribute analysis) is a long-standing problem for computer
vision. This paper introduces a unified and versatile framework (HQNet) for
single-stage multi-person multi-task human-centric perception (HCP). Our
approach centers on learning a unified human query representation, denoted as
Human Query, which captures intricate instance-level features for individual
persons and disentangles complex multi-person scenarios. Although different HCP
tasks have been well-studied individually, single-stage multi-task learning of
HCP tasks has not been fully exploited in the literature due to the absence of
a comprehensive benchmark dataset. To address this gap, we propose
COCO-UniHuman benchmark dataset to enable model development and comprehensive
evaluation. Experimental results demonstrate the proposed method's
state-of-the-art performance among multi-task HCP models and its competitive
performance compared to task-specific HCP models. Moreover, our experiments
underscore Human Query's adaptability to new HCP tasks, thus demonstrating its
robust generalization capability. Codes and data will be publicly accessible.
</p></li>
</ul>

<h3>Title: DPoser: Diffusion Model as Robust 3D Human Pose Prior. (arXiv:2312.05541v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05541">http://arxiv.org/abs/2312.05541</a></li>
<li>Code URL: https://github.com/moonbow721/dposer</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05541]] DPoser: Diffusion Model as Robust 3D Human Pose Prior(http://arxiv.org/abs/2312.05541)</code></li>
<li>Summary: <p>Modeling human pose is a cornerstone in applications from human-robot
interaction to augmented reality, yet crafting a robust human pose prior
remains a challenge due to biomechanical constraints and diverse human
movements. Traditional priors like VAEs and NDFs often fall short in realism
and generalization, especially in extreme conditions such as unseen noisy
poses. To address these issues, we introduce DPoser, a robust and versatile
human pose prior built upon diffusion models. Designed with optimization
frameworks, DPoser seamlessly integrates into various pose-centric
applications, including human mesh recovery, pose completion, and motion
denoising. Specifically, by formulating these tasks as inverse problems, we
employ variational diffusion sampling for efficient solving. Furthermore,
acknowledging the disparity between the articulated poses we focus on and
structured images in previous research, we propose a truncated timestep
scheduling to boost performance on downstream tasks. Our exhaustive experiments
demonstrate DPoser's superiority over existing state-of-the-art pose priors
across multiple tasks.
</p></li>
</ul>

<h3>Title: Enhancing Robustness of Foundation Model Representations under Provenance-related Distribution Shifts. (arXiv:2312.05435v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05435">http://arxiv.org/abs/2312.05435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05435]] Enhancing Robustness of Foundation Model Representations under Provenance-related Distribution Shifts(http://arxiv.org/abs/2312.05435)</code></li>
<li>Summary: <p>Foundation models are a current focus of attention in both industry and
academia. While they have shown their capabilities in a variety of tasks,
in-depth research is required to determine their robustness to distribution
shift when used as a basis for supervised machine learning. This is especially
important in the context of clinical data, with particular limitations related
to data accessibility, lack of pretraining materials, and limited availability
of high-quality annotations. In this work, we examine the stability of models
based on representations from foundation models under distribution shift. We
focus on confounding by provenance, a form of distribution shift that emerges
in the context of multi-institutional datasets when there are differences in
source-specific language use and class distributions. Using a sampling strategy
that synthetically induces varying degrees of distribution shift, we evaluate
the extent to which representations from foundation models result in
predictions that are inherently robust to confounding by provenance.
Additionally, we examine the effectiveness of a straightforward confounding
adjustment method inspired by Pearl's conception of backdoor adjustment.
Results indicate that while foundation models do show some out-of-the-box
robustness to confounding-by-provenance related distribution shifts, this can
be considerably improved through adjustment. These findings suggest a need for
deliberate adjustment of predictive models using representations from
foundation models in the context of source-specific distributional differences.
</p></li>
</ul>

<h3>Title: AI Competitions and Benchmarks: The life cycle of challenges and benchmarks. (arXiv:2312.05296v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05296">http://arxiv.org/abs/2312.05296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05296]] AI Competitions and Benchmarks: The life cycle of challenges and benchmarks(http://arxiv.org/abs/2312.05296)</code></li>
<li>Summary: <p>Data Science research is undergoing a revolution fueled by the transformative
power of technology, the Internet, and an ever increasing computational
capacity. The rate at which sophisticated algorithms can be developed is
unprecedented, yet they remain outpaced by the massive amounts of data that are
increasingly available to researchers. Here we argue for the need to creatively
leverage the scientific research and algorithm development community as an axis
of robust innovation. Engaging these communities in the scientific discovery
enterprise by critical assessments, community experiments, and/or crowdsourcing
will multiply opportunities to develop new data driven, reproducible and well
benchmarked algorithmic solutions to fundamental and applied problems of
current interest. Coordinated community engagement in the analysis of highly
complex and massive data has emerged as one approach to find robust
methodologies that best address these challenges. When community engagement is
done in the form of competitions, also known as challenges, the validation of
the analytical methodology is inherently addressed, establishing performance
benchmarks. Finally, challenges foster open innovation across multiple
disciplines to create communities that collaborate directly or indirectly to
address significant scientific gaps. Together, participants can solve important
problems as varied as health research, climate change, and social equity.
Ultimately, challenges can catalyze and accelerate the synthesis of complex
data into knowledge or actionable information, and should be viewed a powerful
tool to make lasting social and research contributions.
</p></li>
</ul>

<h3>Title: Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph Neural Networks. (arXiv:2312.05502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05502">http://arxiv.org/abs/2312.05502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05502]] Poisoning $\times$ Evasion: Symbiotic Adversarial Robustness for Graph Neural Networks(http://arxiv.org/abs/2312.05502)</code></li>
<li>Summary: <p>It is well-known that deep learning models are vulnerable to small input
perturbations. Such perturbed instances are called adversarial examples.
Adversarial examples are commonly crafted to fool a model either at training
time (poisoning) or test time (evasion). In this work, we study the symbiosis
of poisoning and evasion. We show that combining both threat models can
substantially improve the devastating efficacy of adversarial attacks.
Specifically, we study the robustness of Graph Neural Networks (GNNs) under
structure perturbations and devise a memory-efficient adaptive end-to-end
attack for the novel threat model using first-order optimization.
</p></li>
</ul>

<h3>Title: Sparse Variational Student-t Processes. (arXiv:2312.05568v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05568">http://arxiv.org/abs/2312.05568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05568]] Sparse Variational Student-t Processes(http://arxiv.org/abs/2312.05568)</code></li>
<li>Summary: <p>The theory of Bayesian learning incorporates the use of Student-t Processes
to model heavy-tailed distributions and datasets with outliers. However,
despite Student-t Processes having a similar computational complexity as
Gaussian Processes, there has been limited emphasis on the sparse
representation of this model. This is mainly due to the increased difficulty in
modeling and computation compared to previous sparse Gaussian Processes. Our
motivation is to address the need for a sparse representation framework that
reduces computational complexity, allowing Student-t Processes to be more
flexible for real-world datasets. To achieve this, we leverage the conditional
distribution of Student-t Processes to introduce sparse inducing points.
Bayesian methods and variational inference are then utilized to derive a
well-defined lower bound, facilitating more efficient optimization of our model
through stochastic gradient descent. We propose two methods for computing the
variational lower bound, one utilizing Monte Carlo sampling and the other
employing Jensen's inequality to compute the KL regularization term in the loss
function. We propose adopting these approaches as viable alternatives to
Gaussian processes when the data might contain outliers or exhibit heavy-tailed
behavior, and we provide specific recommendations for their applicability. We
evaluate the two proposed approaches on various synthetic and real-world
datasets from UCI and Kaggle, demonstrating their effectiveness compared to
baseline methods in terms of computational complexity and accuracy, as well as
their robustness to outliers.
</p></li>
</ul>

<h3>Title: Deeper Understanding of Black-box Predictions via Generalized Influence Functions. (arXiv:2312.05586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05586">http://arxiv.org/abs/2312.05586</a></li>
<li>Code URL: https://github.com/hslyu/gif</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05586]] Deeper Understanding of Black-box Predictions via Generalized Influence Functions(http://arxiv.org/abs/2312.05586)</code></li>
<li>Summary: <p>Influence functions (IFs) elucidate how learning data affects model behavior.
However, growing non-convexity and the number of parameters in modern
large-scale models lead to imprecise influence approximation and instability in
computations. We highly suspect that the first-order approximation in large
models causes such fragility, as IFs change all parameters including possibly
nuisance parameters that are irrelevant to the examined data. Thus, we attempt
to selectively analyze parameters associated with the data. However, simply
computing influence from the chosen parameters can be misleading, as it fails
to nullify the subliminal impact of unselected parameters. Our approach
introduces generalized IFs, precisely estimating target parameters' influence
while considering fixed parameters' effects. Unlike the classic IFs, we newly
adopt a method to identify pertinent target parameters closely associated with
the analyzed data. Furthermore, we tackle computational instability with a
robust inverse-Hessian-vector product approximation. Remarkably, the proposed
approximation algorithm guarantees convergence regardless of the network
configurations. We evaluated our approach on ResNet-18 and VGG-11 for class
removal and backdoor model recovery. Modifying just 10\% of the network yields
results comparable to the network retrained from scratch. Aligned with our
first guess, we also confirm that modifying an excessive number of parameters
results in a decline in network utility. We believe our proposal can become a
versatile tool for model analysis across various AI domains, appealing to both
specialists and general readers. Codes are available at
https://github.com/hslyu/GIF.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects. (arXiv:2312.05278v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05278">http://arxiv.org/abs/2312.05278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05278]] Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects(http://arxiv.org/abs/2312.05278)</code></li>
<li>Summary: <p>Large Vision Language Models (LVLMs) have demonstrated impressive zero-shot
capabilities in various vision-language dialogue scenarios. However, the
absence of fine-grained visual object detection hinders the model from
understanding the details of images, leading to irreparable visual
hallucinations and factual errors. In this paper, we propose Lyrics, a novel
multi-modal pre-training and instruction fine-tuning paradigm that bootstraps
vision-language alignment from fine-grained cross-modal collaboration. Building
on the foundation of BLIP-2, Lyrics infuses local visual features extracted
from a visual refiner that includes image tagging, object detection and
semantic segmentation modules into the Querying Transformer, while on the text
side, the language inputs equip the boundary boxes and tags derived from the
visual refiner. We further introduce a two-stage training scheme, in which the
pre-training stage bridges the modality gap through explicit and comprehensive
vision-language alignment targets. During the instruction fine-tuning stage, we
introduce semantic-aware visual feature extraction, a crucial method that
enables the model to extract informative features from concrete visual objects.
Our approach achieves strong performance on 13 held-out datasets across various
vision-language tasks, and demonstrates promising multi-modal understanding and
detailed depiction capabilities in real dialogue scenarios.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Active Learning Guided Federated Online Adaptation: Applications in Medical Image Segmentation. (arXiv:2312.05407v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05407">http://arxiv.org/abs/2312.05407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05407]] Active Learning Guided Federated Online Adaptation: Applications in Medical Image Segmentation(http://arxiv.org/abs/2312.05407)</code></li>
<li>Summary: <p>Data privacy, storage, and distribution shifts are major bottlenecks in
medical image analysis. Data cannot be shared across patients, physicians, and
facilities due to privacy concerns, usually requiring each patient's data to be
analyzed in a discreet setting at a near real-time pace. However, one would
like to take advantage of the accumulated knowledge across healthcare
facilities as the computational systems analyze data of more and more patients
while incorporating feedback provided by physicians to improve accuracy.
Motivated by these, we propose a method for medical image segmentation that
adapts to each incoming data batch (online adaptation), incorporates physician
feedback through active learning, and assimilates knowledge across facilities
in a federated setup. Combining an online adaptation scheme at test time with
an efficient sampling strategy with budgeted annotation helps bridge the gap
between the source and the incoming stream of target domain data. A federated
setup allows collaborative aggregation of knowledge across distinct distributed
models without needing to share the data across different models. This
facilitates the improvement of performance over time by accumulating knowledge
across users. Towards achieving these goals, we propose a computationally
amicable, privacy-preserving image segmentation technique \textbf{DrFRODA} that
uses federated learning to adapt the model in an online manner with feedback
from doctors in the loop. Our experiments on publicly available datasets show
that the proposed distributed active learning-based online adaptation method
outperforms unsupervised online adaptation methods and shows competitive
results with offline active learning-based adaptation methods.
</p></li>
</ul>

<h3>Title: FLoW3 -- Web3 Empowered Federated Learning. (arXiv:2312.05459v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05459">http://arxiv.org/abs/2312.05459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05459]] FLoW3 -- Web3 Empowered Federated Learning(http://arxiv.org/abs/2312.05459)</code></li>
<li>Summary: <p>Federated Learning is susceptible to various kinds of attacks like Data
Poisoning, Model Poisoning and Man in the Middle attack. We perceive Federated
Learning as a hierarchical structure, a federation of nodes with validators as
the head. The process of validation is done through consensus by employing
Novelty Detection and Snowball protocol, to identify valuable and relevant
updates while filtering out potentially malicious or irrelevant updates, thus
preventing Model Poisoning attacks. The opinion of the validators is recorded
in blockchain and trust score is calculated. In case of lack of consensus,
trust score is used to determine the impact of validators on the global model.
A hyperparameter is introduced to guide the model generation process, either to
rely on consensus or on trust score. This approach ensures transparency and
reliability in the aggregation process and allows the global model to benefit
from insights of most trusted nodes. In the training phase, the combination of
IPFS , PGP encryption provides : a) secure and decentralized storage b)
mitigates single point of failure making this system reliable and c) resilient
against man in the middle attack. The system is realized by implementing in
python and Foundry for smart contract development. Global Model is tested
against data poisoning by flipping the labels and by introducing malicious
nodes. Results found to be similar to that of Flower.
</p></li>
</ul>

<h3>Title: Federated Causality Learning with Explainable Adaptive Optimization. (arXiv:2312.05540v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05540">http://arxiv.org/abs/2312.05540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05540]] Federated Causality Learning with Explainable Adaptive Optimization(http://arxiv.org/abs/2312.05540)</code></li>
<li>Summary: <p>Discovering the causality from observational data is a crucial task in
various scientific domains. With increasing awareness of privacy, data are not
allowed to be exposed, and it is very hard to learn causal graphs from
dispersed data, since these data may have different distributions. In this
paper, we propose a federated causal discovery strategy (FedCausal) to learn
the unified global causal graph from decentralized heterogeneous data. We
design a global optimization formula to naturally aggregate the causal graphs
from client data and constrain the acyclicity of the global graph without
exposing local data. Unlike other federated causal learning algorithms,
FedCausal unifies the local and global optimizations into a complete directed
acyclic graph (DAG) learning process with a flexible optimization objective. We
prove that this optimization objective has a high interpretability and can
adaptively handle homogeneous and heterogeneous data. Experimental results on
synthetic and real datasets show that FedCausal can effectively deal with
non-independently and identically distributed (non-iid) data and has a superior
performance.
</p></li>
</ul>

<h3>Title: Multi-dimensional Fair Federated Learning. (arXiv:2312.05551v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05551">http://arxiv.org/abs/2312.05551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05551]] Multi-dimensional Fair Federated Learning(http://arxiv.org/abs/2312.05551)</code></li>
<li>Summary: <p>Federated learning (FL) has emerged as a promising collaborative and secure
paradigm for training a model from decentralized data without compromising
privacy. Group fairness and client fairness are two dimensions of fairness that
are important for FL. Standard FL can result in disproportionate disadvantages
for certain clients, and it still faces the challenge of treating different
groups equitably in a population. The problem of privately training fair FL
models without compromising the generalization capability of disadvantaged
clients remains open. In this paper, we propose a method, called mFairFL, to
address this problem and achieve group fairness and client fairness
simultaneously. mFairFL leverages differential multipliers to construct an
optimization objective for empirical risk minimization with fairness
constraints. Before aggregating locally trained models, it first detects
conflicts among their gradients, and then iteratively curates the direction and
magnitude of gradients to mitigate these conflicts. Theoretical analysis proves
mFairFL facilitates the fairness in model development. The experimental
evaluations based on three benchmark datasets show significant advantages of
mFairFL compared to seven state-of-the-art baselines.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole Slide Image Classification. (arXiv:2312.05490v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05490">http://arxiv.org/abs/2312.05490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05490]] Shapley Values-enabled Progressive Pseudo Bag Augmentation for Whole Slide Image Classification(http://arxiv.org/abs/2312.05490)</code></li>
<li>Summary: <p>In computational pathology, whole slide image (WSI) classification presents a
formidable challenge due to its gigapixel resolution and limited fine-grained
annotations. Multiple instance learning (MIL) offers a weakly supervised
solution, yet refining instance-level information from bag-level labels remains
complex. While most of the conventional MIL methods use attention scores to
estimate instance importance scores (IIS) which contribute to the prediction of
the slide labels, these often lead to skewed attention distributions and
inaccuracies in identifying crucial instances. To address these issues, we
propose a new approach inspired by cooperative game theory: employing Shapley
values to assess each instance's contribution, thereby improving IIS
estimation. The computation of the Shapley value is then accelerated using
attention, meanwhile retaining the enhanced instance identification and
prioritization. We further introduce a framework for the progressive assignment
of pseudo bags based on estimated IIS, encouraging more balanced attention
distributions in MIL models. Our extensive experiments on CAMELYON-16, BRACS,
and TCGA-LUNG datasets show our method's superiority over existing
state-of-the-art approaches, offering enhanced interpretability and class-wise
insights. We will release the code upon acceptance.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Factorized Explainer for Graph Neural Networks. (arXiv:2312.05596v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05596">http://arxiv.org/abs/2312.05596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05596]] Factorized Explainer for Graph Neural Networks(http://arxiv.org/abs/2312.05596)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have received increasing attention due to their
ability to learn from graph-structured data. To open the black-box of these
deep learning models, post-hoc instance-level explanation methods have been
proposed to understand GNN predictions. These methods seek to discover
substructures that explain the prediction behavior of a trained GNN. In this
paper, we show analytically that for a large class of explanation tasks,
conventional approaches, which are based on the principle of graph information
bottleneck (GIB), admit trivial solutions that do not align with the notion of
explainability. Instead, we argue that a modified GIB principle may be used to
avoid the aforementioned trivial solutions. We further introduce a novel
factorized explanation model with theoretical performance guarantees. The
modified GIB is used to analyze the structural properties of the proposed
factorized explainer. We conduct extensive experiments on both synthetic and
real-world datasets to validate the effectiveness of our proposed factorized
explainer over existing approaches.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Target to Source: Guidance-Based Diffusion Model for Test-Time Adaptation. (arXiv:2312.05274v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05274">http://arxiv.org/abs/2312.05274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05274]] Target to Source: Guidance-Based Diffusion Model for Test-Time Adaptation(http://arxiv.org/abs/2312.05274)</code></li>
<li>Summary: <p>Most recent works of test-time adaptation (TTA) aim to alleviate domain shift
problems by re-training source classifiers in each domain. On the other hand,
the emergence of the diffusion model provides another solution to TTA, which
directly maps the test data from the target domain to the source domain based
on a diffusion model pre-trained in the source domain. The source classifier
does not need to be fine-tuned. However, 1) the semantic information loss from
test data to the source domain and 2) the model shift between the source
classifier and diffusion model would prevent the diffusion model from mapping
the test data back to the source domain correctly. In this paper, we propose a
novel guidance-based diffusion-driven adaptation (GDDA) to overcome the data
shift and let the diffusion model find a better way to go back to the source.
Concretely, we first propose detail and global guidance to better keep the
common semantics of the test and source data. The two guidance include a
contrastive loss and mean squared error to alleviate the information loss by
fully exploring the diffusion model and the test data. Meanwhile, we propose a
classifier-aware guidance to reduce the bias caused by the model shift, which
can incorporate the source classifier's information into the generation process
of the diffusion model. Extensive experiments on three image datasets with
three classifier backbones demonstrate that GDDA significantly performs better
than the state-of-the-art baselines. On CIFAR-10C, CIFAR-100C, and ImageNetC,
GDDA achieves 11.54\%, 19.05\%, and 11.63\% average accuracy improvements,
respectively. GDDA even achieves equal performance compared with methods of
re-training classifiers. The code is available in the supplementary material.
</p></li>
</ul>

<h3>Title: MotionCrafter: One-Shot Motion Customization of Diffusion Models. (arXiv:2312.05288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05288">http://arxiv.org/abs/2312.05288</a></li>
<li>Code URL: https://github.com/zyxelsa/motioncrafter</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05288]] MotionCrafter: One-Shot Motion Customization of Diffusion Models(http://arxiv.org/abs/2312.05288)</code></li>
<li>Summary: <p>The essence of a video lies in its dynamic motions, including character
actions, object movements, and camera movements. While text-to-video generative
diffusion models have recently advanced in creating diverse contents,
controlling specific motions through text prompts remains a significant
challenge. A primary issue is the coupling of appearance and motion, often
leading to overfitting on appearance. To tackle this challenge, we introduce
MotionCrafter, a novel one-shot instance-guided motion customization method.
MotionCrafter employs a parallel spatial-temporal architecture that injects the
reference motion into the temporal component of the base model, while the
spatial module is independently adjusted for character or style control. To
enhance the disentanglement of motion and appearance, we propose an innovative
dual-branch motion disentanglement approach, comprising a motion
disentanglement loss and an appearance prior enhancement strategy. During
training, a frozen base model provides appearance normalization, effectively
separating appearance from motion and thereby preserving diversity.
Comprehensive quantitative and qualitative experiments, along with user
preference tests, demonstrate that MotionCrafter can successfully integrate
dynamic motions while preserving the coherence and quality of the base model
with a wide range of appearance generation capabilities. Codes are available at
https://github.com/zyxElsa/MotionCrafter.
</p></li>
</ul>

<h3>Title: NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models. (arXiv:2312.05390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05390">http://arxiv.org/abs/2312.05390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05390]] NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models(http://arxiv.org/abs/2312.05390)</code></li>
<li>Summary: <p>Generative models have been very popular in the recent years for their image
generation capabilities. GAN-based models are highly regarded for their
disentangled latent space, which is a key feature contributing to their success
in controlled image editing. On the other hand, diffusion models have emerged
as powerful tools for generating high-quality images. However, the latent space
of diffusion models is not as thoroughly explored or understood. Existing
methods that aim to explore the latent space of diffusion models usually relies
on text prompts to pinpoint specific semantics. However, this approach may be
restrictive in areas such as art, fashion, or specialized fields like medicine,
where suitable text prompts might not be available or easy to conceive thus
limiting the scope of existing work. In this paper, we propose an unsupervised
method to discover latent semantics in text-to-image diffusion models without
relying on text prompts. Our method takes a small set of unlabeled images from
specific domains, such as faces or cats, and a pre-trained diffusion model, and
discovers diverse semantics in unsupervised fashion using a contrastive
learning objective. Moreover, the learned directions can be applied
simultaneously, either within the same domain (such as various types of facial
edits) or across different domains (such as applying cat and face edits within
the same image) without interfering with each other. Our extensive experiments
show that our method achieves highly disentangled edits, outperforming existing
approaches in both diffusion-based and GAN-based latent space editing methods.
</p></li>
</ul>

<h3>Title: CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling. (arXiv:2312.05412v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05412">http://arxiv.org/abs/2312.05412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05412]] CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional Modeling(http://arxiv.org/abs/2312.05412)</code></li>
<li>Summary: <p>We introduce a multi-modal diffusion model tailored for the bi-directional
conditional generation of video and audio. Recognizing the importance of
accurate alignment between video and audio events in multi-modal generation
tasks, we propose a joint contrastive training loss to enhance the
synchronization between visual and auditory occurrences. Our research
methodology involves conducting comprehensive experiments on multiple datasets
to thoroughly evaluate the efficacy of our proposed model. The assessment of
generation quality and alignment performance is carried out from various
angles, encompassing both objective and subjective metrics. Our findings
demonstrate that the proposed model outperforms the baseline, substantiating
its effectiveness and efficiency. Notably, the incorporation of the contrastive
loss results in improvements in audio-visual alignment, particularly in the
high-correlation video-to-audio generation task. These results indicate the
potential of our proposed model as a robust solution for improving the quality
and alignment of multi-modal generation, thereby contributing to the
advancement of video and audio conditional generation systems.
</p></li>
</ul>

<h3>Title: Efficient Quantization Strategies for Latent Diffusion Models. (arXiv:2312.05431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05431">http://arxiv.org/abs/2312.05431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05431]] Efficient Quantization Strategies for Latent Diffusion Models(http://arxiv.org/abs/2312.05431)</code></li>
<li>Summary: <p>Latent Diffusion Models (LDMs) capture the dynamic evolution of latent
variables over time, blending patterns and multimodality in a generative
system. Despite the proficiency of LDM in various applications, such as
text-to-image generation, facilitated by robust text encoders and a variational
autoencoder, the critical need to deploy large generative models on edge
devices compels a search for more compact yet effective alternatives. Post
Training Quantization (PTQ), a method to compress the operational size of deep
learning models, encounters challenges when applied to LDM due to temporal and
structural complexities. This study proposes a quantization strategy that
efficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR)
as a pivotal metric for evaluation. By treating the quantization discrepancy as
relative noise and identifying sensitive part(s) of a model, we propose an
efficient quantization approach encompassing both global and local strategies.
The global quantization process mitigates relative quantization noise by
initiating higher-precision quantization on sensitive blocks, while local
treatments address specific challenges in quantization-sensitive and
time-sensitive modules. The outcomes of our experiments reveal that the
implementation of both global and local treatments yields a highly efficient
and effective Post Training Quantization (PTQ) of LDMs.
</p></li>
</ul>

<h3>Title: Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation. (arXiv:2312.05464v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05464">http://arxiv.org/abs/2312.05464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05464]] Identifying and Mitigating Model Failures through Few-shot CLIP-aided Diffusion Generation(http://arxiv.org/abs/2312.05464)</code></li>
<li>Summary: <p>Deep learning models can encounter unexpected failures, especially when
dealing with challenging sub-populations. One common reason for these failures
is the occurrence of objects in backgrounds that are rarely seen during
training. To gain a better understanding of these failure modes,
human-interpretable descriptions are crucial for further analysis and
improvement which is expensive. In this study, we propose an end-to-end
framework that utilizes the capabilities of large language models (ChatGPT) and
vision-language deep models (CLIP) to generate text descriptions of failure
modes associated with spurious correlations (e.g. rarely seen backgrounds)
without human-in-the-loop intervention. These descriptions can be used to
generate synthetic data using generative models, such as diffusion models. The
model can now use this generated data to learn from its weaknesses and enhance
its performance on backgrounds that are uncommon for each class of data. Our
approach serves as a broad solution, promising progress in comprehending model
failure modes and strengthening deep learning models across a wide range of
failure scenarios (e.g. bacckgrounds, colors) automatically in a few-shot
manner. Our experiments have shown remarkable \textbf{improvements in accuracy
($\sim \textbf{21%}$)} on hard sub-populations (particularly for wrong
background association) across $40$ different models, such as ResNets,
EfficientNets, DenseNets, Vision Transformer (ViT), SwAVs, MoCos, DINOs, and
CLIPs on various datasets such as ImageNet-1000, CIFAR-10, and CIFAR-100.
</p></li>
</ul>

<h3>Title: BARET : Balanced Attention based Real image Editing driven by Target-text Inversion. (arXiv:2312.05482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05482">http://arxiv.org/abs/2312.05482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05482]] BARET : Balanced Attention based Real image Editing driven by Target-text Inversion(http://arxiv.org/abs/2312.05482)</code></li>
<li>Summary: <p>Image editing approaches with diffusion models have been rapidly developed,
yet their applicability are subject to requirements such as specific editing
types (e.g., foreground or background object editing, style transfer), multiple
conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of
diffusion models. For alleviating these limitations and realizing efficient
real image editing, we propose a novel editing technique that only requires an
input image and target text for various editing types including non-rigid edits
without fine-tuning diffusion model. Our method contains three novelties:(I)
Target-text Inversion Schedule (TTIS) is designed to fine-tune the input target
text embedding to achieve fast image reconstruction without image caption and
acceleration of convergence.(II) Progressive Transition Scheme applies
progressive linear interpolation between target text embedding and its
fine-tuned version to generate transition embedding for maintaining non-rigid
editing capability.(III) Balanced Attention Module (BAM) balances the tradeoff
between textual description and image semantics.By the means of combining
self-attention map from reconstruction process and cross-attention map from
transition process, the guidance of target text embeddings in diffusion process
is optimized.In order to demonstrate editing capability, effectiveness and
efficiency of the proposed BARET, we have conducted extensive qualitative and
quantitative experiments. Moreover, results derived from user study and
ablation study further prove the superiority over other methods.
</p></li>
</ul>

<h3>Title: Cross Domain Generative Augmentation: Domain Generalization with Latent Diffusion Models. (arXiv:2312.05387v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05387">http://arxiv.org/abs/2312.05387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05387]] Cross Domain Generative Augmentation: Domain Generalization with Latent Diffusion Models(http://arxiv.org/abs/2312.05387)</code></li>
<li>Summary: <p>Despite the huge effort in developing novel regularizers for Domain
Generalization (DG), adding simple data augmentation to the vanilla ERM which
is a practical implementation of the Vicinal Risk Minimization principle (VRM)
\citep{chapelle2000vicinal} outperforms or stays competitive with many of the
proposed regularizers. The VRM reduces the estimation error in ERM by replacing
the point-wise kernel estimates with a more precise estimation of true data
distribution that reduces the gap between data points \textbf{within each
domain}. However, in the DG setting, the estimation error of true data
distribution by ERM is mainly caused by the distribution shift \textbf{between
domains} which cannot be fully addressed by simple data augmentation techniques
within each domain. Inspired by this limitation of VRM, we propose a novel data
augmentation named Cross Domain Generative Augmentation (CDGA) that replaces
the pointwise kernel estimates in ERM with new density estimates in the
\textbf{vicinity of domain pairs} so that the gap between domains is further
reduced. To this end, CDGA, which is built upon latent diffusion models (LDM),
generates synthetic images to fill the gap between all domains and as a result,
reduces the non-iidness. We show that CDGA outperforms SOTA DG methods under
the Domainbed benchmark. To explain the effectiveness of CDGA, we generate more
than 5 Million synthetic images and perform extensive ablation studies
including data scaling laws, distribution visualization, domain shift
quantification, adversarial robustness, and loss landscape analysis.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h3>Title: Better Neural PDE Solvers Through Data-Free Mesh Movers. (arXiv:2312.05583v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05583">http://arxiv.org/abs/2312.05583</a></li>
<li>Code URL: https://github.com/mm-pde/mm-pde</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05583]] Better Neural PDE Solvers Through Data-Free Mesh Movers(http://arxiv.org/abs/2312.05583)</code></li>
<li>Summary: <p>Recently, neural networks have been extensively employed to solve partial
differential equations (PDEs) in physical system modeling. While major studies
focus on learning system evolution on predefined static mesh discretizations,
some methods utilize reinforcement learning or supervised learning techniques
to create adaptive and dynamic meshes, due to the dynamic nature of these
systems. However, these approaches face two primary challenges: (1) the need
for expensive optimal mesh data, and (2) the change of the solution space's
degree of freedom and topology during mesh refinement. To address these
challenges, this paper proposes a neural PDE solver with a neural mesh adapter.
To begin with, we introduce a novel data-free neural mesh adaptor, called
Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an
operator that maps the solution to adaptive meshes and is trained using the
Monge-Ampere equation without optimal mesh data. Secondly, it dynamically
changes the mesh by moving existing nodes rather than adding or deleting nodes
and edges. Theoretical analysis shows that meshes generated by DMM have the
lowest interpolation error bound. Based on DMM, to efficiently and accurately
model dynamic systems, we develop a moving mesh based neural PDE solver
(MM-PDE) that embeds the moving mesh with a two-branch architecture and a
learnable interpolation framework to preserve information within the data.
Empirical experiments demonstrate that our method generates suitable meshes and
considerably enhances accuracy when modeling widely considered PDE systems.
</p></li>
</ul>

<h2>transformer</h2>
<h3>Title: PixLore: A Dataset-driven Approach to Rich Image Captioning. (arXiv:2312.05349v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05349">http://arxiv.org/abs/2312.05349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05349]] PixLore: A Dataset-driven Approach to Rich Image Captioning(http://arxiv.org/abs/2312.05349)</code></li>
<li>Summary: <p>In the domain of vision-language integration, generating detailed image
captions poses a significant challenge due to the lack of a curated and rich
dataset. This study introduces PixLore, a novel method that leverages Querying
Transformers through the fine-tuning of the BLIP-2 model using the LoRa method
on a standard commercial GPU. Our approach, which involves training on a
carefully assembled dataset from state-of-the-art Computer Vision models
combined and augmented by ChatGPT, addresses the question of whether intricate
image understanding can be achieved with an ensemble of smaller-scale models.
Comparative evaluations against major models such as GPT-4 and Google Bard
demonstrate that PixLore-2.7B, despite having considerably fewer parameters, is
rated higher than the existing State-of-the-Art models in over half of the
assessments. This research not only presents a groundbreaking approach but also
highlights the importance of well-curated datasets in enhancing the performance
of smaller models.
</p></li>
</ul>

<h3>Title: From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos. (arXiv:2312.05447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05447">http://arxiv.org/abs/2312.05447</a></li>
<li>Code URL: https://github.com/FER-LMC/S2D</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05447]] From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos(http://arxiv.org/abs/2312.05447)</code></li>
<li>Summary: <p>Dynamic facial expression recognition (DFER) in the wild is still hindered by
data limitations, e.g., insufficient quantity and diversity of pose, occlusion
and illumination, as well as the inherent ambiguity of facial expressions. In
contrast, static facial expression recognition (SFER) currently shows much
higher performance and can benefit from more abundant high-quality training
data. Moreover, the appearance features and dynamic dependencies of DFER remain
largely unexplored. To tackle these challenges, we introduce a novel
Static-to-Dynamic model (S2D) that leverages existing SFER knowledge and
dynamic information implicitly encoded in extracted facial landmark-aware
features, thereby significantly improving DFER performance. Firstly, we build
and train an image model for SFER, which incorporates a standard Vision
Transformer (ViT) and Multi-View Complementary Prompters (MCPs) only. Then, we
obtain our video model (i.e., S2D), for DFER, by inserting Temporal-Modeling
Adapters (TMAs) into the image model. MCPs enhance facial expression features
with landmark-aware features inferred by an off-the-shelf facial landmark
detector. And the TMAs capture and model the relationships of dynamic changes
in facial expressions, effectively extending the pre-trained image model for
videos. Notably, MCPs and TMAs only increase a fraction of trainable parameters
(less than +10\%) to the original image model. Moreover, we present a novel
Emotion-Anchors (i.e., reference samples for each emotion category) based
Self-Distillation Loss to reduce the detrimental influence of ambiguous emotion
labels, further enhancing our S2D. Experiments conducted on popular SFER and
DFER datasets show that we achieve the state of the art.
</p></li>
</ul>

<h3>Title: Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal. (arXiv:2312.05454v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05454">http://arxiv.org/abs/2312.05454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05454]] Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal(http://arxiv.org/abs/2312.05454)</code></li>
<li>Summary: <p>Open-World Recognition (OWR) is an emerging field that makes a machine
learning model competent in rejecting the unknowns, managing them, and
incrementally adding novel samples to the base knowledge. However, this broad
objective is not practical for an agent that works on a specific task. Not all
rejected samples will be used for learning continually in the future. Some
novel images in the open environment may not belong to the domain of interest.
Hence, identifying the unknown in the domain of interest is essential for a
machine learning model to learn merely the important samples. In this study, we
propose an evaluation protocol for estimating a model's capability in
separating unknown in-domain (ID) and unknown out-of-domain (OOD). We evaluated
using three approaches with an unknown domain and demonstrated the possibility
of identifying the domain of interest using the pre-trained parameters through
traditional transfer learning, Automated Machine Learning (AutoML), and Nearest
Class Mean (NCM) classifier with First Integer Neighbor Clustering Hierarchy
(FINCH). We experimented with five different domains: garbage, food, dogs,
plants, and birds. The results show that all approaches can be used as an
initial baseline yielding a good accuracy. In addition, a Balanced Accuracy
(BACCU) score from a pre-trained model indicates a tendency to excel in one or
more domains of interest. We observed that MobileNetV3 yielded the highest
BACCU score for the garbage domain and surpassed complex models such as the
transformer network. Meanwhile, our results also suggest that a strong
representation in the pre-trained model is important for identifying unknown
classes in the same domain. This study could open the bridge toward open-world
recognition in domain-specific tasks where the relevancy of the unknown classes
is vital.
</p></li>
</ul>

<h3>Title: Fine-Grained Analysis of Team Collaborative Dialogue. (arXiv:2312.05471v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05471">http://arxiv.org/abs/2312.05471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05471]] Fine-Grained Analysis of Team Collaborative Dialogue(http://arxiv.org/abs/2312.05471)</code></li>
<li>Summary: <p>Natural language analysis of human collaborative chat dialogues is an
understudied domain with many unique challenges: a large number of dialogue act
labels, underspecified and dynamic tasks, interleaved topics, and long-range
contextual dependence. While prior work has studied broad metrics of team
dialogue and associated performance using methods such as LSA, there has been
little effort in generating fine-grained descriptions of team dynamics and
individual performance from dialogue. We describe initial work towards
developing an explainable analytics tool in the software development domain
using Slack chats mined from our organization, including generation of a novel,
hierarchical labeling scheme; design of descriptive metrics based on the
frequency of occurrence of dialogue acts; and initial results using a
transformer + CRF architecture to incorporate long-range context.
</p></li>
</ul>

<h3>Title: Teamwork Dimensions Classification Using BERT. (arXiv:2312.05483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05483">http://arxiv.org/abs/2312.05483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05483]] Teamwork Dimensions Classification Using BERT(http://arxiv.org/abs/2312.05483)</code></li>
<li>Summary: <p>Teamwork is a necessary competency for students that is often inadequately
assessed. Towards providing a formative assessment of student teamwork, an
automated natural language processing approach was developed to identify
teamwork dimensions of students' online team chat. Developments in the field of
natural language processing and artificial intelligence have resulted in
advanced deep transfer learning approaches namely the Bidirectional Encoder
Representations from Transformers (BERT) model that allow for more in-depth
understanding of the context of the text. While traditional machine learning
algorithms were used in the previous work for the automatic classification of
chat messages into the different teamwork dimensions, our findings have shown
that classifiers based on the pre-trained language model BERT provides improved
classification performance, as well as much potential for generalizability in
the language use of varying team chat contexts and team member demographics.
This model will contribute towards an enhanced learning analytics tool for
teamwork assessment and feedback.
</p></li>
</ul>

<h3>Title: Exploring Sparsity in Graph Transformers. (arXiv:2312.05479v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05479">http://arxiv.org/abs/2312.05479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05479]] Exploring Sparsity in Graph Transformers(http://arxiv.org/abs/2312.05479)</code></li>
<li>Summary: <p>Graph Transformers (GTs) have achieved impressive results on various
graph-related tasks. However, the huge computational cost of GTs hinders their
deployment and application, especially in resource-constrained environments.
Therefore, in this paper, we explore the feasibility of sparsifying GTs, a
significant yet under-explored topic. We first discuss the redundancy of GTs
based on the characteristics of existing GT models, and then propose a
comprehensive \textbf{G}raph \textbf{T}ransformer \textbf{SP}arsification
(GTSP) framework that helps to reduce the computational complexity of GTs from
four dimensions: the input graph data, attention heads, model layers, and model
weights. Specifically, GTSP designs differentiable masks for each individual
compressible component, enabling effective end-to-end pruning. We examine our
GTSP through extensive experiments on prominent GTs, including GraphTrans,
Graphormer, and GraphGPS. The experimental results substantiate that GTSP
effectively cuts computational costs, accompanied by only marginal decreases in
accuracy or, in some cases, even improvements. For instance, GTSP yields a
reduction of 30\% in Floating Point Operations while contributing to a 1.8\%
increase in Area Under the Curve accuracy on OGBG-HIV dataset. Furthermore, we
provide several insights on the characteristics of attention heads and the
behavior of attention mechanisms, all of which have immense potential to
inspire future research endeavors in this domain.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: 3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection. (arXiv:2312.05277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05277">http://arxiv.org/abs/2312.05277</a></li>
<li>Code URL: https://github.com/gyhandy/3d-copy-paste</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05277]] 3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection(http://arxiv.org/abs/2312.05277)</code></li>
<li>Summary: <p>A major challenge in monocular 3D object detection is the limited diversity
and quantity of objects in real datasets. While augmenting real scenes with
virtual objects holds promise to improve both the diversity and quantity of the
objects, it remains elusive due to the lack of an effective 3D object insertion
method in complex real captured scenes. In this work, we study augmenting
complex real indoor scenes with virtual objects for monocular 3D object
detection. The main challenge is to automatically identify plausible physical
properties for virtual assets (e.g., locations, appearances, sizes, etc.) in
cluttered real scenes. To address this challenge, we propose a physically
plausible indoor 3D object insertion approach to automatically copy virtual
objects and paste them into real scenes. The resulting objects in scenes have
3D bounding boxes with plausible physical locations and appearances. In
particular, our method first identifies physically feasible locations and poses
for the inserted objects to prevent collisions with the existing room layout.
Subsequently, it estimates spatially-varying illumination for the insertion
location, enabling the immersive blending of the virtual objects into the
original scene with plausible appearances and cast shadows. We show that our
augmentation method significantly improves existing monocular 3D object models
and achieves state-of-the-art performance. For the first time, we demonstrate
that a physically plausible 3D object insertion, serving as a generative data
augmentation technique, can lead to significant improvements for discriminative
downstream tasks such as monocular 3D object detection. Project website:
https://gyhandy.github.io/3D-Copy-Paste/
</p></li>
</ul>

<h3>Title: Multi-view Inversion for 3D-aware Generative Adversarial Networks. (arXiv:2312.05330v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05330">http://arxiv.org/abs/2312.05330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05330]] Multi-view Inversion for 3D-aware Generative Adversarial Networks(http://arxiv.org/abs/2312.05330)</code></li>
<li>Summary: <p>Current 3D GAN inversion methods for human heads typically use only one
single frontal image to reconstruct the whole 3D head model. This leaves out
meaningful information when multi-view data or dynamic videos are available.
Our method builds on existing state-of-the-art 3D GAN inversion techniques to
allow for consistent and simultaneous inversion of multiple views of the same
subject. We employ a multi-latent extension to handle inconsistencies present
in dynamic face videos to re-synthesize consistent 3D representations from the
sequence. As our method uses additional information about the target subject,
we observe significant enhancements in both geometric accuracy and image
quality, particularly when rendering from wide viewing angles. Moreover, we
demonstrate the editability of our inverted 3D renderings, which distinguishes
them from NeRF-based scene reconstructions.
</p></li>
</ul>

<h3>Title: Using Captum to Explain Generative Language Models. (arXiv:2312.05491v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05491">http://arxiv.org/abs/2312.05491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05491]] Using Captum to Explain Generative Language Models(http://arxiv.org/abs/2312.05491)</code></li>
<li>Summary: <p>Captum is a comprehensive library for model explainability in PyTorch,
offering a range of methods from the interpretability literature to enhance
users' understanding of PyTorch models. In this paper, we introduce new
features in Captum that are specifically designed to analyze the behavior of
generative language models. We provide an overview of the available
functionalities and example applications of their potential for understanding
learned associations within generative language models.
</p></li>
</ul>

<h3>Title: Consistency Models for Scalable and Fast Simulation-Based Inference. (arXiv:2312.05440v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05440">http://arxiv.org/abs/2312.05440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05440]] Consistency Models for Scalable and Fast Simulation-Based Inference(http://arxiv.org/abs/2312.05440)</code></li>
<li>Summary: <p>Simulation-based inference (SBI) is constantly in search of more expressive
algorithms for accurately inferring the parameters of complex models from noisy
data. We present consistency models for neural posterior estimation (CMPE), a
new free-form conditional sampler for scalable, fast, and amortized SBI with
generative neural networks. CMPE combines the advantages of normalizing flows
and flow matching methods into a single generative architecture: It essentially
distills a continuous probability flow and enables rapid few-shot inference
with an unconstrained architecture that can be tailored to the structure of the
estimation problem. Our empirical evaluation demonstrates that CMPE not only
outperforms current state-of-the-art algorithms on three hard low-dimensional
problems, but also achieves competitive performance in a high-dimensional
Bayesian denoising experiment and in estimating a computationally demanding
multi-scale model of tumor spheroid growth.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos. (arXiv:2312.05269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05269">http://arxiv.org/abs/2312.05269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05269]] LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos(http://arxiv.org/abs/2312.05269)</code></li>
<li>Summary: <p>The egocentric video natural language query (NLQ) task involves localizing a
temporal window in an egocentric video that provides an answer to a posed
query, which has wide applications in building personalized AI assistants.
Prior methods for this task have focused on improvements of network
architecture and leveraging pre-training for enhanced image and video features,
but have struggled with capturing long-range temporal dependencies in lengthy
videos, and cumbersome end-to-end training. Motivated by recent advancements in
Large Language Models (LLMs) and vision language models, we introduce
LifelongMemory, a novel framework that utilizes multiple pre-trained models to
answer queries from extensive egocentric video content. We address the unique
challenge by employing a pre-trained captioning model to create detailed
narratives of the videos. These narratives are then used to prompt a frozen LLM
to generate coarse-grained temporal window predictions, which are subsequently
refined using a pre-trained NLQ model. Empirical results demonstrate that our
method achieves competitive performance against existing supervised end-to-end
learning methods, underlining the potential of integrating multiple pre-trained
multimodal large language models in complex vision-language tasks. We provide a
comprehensive analysis of key design decisions and hyperparameters in our
pipeline, offering insights and practical guidelines.
</p></li>
</ul>

<h3>Title: GlitchBench: Can large multimodal models detect video game glitches?. (arXiv:2312.05291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05291">http://arxiv.org/abs/2312.05291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05291]] GlitchBench: Can large multimodal models detect video game glitches?(http://arxiv.org/abs/2312.05291)</code></li>
<li>Summary: <p>Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/
</p></li>
</ul>

<h3>Title: Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models. (arXiv:2312.05434v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05434">http://arxiv.org/abs/2312.05434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05434]] Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models(http://arxiv.org/abs/2312.05434)</code></li>
<li>Summary: <p>The age of social media is rife with memes. Understanding and detecting
harmful memes pose a significant challenge due to their implicit meaning that
is not explicitly conveyed through the surface text and image. However,
existing harmful meme detection approaches only recognize superficial
harm-indicative signals in an end-to-end classification manner but ignore
in-depth cognition of the meme text and image. In this paper, we attempt to
detect harmful memes based on advanced reasoning over the interplay of
multimodal information in memes. Inspired by the success of Large Language
Models (LLMs) on complex reasoning, we first conduct abductive reasoning with
LLMs. Then we propose a novel generative framework to learn reasonable thoughts
from LLMs for better multimodal fusion and lightweight fine-tuning, which
consists of two training stages: 1) Distill multimodal reasoning knowledge from
LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive
experiments conducted on three meme datasets demonstrate that our proposed
approach achieves superior performance than state-of-the-art methods on the
harmful meme detection task.
</p></li>
</ul>

<h3>Title: History Matters: Temporal Knowledge Editing in Large Language Model. (arXiv:2312.05497v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05497">http://arxiv.org/abs/2312.05497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05497]] History Matters: Temporal Knowledge Editing in Large Language Model(http://arxiv.org/abs/2312.05497)</code></li>
<li>Summary: <p>The imperative task of revising or updating the knowledge stored within large
language models arises from two distinct sources: intrinsic errors inherent in
the model which should be corrected and outdated knowledge due to external
shifts in the real world which should be updated. Prevailing efforts in model
editing conflate these two distinct categories of edits arising from distinct
reasons and directly modify the original knowledge in models into new
knowledge. However, we argue that preserving the model's original knowledge
remains pertinent. Specifically, if a model's knowledge becomes outdated due to
evolving worldly dynamics, it should retain recollection of the historical
knowledge while integrating the newfound knowledge. In this work, we introduce
the task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe
(Assessment of TempOral Knowledge Editing) to evaluate current model editing
methods. We find that while existing model editing methods are effective at
making models remember new knowledge, the edited model catastrophically forgets
historical knowledge. To address this gap, we propose a simple and general
framework termed Multi-Editing with Time Objective (METO) for enhancing
existing editing models, which edits both historical and new knowledge
concurrently and optimizes the model's prediction for the time of each fact.
Our assessments demonstrate that while AToKe is still difficult, METO maintains
the effectiveness of learning new knowledge and meanwhile substantially
improves the performance of edited models on utilizing historical knowledge.
</p></li>
</ul>

<h3>Title: Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models. (arXiv:2312.05503v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05503">http://arxiv.org/abs/2312.05503</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05503]] Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models(http://arxiv.org/abs/2312.05503)</code></li>
<li>Summary: <p>We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method
for aligning multi-billion-parameter-sized Large Language Models (LLMs).
Aligner employs a unique design that constructs a globally shared set of
tunable tokens that modify the attention of every layer. Remarkably with this
method, even when using one token accounting for a mere 5,000 parameters,
Aligner can still perform comparably well to state-of-the-art LLM adaptation
methods like LoRA that require millions of parameters. This capacity is
substantiated in both instruction following and value alignment tasks. Besides
the multiple order-of-magnitude improvement in parameter efficiency, the
insight Aligner provides into the internal mechanisms of LLMs is also valuable.
The architectural features and efficacy of our method, in addition to our
experiments demonstrate that an LLM separates its internal handling of "form"
and "knowledge" in a somewhat orthogonal manner. This finding promises to
motivate new research into LLM mechanism understanding and value alignment.
</p></li>
</ul>

<h3>Title: Enhancing Medical Specialty Assignment to Patients using NLP Techniques. (arXiv:2312.05585v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05585">http://arxiv.org/abs/2312.05585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05585]] Enhancing Medical Specialty Assignment to Patients using NLP Techniques(http://arxiv.org/abs/2312.05585)</code></li>
<li>Summary: <p>The introduction of Large Language Models (LLMs), and the vast volume of
publicly available medical data, amplified the application of NLP to the
medical domain. However, LLMs are pretrained on data that are not explicitly
relevant to the domain that are applied to and are often biased towards the
original data they were pretrained upon. Even when pretrained on domainspecific
data, these models typically require time-consuming fine-tuning to achieve good
performance for a specific task. To address these limitations, we propose an
alternative approach that achieves superior performance while being
computationally efficient. Specifically, we utilize keywords to train a deep
learning architecture that outperforms a language model pretrained on a large
corpus of text. Our proposal does not require pretraining nor fine-tuning and
can be applied directly to a specific setting for performing multi-label
classification. Our objective is to automatically assign a new patient to the
specialty of the medical professional they require, using a dataset that
contains medical transcriptions and relevant keywords. To this end, we
fine-tune the PubMedBERT model on this dataset, which serves as the baseline
for our experiments. We then twice train/fine-tune a DNN and the RoBERTa
language model, using both the keywords and the full transcriptions as input.
We compare the performance of these approaches using relevant metrics. Our
results demonstrate that utilizing keywords for text classification
significantly improves classification performance, for both a basic DL
architecture and a large language model. Our approach represents a promising
and efficient alternative to traditional methods for finetuning language models
on domain-specific data and has potential applications in various medical
domains
</p></li>
</ul>

<h3>Title: Stateful Large Language Model Serving with Pensieve. (arXiv:2312.05516v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05516">http://arxiv.org/abs/2312.05516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05516]] Stateful Large Language Model Serving with Pensieve(http://arxiv.org/abs/2312.05516)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have recently experienced great success, as
evident in the widespread popularity of ChatGPT. Existing LLM serving systems
are stateless across requests. Consequently, when LLMs are used in the common
setting of multi-turn conversations, a growing log of the conversation history
must be processed alongside any request by the serving system at each turn,
resulting in repeated history processing. In this paper, we design $Pensieve$,
a system optimized for multi-turn conversation LLM serving. $Pensieve$
maintains the conversation state across requests by caching previously
processed history to avoid duplicate processing. $Pensieve$'s multi-tier
caching strategy can utilize both GPU and CPU memory to efficiently store and
retrieve cached data. $Pensieve$ also generalizes the recent PagedAttention
kernel to support attention between multiple input tokens with a GPU cache
spread over non-contiguous memory. Our evaluation shows that $Pensieve$ is able
to achieve 1.51-1.95x throughput compared to vLLM and reduce latency by 60-75%.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Loss Functions in the Era of Semantic Segmentation: A Survey and Outlook. (arXiv:2312.05391v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05391">http://arxiv.org/abs/2312.05391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05391]] Loss Functions in the Era of Semantic Segmentation: A Survey and Outlook(http://arxiv.org/abs/2312.05391)</code></li>
<li>Summary: <p>Semantic image segmentation, the process of classifying each pixel in an
image into a particular class, plays an important role in many visual
understanding systems. As the predominant criterion for evaluating the
performance of statistical models, loss functions are crucial for shaping the
development of deep learning-based segmentation algorithms and improving their
overall performance. To aid researchers in identifying the optimal loss
function for their particular application, this survey provides a comprehensive
and unified review of $25$ loss functions utilized in image segmentation. We
provide a novel taxonomy and thorough review of how these loss functions are
customized and leveraged in image segmentation, with a systematic
categorization emphasizing their significant features and applications.
Furthermore, to evaluate the efficacy of these methods in real-world scenarios,
we propose unbiased evaluations of some distinct and renowned loss functions on
established medical and natural image datasets. We conclude this review by
identifying current challenges and unveiling future research opportunities.
Finally, we have compiled the reviewed studies that have open-source
implementations on our GitHub page.
</p></li>
</ul>

<h3>Title: CSL: Class-Agnostic Structure-Constrained Learning for Segmentation Including the Unseen. (arXiv:2312.05538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05538">http://arxiv.org/abs/2312.05538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05538]] CSL: Class-Agnostic Structure-Constrained Learning for Segmentation Including the Unseen(http://arxiv.org/abs/2312.05538)</code></li>
<li>Summary: <p>Addressing Out-Of-Distribution (OOD) Segmentation and Zero-Shot Semantic
Segmentation (ZS3) is challenging, necessitating segmenting unseen classes.
Existing strategies adapt the class-agnostic Mask2Former (CA-M2F) tailored to
specific tasks. However, these methods cater to singular tasks, demand training
from scratch, and we demonstrate certain deficiencies in CA-M2F, which affect
performance. We propose the Class-Agnostic Structure-Constrained Learning
(CSL), a plug-in framework that can integrate with existing methods, thereby
embedding structural constraints and achieving performance gain, including the
unseen, specifically OOD, ZS3, and domain adaptation (DA) tasks. There are two
schemes for CSL to integrate with existing methods (1) by distilling knowledge
from a base teacher network, enforcing constraints across training and
inference phrases, or (2) by leveraging established models to obtain per-pixel
distributions without retraining, appending constraints during the inference
phase. We propose soft assignment and mask split methodologies that enhance OOD
object segmentation. Empirical evaluations demonstrate CSL's prowess in
boosting the performance of existing algorithms spanning OOD segmentation, ZS3,
and DA segmentation, consistently transcending the state-of-art across all
three tasks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
