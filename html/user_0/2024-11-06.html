<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-06</h1>
<h3>Title: NMformer: A Transformer for Noisy Modulation Classification in Wireless Communication</h3>
<ul>
<li><strong>Authors: </strong>Atik Faysal, Mohammad Rostami, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02428">https://arxiv.org/abs/2411.02428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02428">https://arxiv.org/pdf/2411.02428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02428]] NMformer: A Transformer for Noisy Modulation Classification in Wireless Communication(https://arxiv.org/abs/2411.02428)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modulation classification is a very challenging task since the signals intertwine with various ambient noises. Methods are required that can classify them without adding extra steps like denoising, which introduces computational complexity. In this study, we propose a vision transformer (ViT) based model named NMformer to predict the channel modulation images with different noise levels in wireless communication. Since ViTs are most effective for RGB images, we generated constellation diagrams from the modulated signals. The diagrams provide the information from the signals in a 2-D representation form. We trained NMformer on 106, 800 modulation images to build the base classifier and only used 3, 000 images to fine-tune for specific tasks. Our proposed model has two different kinds of prediction setups: in-distribution and out-of-distribution. Our model achieves 4.67% higher accuracy than the base classifier when finetuned and tested on high signal-to-noise ratios (SNRs) in-distribution classes. Moreover, the fine-tuned low SNR task achieves a higher accuracy than the base classifier. The fine-tuned classifier becomes much more effective than the base classifier by achieving higher accuracy when predicted, even on unseen data from out-of-distribution classes. Extensive experiments show the effectiveness of NMformer for a wide range of SNRs.</li>
</ul>

<h3>Title: IdeaBench: Benchmarking Large Language Models for Research Idea Generation</h3>
<ul>
<li><strong>Authors: </strong>Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Eric Xie, Stefan Bekiranov, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02429">https://arxiv.org/abs/2411.02429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02429">https://arxiv.org/pdf/2411.02429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02429]] IdeaBench: Benchmarking Large Language Models for Research Idea Generation(https://arxiv.org/abs/2411.02429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed how people interact with artificial intelligence (AI) systems, achieving state-of-the-art results in various tasks, including scientific discovery and hypothesis generation. However, the lack of a comprehensive and systematic evaluation framework for generating research ideas using LLMs poses a significant obstacle to understanding and assessing their generative capabilities in scientific discovery. To address this gap, we propose IdeaBench, a benchmark system that includes a comprehensive dataset and an evaluation framework for standardizing the assessment of research idea generation using LLMs. Our dataset comprises titles and abstracts from a diverse range of influential papers, along with their referenced works. To emulate the human process of generating research ideas, we profile LLMs as domain-specific researchers and ground them in the same context considered by human researchers. This maximizes the utilization of the LLMs' parametric knowledge to dynamically generate new research ideas. We also introduce an evaluation framework for assessing the quality of generated research ideas. Our evaluation framework is a two-stage process: first, using GPT-4o to rank ideas based on user-specified quality indicators such as novelty and feasibility, enabling scalable personalization; and second, calculating relative ranking based "Insight Score" to quantify the chosen quality indicator. The proposed benchmark system will be a valuable asset for the community to measure and compare different LLMs, ultimately advancing the automation of the scientific discovery process.</li>
</ul>

<h3>Title: Generative Emotion Cause Explanation in Multimodal Conversations</h3>
<ul>
<li><strong>Authors: </strong>Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02430">https://arxiv.org/abs/2411.02430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02430">https://arxiv.org/pdf/2411.02430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02430]] Generative Emotion Cause Explanation in Multimodal Conversations(https://arxiv.org/abs/2411.02430)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically uses clause selection methods to locate the reason utterance, without providing a detailed explanation of the emotional causes. In this paper, we propose a new task, \textbf{M}ultimodal \textbf{C}onversation \textbf{E}motion \textbf{C}ause \textbf{E}xplanation (MCECE), aiming to generate a detailed explanation of the emotional cause to the target utterance within a multimodal conversation scenario. Building upon the MELD dataset, we develop a new dataset (ECEM) that integrates video clips with detailed explanations of character emotions, facilitating an in-depth examination of the causal factors behind emotional expressions in multimodal conversations.A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net significantly outperforms several excellent large language model baselines. Code and dataset are available at \url{this https URL}</li>
</ul>

<h3>Title: Can LLMs make trade-offs involving stipulated pain and pleasure states?</h3>
<ul>
<li><strong>Authors: </strong>Geoff Keeling, Winnie Street, Martyna Stachaczyk, Daria Zakharova, Iulia M. Comsa, Anastasiya Sakovych, Isabella Logothesis, Zejia Zhang, Blaise Ag√ºera y Arcas, Jonathan Birch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02432">https://arxiv.org/abs/2411.02432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02432">https://arxiv.org/pdf/2411.02432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02432]] Can LLMs make trade-offs involving stipulated pain and pleasure states?(https://arxiv.org/abs/2411.02432)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pleasure and pain play an important role in human decision making by providing a common currency for resolving motivational conflicts. While Large Language Models (LLMs) can generate detailed descriptions of pleasure and pain experiences, it is an open question whether LLMs can recreate the motivational force of pleasure and pain in choice scenarios - a question which may bear on debates about LLM sentience, understood as the capacity for valenced experiential states. We probed this question using a simple game in which the stated goal is to maximise points, but where either the points-maximising option is said to incur a pain penalty or a non-points-maximising option is said to incur a pleasure reward, providing incentives to deviate from points-maximising behaviour. Varying the intensity of the pain penalties and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-4o, and GPT-4o mini each demonstrated at least one trade-off in which the majority of responses switched from points-maximisation to pain-minimisation or pleasure-maximisation after a critical threshold of stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance over points-maximisation regardless of intensity, while tending to prioritise points over pleasure regardless of intensity. We discuss the implications of these findings for debates about the possibility of LLM sentience.</li>
</ul>

<h3>Title: SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02433">https://arxiv.org/abs/2411.02433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02433">https://arxiv.org/pdf/2411.02433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02433]] SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models(https://arxiv.org/abs/2411.02433)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.</li>
</ul>

<h3>Title: Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Leng, Jason Liang, Jack Mauro, Xu Wang, Andrea L. Bertozzi, James Chapman, Junyuan Lin, Bohan Chen, Chenchen Ye, Temple Daniel, P. Jeffrey Brantingham</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02435">https://arxiv.org/abs/2411.02435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02435">https://arxiv.org/pdf/2411.02435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02435]] Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models(https://arxiv.org/abs/2411.02435)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Narrative data spans all disciplines and provides a coherent model of the world to the reader or viewer. Recent advancement in machine learning and Large Language Models (LLMs) have enable great strides in analyzing natural language. However, Large language models (LLMs) still struggle with complex narrative arcs as well as narratives containing conflicting information. Recent work indicates LLMs augmented with external knowledge bases can improve the accuracy and interpretability of the resulting models. In this work, we analyze the effectiveness of applying knowledge graphs (KGs) in understanding true-crime podcast data from both classical Natural Language Processing (NLP) and LLM approaches. We directly compare KG-augmented LLMs (KGLLMs) with classical methods for KG construction, topic modeling, and sentiment analysis. Additionally, the KGLLM allows us to query the knowledge base in natural language and test its ability to factually answer questions. We examine the robustness of the model to adversarial prompting in order to test the model's ability to deal with conflicting information. Finally, we apply classical methods to understand more subtle aspects of the text such as the use of hearsay and sentiment in narrative construction and propose future directions. Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are more robust to adversarial prompts, and are more capable of summarizing the text into topics.</li>
</ul>

<h3>Title: TypeScore: A Text Fidelity Metric for Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Georgia Gabriela Sampaio, Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Josh Susskind, Navdeep Jaitly, Yizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02437">https://arxiv.org/abs/2411.02437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02437">https://arxiv.org/pdf/2411.02437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02437]] TypeScore: A Text Fidelity Metric for Text-to-Image Generative Models(https://arxiv.org/abs/2411.02437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating text-to-image generative models remains a challenge, despite the remarkable progress being made in their overall performances. While existing metrics like CLIPScore work for coarse evaluations, they lack the sensitivity to distinguish finer differences as model performance rapidly improves. In this work, we focus on the text rendering aspect of these models, which provides a lens for evaluating a generative model's fine-grained instruction-following capabilities. To this end, we introduce a new evaluation framework called TypeScore to sensitively assess a model's ability to generate images with high-fidelity embedded text by following precise instructions. We argue that this text generation capability serves as a proxy for general instruction-following ability in image synthesis. TypeScore uses an additional image description model and leverages an ensemble dissimilarity measure between the original and extracted text to evaluate the fidelity of the rendered text. Our proposed metric demonstrates greater resolution than CLIPScore to differentiate popular image generation models across a range of instructions with diverse text styles. Our study also evaluates how well these vision-language models (VLMs) adhere to stylistic instructions, disentangling style evaluation from embedded-text fidelity. Through human evaluation studies, we quantitatively meta-evaluate the effectiveness of the metric. Comprehensive analysis is conducted to explore factors such as text length, captioning models, and current progress towards human parity on this task. The framework provides insights into remaining gaps in instruction-following for image generation with embedded text.</li>
</ul>

<h3>Title: Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier Shifting Operation</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Can Yavuz, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02441">https://arxiv.org/abs/2411.02441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02441">https://arxiv.org/pdf/2411.02441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02441]] Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier Shifting Operation(https://arxiv.org/abs/2411.02441)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In biomedical imaging analysis, the dichotomy between 2D and 3D data presents a significant challenge. While 3D volumes offer superior real-world applicability, they are less available for each modality and not easy to train in large scale, whereas 2D samples are abundant but less comprehensive. This paper introduces the Cross-D Conv operation, a novel approach that bridges the dimensional gap by learning the phase shifting in the Fourier domain. Our method enables seamless weight transfer between 2D and 3D convolution operations, effectively facilitating cross-dimensional learning. The proposed architecture leverages the abundance of 2D training data to enhance 3D model performance, offering a practical solution to the multimodal data scarcity challenge in 3D medical model pretraining. Experimental validation on the RadImagenet (2D) and multimodal (3D) sets demonstrates that our approach achieves comparable or superior performance in feature quality assessment comparable to conventional methods. The enhanced convolution operation presents new opportunities for developing efficient classification and segmentation models in medical imaging. This work represents an advancement in cross-dimensional and multi-modal medical image analysis, offering a robust framework for utilizing 2D priors in 3D model pretraining or vice versa while maintaining computational efficiency.</li>
</ul>

<h3>Title: TODO: Enhancing LLM Alignment with Ternary Preferences</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Guo, Lu Yin, Bo Jiang, Jiaqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02442">https://arxiv.org/abs/2411.02442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02442">https://arxiv.org/pdf/2411.02442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02442]] TODO: Enhancing LLM Alignment with Ternary Preferences(https://arxiv.org/abs/2411.02442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human intent is critical for enhancing their performance across a variety of tasks. Standard alignment techniques, such as Direct Preference Optimization (DPO), often rely on the binary Bradley-Terry (BT) model, which can struggle to capture the complexities of human preferences -- particularly in the presence of noisy or inconsistent labels and frequent ties. To address these limitations, we introduce the Tie-rank Oriented Bradley-Terry model (TOBT), an extension of the BT model that explicitly incorporates ties, enabling more nuanced preference representation. Building on this, we propose Tie-rank Oriented Direct Preference Optimization (TODO), a novel alignment algorithm that leverages TOBT's ternary ranking system to improve preference alignment. In evaluations on Mistral-7B and Llama 3-8B models, TODO consistently outperforms DPO in modeling preferences across both in-distribution and out-of-distribution datasets. Additional assessments using MT Bench and benchmarks such as Piqa, ARC-c, and MMLU further demonstrate TODO's superior alignment performance. Notably, TODO also shows strong results in binary preference alignment, highlighting its versatility and potential for broader integration into LLM alignment. The implementation details can be found in this https URL.</li>
</ul>

<h3>Title: MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoliang Wang, Chen Zhao, Feng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02444">https://arxiv.org/abs/2411.02444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02444">https://arxiv.org/pdf/2411.02444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02444]] MADOD: Generalizing OOD Detection to Unseen Domains via G-Invariance Meta-Learning(https://arxiv.org/abs/2411.02444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world machine learning applications often face simultaneous covariate and semantic shifts, challenging traditional domain generalization and out-of-distribution (OOD) detection methods. We introduce Meta-learned Across Domain Out-of-distribution Detection (MADOD), a novel framework designed to address both shifts concurrently. MADOD leverages meta-learning and G-invariance to enhance model generalizability and OOD detection in unseen domains. Our key innovation lies in task construction: we randomly designate in-distribution classes as pseudo-OODs within each meta-learning task, simulating OOD scenarios using existing data. This approach, combined with energy-based regularization, enables the learning of robust, domain-invariant features while calibrating decision boundaries for effective OOD detection. Operating in a test domain-agnostic setting, MADOD eliminates the need for adaptation during inference, making it suitable for scenarios where test data is unavailable. Extensive experiments on real-world and synthetic datasets demonstrate MADOD's superior performance in semantic OOD detection across unseen domains, achieving an AUPR improvement of 8.48% to 20.81%, while maintaining competitive in-distribution classification accuracy, representing a significant advancement in handling both covariate and semantic shifts.</li>
</ul>

<h3>Title: Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva K. Pentyala, Katherine Tan, Xiang-Bo Mao, Roshanak Omrani, Sougata Chaudhuri, Regunathan Radhakrishnan, Sitaram Asur, Claire Na Cheng, Bin Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02448">https://arxiv.org/abs/2411.02448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02448">https://arxiv.org/pdf/2411.02448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02448]] Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models(https://arxiv.org/abs/2411.02448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucinations. This paper introduces two fine-tuned general-purpose LLM autoevaluators, REC-12B and REC-70B, specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanations and citations with minimal bias. It achieves Rank \#1 as a generative model on the RewardBench leaderboard\footnote{\url{this https URL}} under the model name \texttt{TextEval-Llama3.1-70B}. Our REC dataset and models are released at \url{this https URL}.</li>
</ul>

<h3>Title: High-performance automated abstract screening with large language model ensembles</h3>
<ul>
<li><strong>Authors: </strong>Rohan Sanghera, Arun James Thirunavukarasu, Marc El Khoury, Jessica O'Logbon, Yuqing Chen, Archie Watt, Mustafa Mahmood, Hamid Butt, George Nishimura, Andrew Soltan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02451">https://arxiv.org/abs/2411.02451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02451">https://arxiv.org/pdf/2411.02451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02451]] High-performance automated abstract screening with large language model ensembles(https://arxiv.org/abs/2411.02451)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in tasks requiring processing and interpretation of input text. Abstract screening is a labour-intensive component of systematic review involving repetitive application of inclusion and exclusion criteria on a large volume of studies identified by a literature search. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5 Pro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue of the Cochrane Library to evaluate their accuracy in zero-shot binary classification for abstract screening. Trials over a subset of 800 records identified optimal prompting strategies and demonstrated superior performance of LLMs to human researchers in terms of sensitivity (LLMmax = 1.000, humanmax = 0.775), precision (LLMmax = 0.927, humanmax = 0.911), and balanced accuracy (LLMmax = 0.904, humanmax = 0.865). The best performing LLM-prompt combinations were trialled across every replicated search result (n = 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but diminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles exhibited perfect sensitivity with a maximal precision of 0.458, with less observed performance drop in larger trials. Significant variation in performance was observed between reviews, highlighting the importance of domain-specific validation before deployment. LLMs may reduce the human labour cost of systematic review with maintained or improved accuracy and sensitivity. Systematic review is the foundation of evidence-based medicine, and LLMs can contribute to increasing the efficiency and quality of this mode of research.</li>
</ul>

<h3>Title: Goal-Oriented Semantic Communication for Wireless Visual Question Answering with Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sige Liu, Nan Li, Yansha Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02452">https://arxiv.org/abs/2411.02452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02452">https://arxiv.org/pdf/2411.02452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02452]] Goal-Oriented Semantic Communication for Wireless Visual Question Answering with Scene Graphs(https://arxiv.org/abs/2411.02452)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>As demands for communication and computational capabilities escalate, traditional bit-oriented communication falls short of these stringent requirements, especially for mission-critical and computation-intensive applications. Visual Question Answering (VQA), a representative application, has adopted edge computing to mitigate local computational constraints and accelerate visual perception with natural language. However, it encounters significant communication challenges such as limited bandwidth, reduced transmission power, and increased noise levels, leading to considerable latency and reduced efficiency in image and question transmission. we propose a goal-oriented semantic communication (GSC) framework that focuses on effectively extracting and transmitting semantic information most relevant to the VQA goals, improving the answering accuracy and enhancing the effectiveness and efficiency. The objective is to maximize the answering accuracy, and we propose a scene graphs (SG)-based image semantic extraction and ranking approach to prioritize the semantic information based on the goal of questions. Experimental results demonstrate that our GSC framework improves answering accuracy by up to 59% under Rayleigh channels while reducing total latency by up to 65% compared to traditional bit-oriented transmission.</li>
</ul>

<h3>Title: Graph-based Confidence Calibration for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yukun Li, Sijia Wang, Lifu Huang, Li-Ping Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02454">https://arxiv.org/abs/2411.02454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02454">https://arxiv.org/pdf/2411.02454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02454]] Graph-based Confidence Calibration for Large Language Models(https://arxiv.org/abs/2411.02454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.</li>
</ul>

<h3>Title: An Exploration of Higher Education Course Evaluation by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bo Yuan, Jiazi Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02455">https://arxiv.org/abs/2411.02455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02455">https://arxiv.org/pdf/2411.02455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02455]] An Exploration of Higher Education Course Evaluation by Large Language Models(https://arxiv.org/abs/2411.02455)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Course evaluation is a critical component in higher education pedagogy. It not only serves to identify limitations in existing course designs and provide a basis for curricular innovation, but also to offer quantitative insights for university administrative decision-making. Traditional evaluation methods, primarily comprising student surveys, instructor self-assessments, and expert reviews, often encounter challenges, including inherent subjectivity, feedback delays, inefficiencies, and limitations in addressing innovative teaching approaches. Recent advancements in large language models (LLMs) within artificial intelligence (AI) present promising new avenues for enhancing course evaluation processes. This study explores the application of LLMs in automated course evaluation from multiple perspectives and conducts rigorous experiments across 100 courses at a major university in China. The findings indicate that: (1) LLMs can be an effective tool for course evaluation; (2) their effectiveness is contingent upon appropriate fine-tuning and prompt engineering; and (3) LLM-generated evaluation results demonstrate a notable level of rationality and interpretability.</li>
</ul>

<h3>Title: A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Harini Narayanan, Sindhu Ghanta</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02456">https://arxiv.org/abs/2411.02456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02456">https://arxiv.org/pdf/2411.02456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02456]] A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning(https://arxiv.org/abs/2411.02456)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.</li>
</ul>

<h3>Title: A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Chen, Qingyi Si, Chenxu Yang, Yunzhi Liang, Zheng Lin, Huan Liu, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02457">https://arxiv.org/abs/2411.02457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02457">https://arxiv.org/pdf/2411.02457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02457]] A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles(https://arxiv.org/abs/2411.02457)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) has significantly propelled the advancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents predominantly focus on mimicking a character's fundamental attributes while neglecting the replication of linguistic style, and they are incapable of effectively replicating characters when performing tasks beyond multi-turn dialogues, which results in generated responses that lack authenticity. The reason current RPAs lack this capability is due to the nature of existing character datasets, which lack collections of character quotations and are limited to multi-turn dialogue tasks, constraining the RPA's performance across other task domains and failing to mimic a character's linguistic style. To address this gap, we developed a multi-task role-playing dataset named MRstyle, which encompasses a substantial number of real individuals along with their quotations and covers seven different tasks. On this basis, we develop StyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms recent open-source LLMs and RPAs baselines on 7 tasks including Dialogue, Dictionary, Composition, Story Generation, Product Description, Music Commentary, and Open Question Answering. The code and data will be released.</li>
</ul>

<h3>Title: Code-Switching Curriculum Learning for Multilingual Transfer in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, Hwaran Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02460">https://arxiv.org/abs/2411.02460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02460">https://arxiv.org/pdf/2411.02460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02460]] Code-Switching Curriculum Learning for Multilingual Transfer in LLMs(https://arxiv.org/abs/2411.02460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now exhibit near human-level performance in various tasks, but their performance drops drastically after a handful of high-resource languages due to the imbalance in pre-training data. Inspired by the human process of second language acquisition, particularly code-switching (the practice of language alternation in a conversation), we propose code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of human language learning by progressively training models with a curriculum consisting of 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as our underlying model, we demonstrate the efficacy of the CSCL in improving language transfer to Korean, achieving significant performance gains compared to monolingual continual pre-training methods. Ablation studies reveal that both token- and sentence-level code-switching significantly enhance cross-lingual transfer and that curriculum learning amplifies these effects. We also extend our findings into various languages, including Japanese (high-resource) and Indonesian (low-resource), and using two additional models (Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious correlations between language resources and safety alignment, presenting a robust, efficient framework for more equitable language transfer in LLMs. We observe that CSCL is effective for low-resource settings where high-quality, monolingual corpora for language transfer are hardly available.</li>
</ul>

<h3>Title: Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02461">https://arxiv.org/abs/2411.02461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02461">https://arxiv.org/pdf/2411.02461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02461]] Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control(https://arxiv.org/abs/2411.02461)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality. In this work, we address this issue through ``Sparse Activation Control''. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint components that are closely related to specific tasks within the model, i.e., attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factuality, and bias concurrently.</li>
</ul>

<h3>Title: You are out of context!</h3>
<ul>
<li><strong>Authors: </strong>Giancarlo Cobino, Simone Farci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02464">https://arxiv.org/abs/2411.02464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02464">https://arxiv.org/pdf/2411.02464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02464]] You are out of context!(https://arxiv.org/abs/2411.02464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This research proposes a novel drift detection methodology for machine learning (ML) models based on the concept of ''deformation'' in the vector space representation of data. Recognizing that new data can act as forces stretching, compressing, or twisting the geometric relationships learned by a model, we explore various mathematical frameworks to quantify this deformation. We investigate measures such as eigenvalue analysis of covariance matrices to capture global shape changes, local density estimation using kernel density estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in data concentration. Additionally, we draw inspiration from continuum mechanics by proposing a ''strain tensor'' analogy to capture multi-faceted deformations across different data types. This requires careful estimation of the displacement field, and we delve into strategies ranging from density-based approaches to manifold learning and neural network methods. By continuously monitoring these deformation metrics and correlating them with model performance, we aim to provide a sensitive, interpretable, and adaptable drift detection system capable of distinguishing benign data evolution from true drift, enabling timely interventions and ensuring the reliability of machine learning systems in dynamic environments. Addressing the computational challenges of this methodology, we discuss mitigation strategies like dimensionality reduction, approximate algorithms, and parallelization for real-time and large-scale applications. The method's effectiveness is demonstrated through experiments on real-world text data, focusing on detecting context shifts in Generative AI. Our results, supported by publicly available code, highlight the benefits of this deformation-based approach in capturing subtle drifts that traditional statistical methods often miss. Furthermore, we present a detailed application example within the healthcare domain, showcasing the methodology's potential in diverse fields. Future work will focus on further improving computational efficiency and exploring additional applications across different ML domains.</li>
</ul>

<h3>Title: See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhuang, Leon Yan, Zhenwei Zhang, Ruiqi Wang, Jiawei Zhang, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02465">https://arxiv.org/abs/2411.02465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02465">https://arxiv.org/pdf/2411.02465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02465]] See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers(https://arxiv.org/abs/2411.02465)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is becoming increasingly vital due to the rapid growth of time series data across various sectors. Anomalies in web service data, for example, can signal critical incidents such as system failures or server malfunctions, necessitating timely detection and response. However, most existing TSAD methodologies rely heavily on manual feature engineering or require extensive labeled training data, while also offering limited interpretability. To address these challenges, we introduce a pioneering framework called the Time Series Anomaly Multimodal Analyzer (TAMA), which leverages the power of Large Multimodal Models (LMMs) to enhance both the detection and interpretation of anomalies in time series data. By converting time series into visual formats that LMMs can efficiently process, TAMA leverages few-shot in-context learning capabilities to reduce dependence on extensive labeled datasets. Our methodology is validated through rigorous experimentation on multiple real-world datasets, where TAMA consistently outperforms state-of-the-art methods in TSAD tasks. Additionally, TAMA provides rich, natural language-based semantic analysis, offering deeper insights into the nature of detected anomalies. Furthermore, we contribute one of the first open-source datasets that includes anomaly detection labels, anomaly type labels, and contextual description, facilitating broader exploration and advancement within this critical field. Ultimately, TAMA not only excels in anomaly detection but also provides a comprehensive approach for understanding the underlying causes of anomalies, pushing TSAD forward through innovative methodologies and insights.</li>
</ul>

<h3>Title: Towards Harmless Rawlsian Fairness Regardless of Demographic Prior</h3>
<ul>
<li><strong>Authors: </strong>Xuanqian Wang, Jing Li, Ivor W. Tsang, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02467">https://arxiv.org/abs/2411.02467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02467">https://arxiv.org/pdf/2411.02467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02467]] Towards Harmless Rawlsian Fairness Regardless of Demographic Prior(https://arxiv.org/abs/2411.02467)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, fair</a></li>
<li><strong>Abstract: </strong>Due to privacy and security concerns, recent advancements in group fairness advocate for model training regardless of demographic information. However, most methods still require prior knowledge of demographics. In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely \emph{harmless Rawlsian fairness}. We ascertain that such a fairness requirement with no prior demographic information essential promotes training losses to exhibit a Dirac delta distribution. To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses. This problem is then optimized by a tailored dynamic update approach that operates in both loss and gradient dimensions, directing the model towards relatively fairer solutions while preserving its intact utility. Our experimental findings indicate that regression tasks, which are relatively unexplored from literature, can achieve significant fairness improvement through VFair regardless of any prior, whereas classification tasks usually do not because of their quantized utility measurements. The implementation of our method is publicly available at \url{this https URL}.</li>
</ul>

<h3>Title: Benchmarking XAI Explanations with Human-Aligned Evaluations</h3>
<ul>
<li><strong>Authors: </strong>R√©mi Kazmierczak, Steve Azzolin, Elo√Øse Berthier, Anna Hedstr√∂m, Patricia Delhomme, Nicolas Bousquet, Goran Frehse, Massimiliano Mancini, Baptiste Caramiaux, Andrea Passerini, Gianni Franchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02470">https://arxiv.org/abs/2411.02470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02470">https://arxiv.org/pdf/2411.02470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02470]] Benchmarking XAI Explanations with Human-Aligned Evaluations(https://arxiv.org/abs/2411.02470)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce PASTA (Perceptual Assessment System for explanaTion of Artificial intelligence), a novel framework for a human-centric evaluation of XAI techniques in computer vision. Our first key contribution is a human evaluation of XAI explanations on four diverse datasets (COCO, Pascal Parts, Cats Dogs Cars, and MonumAI) which constitutes the first large-scale benchmark dataset for XAI, with annotations at both the image and concept levels. This dataset allows for robust evaluation and comparison across various XAI methods. Our second major contribution is a data-based metric for assessing the interpretability of explanations. It mimics human preferences, based on a database of human evaluations of explanations in the PASTA-dataset. With its dataset and metric, the PASTA framework provides consistent and reliable comparisons between XAI techniques, in a way that is scalable but still aligned with human evaluations. Additionally, our benchmark allows for comparisons between explanations across different modalities, an aspect previously unaddressed. Our findings indicate that humans tend to prefer saliency maps over other explanation types. Moreover, we provide evidence that human assessments show a low correlation with existing XAI metrics that are numerically simulated by probing the model.</li>
</ul>

<h3>Title: A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Sorouralsadat Fatemi, Yuheng Hu, Maryam Mousavi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02476">https://arxiv.org/abs/2411.02476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02476">https://arxiv.org/pdf/2411.02476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02476]] A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification(https://arxiv.org/abs/2411.02476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.</li>
</ul>

<h3>Title: Building a Synthetic Vascular Model: Evaluation in an Intracranial Aneurysms Detection Scenario</h3>
<ul>
<li><strong>Authors: </strong>Rafic Nader, Florent Autrusseau, Vincent L'Allinec, Romain Bourcier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02477">https://arxiv.org/abs/2411.02477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02477">https://arxiv.org/pdf/2411.02477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02477]] Building a Synthetic Vascular Model: Evaluation in an Intracranial Aneurysms Detection Scenario(https://arxiv.org/abs/2411.02477)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We hereby present a full synthetic model, able to mimic the various constituents of the cerebral vascular tree, including the cerebral arteries, bifurcations and intracranial aneurysms. This model intends to provide a substantial dataset of brain arteries which could be used by a 3D convolutional neural network to efficiently detect Intra-Cranial Aneurysms. The cerebral aneurysms most often occur on a particular structure of the vascular tree named the Circle of Willis. Various studies have been conducted to detect and monitor the aneurysms and those based on Deep Learning achieve the best performance. Specifically, in this work, we propose a full synthetic 3D model able to mimic the brain vasculature as acquired by Magnetic Resonance Angiography, Time Of Flight principle. Among the various MRI modalities, this latter allows for a good rendering of the blood vessels and is non-invasive. Our model has been designed to simultaneously mimic the arteries' geometry, the aneurysm shape, and the background noise. The vascular tree geometry is modeled thanks to an interpolation with 3D Spline functions, and the statistical properties of the background noise is collected from angiography acquisitions and reproduced within the model. In this work, we thoroughly describe the synthetic vasculature model, we build up a neural network designed for aneurysm segmentation and detection, finally, we carry out an in-depth evaluation of the performance gap gained thanks to the synthetic model data augmentation.</li>
</ul>

<h3>Title: Fantastic LLMs for Preference Data Annotation and How to (not) Find Them</h3>
<ul>
<li><strong>Authors: </strong>Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02481">https://arxiv.org/abs/2411.02481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02481">https://arxiv.org/pdf/2411.02481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02481]] Fantastic LLMs for Preference Data Annotation and How to (not) Find Them(https://arxiv.org/abs/2411.02481)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Preference tuning of large language models (LLMs) relies on high-quality human preference data, which is often expensive and time-consuming to gather. While existing methods can use trained reward models or proprietary model as judges for preference annotation, they have notable drawbacks: training reward models remain dependent on initial human data, and using proprietary model imposes license restrictions that inhibits commercial usage. In this paper, we introduce customized density ratio (CDR) that leverages open-source LLMs for data annotation, offering an accessible and effective solution. Our approach uses the log-density ratio between a well-aligned LLM and a less aligned LLM as a reward signal. We explores 221 different LLMs pairs and empirically demonstrate that increasing the performance gap between paired LLMs correlates with better reward generalization. Furthermore, we show that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas. In our experiment using density ratio from a pair of Mistral-7B models, CDR achieves a RewardBench score of 82.6, outperforming the best in-class trained reward functions and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an on-policy preference dataset with which we preference tune Llama-3-8B-Instruct with SimPO. The final model achieves a 37.4% (+15.1%) win rate on ArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a score of 8.0 on MT-Bench.</li>
</ul>

<h3>Title: Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes</h3>
<ul>
<li><strong>Authors: </strong>Balu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang, Karim Hanna, Jennifer Costa, Cindy Shavor, Zhiyong Lu, Zhe He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02523">https://arxiv.org/abs/2411.02523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02523">https://arxiv.org/pdf/2411.02523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02523]] Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes(https://arxiv.org/abs/2411.02523)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Differential diagnosis is crucial for medicine as it helps healthcare providers systematically distinguish between conditions that share similar symptoms. This study assesses the impact of lab test results on differential diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from 50 case reports from PubMed Central were created incorporating patient demographics, symptoms, and lab results. Five LLMs GPT-4, GPT-3.5, Llama-2-70b, Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx with and without lab data. A comprehensive evaluation involving GPT-4, a knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving 55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and Mixtral excelling, though exact match rates were low. Lab tests, including liver function, metabolic/toxicology panels, and serology/immune tests, were generally interpreted correctly by LLMs for differential diagnosis.</li>
</ul>

<h3>Title: Strongly Topology-preserving GNNs for Brain Graph Super-resolution</h3>
<ul>
<li><strong>Authors: </strong>Pragya Singh, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02525">https://arxiv.org/abs/2411.02525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02525">https://arxiv.org/pdf/2411.02525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02525]] Strongly Topology-preserving GNNs for Brain Graph Super-resolution(https://arxiv.org/abs/2411.02525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Brain graph super-resolution (SR) is an under-explored yet highly relevant task in network neuroscience. It circumvents the need for costly and time-consuming medical imaging data collection, preparation, and processing. Current SR methods leverage graph neural networks (GNNs) thanks to their ability to natively handle graph-structured datasets. However, most GNNs perform node feature learning, which presents two significant limitations: (1) they require computationally expensive methods to learn complex node features capable of inferring connectivity strength or edge features, which do not scale to larger graphs; and (2) computations in the node space fail to adequately capture higher-order brain topologies such as cliques and hubs. However, numerous studies have shown that brain graph topology is crucial in identifying the onset and presence of various neurodegenerative disorders like Alzheimer and Parkinson. Motivated by these challenges and applications, we propose our STP-GSR framework. It is the first graph SR architecture to perform representation learning in higher-order topological space. Specifically, using the primal-dual graph formulation from graph theory, we develop an efficient mapping from the edge space of our low-resolution (LR) brain graphs to the node space of a high-resolution (HR) dual graph. This approach ensures that node-level computations on this dual graph correspond naturally to edge-level learning on our HR brain graphs, thereby enforcing strong topological consistency within our framework. Additionally, our framework is GNN layer agnostic and can easily learn from smaller, scalable GNNs, reducing computational requirements. We comprehensively benchmark our framework across seven key topological measures and observe that it significantly outperforms the previous state-of-the-art methods and baselines.</li>
</ul>

<h3>Title: What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length</h3>
<ul>
<li><strong>Authors: </strong>Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02528">https://arxiv.org/abs/2411.02528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02528">https://arxiv.org/pdf/2411.02528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02528]] What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length(https://arxiv.org/abs/2411.02528)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability--SLOR (Pauls and Klein, 2012; Lau et al. 2017)--across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs' lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context.</li>
</ul>

<h3>Title: A Comprehensive Study on Quantization Techniques for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiedong Lang, Zhehao Guo, Shuyu Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02530">https://arxiv.org/abs/2411.02530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02530">https://arxiv.org/pdf/2411.02530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02530]] A Comprehensive Study on Quantization Techniques for Large Language Models(https://arxiv.org/abs/2411.02530)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.</li>
</ul>

<h3>Title: MILU: A Multi-task Indic Language Understanding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02538">https://arxiv.org/abs/2411.02538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02538">https://arxiv.org/pdf/2411.02538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02538]] MILU: A Multi-task Indic Language Understanding Benchmark(https://arxiv.org/abs/2411.02538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating Large Language Models (LLMs) in low-resource and linguistically diverse languages remains a significant challenge in NLP, particularly for languages using non-Latin scripts like those spoken in India. Existing benchmarks predominantly focus on English, leaving substantial gaps in assessing LLM capabilities in these languages. We introduce MILU, a Multi task Indic Language Understanding Benchmark, a comprehensive evaluation benchmark designed to address this gap. MILU spans 8 domains and 42 subjects across 11 Indic languages, reflecting both general and culturally specific knowledge. With an India-centric design, incorporates material from regional and state-level examinations, covering topics such as local history, arts, festivals, and laws, alongside standard subjects like science and mathematics. We evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with GPT-4o achieving the highest average accuracy at 72 percent. Open multilingual models outperform language-specific fine-tuned models, which perform only slightly better than random baselines. Models also perform better in high resource languages as compared to low resource ones. Domain-wise analysis indicates that models perform poorly in culturally relevant areas like Arts and Humanities, Law and Governance compared to general fields like STEM. To the best of our knowledge, MILU is the first of its kind benchmark focused on Indic languages, serving as a crucial step towards comprehensive cultural evaluation. All code, benchmarks, and artifacts will be made publicly available to foster open research.</li>
</ul>

<h3>Title: GraphXAIN: Narratives to Explain Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Cedro, David Martens</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02540">https://arxiv.org/abs/2411.02540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02540">https://arxiv.org/pdf/2411.02540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02540]] GraphXAIN: Narratives to Explain Graph Neural Networks(https://arxiv.org/abs/2411.02540)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are a powerful technique for machine learning on graph-structured data, yet they pose interpretability challenges, especially for non-expert users. Existing GNN explanation methods often yield technical outputs such as subgraphs and feature importance scores, which are not easily understood. Building on recent insights from social science and other Explainable AI (XAI) methods, we propose GraphXAIN, a natural language narrative that explains individual predictions made by GNNs. We present a model-agnostic and explainer-agnostic XAI approach that complements graph explainers by generating GraphXAINs, using Large Language Models (LLMs) and integrating graph data, individual predictions from GNNs, explanatory subgraphs, and feature importances. We define XAI Narratives and XAI Descriptions, highlighting their distinctions and emphasizing the importance of narrative principles in effective explanations. By incorporating natural language narratives, our approach supports graph practitioners and non-expert users, aligning with social science research on explainability and enhancing user understanding and trust in complex GNN models. We demonstrate GraphXAIN's capabilities on a real-world graph dataset, illustrating how its generated narratives can aid understanding compared to traditional graph explainer outputs or other descriptive explanation methods.</li>
</ul>

<h3>Title: Pretrained transformer efficiently learns low-dimensional target functions in-context</h3>
<ul>
<li><strong>Authors: </strong>Kazusato Oko, Yujin Song, Taiji Suzuki, Denny Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02544">https://arxiv.org/abs/2411.02544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02544">https://arxiv.org/pdf/2411.02544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02544]] Pretrained transformer efficiently learns low-dimensional target functions in-context(https://arxiv.org/abs/2411.02544)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers can efficiently learn in-context from example demonstrations. Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective. However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt. In this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of \textit{single-index} target functions $f_*(\boldsymbol{x}) = \sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the \textit{information exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a prompt length that only depends on the dimension of the distribution of target functions $r$; in contrast, any algorithm that directly learns $f_*$ on test prompt yields a statistical complexity that scales with the ambient dimension $d$. Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.</li>
</ul>

<h3>Title: Analysing the cultural dimensions of cybercriminal groups -- A case study on the Conti ransomware group</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Mersinas, Aimee Liu, Niki Panteli</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02548">https://arxiv.org/abs/2411.02548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02548">https://arxiv.org/pdf/2411.02548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02548]] Analysing the cultural dimensions of cybercriminal groups -- A case study on the Conti ransomware group(https://arxiv.org/abs/2411.02548)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Cybercriminal profiling and cyber-attack attribution have been elusive goals world-wide, due to their effects on societal and geopolitical balance and stability. Attributing actions to a group or state is a complex endeavour, with traditional established approaches including cyber threat intelligence and analysis of technical means such as malware analysis, network forensics and geopolitical intelligence. However, we propose an additional component for profiling threat actor groups through analysing cultural aspects of human behaviours and interactions. We utilise a set of variables which determine characteristics of national and organisational culture to create a cultural "footprint" of cybercriminal groups. As a case study, we conduct thematic analysis across the six dimensions of the Hofstede national culture classification and the eight dimensions of the Meyer classification on leaked internal communications of the ransomware group Conti. We propose that a systematic analysis of similar communications can serve as a practical tool for a) understanding the modus operandi of cybercrime and cyberwarfare-related groups, and b) profiling cybercriminal groups and/or nation-state actors. Insights from such applications can, first, assist in combating cybercrime and, second, if combined with additional cyber threat intelligence, can provide a level of confidence in nuanced cyber-attack attribution processes.</li>
</ul>

<h3>Title: Leveraging Transformer-Based Models for Predicting Inflection Classes of Words in an Endangered Sami Language</h3>
<ul>
<li><strong>Authors: </strong>Khalid Alnajjar, Mika H√§m√§l√§inen, Jack Rueter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02556">https://arxiv.org/abs/2411.02556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02556">https://arxiv.org/pdf/2411.02556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02556]] Leveraging Transformer-Based Models for Predicting Inflection Classes of Words in an Endangered Sami Language(https://arxiv.org/abs/2411.02556)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a methodology for training a transformer-based model to classify lexical and morphosyntactic features of Skolt Sami, an endangered Uralic language characterized by complex morphology. The goal of our approach is to create an effective system for understanding and analyzing Skolt Sami, given the limited data availability and linguistic intricacies inherent to the language. Our end-to-end pipeline includes data extraction, augmentation, and training a transformer-based model capable of predicting inflection classes. The motivation behind this work is to support language preservation and revitalization efforts for minority languages like Skolt Sami. Accurate classification not only helps improve the state of Finite-State Transducers (FSTs) by providing greater lexical coverage but also contributes to systematic linguistic documentation for researchers working with newly discovered words from literature and native speakers. Our model achieves an average weighted F1 score of 1.00 for POS classification and 0.81 for inflection class classification. The trained model and code will be released publicly to facilitate future research in endangered NLP.</li>
</ul>

<h3>Title: Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Zhang, Henry Xie, Xinhao Zhang, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02558">https://arxiv.org/abs/2411.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02558">https://arxiv.org/pdf/2411.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02558]] Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions(https://arxiv.org/abs/2411.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the financial field, precise risk assessment tools are essential for decision-making. Recent studies have challenged the notion that traditional network loss functions like Mean Square Error (MSE) are adequate, especially under extreme risk conditions that can lead to significant losses during market upheavals. Transformers and Transformer-based models are now widely used in financial forecasting according to their outstanding performance in time-series-related predictions. However, these models typically lack sensitivity to extreme risks and often underestimate great financial losses. To address this problem, we introduce a novel loss function, the Loss-at-Risk, which incorporates Value at Risk (VaR) and Conditional Value at Risk (CVaR) into Transformer models. This integration allows Transformer models to recognize potential extreme losses and further improves their capability to handle high-stakes financial decisions. Moreover, we conduct a series of experiments with highly volatile financial datasets to demonstrate that our Loss-at-Risk function improves the Transformers' risk prediction and management capabilities without compromising their decision-making accuracy or efficiency. The results demonstrate that integrating risk-aware metrics during training enhances the Transformers' risk assessment capabilities while preserving their core strengths in decision-making and reasoning across diverse scenarios.</li>
</ul>

<h3>Title: Segment Anything for Dendrites from Electron Microscopy</h3>
<ul>
<li><strong>Authors: </strong>Zewen Zhuo, Ilya Belevich, Ville Leinonen, Eija Jokitalo, Tarja Malm, Alejandra Sierra, Jussi Tohka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02562">https://arxiv.org/abs/2411.02562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02562">https://arxiv.org/pdf/2411.02562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02562]] Segment Anything for Dendrites from Electron Microscopy(https://arxiv.org/abs/2411.02562)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation of cellular structures in electron microscopy (EM) images is fundamental to analyzing the morphology of neurons and glial cells in the healthy and diseased brain tissue. Current neuronal segmentation applications are based on convolutional neural networks (CNNs) and do not effectively capture global relationships within images. Here, we present DendriteSAM, a vision foundation model based on Segment Anything, for interactive and automatic segmentation of dendrites in EM images. The model is trained on high-resolution EM data from healthy rat hippocampus and is tested on diseased rat and human data. Our evaluation results demonstrate better mask quality compared to the original and other fine-tuned models, leveraging the features learned during training. This study introduces the first implementation of vision foundation models in dendrite segmentation, paving the path for computer-assisted diagnosis of neuronal anomalies.</li>
</ul>

<h3>Title: The Intersectionality Problem for Algorithmic Fairness</h3>
<ul>
<li><strong>Authors: </strong>Johannes Himmelreich, Arbie Hsu, Kristian Lum, Ellen Veomett</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02569">https://arxiv.org/abs/2411.02569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02569">https://arxiv.org/pdf/2411.02569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02569]] The Intersectionality Problem for Algorithmic Fairness(https://arxiv.org/abs/2411.02569)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups -- and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.</li>
</ul>

<h3>Title: TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Plini, Luca Scofano, Edoardo De Matteis, Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Andrea Sanchietti, Giovanni Maria Farinella, Fabio Galasso, Antonino Furnari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02570">https://arxiv.org/abs/2411.02570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02570">https://arxiv.org/pdf/2411.02570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02570]] TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos(https://arxiv.org/abs/2411.02570)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online. We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones. Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios. Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications.</li>
</ul>

<h3>Title: MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02571">https://arxiv.org/abs/2411.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02571">https://arxiv.org/pdf/2411.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02571]] MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs(https://arxiv.org/abs/2411.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future.</li>
</ul>

<h3>Title: ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy</h3>
<ul>
<li><strong>Authors: </strong>Kian Kenyon-Dean, Zitong Jerry Wang, John Urbanik, Konstantin Donhauser, Jason Hartford, Saber Saberian, Nil Sahin, Ihab Bendidi, Safiye Celik, Marta Fay, Juan Sebastian Rodriguez Vera, Imran S Haque, Oren Kraus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02572">https://arxiv.org/abs/2411.02572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02572">https://arxiv.org/pdf/2411.02572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02572]] ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy(https://arxiv.org/abs/2411.02572)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations. In this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks. Beyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological data.</li>
</ul>

<h3>Title: Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis</h3>
<ul>
<li><strong>Authors: </strong>Juanqin Liu, Leonardo Plotegher, Eloy Roura, Cristino de Souza Junior, Shaoming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02582">https://arxiv.org/abs/2411.02582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02582">https://arxiv.org/pdf/2411.02582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02582]] Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis(https://arxiv.org/abs/2411.02582)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle (UAV) detection technology plays a critical role in mitigating security risks and safeguarding privacy in both military and civilian applications. However, traditional detection methods face significant challenges in identifying UAV targets with extremely small pixels at long distances. To address this issue, we propose the Global-Local YOLO-Motion (GL-YOMO) detection algorithm, which combines You Only Look Once (YOLO) object detection with multi-frame motion detection techniques, markedly enhancing the accuracy and stability of small UAV target detection. The YOLO detection algorithm is optimized through multi-scale feature fusion and attention mechanisms, while the integration of the Ghost module further improves efficiency. Additionally, a motion detection approach based on template matching is being developed to augment detection capabilities for minute UAV targets. The system utilizes a global-local collaborative detection strategy to achieve high precision and efficiency. Experimental results on a self-constructed fixed-wing UAV dataset demonstrate that the GL-YOMO algorithm significantly enhances detection accuracy and stability, underscoring its potential in UAV detection applications.</li>
</ul>

<h3>Title: Context-Informed Machine Translation of Manga using Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Philip Lippmann, Konrad Skublicki, Joshua Tanner, Shonosuke Ishiwatari, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02589">https://arxiv.org/abs/2411.02589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02589">https://arxiv.org/pdf/2411.02589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02589]] Context-Informed Machine Translation of Manga using Multimodal Large Language Models(https://arxiv.org/abs/2411.02589)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the significant time and effort required for handcrafting translations, most manga never leave the domestic Japanese market. Automatic manga translation is a promising potential solution. However, it is a budding and underdeveloped field and presents complexities even greater than those found in standard translation due to the need to effectively incorporate visual elements into the translation process to resolve ambiguities. In this work, we investigate to what extent multimodal large language models (LLMs) can provide effective manga translation, thereby assisting manga authors and publishers in reaching wider audiences. Specifically, we propose a methodology that leverages the vision component of multimodal LLMs to improve translation quality and evaluate the impact of translation unit size, context length, and propose a token efficient approach for manga translation. Moreover, we introduce a new evaluation dataset -- the first parallel Japanese-Polish manga translation dataset -- as part of a benchmark to be used in future research. Finally, we contribute an open-source software suite, enabling others to benchmark LLMs for manga translation. Our findings demonstrate that our proposed methods achieve state-of-the-art results for Japanese-English translation and set a new standard for Japanese-Polish.</li>
</ul>

<h3>Title: Decoupled Data Augmentation for Improving Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Ruoxin Chen, Zhe Wang, Ke-Yue Zhang, Shuang Wu, Jiamu Sun, Shouli Wang, Taiping Yao, Shouhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02592">https://arxiv.org/abs/2411.02592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02592">https://arxiv.org/pdf/2411.02592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02592]] Decoupled Data Augmentation for Improving Image Classification(https://arxiv.org/abs/2411.02592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in image mixing and generative data augmentation have shown promise in enhancing image classification. However, these techniques face the challenge of balancing semantic fidelity with diversity. Specifically, image mixing involves interpolating two images to create a new one, but this pixel-level interpolation can compromise fidelity. Generative augmentation uses text-to-image generative models to synthesize or modify images, often limiting diversity to avoid generating out-of-distribution data that potentially affects accuracy. We propose that this fidelity-diversity dilemma partially stems from the whole-image paradigm of existing methods. Since an image comprises the class-dependent part (CDP) and the class-independent part (CIP), where each part has fundamentally different impacts on the image's fidelity, treating different parts uniformly can therefore be misleading. To address this fidelity-diversity dilemma, we introduce Decoupled Data Augmentation (De-DA), which resolves the dilemma by separating images into CDPs and CIPs and handling them adaptively. To maintain fidelity, we use generative models to modify real CDPs under controlled conditions, preserving semantic consistency. To enhance diversity, we replace the image's CIP with inter-class variants, creating diverse CDP-CIP combinations. Additionally, we implement an online randomized combination strategy during training to generate numerous distinct CDP-CIP combinations cost-effectively. Comprehensive empirical evaluations validate the effectiveness of our method.</li>
</ul>

<h3>Title: Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach</h3>
<ul>
<li><strong>Authors: </strong>Nodirbek Korchiev, Akash Pateria, Vodelina Samatova, Sogolsadat Mansouri, Kemafor Anyanwu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02597">https://arxiv.org/abs/2411.02597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02597">https://arxiv.org/pdf/2411.02597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02597]] Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach(https://arxiv.org/abs/2411.02597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Blockchains are being positioned as the "technology of trust" that can be used to mediate transactions between non-trusting parties without the need for a central authority. They support transaction types that are native to the blockchain platform or user-defined via user programs called smart contracts. Despite the significant flexibility in transaction programmability that smart contracts offer, they pose several usability, robustness, and performance challenges. This paper proposes an alternative transaction framework that incorporates more primitives into the native set of transaction types (reducing the likelihood of requiring user-defined transaction programs often). The framework is based on the concept of declarative blockchain transactions whose strength lies in the fact that it addresses several of the limitations of smart contracts simultaneously. A formal and implementation framework is presented, and a subset of commonly occurring transaction behaviors are modeled and implemented as use cases, using an open-source blockchain database, BigchchainDB, as the implementation context. A performance study comparing the declarative transaction approach to equivalent smart contract transaction models reveals several advantages of the proposed approach.</li>
</ul>

<h3>Title: FactTest: Factuality Testing in Large Language Models with Statistical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02603">https://arxiv.org/abs/2411.02603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02603">https://arxiv.org/pdf/2411.02603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02603]] FactTest: Factuality Testing in Large Language Models with Statistical Guarantees(https://arxiv.org/abs/2411.02603)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored. In this paper, we introduce FactTest, a novel framework that statistically assesses whether an LLM can confidently provide correct answers to given questions with high-probability correctness guarantees. We formulate factuality testing as hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that our framework also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. %These analyses are amenable to the principled NP framework. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) and multiple-choice benchmarks demonstrate that \approach effectively detects hallucinations and improves the model's ability to abstain from answering unknown questions, leading to an over 40% accuracy improvement.</li>
</ul>

<h3>Title: TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context Support for Network</h3>
<ul>
<li><strong>Authors: </strong>Nouf Alabbasi, Omar Erak, Omar Alhussein, Ismail Lotfi, Sami Muhaidat, Merouane Debbah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02617">https://arxiv.org/abs/2411.02617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02617">https://arxiv.org/pdf/2411.02617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02617]] TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context Support for Network(https://arxiv.org/abs/2411.02617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The telecommunications industry's rapid evolution demands intelligent systems capable of managing complex networks and adapting to emerging technologies. While large language models (LLMs) show promise in addressing these challenges, their deployment in telecom environments faces significant constraints due to edge device limitations and inconsistent documentation. To bridge this gap, we present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG) system built on the Phi-2 small language model (SLM). To improve context retrieval, TeleOracle employs a two-stage retriever that incorporates semantic chunking and hybrid keyword and semantic search. Additionally, we expand the context window during inference to enhance the model's performance on open-ended queries. We also employ low-rank adaption for efficient fine-tuning. A thorough analysis of the model's performance indicates that our RAG framework is effective in aligning Phi-2 to the telecom domain in a downstream question and answer (QnA) task, achieving a 30% improvement in accuracy over the base Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our model not only performs on par with the much larger LLMs but also achieves a higher faithfulness score, indicating higher adherence to the retrieved context.</li>
</ul>

<h3>Title: Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhao, Yijiang Li, Yuchen Yang, Wenqing Zhang, Nuno Vasconcelos, Yinzhi Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02622">https://arxiv.org/abs/2411.02622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02622">https://arxiv.org/pdf/2411.02622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02622]] Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning(https://arxiv.org/abs/2411.02622)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Machine unlearning--enabling a trained model to forget specific data--is crucial for addressing biased data and adhering to privacy regulations like the General Data Protection Regulation (GDPR)'s "right to be forgotten". Recent works have paid little attention to privacy concerns, leaving the data intended for forgetting vulnerable to membership inference attacks. Moreover, they often come with high computational overhead. In this work, we propose Pseudo-Probability Unlearning (PPU), a novel method that enables models to forget data efficiently and in a privacy-preserving manner. Our method replaces the final-layer output probabilities of the neural network with pseudo-probabilities for the data to be forgotten. These pseudo-probabilities follow either a uniform distribution or align with the model's overall distribution, enhancing privacy and reducing risk of membership inference attacks. Our optimization strategy further refines the predictive probability distributions and updates the model's weights accordingly, ensuring effective forgetting with minimal impact on the model's overall performance. Through comprehensive experiments on multiple benchmarks, our method achieves over 20% improvements in forgetting error compared to the state-of-the-art. Additionally, our method enhances privacy by preventing the forgotten set from being inferred to around random guesses.</li>
</ul>

<h3>Title: Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach</h3>
<ul>
<li><strong>Authors: </strong>Minghao Ning, Yaodong Cui, Yufeng Yang, Shucheng Huang, Zhenan Liu, Ahmad Reza Alghooneh, Ehsan Hashemi, Amir Khajepour</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02624">https://arxiv.org/abs/2411.02624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02624">https://arxiv.org/pdf/2411.02624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02624]] Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach(https://arxiv.org/abs/2411.02624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel real-time, delay-aware cooperative perception system designed for intelligent mobility platforms operating in dynamic indoor environments. The system contains a network of multi-modal sensor nodes and a central node that collectively provide perception services to mobility platforms. The proposed Hierarchical Clustering Considering the Scanning Pattern and Ground Contacting Feature based Lidar Camera Fusion improve intra-node perception for crowded environment. The system also features delay-aware global perception to synchronize and aggregate data across nodes. To validate our approach, we introduced the Indoor Pedestrian Tracking dataset, compiled from data captured by two indoor sensor nodes. Our experiments, compared to baselines, demonstrate significant improvements in detection accuracy and robustness against delays. The dataset is available in the repository: this https URL</li>
</ul>

<h3>Title: Extracting Unlearned Information from LLMs with Activation Steering</h3>
<ul>
<li><strong>Authors: </strong>Atakan Seyitoƒülu, Aleksei Kuvshinov, Leo Schwinn, Stephan G√ºnnemann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02631">https://arxiv.org/abs/2411.02631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02631">https://arxiv.org/pdf/2411.02631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02631]] Extracting Unlearned Information from LLMs with Activation Steering(https://arxiv.org/abs/2411.02631)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>An unintended consequence of the vast pretraining of Large Language Models (LLMs) is the verbatim memorization of fragments of their training data, which may contain sensitive or copyrighted information. In recent years, unlearning has emerged as a solution to effectively remove sensitive knowledge from models after training. Yet, recent work has shown that supposedly deleted information can still be extracted by malicious actors through various attacks. Still, current attacks retrieve sets of possible candidate generations and are unable to pinpoint the output that contains the actual target information. We propose activation steering as a method for exact information retrieval from unlearned LLMs. We introduce a novel approach to generating steering vectors, named Anonymized Activation Steering. Additionally, we develop a simple word frequency method to pinpoint the correct answer among a set of candidates when retrieving unlearned information. Our evaluation across multiple unlearning techniques and datasets demonstrates that activation steering successfully recovers general knowledge (e.g., widely known fictional characters) while revealing limitations in retrieving specific information (e.g., details about non-public individuals). Overall, our results demonstrate that exact information retrieval from unlearned models is possible, highlighting a severe vulnerability of current unlearning techniques.</li>
</ul>

<h3>Title: Data-Driven Hierarchical Open Set Recognition</h3>
<ul>
<li><strong>Authors: </strong>Andrew Hannum, Max Conway, Mario Lopez, Andr√© Harrison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02635">https://arxiv.org/abs/2411.02635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02635">https://arxiv.org/pdf/2411.02635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02635]] Data-Driven Hierarchical Open Set Recognition(https://arxiv.org/abs/2411.02635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel data-driven hierarchical approach to open set recognition (OSR) for robust perception in robotics and computer vision, utilizing constrained agglomerative clustering to automatically build a hierarchy of known classes in embedding space without requiring manual relational information. The method, demonstrated on the Animals with Attributes 2 (AwA2) dataset, achieves competitive results with an AUC ROC score of 0.82 and utility score of 0.85, while introducing two classification approaches (score-based and traversal-based) and a new Concentration Centrality (CC) metric for measuring hierarchical classification consistency. Although not surpassing existing models in accuracy, the approach provides valuable additional information about unknown classes through automatically generated hierarchies, requires no supplementary information beyond typical supervised model requirements, and introduces the Class Concentration Centrality (CCC) metric for evaluating unknown class placement consistency, with future work aimed at improving accuracy, validating the CC metric, and expanding to Large-Scale Open-Set Classification Protocols for ImageNet.</li>
</ul>

<h3>Title: A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Stephen McAleese, Mark Keane</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02643">https://arxiv.org/abs/2411.02643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02643">https://arxiv.org/pdf/2411.02643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02643]] A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers(https://arxiv.org/abs/2411.02643)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations can be used to interpret and debug text classifiers by producing minimally altered text inputs that change a classifier's output. In this work, we evaluate five methods for generating counterfactual explanations for a BERT text classifier on two datasets using three evaluation metrics. The results of our experiments suggest that established white-box substitution-based methods are effective at generating valid counterfactuals that change the classifier's output. In contrast, newer methods based on large language models (LLMs) excel at producing natural and linguistically plausible text counterfactuals but often fail to generate valid counterfactuals that alter the classifier's output. Based on these results, we recommend developing new counterfactual explanation methods that combine the strengths of established gradient-based approaches and newer LLM-based techniques to generate high-quality, valid, and plausible text counterfactual explanations.</li>
</ul>

<h3>Title: Fine Grained Insider Risk Detection</h3>
<ul>
<li><strong>Authors: </strong>Birkett Huber, Casper Neo, Keiran Sampson, Alex Kantchelian, Brett Ksobiech, Yanis Pavlidis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02645">https://arxiv.org/abs/2411.02645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02645">https://arxiv.org/pdf/2411.02645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02645]] Fine Grained Insider Risk Detection(https://arxiv.org/abs/2411.02645)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We present a method to detect departures from business-justified workflows among support agents. Our goal is to assist auditors in identifying agent actions that cannot be explained by the activity within their surrounding context, where normal activity patterns are established from historical data. We apply our method to help audit millions of actions of over three thousand support agents. We collect logs from the tools used by support agents and construct a bipartite graph of Actions and Entities representing all the actions of the agents, as well as background information about entities. From this graph, we sample subgraphs rooted on security-significant actions taken by the agents. Each subgraph captures the relevant context of the root action in terms of other actions, entities and their relationships. We then prioritize the rooted-subgraphs for auditor review using feed-forward and graph neural networks, as well as nearest neighbors techniques. To alleviate the issue of scarce labeling data, we use contrastive learning and domain-specific data augmentations. Expert auditors label the top ranked subgraphs as ``worth auditing" or ``not worth auditing" based on the company's business policies. This system finds subgraphs that are worth auditing with high enough precision to be used in production.</li>
</ul>

<h3>Title: M-CELS: Counterfactual Explanation for Multivariate Time Series Data Guided by Learned Saliency Maps</h3>
<ul>
<li><strong>Authors: </strong>Peiyu Li, Omar Bahri, Soukaina Filali Boubrahimi, Shah Muhammad Hamdi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02649">https://arxiv.org/abs/2411.02649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02649">https://arxiv.org/pdf/2411.02649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02649]] M-CELS: Counterfactual Explanation for Multivariate Time Series Data Guided by Learned Saliency Maps(https://arxiv.org/abs/2411.02649)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Over the past decade, multivariate time series classification has received great attention. Machine learning (ML) models for multivariate time series classification have made significant strides and achieved impressive success in a wide range of applications and tasks. The challenge of many state-of-the-art ML models is a lack of transparency and interpretability. In this work, we introduce M-CELS, a counterfactual explanation model designed to enhance interpretability in multidimensional time series classification tasks. Our experimental validation involves comparing M-CELS with leading state-of-the-art baselines, utilizing seven real-world time-series datasets from the UEA repository. The results demonstrate the superior performance of M-CELS in terms of validity, proximity, and sparsity, reinforcing its effectiveness in providing transparent insights into the decisions of machine learning models applied to multivariate time series data.</li>
</ul>

<h3>Title: Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Karthik Soman, Andrew Langdon, Catalina Villouta, Chinmay Agrawal, Lashaw Salta, Braian Peetoom, Gianmarco Bellucci, Orion J Buske</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02657">https://arxiv.org/abs/2411.02657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02657">https://arxiv.org/pdf/2411.02657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02657]] Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge(https://arxiv.org/abs/2411.02657)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.</li>
</ul>

<h3>Title: From Twitter to Reasoner: Understand Mobility Travel Modes and Sentiment Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kangrui Ruan, Xinyang Wang, Xuan Di</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02666">https://arxiv.org/abs/2411.02666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02666">https://arxiv.org/pdf/2411.02666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02666]] From Twitter to Reasoner: Understand Mobility Travel Modes and Sentiment Using Large Language Models(https://arxiv.org/abs/2411.02666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social media has become an important platform for people to express their opinions towards transportation services and infrastructure, which holds the potential for researchers to gain a deeper understanding of individuals' travel choices, for transportation operators to improve service quality, and for policymakers to regulate mobility services. A significant challenge, however, lies in the unstructured nature of social media data. In other words, textual data like social media is not labeled, and large-scale manual annotations are cost-prohibitive. In this study, we introduce a novel methodological framework utilizing Large Language Models (LLMs) to infer the mentioned travel modes from social media posts, and reason people's attitudes toward the associated travel mode, without the need for manual annotation. We compare different LLMs along with various prompting engineering methods in light of human assessment and LLM verification. We find that most social media posts manifest negative rather than positive sentiments. We thus identify the contributing factors to these negative posts and, accordingly, propose recommendations to traffic operators and policymakers.</li>
</ul>

<h3>Title: Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Jia, Sensen Gao, Qing Guo, Ke Ma, Yihao Huang, Simeng Qin, Yang Liu, Ivor Tsang Fellow, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02669">https://arxiv.org/abs/2411.02669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02669">https://arxiv.org/pdf/2411.02669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02669]] Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack(https://arxiv.org/abs/2411.02669)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-language pre-training (VLP) models excel at interpreting both images and text but remain vulnerable to multimodal adversarial examples (AEs). Advancing the generation of transferable AEs, which succeed across unseen models, is key to developing more robust and practical VLP models. Previous approaches augment image-text pairs to enhance diversity within the adversarial example generation process, aiming to improve transferability by expanding the contrast space of image-text features. However, these methods focus solely on diversity around the current AEs, yielding limited gains in transferability. To address this issue, we propose to increase the diversity of AEs by leveraging the intersection regions along the adversarial trajectory during optimization. Specifically, we propose sampling from adversarial evolution triangles composed of clean, historical, and current adversarial examples to enhance adversarial diversity. We provide a theoretical analysis to demonstrate the effectiveness of the proposed adversarial evolution triangle. Moreover, we find that redundant inactive dimensions can dominate similarity calculations, distorting feature matching and making AEs model-dependent with reduced transferability. Hence, we propose to generate AEs in the semantic image-text feature contrast space, which can project the original feature space into a semantic corpus subspace. The proposed semantic-aligned subspace can reduce the image feature redundancy, thereby improving adversarial transferability. Extensive experiments across different datasets and models demonstrate that the proposed method can effectively improve adversarial transferability and outperform state-of-the-art adversarial attack methods. The code is released at this https URL.</li>
</ul>

<h3>Title: Visually Analyze SHAP Plots to Diagnose Misclassifications in ML-based Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Maraz Mia, Mir Mehedi A. Pritom, Tariqul Islam, Kamrul Hasan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02670">https://arxiv.org/abs/2411.02670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02670">https://arxiv.org/pdf/2411.02670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02670]] Visually Analyze SHAP Plots to Diagnose Misclassifications in ML-based Intrusion Detection(https://arxiv.org/abs/2411.02670)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Intrusion detection has been a commonly adopted detective security measures to safeguard systems and networks from various threats. A robust intrusion detection system (IDS) can essentially mitigate threats by providing alerts. In networks based IDS, typically we deal with cyber threats like distributed denial of service (DDoS), spoofing, reconnaissance, brute-force, botnets, and so on. In order to detect these threats various machine learning (ML) and deep learning (DL) models have been proposed. However, one of the key challenges with these predictive approaches is the presence of false positive (FP) and false negative (FN) instances. This FPs and FNs within any black-box intrusion detection system (IDS) make the decision-making task of an analyst further complicated. In this paper, we propose an explainable artificial intelligence (XAI) based visual analysis approach using overlapping SHAP plots that presents the feature explanation to identify potential false positive and false negatives in IDS. Our approach can further provide guidance to security analysts for effective decision-making. We present case study with multiple publicly available network traffic datasets to showcase the efficacy of our approach for identifying false positive and false negative instances. Our use-case scenarios provide clear guidance for analysts on how to use the visual analysis approach for reliable course-of-actions against such threats.</li>
</ul>

<h3>Title: Fair In-Context Learning via Latent Concept Variables</h3>
<ul>
<li><strong>Authors: </strong>Karuna Bhaila, Minh-Hao Van, Kennedy Edemacu, Chen Zhao, Feng Chen, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02671">https://arxiv.org/abs/2411.02671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02671">https://arxiv.org/pdf/2411.02671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02671]] Fair In-Context Learning via Latent Concept Variables(https://arxiv.org/abs/2411.02671)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different types of data facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate this inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce correlation between predictive outcomes and sensitive variables helping to promote fairness during latent concept learning. We utilize the learned concept and select demonstrations from a training dataset to obtain fair predictions during inference while maintaining model utility. The latent concept variable is learned using a smaller internal LLM and the selected demonstrations can be used for inference with larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods.</li>
</ul>

<h3>Title: Multi-Transmotion: Pre-trained Model for Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yang Gao, Po-Chien Luan, Alexandre Alahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02673">https://arxiv.org/abs/2411.02673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02673">https://arxiv.org/pdf/2411.02673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02673]] Multi-Transmotion: Pre-trained Model for Human Motion Prediction(https://arxiv.org/abs/2411.02673)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The ability of intelligent systems to predict human behaviors is crucial, particularly in fields such as autonomous vehicle navigation and social robotics. However, the complexity of human motion have prevented the development of a standardized dataset for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model designed for cross-modality pre-training. Additionally, we present a novel masking strategy to capture rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code is publicly available: this https URL</li>
</ul>

<h3>Title: Wave Network: An Ultra-Small Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Victor S.Sheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02674">https://arxiv.org/abs/2411.02674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02674">https://arxiv.org/pdf/2411.02674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02674]] Wave Network: An Ultra-Small Language Model(https://arxiv.org/abs/2411.02674)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose an innovative token representation and update method in a new ultra-small language model: the Wave network. Specifically, we use a \textbf{complex vector} to represent each token, encoding both global and local semantics of the input text. A \textbf{complex vector} consists of two components: a magnitude vector representing the \textit{global semantics} of the input text, and a phase vector capturing the \textit{relationships between individual tokens and global semantics}. Experiments on the AG News text classification task demonstrate that, when generating complex vectors from randomly initialized token embeddings, our single-layer Wave Network achieves 90.91\% accuracy with wave interference and 91.66\% with wave modulation -- outperforming a single Transformer layer using BERT pre-trained embeddings by 19.23\% and 19.98\%, respectively, and approaching the accuracy of the pre-trained and fine-tuned BERT base model (94.64\%). Additionally, compared to BERT base, the Wave Network reduces video memory usage and training time by 77.34\% and 85.62\% during wave modulation. In summary, we used a 2.4-million-parameter small language model to achieve accuracy comparable to a 100-million-parameter BERT model in text classification.</li>
</ul>

<h3>Title: On the loss of context-awareness in general instruction fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wang, Andrew Bai, Nanyun Peng, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02688">https://arxiv.org/abs/2411.02688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02688">https://arxiv.org/pdf/2411.02688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02688]] On the loss of context-awareness in general instruction fine-tuning(https://arxiv.org/abs/2411.02688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pretrained Large Language Models (LLMs) require post-training methods such as supervised fine-tuning (SFT) on instruction-response pairs to enable instruction following. However, this process can potentially harm existing capabilities learned during pretraining. In this paper, we investigate the loss of context awareness after SFT, defined as the capability to extract and understand information from the user-provided context and respond accordingly. We are the first to identify and show that the loss of context-awareness appears on instruction-finetuned LLMs when the chat template is applied to the input prompts. We identify the performance decline is partially caused by the bias embedded into the chat template to focus less on the user-provided context. Based on these observations, we propose two methods to mitigate the loss of context awareness in instruct models: post-hoc attention steering on user prompts and conditional instruction fine-tuning with a context-dependency indicator. Empirical experiments on 4 context-dependent downstream tasks and 3 pretrained LLMs of different sizes show that our methods effectively mitigates the loss of context awareness without compromising the general ability to follow instructions. Our findings also strongly advocate the necessity to carefully benchmark context awareness after instruction fine-tuning.</li>
</ul>

<h3>Title: Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yunkai Dang, Mengxi Gao, Yibo Yan, Xin Zou, Yanggan Gu, Aiwei Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02708">https://arxiv.org/abs/2411.02708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02708">https://arxiv.org/pdf/2411.02708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02708]] Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios(https://arxiv.org/abs/2411.02708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence. However, existing benchmarks include many samples where all MLLMs \textit{exhibit high response uncertainty when encountering misleading information}, requiring even 5-15 response attempts per sample to effectively assess uncertainty. Therefore, we propose a two-stage pipeline: first, we collect MLLMs' responses without misleading information, and then gather misleading ones via specific misleading instructions. By calculating the misleading rate, and capturing both correct-to-incorrect and incorrect-to-correct shifts between the two sets of responses, we can effectively metric the model's response uncertainty. Eventually, we establish a \textbf{\underline{M}}ultimodal \textbf{\underline{U}}ncertainty \textbf{\underline{B}}enchmark (\textbf{MUB}) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains. Our experiments reveal that all open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate exceeding 86\%. To enhance the robustness of MLLMs, we further fine-tune all open-source MLLMs by incorporating explicit and implicit misleading data, which demonstrates a significant reduction in misleading rates. Our code is available at: \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach</h3>
<ul>
<li><strong>Authors: </strong>H. Wang, Y. Pang, D. Shang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02709">https://arxiv.org/abs/2411.02709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02709">https://arxiv.org/pdf/2411.02709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02709]] Carbon price fluctuation prediction using blockchain information A new hybrid machine learning approach(https://arxiv.org/abs/2411.02709)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this study, the novel hybrid machine learning approach is proposed in carbon price fluctuation prediction. Specifically, a research framework integrating DILATED Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) neural network algorithm is proposed. The advantage of the combined framework is that it can make feature extraction more efficient. Then, based on the DILATED CNN-LSTM framework, the L1 and L2 parameter norm penalty as regularization method is adopted to predict. Referring to the characteristics of high correlation between energy indicator price and blockchain information in previous literature, and we primarily includes indicators related to blockchain information through regularization process. Based on the above methods, this paper uses a dataset containing an amount of data to carry out the carbon price prediction. The experimental results show that the DILATED CNN-LSTM framework is superior to the traditional CNN-LSTM architecture. Blockchain information can effectively predict the price. Since parameter norm penalty as regularization, Ridge Regression (RR) as L2 regularization is better than Smoothly Clipped Absolute Deviation Penalty (SCAD) as L1 regularization in price forecasting. Thus, the proposed RR-DILATED CNN-LSTM approach can effectively and accurately predict the fluctuation trend of the carbon price. Therefore, the new forecasting methods and theoretical ecology proposed in this study provide a new basis for trend prediction and evaluating digital assets policy represented by the carbon price for both the academia and practitioners.</li>
</ul>

<h3>Title: V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Xie, Guanzhen Li, Xiao Xu, Min-Yen Kan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02712">https://arxiv.org/abs/2411.02712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02712">https://arxiv.org/pdf/2411.02712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02712]] V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization(https://arxiv.org/abs/2411.02712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: CIT: Rethinking Class-incremental Semantic Segmentation with a Class Independent Transformation</h3>
<ul>
<li><strong>Authors: </strong>Jinchao Ge, Bowen Zhang, Akide Liu, Minh Hieu Phan, Qi Chen, Yangyang Shu, Yang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02715">https://arxiv.org/abs/2411.02715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02715">https://arxiv.org/pdf/2411.02715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02715]] CIT: Rethinking Class-incremental Semantic Segmentation with a Class Independent Transformation(https://arxiv.org/abs/2411.02715)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Class-incremental semantic segmentation (CSS) requires that a model learn to segment new classes without forgetting how to segment previous ones: this is typically achieved by distilling the current knowledge and incorporating the latest data. However, bypassing iterative distillation by directly transferring outputs of initial classes to the current learning task is not supported in existing class-specific CSS methods. Via Softmax, they enforce dependency between classes and adjust the output distribution at each learning step, resulting in a large probability distribution gap between initial and current tasks. We introduce a simple, yet effective Class Independent Transformation (CIT) that converts the outputs of existing semantic segmentation models into class-independent forms with negligible cost or performance loss. By utilizing class-independent predictions facilitated by CIT, we establish an accumulative distillation framework, ensuring equitable incorporation of all class information. We conduct extensive experiments on various segmentation architectures, including DeepLabV3, Mask2Former, and SegViTv2. Results from these experiments show minimal task forgetting across different datasets, with less than 5% for ADE20K in the most challenging 11 task configurations and less than 1% across all configurations for the PASCAL VOC 2012 dataset.</li>
</ul>

<h3>Title: Multimodal Commonsense Knowledge Distillation for Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yang, Siwen Luo, Soyeon Caren Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02722">https://arxiv.org/abs/2411.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02722">https://arxiv.org/pdf/2411.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02722]] Multimodal Commonsense Knowledge Distillation for Visual Question Answering(https://arxiv.org/abs/2411.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing Multimodal Large Language Models (MLLMs) and Visual Language Pretrained Models (VLPMs) have shown remarkable performances in the general Visual Question Answering (VQA). However, these models struggle with VQA questions that require external commonsense knowledge due to the challenges in generating high-quality prompts and the high computational costs of fine-tuning. In this work, we propose a novel graph-based multimodal commonsense knowledge distillation framework that constructs a unified relational graph over commonsense knowledge, visual objects and questions through a Graph Convolutional Network (GCN) following a teacher-student environment. This proposed framework is flexible with any type of teacher and student models without further fine-tuning, and has achieved competitive performances on the ScienceQA dataset.</li>
</ul>

<h3>Title: A Natural Language Processing Approach to Support Biomedical Data Harmonization: Leveraging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zexu Li, Suraj P. Prabhu, Zachary T. Popp, Shubhi S. Jain, Vijetha Balakundi, Ting Fang Alvin Ang, Rhoda Au, Jinying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02730">https://arxiv.org/abs/2411.02730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02730">https://arxiv.org/pdf/2411.02730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02730]] A Natural Language Processing Approach to Support Biomedical Data Harmonization: Leveraging Large Language Models(https://arxiv.org/abs/2411.02730)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Biomedical research requires large, diverse samples to produce unbiased results. Automated methods for matching variables across datasets can accelerate this process. Research in this area has been limited, primarily focusing on lexical matching and ontology based semantic matching. We aimed to develop new methods, leveraging large language models (LLM) and ensemble learning, to automate variable matching. Methods: We utilized data from two GERAS cohort (European and Japan) studies to develop variable matching methods. We first manually created a dataset by matching 352 EU variables with 1322 candidate JP variables, where matched variable pairs were positive and unmatched pairs were negative instances. Using this dataset, we developed and evaluated two types of natural language processing (NLP) methods, which matched variables based on variable labels and definitions from data dictionaries: (1) LLM-based and (2) fuzzy matching. We then developed an ensemble-learning method, using the Random Forest model, to integrate individual NLP methods. RF was trained and evaluated on 50 trials. Each trial had a random split (4:1) of training and test sets, with the model's hyperparameters optimized through cross-validation on the training set. For each EU variable, 1322 candidate JP variables were ranked based on NLP-derived similarity scores or RF's probability scores, denoting their likelihood to match the EU variable. Ranking performance was measured by top-n hit ratio (HRn) and mean reciprocal rank (MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30 and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived features contributed most to RF's performance. One major cause of errors in automatic variable matching was ambiguous variable definitions within data dictionaries.</li>
</ul>

<h3>Title: Novelty-focused R&D landscaping using transformer and local outlier factor</h3>
<ul>
<li><strong>Authors: </strong>Jaewoong Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02738">https://arxiv.org/abs/2411.02738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02738">https://arxiv.org/pdf/2411.02738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02738]] Novelty-focused R&D landscaping using transformer and local outlier factor(https://arxiv.org/abs/2411.02738)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While numerous studies have explored the field of research and development (R&D) landscaping, the preponderance of these investigations has emphasized predictive analysis based on R&D outcomes, specifically patents, and academic literature. However, the value of research proposals and novelty analysis has seldom been addressed. This study proposes a systematic approach to constructing and navigating the R&D landscape that can be utilized to guide organizations to respond in a reproducible and timely manner to the challenges presented by increasing number of research proposals. At the heart of the proposed approach is the composite use of the transformer-based language model and the local outlier factor (LOF). The semantic meaning of the research proposals is captured with our further-trained transformers, thereby constructing a comprehensive R&D landscape. Subsequently, the novelty of the newly selected research proposals within the annual landscape is quantified on a numerical scale utilizing the LOF by assessing the dissimilarity of each proposal to others preceding and within the same year. A case study examining research proposals in the energy and resource sector in South Korea is presented. The systematic process and quantitative outcomes are expected to be useful decision-support tools, providing future insights regarding R&D planning and roadmapping.</li>
</ul>

<h3>Title: A Bayesian explanation of machine learning models based on modes and functional ANOVA</h3>
<ul>
<li><strong>Authors: </strong>Quan Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02746">https://arxiv.org/abs/2411.02746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02746">https://arxiv.org/pdf/2411.02746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02746]] A Bayesian explanation of machine learning models based on modes and functional ANOVA(https://arxiv.org/abs/2411.02746)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most methods in explainable AI (XAI) focus on providing reasons for the prediction of a given set of features. However, we solve an inverse explanation problem, i.e., given the deviation of a label, find the reasons of this deviation. We use a Bayesian framework to recover the ``true'' features, conditioned on the observed label value. We efficiently explain the deviation of a label value from the mode, by identifying and ranking the influential features using the ``distances'' in the ANOVA functional decomposition. We show that the new method is more human-intuitive and robust than methods based on mean values, e.g., SHapley Additive exPlanations (SHAP values). The extra costs of solving a Bayesian inverse problem are dimension-independent.</li>
</ul>

<h3>Title: EcoCropsAID: Economic Crops Aerial Image Dataset for Land Use Classification</h3>
<ul>
<li><strong>Authors: </strong>Sangdaow Noppitak, Emmanuel Okafor, Olarik Surinta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02762">https://arxiv.org/abs/2411.02762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02762">https://arxiv.org/pdf/2411.02762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02762]] EcoCropsAID: Economic Crops Aerial Image Dataset for Land Use Classification(https://arxiv.org/abs/2411.02762)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The EcoCropsAID dataset is a comprehensive collection of 5,400 aerial images captured between 2014 and 2018 using the Google Earth application. This dataset focuses on five key economic crops in Thailand: rice, sugarcane, cassava, rubber, and longan. The images were collected at various crop growth stages: early cultivation, growth, and harvest, resulting in significant variability within each category and similarities across different categories. These variations, coupled with differences in resolution, color, and contrast introduced by multiple remote imaging sensors, present substantial challenges for land use classification. The dataset is an interdisciplinary resource that spans multiple research domains, including remote sensing, geoinformatics, artificial intelligence, and computer vision. The unique features of the EcoCropsAID dataset offer opportunities for researchers to explore novel approaches, such as extracting spatial and temporal features, developing deep learning architectures, and implementing transformer-based models. The EcoCropsAID dataset provides a valuable platform for advancing research in land use classification, with implications for optimizing agricultural practices and enhancing sustainable development. This study explicitly investigates the use of deep learning algorithms to classify economic crop areas in northeastern Thailand, utilizing satellite imagery to address the challenges posed by diverse patterns and similarities across categories.</li>
</ul>

<h3>Title: One-Stage-TFS: Thai One-Stage Fingerspelling Dataset for Fingerspelling Recognition Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Siriwiwat Lata, Sirawan Phiphitphatphaisit, Emmanuel Okafor, Olarik Surinta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02768">https://arxiv.org/abs/2411.02768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02768">https://arxiv.org/pdf/2411.02768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02768]] One-Stage-TFS: Thai One-Stage Fingerspelling Dataset for Fingerspelling Recognition Frameworks(https://arxiv.org/abs/2411.02768)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The Thai One-Stage Fingerspelling (One-Stage-TFS) dataset is a comprehensive resource designed to advance research in hand gesture recognition, explicitly focusing on the recognition of Thai sign language. This dataset comprises 7,200 images capturing 15 one-stage consonant gestures performed by undergraduate students from Rajabhat Maha Sarakham University, Thailand. The contributors include both expert students from the Special Education Department with proficiency in Thai sign language and students from other departments without prior sign language experience. Images were collected between July and December 2021 using a DSLR camera, with contributors demonstrating hand gestures against both simple and complex backgrounds. The One-Stage-TFS dataset presents challenges in detecting and recognizing hand gestures, offering opportunities to develop novel end-to-end recognition frameworks. Researchers can utilize this dataset to explore deep learning methods, such as YOLO, EfficientDet, RetinaNet, and Detectron, for hand detection, followed by feature extraction and recognition using techniques like convolutional neural networks, transformers, and adaptive feature fusion networks. The dataset is accessible via the Mendeley Data repository and supports a wide range of applications in computer science, including deep learning, computer vision, and pattern recognition, thereby encouraging further innovation and exploration in these fields.</li>
</ul>

<h3>Title: FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Duong H. Nguyen, Phi L. Nguyen, Truong T. Nguyen, Hieu H. Pham, Duc A. Tran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02773">https://arxiv.org/abs/2411.02773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02773">https://arxiv.org/pdf/2411.02773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02773]] FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks(https://arxiv.org/abs/2411.02773)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning. Despite its promises, FL is prone to critical security risks. First, because FL depends on a central server to aggregate local training models, this is a single point of failure. The server might function maliciously. Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients. They can poison the local model before submitting to the server. Either type of attack, on the server or the client side, would severely degrade learning accuracy. We propose FedBlock, a novel blockchain-based FL framework that addresses both of these security risks. FedBlock is uniquely desirable in that it involves only smart contract programming, thus deployable atop any blockchain network. Our framework is substantiated with a comprehensive evaluation study using real-world datasets. Its robustness against backdoor attacks is competitive with the literature of FL backdoor defense. The latter, however, does not address the server risk as we do.</li>
</ul>

<h3>Title: Brewing Vodka: Distilling Pure Knowledge for Lightweight Threat Detection in Audit Logs</h3>
<ul>
<li><strong>Authors: </strong>Weiheng Wu, Wei Qiao, Wenhao Yan, Bo Jiang, Yuling Liu, Baoxu Liu, Zhigang Lu, JunRong Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02775">https://arxiv.org/abs/2411.02775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02775">https://arxiv.org/pdf/2411.02775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02775]] Brewing Vodka: Distilling Pure Knowledge for Lightweight Threat Detection in Audit Logs(https://arxiv.org/abs/2411.02775)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) are continuously evolving, leveraging their stealthiness and persistence to put increasing pressure on current provenance-based Intrusion Detection Systems (IDS). This evolution exposes several critical issues: (1) The dense interaction between malicious and benign nodes within provenance graphs introduces neighbor noise, hindering effective detection; (2) The complex prediction mechanisms of existing APTs detection models lead to the insufficient utilization of prior knowledge embedded in the data; (3) The high computational cost makes detection impractical. To address these challenges, we propose Vodka, a lightweight threat detection system built on a knowledge distillation framework, capable of node-level detection within audit log provenance graphs. Specifically, Vodka applies graph Laplacian regularization to reduce neighbor noise, obtaining smoothed and denoised graph signals. Subsequently, Vodka employs a teacher model based on GNNs to extract knowledge, which is then distilled into a lightweight student model. The student model is designed as a trainable combination of a feature transformation module and a personalized PageRank random walk label propagation module, with the former capturing feature knowledge and the latter learning label and structural knowledge. After distillation, the student model benefits from the knowledge of the teacher model to perform precise threat detection. Finally, Vodka reconstructs attack paths from anomalous nodes, providing insight into the attackers' strategies. We evaluate Vodka through extensive experiments on three public datasets and compare its performance against several state-of-the-art IDS solutions. The results demonstrate that Vodka achieves outstanding detection accuracy across all scenarios and the detection time is 1.4 to 5.2 times faster than the current state-of-the-art methods.</li>
</ul>

<h3>Title: Advancing Recycling Efficiency: A Comparative Analysis of Deep Learning Models in Waste Classification</h3>
<ul>
<li><strong>Authors: </strong>Zhanshan Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02779">https://arxiv.org/abs/2411.02779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02779">https://arxiv.org/pdf/2411.02779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02779]] Advancing Recycling Efficiency: A Comparative Analysis of Deep Learning Models in Waste Classification(https://arxiv.org/abs/2411.02779)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the ongoing increase in the worldwide population and escalating consumption habits,there's a surge in the amount of waste this http URL situation poses considerable challenges for waste management and the optimization of recycling this http URL research tackles the pressing issue of waste classification for recycling by analyzing various deep learning models,including Convolutional Neural Network(CNN),AlexNet,ResNet,ResNet50 plus Support Vector Machine(SVM),and transformers,across a wide array of waste this http URL research meticulously compares these models on several targets like parameters settings,category accuracy,total accuracy and model parameters to establish a uniform evaluation this http URL research presents a novel method that incorporates SVM with deep learning frameworks,particularly this http URL results indicate the method significantly boosts accuracy in complex waste this http URL,the transformer model outshines others in average accuracy,showcasing its aptitude for intricate classification this http URL improve performance in poorly performing categories,the research advocates for enlarging the dataset,employing data augmentation,and leveraging sophisticated models such as transformers,along with refining training this http URL research paves the way for future advancements in multi-category waste recycling and underscores the pivotal role of deep learning in promoting environmental sustainability.</li>
</ul>

<h3>Title: How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Giannis Daras, Yeshwanth Cherapanamjeri, Constantinos Daskalakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02780">https://arxiv.org/abs/2411.02780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02780">https://arxiv.org/pdf/2411.02780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02780]] How much is a noisy image worth? Data Scaling Laws for Ambient Diffusion(https://arxiv.org/abs/2411.02780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than $80$ models on data with different corruption levels across three datasets ranging from $30,000$ to $\approx 1.3$M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g.~$10\%$ of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.</li>
</ul>

<h3>Title: BrainBits: How Much of the Brain are Generative Reconstruction Methods Using?</h3>
<ul>
<li><strong>Authors: </strong>David Mayo, Christopher Wang, Asa Harbin, Abdulrahman Alabdulkareem, Albert Eaton Shaw, Boris Katz, Andrei Barbu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02783">https://arxiv.org/abs/2411.02783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02783">https://arxiv.org/pdf/2411.02783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02783]] BrainBits: How Much of the Brain are Generative Reconstruction Methods Using?(https://arxiv.org/abs/2411.02783)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>When evaluating stimuli reconstruction results it is tempting to assume that higher fidelity text and image generation is due to an improved understanding of the brain or more powerful signal extraction from neural recordings. However, in practice, new reconstruction methods could improve performance for at least three other reasons: learning more about the distribution of stimuli, becoming better at reconstructing text or images in general, or exploiting weaknesses in current image and/or text evaluation metrics. Here we disentangle how much of the reconstruction is due to these other factors vs. productively using the neural recordings. We introduce BrainBits, a method that uses a bottleneck to quantify the amount of signal extracted from neural recordings that is actually necessary to reproduce a method's reconstruction fidelity. We find that it takes surprisingly little information from the brain to produce reconstructions with high fidelity. In these cases, it is clear that the priors of the methods' generative models are so powerful that the outputs they produce extrapolate far beyond the neural signal they decode. Given that reconstructing stimuli can be improved independently by either improving signal extraction from the brain or by building more powerful generative models, improving the latter may fool us into thinking we are improving the former. We propose that methods should report a method-specific random baseline, a reconstruction ceiling, and a curve of performance as a function of bottleneck size, with the ultimate goal of using more of the neural recordings.</li>
</ul>

<h3>Title: Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jason Vega, Junsheng Huang, Gaokai Zhang, Hangoo Kang, Minjia Zhang, Gagandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02785">https://arxiv.org/abs/2411.02785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02785">https://arxiv.org/pdf/2411.02785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02785]] Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment(https://arxiv.org/abs/2411.02785)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment of Large Language Models (LLMs) has recently become a critical objective of model developers. In response, a growing body of work has been investigating how safety alignment can be bypassed through various jailbreaking methods, such as adversarial attacks. However, these jailbreak methods can be rather costly or involve a non-trivial amount of creativity and effort, introducing the assumption that malicious users are high-resource or sophisticated. In this paper, we study how simple random augmentations to the input prompt affect safety alignment effectiveness in state-of-the-art LLMs, such as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different models and investigate the intersection of safety under random augmentations with multiple dimensions: augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies (e.g., sampling temperature). We show that low-resource and unsophisticated attackers, i.e. $\textit{stochastic monkeys}$, can significantly improve their chances of bypassing alignment with just 25 random augmentations per prompt.</li>
</ul>

<h3>Title: Language Models and Cycle Consistency for Self-Reflective Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Jianqiao Wangni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02791">https://arxiv.org/abs/2411.02791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02791">https://arxiv.org/pdf/2411.02791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02791]] Language Models and Cycle Consistency for Self-Reflective Machine Translation(https://arxiv.org/abs/2411.02791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A to a target language B, and subsequently translate these candidates back to the original language A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law and test-time computation scaling law. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.</li>
</ul>

<h3>Title: Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingcheng Li, Dingkang Yang, Yang Liu, Shunli Wang, Jiawei Chen, Shuaibing Wang, Jinjie Wei, Yue Jiang, Qingyao Xu, Xiaolu Hou, Mingyang Sun, Ziyun Qian, Dongliang Kou, Lihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02793">https://arxiv.org/abs/2411.02793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02793">https://arxiv.org/pdf/2411.02793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02793]] Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning(https://arxiv.org/abs/2411.02793)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model's performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.</li>
</ul>

<h3>Title: Real-Time Text Detection with Similar Mask in Traffic, Industrial, and Natural Scenes</h3>
<ul>
<li><strong>Authors: </strong>Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02794">https://arxiv.org/abs/2411.02794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02794">https://arxiv.org/pdf/2411.02794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02794]] Real-Time Text Detection with Similar Mask in Traffic, Industrial, and Natural Scenes(https://arxiv.org/abs/2411.02794)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Texts on the intelligent transportation scene include mass information. Fully harnessing this information is one of the critical drivers for advancing intelligent transportation. Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy. Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing. In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process. To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM). Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible. Its post-progressing saves 50$\%$ of the time, accurately and efficiently reconstructing text contours. The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level. Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM. Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur. In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets. Extensive experiments verify it achieves (SOTA) performance on several benchmarks. The code and dataset are available at: \url{this https URL}.</li>
</ul>

<h3>Title: The Evolution of RWKV: Advancements in Efficient Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Akul Datta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02795">https://arxiv.org/abs/2411.02795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02795">https://arxiv.org/pdf/2411.02795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02795]] The Evolution of RWKV: Advancements in Efficient Language Modeling(https://arxiv.org/abs/2411.02795)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper reviews the development of the Receptance Weighted Key Value (RWKV) architecture, emphasizing its advancements in efficient language modeling. RWKV combines the training efficiency of Transformers with the inference efficiency of RNNs through a novel linear attention mechanism. We examine its core innovations, adaptations across various domains, and performance advantages over traditional models. The paper also discusses challenges and future directions for RWKV as a versatile architecture in deep learning.</li>
</ul>

<h3>Title: TRANSPOSE: Transitional Approaches for Spatially-Aware LFI Resilient FSM Encoding</h3>
<ul>
<li><strong>Authors: </strong>Muhtadi Choudhury, Minyan Gao, Avinash Varna, Elad Peer, Domenic Forte</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02798">https://arxiv.org/abs/2411.02798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02798">https://arxiv.org/pdf/2411.02798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02798]] TRANSPOSE: Transitional Approaches for Spatially-Aware LFI Resilient FSM Encoding(https://arxiv.org/abs/2411.02798)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Finite state machines (FSMs) regulate sequential circuits, including access to sensitive information and privileged CPU states. Courtesy of contemporary research on laser attacks, laser-based fault injection (LFI) is becoming even more precise where an adversary can thwart chip security by altering individual flip-flop (FF) values. Different laser models, e.g., bit flip, bit set, and bit reset, have been developed to appreciate LFI on practical targets. As traditional approaches may incorporate substantial overhead, state-based SPARSE and transition-based TAMED countermeasures were proposed in our prior work to improve FSM resiliency efficiently. TAMED overcame SPARSE's limitation of being too conservative, and generating multiple LFI resilient encodings for contemporary LFI models on demand. SPARSE, however, incorporated design layout information into its vulnerability estimation which makes its vulnerability estimation metric more accurate. In this paper, we extend TAMED by proposing a transition-based encoding CAD framework (TRANSPOSE), that incorporates spatial transitional vulnerability metrics to quantify design susceptibility of FSMs based on both the bit flip model and the set-reset models. TRANSPOSE also incorporates floorplan optimization into its framework to accommodate secure spatial inter-distance of FF-sensitive regions. All TRANSPOSE approaches are demonstrated on 5 multifarious benchmarks and outperform existing FSM encoding schemes/frameworks in terms of security and overhead.</li>
</ul>

<h3>Title: ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather Condition by Unified Image-Adaptive Processing</h3>
<ul>
<li><strong>Authors: </strong>Yuka Ogino, Yuho Shoji, Takahiro Toizumi, Atsushi Ito</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02799">https://arxiv.org/abs/2411.02799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02799">https://arxiv.org/pdf/2411.02799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02799]] ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather Condition by Unified Image-Adaptive Processing(https://arxiv.org/abs/2411.02799)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a B√©zier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation. We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions.</li>
</ul>

<h3>Title: NinjaDoH: A Censorship-Resistant Moving Target DoH Server Using Hyperscalers and IPNS</h3>
<ul>
<li><strong>Authors: </strong>Scott Seidenberger, Marc Beret, Raveen Wijewickrama, Murtuza Jadliwala, Anindya Maiti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02805">https://arxiv.org/abs/2411.02805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02805">https://arxiv.org/pdf/2411.02805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02805]] NinjaDoH: A Censorship-Resistant Moving Target DoH Server Using Hyperscalers and IPNS(https://arxiv.org/abs/2411.02805)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>We introduce NinjaDoH, a novel DNS over HTTPS (DoH) protocol that leverages the InterPlanetary Name System (IPNS), along with public cloud infrastructure, to create a censorship-resistant moving target DoH service. NinjaDoH is specifically designed to evade traditional censorship methods that involve blocking DoH servers by IP addresses or domains by continually altering the server's network identifiers, significantly increasing the complexity of effectively censoring NinjaDoH traffic without disruption of other web traffic. We also present an analysis that quantifies the DNS query latency and financial costs of running our implementation of this protocol as a service. Further tests assess the ability of NinjaDoH to elude detection mechanisms, including both commercial firewall products and advanced machine learning-based detection systems. The results broadly support NinjaDoH's efficacy as a robust, moving target DNS solution that can ensure continuous and secure internet access in environments with heavy DNS-based censorship.</li>
</ul>

<h3>Title: Query-Efficient Adversarial Attack Against Vertical Federated Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinyin Chen, Wenbo Mu, Luxin Zhang, Guohan Huang, Haibin Zheng, Yao Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02809">https://arxiv.org/abs/2411.02809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02809">https://arxiv.org/pdf/2411.02809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02809]] Query-Efficient Adversarial Attack Against Vertical Federated Graph Learning(https://arxiv.org/abs/2411.02809)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal, federate</a></li>
<li><strong>Abstract: </strong>Graph neural network (GNN) has captured wide attention due to its capability of graph representation learning for graph-structured data. However, the distributed data silos limit the performance of GNN. Vertical federated learning (VFL), an emerging technique to process distributed data, successfully makes GNN possible to handle the distributed graph-structured data. Despite the prosperous development of vertical federated graph learning (VFGL), the robustness of VFGL against the adversarial attack has not been explored yet. Although numerous adversarial attacks against centralized GNNs are proposed, their attack performance is challenged in the VFGL scenario. To the best of our knowledge, this is the first work to explore the adversarial attack against VFGL. A query-efficient hybrid adversarial attack framework is proposed to significantly improve the centralized adversarial attacks against VFGL, denoted as NA2, short for Neuron-based Adversarial Attack. Specifically, a malicious client manipulates its local training data to improve its contribution in a stealthy fashion. Then a shadow model is established based on the manipulated data to simulate the behavior of the server model in VFGL. As a result, the shadow model can improve the attack success rate of various centralized attacks with a few queries. Extensive experiments on five real-world benchmarks demonstrate that NA2 improves the performance of the centralized adversarial attacks against VFGL, achieving state-of-the-art performance even under potential adaptive defense where the defender knows the attack method. Additionally, we provide interpretable experiments of the effectiveness of NA2 via sensitive neurons identification and visualization of t-SNE.</li>
</ul>

<h3>Title: Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Jalali, Azim Ospanov, Amin Gohari, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02817">https://arxiv.org/abs/2411.02817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02817">https://arxiv.org/pdf/2411.02817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02817]] Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models(https://arxiv.org/abs/2411.02817)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-conditioned generation models are commonly evaluated based on the quality of the generated data and its alignment with the input text prompt. On the other hand, several applications of prompt-based generative models require sufficient diversity in the generated data to ensure the models' capability of generating image and video samples possessing a variety of features. However, most existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity arising from variations in text prompts and that contributed by the generative model itself. In this work, our goal is to quantify the prompt-induced and model-induced diversity in samples generated by prompt-based models. We propose an information-theoretic approach for internal diversity quantification, where we decompose the kernel-based entropy $H(X)$ of the generated data $X$ into the sum of the conditional entropy $H(X|T)$, given text variable $T$, and the mutual information $I(X; T)$ between the text and data variables. We introduce the \emph{Conditional-Vendi} score based on $H(X|T)$ to quantify the internal diversity of the model and the \emph{Information-Vendi} score based on $I(X; T)$ to measure the statistical relevance between the generated data and text prompts. We provide theoretical results to statistically interpret these scores and relate them to the unconditional Vendi score. We conduct several numerical experiments to show the correlation between the Conditional-Vendi score and the internal diversity of text-conditioned generative models. The codebase is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: LiVOS: Light Video Object Segmentation with Gated Linear Matching</h3>
<ul>
<li><strong>Authors: </strong>Qin Liu, Jianfeng Wang, Zhengyuan Yang, Linjie Li, Kevin Lin, Marc Niethammer, Lijuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02818">https://arxiv.org/abs/2411.02818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02818">https://arxiv.org/pdf/2411.02818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02818]] LiVOS: Light Video Object Segmentation with Gated Linear Matching(https://arxiv.org/abs/2411.02818)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention. However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase. To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard. Experiments on diverse benchmarks demonstrated the effectiveness of our method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU--a previously cost-prohibitive capability--opening the door for long and high-resolution video foundation models.</li>
</ul>

<h3>Title: Mixtures of In-Context Learners</h3>
<ul>
<li><strong>Authors: </strong>Giwon Hong, Emile van Krieken, Edoardo Ponti, Nikolay Malkin, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02830">https://arxiv.org/abs/2411.02830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02830">https://arxiv.org/pdf/2411.02830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02830]] Mixtures of In-Context Learners(https://arxiv.org/abs/2411.02830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs, exhausting the memory. As a solution, we propose Mixtures of In-Context Learners (MoICL), a novel approach to treat subsets of demonstrations as experts and learn a weighting function to merge their output distributions based on a training set. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (up to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can filter these out from datasets. Overall, MoICL is a more expressive approach to learning from demonstrations without exhausting the context window or memory.</li>
</ul>

<h3>Title: PersianRAG: A Retrieval-Augmented Generation System for Persian Language</h3>
<ul>
<li><strong>Authors: </strong>Hossein Hosseini, Mohammad Siobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02832">https://arxiv.org/abs/2411.02832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02832">https://arxiv.org/pdf/2411.02832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02832]] PersianRAG: A Retrieval-Augmented Generation System for Persian Language(https://arxiv.org/abs/2411.02832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) models, which integrate large-scale pre-trained generative models with external retrieval mechanisms, have shown significant success in various natural language processing (NLP) tasks. However, applying RAG models in Persian language as a low-resource language, poses distinct challenges. These challenges primarily involve the preprocessing, embedding, retrieval, prompt construction, language modeling, and response evaluation of the system. In this paper, we address the challenges towards implementing a real-world RAG system for Persian language called PersianRAG. We propose novel solutions to overcome these obstacles and evaluate our approach using several Persian benchmark datasets. Our experimental results demonstrate the capability of the PersianRAG framework to enhance question answering task in Persian.</li>
</ul>

<h3>Title: On the Comparison between Multi-modal and Single-modal Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Andi Han, Yongqiang Chen, Yuan Cao, Zhiqiang Xu, Taiji Suzuki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02837">https://arxiv.org/abs/2411.02837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02837">https://arxiv.org/pdf/2411.02837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02837]] On the Comparison between Multi-modal and Single-modal Contrastive Learning(https://arxiv.org/abs/2411.02837)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.</li>
</ul>

<h3>Title: Test-Time Dynamic Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Bing Cao, Yinan Xia, Yi Ding, Changqing Zhang, Qinghua Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02840">https://arxiv.org/abs/2411.02840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02840">https://arxiv.org/pdf/2411.02840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02840]] Test-Time Dynamic Image Fusion(https://arxiv.org/abs/2411.02840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the Relative Dominability (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at this https URL.</li>
</ul>

<h3>Title: ADOPT: Modified Adam Can Converge with Any $\beta_2$ with the Optimal Rate</h3>
<ul>
<li><strong>Authors: </strong>Shohei Taniguchi, Keno Harada, Gouki Minegishi, Yuta Oshima, Seong Cheol Jeong, Go Nagahara, Tomoshi Iiyama, Masahiro Suzuki, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02853">https://arxiv.org/abs/2411.02853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02853">https://arxiv.org/pdf/2411.02853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02853]] ADOPT: Modified Adam Can Converge with Any $\beta_2$ with the Optimal Rate(https://arxiv.org/abs/2411.02853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\beta_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\mathcal{O} ( 1 / \sqrt{T} )$ with any choice of $\beta_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at this https URL.</li>
</ul>

<h3>Title: OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing</h3>
<ul>
<li><strong>Authors: </strong>Pranav Gupta, Rishubh Singh, Pradeep Shenoy, Ravikiran Sarvadevabhatla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02858">https://arxiv.org/abs/2411.02858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02858">https://arxiv.org/pdf/2411.02858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02858]] OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing(https://arxiv.org/abs/2411.02858)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-object multi-part scene segmentation is a challenging task whose complexity scales exponentially with part granularity and number of scene objects. To address the task, we propose a plug-and-play approach termed OLAF. First, we augment the input (RGB) with channels containing object-based structural cues (fg/bg mask, boundary edge mask). We propose a weight adaptation technique which enables regular (RGB) pre-trained models to process the augmented (5-channel) input in a stable manner during optimization. In addition, we introduce an encoder module termed LDF to provide low-level dense feature guidance. This assists segmentation, particularly for smaller parts. OLAF enables significant mIoU gains of $\mathbf{3.3}$ (Pascal-Parts-58), $\mathbf{3.5}$ (Pascal-Parts-108) over the SOTA model. On the most challenging variant (Pascal-Parts-201), the gain is $\mathbf{4.0}$. Experimentally, we show that OLAF's broad applicability enables gains across multiple architectures (CNN, U-Net, Transformer) and datasets. The code is available at this http URL</li>
</ul>

<h3>Title: Continual Audio-Visual Sound Separation</h3>
<ul>
<li><strong>Authors: </strong>Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02860">https://arxiv.org/abs/2411.02860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02860">https://arxiv.org/pdf/2411.02860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02860]] Continual Audio-Visual Sound Separation(https://arxiv.org/abs/2411.02860)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (\textbf{Cont}inual \textbf{A}udio-\textbf{V}isual Sound \textbf{Sep}aration). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: \url{this https URL}.</li>
</ul>

<h3>Title: Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02864">https://arxiv.org/abs/2411.02864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02864">https://arxiv.org/pdf/2411.02864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02864]] Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning(https://arxiv.org/abs/2411.02864)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning capability on many NLP tasks. Recasting an NLP task into a text-to-text generation task is a common practice so that generative LLMs can be prompted to resolve it. However, performing document-level relation extraction (DocRE) tasks with generative LLM models is still challenging due to the structured output format of DocRE, which complicates the conversion to plain text. Limited information available in few-shot samples and prompt instructions induce further difficulties and challenges in relation extraction for mentioned entities in a document. In this paper, we represent the structured output as a graph-style triplet rather than natural language expressions and leverage generative LLMs for the DocRE task. Our approach, the Graph-DPEP framework is grounded in the reasoning behind triplet explanation thoughts presented in natural language. In this framework, we first introduce a ``decomposed-plug" method for performing the generation from LLMs over prompts with type-space decomposition to alleviate the burden of distinguishing all relation types. Second, we employ a verifier for calibrating the generation and identifying overlooked query entity pairs. Third, we develop "ensemble-play", reapplying generation on the entire type list by leveraging the reasoning thoughts embedded in a sub-graph associated with the missing query pair to address the missingness issue. Through extensive comparisons with existing prompt techniques and alternative Language Models (LLMs), our framework demonstrates superior performance on publicly available benchmarks in experiments.</li>
</ul>

<h3>Title: Enhancing Adversarial Robustness via Uncertainty-Aware Distributional Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Junhao Dong, Xinghua Qu, Z. Jane Wang, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02871">https://arxiv.org/abs/2411.02871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02871">https://arxiv.org/pdf/2411.02871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02871]] Enhancing Adversarial Robustness via Uncertainty-Aware Distributional Adversarial Training(https://arxiv.org/abs/2411.02871)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite remarkable achievements in deep learning across various domains, its inherent vulnerability to adversarial examples still remains a critical concern for practical deployment. Adversarial training has emerged as one of the most effective defensive techniques for improving model robustness against such malicious inputs. However, existing adversarial training schemes often lead to limited generalization ability against underlying adversaries with diversity due to their overreliance on a point-by-point augmentation strategy by mapping each clean example to its adversarial counterpart during training. In addition, adversarial examples can induce significant disruptions in the statistical information w.r.t. the target model, thereby introducing substantial uncertainty and challenges to modeling the distribution of adversarial examples. To circumvent these issues, in this paper, we propose a novel uncertainty-aware distributional adversarial training method, which enforces adversary modeling by leveraging both the statistical information of adversarial examples and its corresponding uncertainty estimation, with the goal of augmenting the diversity of adversaries. Considering the potentially negative impact induced by aligning adversaries to misclassified clean examples, we also refine the alignment reference based on the statistical proximity to clean examples during adversarial training, thereby reframing adversarial training within a distribution-to-distribution matching framework interacted between the clean and adversarial domains. Furthermore, we design an introspective gradient alignment approach via matching input gradients between these domains without introducing external models. Extensive experiments across four benchmark datasets and various network architectures demonstrate that our approach achieves state-of-the-art adversarial robustness and maintains natural performance.</li>
</ul>

<h3>Title: TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</h3>
<ul>
<li><strong>Authors: </strong>Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02886">https://arxiv.org/abs/2411.02886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02886">https://arxiv.org/pdf/2411.02886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02886]] TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection(https://arxiv.org/abs/2411.02886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28x acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.</li>
</ul>

<h3>Title: Membership Inference Attacks against Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02902">https://arxiv.org/abs/2411.02902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02902">https://arxiv.org/pdf/2411.02902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02902]] Membership Inference Attacks against Large Vision-Language Models(https://arxiv.org/abs/2411.02902)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR√©nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Photon: Federated LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Sani, Alex Iacob, Zeyu Cao, Royson Lee, Bill Marino, Yan Gao, Dongqi Cai, Zexi Li, Wanru Zhao, Xinchi Qiu, Nicholas D. Lane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02908">https://arxiv.org/abs/2411.02908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02908">https://arxiv.org/pdf/2411.02908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02908]] Photon: Federated LLM Pre-Training(https://arxiv.org/abs/2411.02908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Scaling large language models (LLMs) demands extensive data and computing resources, which are traditionally constrained to data centers by the high-bandwidth requirements of distributed training. Low-bandwidth methods like federated learning (FL) could enable collaborative training of larger models across weakly-connected GPUs if they can effectively be used for pre-training. To achieve this, we introduce Photon, the first complete system for federated end-to-end LLM training, leveraging cross-silo FL for global-scale training with minimal communication overheads. Using Photon, we train the first federated family of decoder-only LLMs from scratch. We show that: (1) Photon can train model sizes up to 7B in a federated fashion while reaching an even better perplexity than centralized pre-training; (2) Photon model training time decreases with available compute, achieving a similar compute-time trade-off to centralized; and (3) Photon outperforms the wall-time of baseline distributed training methods by 35% via communicating 64x-512xless. Our proposal is robust to data heterogeneity and converges twice as fast as previous methods like DiLoCo. This surprising data efficiency stems from a unique approach combining small client batch sizes with extremely high learning rates, enabled by federated averaging's robustness to hyperparameters. Photon thus represents the first economical system for global internet-wide LLM pre-training.</li>
</ul>

<h3>Title: Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Pengkun Jiao, Na Zhao, Jingjing Chen, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02920">https://arxiv.org/abs/2411.02920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02920">https://arxiv.org/pdf/2411.02920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02920]] Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization(https://arxiv.org/abs/2411.02920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.</li>
</ul>

<h3>Title: Theoretically Guaranteed Distribution Adaptable Learning</h3>
<ul>
<li><strong>Authors: </strong>Chao Xu, Xijia Tang, Guoqing Liu, Yuhua Qian, Chenping Hou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02921">https://arxiv.org/abs/2411.02921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02921">https://arxiv.org/pdf/2411.02921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02921]] Theoretically Guaranteed Distribution Adaptable Learning(https://arxiv.org/abs/2411.02921)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In many open environment applications, data are collected in the form of a stream, which exhibits an evolving distribution over time. How to design algorithms to track these evolving data distributions with provable guarantees, particularly in terms of the generalization ability, remains a formidable challenge. To handle this crucial but rarely studied problem and take a further step toward robust artificial intelligence, we propose a novel framework called Distribution Adaptable Learning (DAL). It enables the model to effectively track the evolving data distributions. By Encoding Feature Marginal Distribution Information (EFMDI), we broke the limitations of optimal transport to characterize the environmental changes and enable model reuse across diverse data distributions. It can enhance the reusable and evolvable properties of DAL in accommodating evolving distributions. Furthermore, to obtain the model interpretability, we not only analyze the generalization error bound of the local step in the evolution process, but also investigate the generalization error bound associated with the entire classifier trajectory of the evolution based on the Fisher-Rao distance. For demonstration, we also present two special cases within the framework, together with their optimizations and convergence analyses. Experimental results over both synthetic and real-world data distribution evolving tasks validate the effectiveness and practical utility of the proposed framework.</li>
</ul>

<h3>Title: Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering</h3>
<ul>
<li><strong>Authors: </strong>Fabrianne Effendi, Anupam Chattopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02926">https://arxiv.org/abs/2411.02926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02926">https://arxiv.org/pdf/2411.02926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02926]] Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering(https://arxiv.org/abs/2411.02926)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions. Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks. However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy. This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance. Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data. Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML. The research contributes two key privacy-preserving pipelines. First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored. Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible. Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed. Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings. On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%. The research highlights the need to balance the trade-off between privacy and computational efficiency.</li>
</ul>

<h3>Title: Textual Aesthetics in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lingjie Jiang, Shaohan Huang, Xun Wu, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02930">https://arxiv.org/abs/2411.02930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02930">https://arxiv.org/pdf/2411.02930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02930]] Textual Aesthetics in Large Language Models(https://arxiv.org/abs/2411.02930)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses. Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content. In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TexAes. We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness. Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively. Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Anera-hard.</li>
</ul>

<h3>Title: Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep Learning and Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Kakooei, James Bailie, Albin S√∂derberg, Albin Becevic, Adel Daoud</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02935">https://arxiv.org/abs/2411.02935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02935">https://arxiv.org/pdf/2411.02935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02935]] Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep Learning and Satellite Imagery(https://arxiv.org/abs/2411.02935)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate Land Use and Land Cover (LULC) maps are essential for understanding the drivers of sustainable development, in terms of its complex interrelationships between human activities and natural resources. However, existing LULC maps often lack precise urban and rural classifications, particularly in diverse regions like Africa. This study presents a novel construction of a high-resolution rural-urban map using deep learning techniques and satellite imagery. We developed a deep learning model based on the DeepLabV3 architecture, which was trained on satellite imagery from Landsat-8 and the ESRI LULC dataset, augmented with human settlement data from the GHS-SMOD. The model utilizes semantic segmentation to classify land into detailed categories, including urban and rural areas, at a 10-meter resolution. Our findings demonstrate that incorporating LULC along with urban and rural classifications significantly enhances the model's ability to accurately distinguish between urban, rural, and non-human settlement areas. Therefore, our maps can support more informed decision-making for policymakers, researchers, and stakeholders. We release a continent wide urban-rural map, covering the period 2016 and 2022.</li>
</ul>

<h3>Title: Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent</h3>
<ul>
<li><strong>Authors: </strong>Yangning Li, Yinghui Li, Xingyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S. Yu, Fei Huang, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02937">https://arxiv.org/abs/2411.02937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02937">https://arxiv.org/pdf/2411.02937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02937]] Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent(https://arxiv.org/abs/2411.02937)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the "hallucination" issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws cannot be adequately reflected by current knowledge-seeking visual question answering (VQA) datasets, since the most required knowledge can be readily obtained with a standard two-step retrieval. To bridge the dataset gap, we first construct Dyn-VQA dataset, consisting of three types of "dynamic" questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes. Hence, we further propose the first self-adaptive planning agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. Extensive experiments prove the effectiveness of our OmniSearch, also provide direction for advancing mRAG. The code and dataset will be open-sourced at this https URL.</li>
</ul>

<h3>Title: A Mamba Foundation Model for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Ma, Yushu Chen, Wenlai Zhao, Jinzhe Yang, Yingsheng Ji, Xinghua Xu, Xiaozhu Liu, Hao Jing, Shengzhuo Liu, Guangwen Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02941">https://arxiv.org/abs/2411.02941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02941">https://arxiv.org/pdf/2411.02941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02941]] A Mamba Foundation Model for Time Series Forecasting(https://arxiv.org/abs/2411.02941)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series foundation models have demonstrated strong performance in zero-shot learning, making them well-suited for predicting rapidly evolving patterns in real-world applications where relevant training data are scarce. However, most of these models rely on the Transformer architecture, which incurs quadratic complexity as input length increases. To address this, we introduce TSMamba, a linear-complexity foundation model for time series forecasting built on the Mamba architecture. The model captures temporal dependencies through both forward and backward Mamba encoders, achieving high prediction accuracy. To reduce reliance on large datasets and lower training costs, TSMamba employs a two-stage transfer learning process that leverages pretrained Mamba LLMs, allowing effective time series modeling with a moderate training set. In the first stage, the forward and backward backbones are optimized via patch-wise autoregressive prediction; in the second stage, the model trains a prediction head and refines other components for long-term forecasting. While the backbone assumes channel independence to manage varying channel numbers across datasets, a channel-wise compressed attention module is introduced to capture cross-channel dependencies during fine-tuning on specific multivariate datasets. Experiments show that TSMamba's zero-shot performance is comparable to state-of-the-art time series foundation models, despite using significantly less training data. It also achieves competitive or superior full-shot performance compared to task-specific prediction models. The code will be made publicly available.</li>
</ul>

<h3>Title: Capturing research literature attitude towards Sustainable Development Goals: an LLM-based topic modeling approach</h3>
<ul>
<li><strong>Authors: </strong>Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02943">https://arxiv.org/abs/2411.02943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02943">https://arxiv.org/pdf/2411.02943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02943]] Capturing research literature attitude towards Sustainable Development Goals: an LLM-based topic modeling approach(https://arxiv.org/abs/2411.02943)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet. The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030. Natural language processing techniques can help uncover discussions on SDGs within research literature. We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction. For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets. We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution. Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process. Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span. All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents.</li>
</ul>

<h3>Title: Time-Causal VAE: Robust Financial Time Series Generator</h3>
<ul>
<li><strong>Authors: </strong>Beatrice Acciaio, Stephan Eckstein, Songyan Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02947">https://arxiv.org/abs/2411.02947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02947">https://arxiv.org/pdf/2411.02947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02947]] Time-Causal VAE: Robust Financial Time Series Generator(https://arxiv.org/abs/2411.02947)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We build a time-causal variational autoencoder (TC-VAE) for robust generation of financial time series data. Our approach imposes a causality constraint on the encoder and decoder networks, ensuring a causal transport from the real market time series to the fake generated time series. Specifically, we prove that the TC-VAE loss provides an upper bound on the causal Wasserstein distance between market distributions and generated distributions. Consequently, the TC-VAE loss controls the discrepancy between optimal values of various dynamic stochastic optimization problems under real and generated distributions. To further enhance the model's ability to approximate the latent representation of the real market distribution, we integrate a RealNVP prior into the TC-VAE framework. Finally, extensive numerical experiments show that TC-VAE achieves promising results on both synthetic and real market data. This is done by comparing real and generated distributions according to various statistical distances, demonstrating the effectiveness of the generated data for downstream financial optimization tasks, as well as showcasing that the generated data reproduces stylized facts of real financial market data.</li>
</ul>

<h3>Title: A scalable generative model for dynamical system reconstruction from neuroimaging data</h3>
<ul>
<li><strong>Authors: </strong>Eric Volkmann, Alena Br√§ndle, Daniel Durstewitz, Georgia Koppe</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, nlin.CD, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02949">https://arxiv.org/abs/2411.02949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02949">https://arxiv.org/pdf/2411.02949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02949]] A scalable generative model for dynamical system reconstruction from neuroimaging data(https://arxiv.org/abs/2411.02949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal's filtering properties, as common in neuroscience (and physiology more generally). Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data. Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods. Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.</li>
</ul>

<h3>Title: IMUDiffusion: A Diffusion Model for Multivariate Time Series Synthetisation for Inertial Motion Capturing Systems</h3>
<ul>
<li><strong>Authors: </strong>Heiko Oppel, Michael Munz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02954">https://arxiv.org/abs/2411.02954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02954">https://arxiv.org/pdf/2411.02954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02954]] IMUDiffusion: A Diffusion Model for Multivariate Time Series Synthetisation for Inertial Motion Capturing Systems(https://arxiv.org/abs/2411.02954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Kinematic sensors are often used to analyze movement behaviors in sports and daily activities due to their ease of use and lack of spatial restrictions, unlike video-based motion capturing systems. Still, the generation, and especially the labeling of motion data for specific activities can be time-consuming and costly. Additionally, many models struggle with limited data, which limits their performance in recognizing complex movement patterns. To address those issues, generating synthetic data can help expand the diversity and variability. In this work, we propose IMUDiffusion, a probabilistic diffusion model specifically designed for multivariate time series generation. Our approach enables the generation of high-quality time series sequences which accurately capture the dynamics of human activities. Moreover, by joining our dataset with synthetic data, we achieve a significant improvement in the performance of our baseline human activity classifier. In some cases, we are able to improve the macro F1-score by almost 30%. IMUDiffusion provides a valuable tool for generating realistic human activity movements and enhance the robustness of models in scenarios with limited training data.</li>
</ul>

<h3>Title: Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02969">https://arxiv.org/abs/2411.02969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02969">https://arxiv.org/pdf/2411.02969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02969]] Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation(https://arxiv.org/abs/2411.02969)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times. We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR. We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.</li>
</ul>

<h3>Title: [Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Maren Pielka, Tobias Schneider, Jan Terheyden, Rafet Sifa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02973">https://arxiv.org/abs/2411.02973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02973">https://arxiv.org/pdf/2411.02973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02973]] [Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI(https://arxiv.org/abs/2411.02973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application. The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options. Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment. Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status. The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment. The approach needs to be further validated using a survey and a clinical study.</li>
</ul>

<h3>Title: Region-Guided Attack on the Segment Anything Model (SAM)</h3>
<ul>
<li><strong>Authors: </strong>Xiaoliang Liu, Furao Shen, Jian Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02974">https://arxiv.org/abs/2411.02974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02974">https://arxiv.org/pdf/2411.02974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02974]] Region-Guided Attack on the Segment Anything Model (SAM)(https://arxiv.org/abs/2411.02974)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) is a cornerstone of image segmentation, demonstrating exceptional performance across various applications, particularly in autonomous driving and medical imaging, where precise segmentation is crucial. However, SAM is vulnerable to adversarial attacks that can significantly impair its functionality through minor input perturbations. Traditional techniques, such as FGSM and PGD, are often ineffective in segmentation tasks due to their reliance on global perturbations that overlook spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address these challenges, but they frequently depend on external cues and do not fully leverage the structural interdependencies within segmentation processes. This limitation underscores the need for a novel adversarial strategy that exploits the unique characteristics of segmentation tasks. In response, we introduce the Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted perturbations that fragment large segments and expand smaller ones, resulting in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves high success rates in both white-box and black-box scenarios, emphasizing the need for robust defenses against such sophisticated attacks. RGA not only reveals SAM's vulnerabilities but also lays the groundwork for developing more resilient defenses against adversarial threats in image segmentation.</li>
</ul>

<h3>Title: Growing a Tail: Increasing Output Diversity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michal Shur-Ofry, Bar Horowitz-Amsalem, Adir Rahamim, Yonatan Belinkov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.02989">https://arxiv.org/abs/2411.02989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.02989">https://arxiv.org/pdf/2411.02989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.02989]] Growing a Tail: Increasing Output Diversity in Large Language Models(https://arxiv.org/abs/2411.02989)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How diverse are the outputs of large language models when diversity is desired? We examine the diversity of responses of various models to questions with multiple possible answers, comparing them with human responses. Our findings suggest that models' outputs are highly concentrated, reflecting a narrow, mainstream 'worldview', in comparison to humans, whose responses exhibit a much longer-tail. We examine three ways to increase models' output diversity: 1) increasing generation randomness via temperature sampling; 2) prompting models to answer from diverse perspectives; 3) aggregating outputs from several models. A combination of these measures significantly increases models' output diversity, reaching that of humans. We discuss implications of these findings for AI policy that wishes to preserve cultural diversity, an essential building block of a democratic social fabric.</li>
</ul>

<h3>Title: Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status</h3>
<ul>
<li><strong>Authors: </strong>Samuel Lee, Zach Wood-Doughty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03004">https://arxiv.org/abs/2411.03004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03004">https://arxiv.org/pdf/2411.03004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03004]] Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status(https://arxiv.org/abs/2411.03004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Causal understanding is a fundamental goal of evidence-based medicine. When randomization is impossible, causal inference methods allow the estimation of treatment effects from retrospective analysis of observational data. However, such analyses rely on a number of assumptions, often including that of no unobserved confounding. In many practical settings, this assumption is violated when important variables are not explicitly measured in the clinical record. Prior work has proposed to address unobserved confounding with machine learning by imputing unobserved variables and then correcting for the classifier's mismeasurement. When such a classifier can be trained and the necessary assumptions are met, this method can recover an unbiased estimate of a causal effect. However, such work has been limited to synthetic data, simple classifiers, and binary variables. This paper extends this methodology by using a large language model trained on clinical notes to predict patients' smoking status, which would otherwise be an unobserved confounder. We then apply a measurement error correction on the categorical predicted smoking status to estimate the causal effect of transthoracic echocardiography on mortality in the MIMIC dataset.</li>
</ul>

<h3>Title: Hierarchical Orchestra of Policies</h3>
<ul>
<li><strong>Authors: </strong>Thomas P Cannon, √ñzg√ºr Simsek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03008">https://arxiv.org/abs/2411.03008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03008">https://arxiv.org/pdf/2411.03008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03008]] Hierarchical Orchestra of Policies(https://arxiv.org/abs/2411.03008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual reinforcement learning poses a major challenge due to the tendency of agents to experience catastrophic forgetting when learning sequential tasks. In this paper, we introduce a modularity-based approach, called Hierarchical Orchestra of Policies (HOP), designed to mitigate catastrophic forgetting in lifelong reinforcement learning. HOP dynamically forms a hierarchy of policies based on a similarity metric between the current observations and previously encountered observations in successful tasks. Unlike other state-of-the-art methods, HOP does not require task labelling, allowing for robust adaptation in environments where boundaries between tasks are ambiguous. Our experiments, conducted across multiple tasks in a procedurally generated suite of environments, demonstrate that HOP significantly outperforms baseline methods in retaining knowledge across tasks and performs comparably to state-of-the-art transfer methods that require task labelling. Moreover, HOP achieves this without compromising performance when tasks remain constant, highlighting its versatility.</li>
</ul>

<h3>Title: Leveraging Large Language Models in Code Question Answering: Baselines and Issues</h3>
<ul>
<li><strong>Authors: </strong>Georgy Andryushchenko, Vladimir Ivanov, Vladimir Makharev, Elizaveta Tukhtina, Aidar Valeev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03012">https://arxiv.org/abs/2411.03012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03012">https://arxiv.org/pdf/2411.03012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03012]] Leveraging Large Language Models in Code Question Answering: Baselines and Issues(https://arxiv.org/abs/2411.03012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at this https URL.</li>
</ul>

<h3>Title: CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jisong Kim, Minjae Seong, Jun Won Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03013">https://arxiv.org/abs/2411.03013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03013">https://arxiv.org/pdf/2411.03013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03013]] CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for 3D Object Detection(https://arxiv.org/abs/2411.03013)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird's-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection.</li>
</ul>

<h3>Title: FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Isaac Baglin, Xiatian Zhu, Simon Hadfield</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03019">https://arxiv.org/abs/2411.03019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03019">https://arxiv.org/pdf/2411.03019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03019]] FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses(https://arxiv.org/abs/2411.03019)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local. Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage. While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios. This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context. By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states. This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses.</li>
</ul>

<h3>Title: Testing Generalizability in Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Daniel de Vassimon Manela, Linying Yang, Robin J. Evans</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03021">https://arxiv.org/abs/2411.03021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03021">https://arxiv.org/pdf/2411.03021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03021]] Testing Generalizability in Causal Inference(https://arxiv.org/abs/2411.03021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges. However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference. Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability. To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings. Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods. By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets. Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics. Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications.</li>
</ul>

<h3>Title: Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras</h3>
<ul>
<li><strong>Authors: </strong>Roberto Ria√±o, Gorka Abad, Stjepan Picek, Aitor Urbieta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03022">https://arxiv.org/abs/2411.03022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03022">https://arxiv.org/pdf/2411.03022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03022]] Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras(https://arxiv.org/abs/2411.03022)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>While security vulnerabilities in traditional Deep Neural Networks (DNNs) have been extensively studied, the susceptibility of Spiking Neural Networks (SNNs) to adversarial attacks remains mostly underexplored. Until now, the mechanisms to inject backdoors into SNN models have been limited to digital scenarios; thus, we present the first evaluation of backdoor attacks in real-world environments. We begin by assessing the applicability of existing digital backdoor attacks and identifying their limitations for deployment in physical environments. To address each of the found limitations, we present three novel backdoor attack methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess the effectiveness of traditional backdoor procedures and defenses adapted for SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that while these procedures and defenses can mitigate some attacks, they often fail against stronger methods like Flashy Backdoor or sacrifice too much clean accuracy, rendering the models unusable. Overall, all our methods can achieve up to a 100% Attack Success Rate while maintaining high clean accuracy in every tested dataset. Additionally, we evaluate the stealthiness of the triggers with commonly used metrics, finding them highly stealthy. Thus, we propose new alternatives more suited for identifying poisoned samples in these scenarios. Our results show that further research is needed to ensure the security of SNN-based systems against backdoor attacks and their safe application in real-world scenarios. The code, experiments, and results are available in our repository.</li>
</ul>

<h3>Title: Rethinking Decoders for Transformer-based Semantic Segmentation: Compression is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Qishuai Wen, Chun-Guang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03033">https://arxiv.org/abs/2411.03033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03033">https://arxiv.org/pdf/2411.03033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03033]] Rethinking Decoders for Transformer-based Semantic Segmentation: Compression is All You Need(https://arxiv.org/abs/2411.03033)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>State-of-the-art methods for Transformer-based semantic segmentation typically adopt Transformer decoders that are used to extract additional embeddings from image embeddings via cross-attention, refine either or both types of embeddings via self-attention, and project image embeddings onto the additional embeddings via dot-product. Despite their remarkable success, these empirical designs still lack theoretical justifications or interpretations, thus hindering potentially principled improvements. In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA). From such a perspective, we derive a white-box, fully attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the interpretations as follows: 1) the self-attention operator refines image embeddings to construct an ideal principal subspace that aligns with the supervision and retains most information; 2) the cross-attention operator seeks to find a low-rank approximation of the refined image embeddings, which is expected to be a set of orthonormal bases of the principal subspace and corresponds to the predefined classes; 3) the dot-product operation yields compact representation for image embeddings as segmentation masks. Experiments conducted on dataset ADE20K find that DEPICT consistently outperforms its black-box counterpart, Segmenter, and it is light weight and more robust.</li>
</ul>

<h3>Title: Can Transformers Smell Like Humans?</h3>
<ul>
<li><strong>Authors: </strong>Farzaneh Taleb, Miguel Vasco, Ant√¥nio H. Ribeiro, M√•rten Bj√∂rkman, Danica Kragic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03038">https://arxiv.org/abs/2411.03038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03038">https://arxiv.org/pdf/2411.03038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03038]] Can Transformers Smell Like Humans?(https://arxiv.org/abs/2411.03038)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The human brain encodes stimuli from the environment into representations that form a sensory perception of the world. Despite recent advances in understanding visual and auditory perception, olfactory perception remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels of human olfactory perception. In this work, we ask the question of whether pre-trained transformer models of chemical structures encode representations that are aligned with human olfactory perception, i.e., can transformers smell like humans? We demonstrate that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception. We use multiple datasets and different types of perceptual representations to show that the representations encoded by transformer models are able to predict: (i) labels associated with odorants provided by experts; (ii) continuous ratings provided by human participants with respect to pre-defined descriptors; and (iii) similarity ratings between odorants provided by human participants. Finally, we evaluate the extent to which this alignment is associated with physicochemical features of odorants known to be relevant for olfactory decoding.</li>
</ul>

<h3>Title: Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for Semi-supervised Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Zhang Qixiang, Yang Yuxiang, Zu Chen, Zhang Jianjia, Wu Xi, Zhou Jiliu, Wang Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03041">https://arxiv.org/abs/2411.03041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03041">https://arxiv.org/pdf/2411.03041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03041]] Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for Semi-supervised Medical Image Classification(https://arxiv.org/abs/2411.03041)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) is a popular solution to alleviate the high annotation cost in medical image classification. As a main branch of SSL, consistency regularization engages in imposing consensus between the predictions of a single sample from different views, termed as Absolute Location consistency (AL-c). However, only AL-c may be insufficient. Just like when diagnosing a case in practice, besides the case itself, the doctor usually refers to certain related trustworthy cases to make more reliable this http URL, we argue that solely relying on AL-c may ignore the relative differences across samples, which we interpret as relative locations, and only exploit limited information from one perspective. To address this issue, we propose a Sample Consistency Mean Teacher (SCMT) which not only incorporates AL c but also additionally enforces consistency between the samples' relative similarities to its related samples, called Relative Location consistency (RL c). AL c and RL c conduct consistency regularization from two different perspectives, jointly extracting more diverse semantic information for classification. On the other hand, due to the highly similar structures in medical images, the sample distribution could be overly dense in feature space, making their relative locations susceptible to noise. To tackle this problem, we further develop a Sample Scatter Mean Teacher (SSMT) by utilizing contrastive learning to sparsify the sample distribution and obtain robust and effective relative locations. Extensive experiments on different datasets demonstrate the superiority of our method.</li>
</ul>

<h3>Title: Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning</h3>
<ul>
<li><strong>Authors: </strong>Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03042">https://arxiv.org/abs/2411.03042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03042">https://arxiv.org/pdf/2411.03042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03042]] Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning(https://arxiv.org/abs/2411.03042)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true ``solution.'' First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consists of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT'14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.</li>
</ul>

<h3>Title: GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details</h3>
<ul>
<li><strong>Authors: </strong>Zhongjin Luo, Haolin Liu, Chenghong Li, Wanghao Du, Zirong Jin, Wanhu Sun, Yinyu Nie, Weikai Chen, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03047">https://arxiv.org/abs/2411.03047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03047">https://arxiv.org/pdf/2411.03047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03047]] GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details(https://arxiv.org/abs/2411.03047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: this https URL</li>
</ul>

<h3>Title: Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising</h3>
<ul>
<li><strong>Authors: </strong>Tao Huang, Jiayang Meng, Hong Chen, Guolong Zheng, Xu Yang, Xun Yi, Hua Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03053">https://arxiv.org/abs/2411.03053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03053">https://arxiv.org/pdf/2411.03053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03053]] Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising(https://arxiv.org/abs/2411.03053)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, diffusion</a></li>
<li><strong>Abstract: </strong>We investigate the construction of gradient-guided conditional diffusion models for reconstructing private images, focusing on the adversarial interplay between differential privacy noise and the denoising capabilities of diffusion models. While current gradient-based reconstruction methods struggle with high-resolution images due to computational complexity and prior knowledge requirements, we propose two novel methods that require minimal modifications to the diffusion model's generation process and eliminate the need for prior knowledge. Our approach leverages the strong image generation capabilities of diffusion models to reconstruct private images starting from randomly generated noise, even when a small amount of differentially private noise has been added to the gradients. We also conduct a comprehensive theoretical analysis of the impact of differential privacy noise on the quality of reconstructed images, revealing the relationship among noise magnitude, the architecture of attacked models, and the attacker's reconstruction capability. Additionally, extensive experiments validate the effectiveness of our proposed methods and the accuracy of our theoretical findings, suggesting new directions for privacy risk auditing using conditional diffusion models.</li>
</ul>

<h3>Title: Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight</h3>
<ul>
<li><strong>Authors: </strong>Tao Huang, Qingyu Huang, Xin Shi, Jiayang Meng, Guolong Zheng, Xu Yang, Xun Yi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03059">https://arxiv.org/abs/2411.03059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03059">https://arxiv.org/pdf/2411.03059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03059]] Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight(https://arxiv.org/abs/2411.03059)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>In the domain of deep learning, the challenge of protecting sensitive data while maintaining model utility is significant. Traditional Differential Privacy (DP) techniques such as Differentially Private Stochastic Gradient Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive gradient clipping. These methods, however, compromise model accuracy due to their critical influence on gradient handling, particularly neglecting the significant contribution of small gradients during later training stages. In this paper, we introduce an enhanced version of DP-SGD, named Differentially Private Per-sample Adaptive Scaling Clipping (DP-PSASC). This approach replaces traditional clipping with non-monotonous adaptive gradient scaling, which alleviates the need for intensive threshold setting and rectifies the disproportionate weighting of smaller gradients. Our contribution is twofold. First, we develop a novel gradient scaling technique that effectively assigns proper weights to gradients, particularly small ones, thus improving learning under differential privacy. Second, we integrate a momentum-based method into DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates. Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy and delivers superior performance across diverse datasets, setting new standards for privacy-sensitive applications.</li>
</ul>

<h3>Title: Alpha and Prejudice: Improving $\alpha$-sized Worst-case Fairness via Intrinsic Reweighting</h3>
<ul>
<li><strong>Authors: </strong>Jing Li, Yinghua Yao, Yuangang Pan, Xuanqian Wang, Ivor W. Tsang, Xiuju Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03068">https://arxiv.org/abs/2411.03068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03068">https://arxiv.org/pdf/2411.03068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03068]] Alpha and Prejudice: Improving $\alpha$-sized Worst-case Fairness via Intrinsic Reweighting(https://arxiv.org/abs/2411.03068)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair</a></li>
<li><strong>Abstract: </strong>Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group. Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation. Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $\alpha$, as side information, referred to as ``$\alpha$-sized worst-case fairness'' in this paper. We first justify the practical significance of this setting by presenting noteworthy evidence from the data privacy perspective, which has been overlooked by existing research. Without imposing specific requirements on loss functions, we propose reweighting the training samples based on their intrinsic importance to fairness. Given the global nature of the worst-case formulation, we further develop a stochastic learning scheme to simplify the training process without compromising model performance. Additionally, we address the issue of outliers and provide a robust variant to handle potential outliers during model training. Our theoretical analysis and experimental observations reveal the connections between the proposed approaches and existing ``fairness-through-reweighting'' studies, with extensive experimental results on fairness benchmarks demonstrating the superiority of our methods.</li>
</ul>

<h3>Title: Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data</h3>
<ul>
<li><strong>Authors: </strong>Irum Mehboob, Li Sun, Alireza Astegarpanah, Rustam Stolkin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03082">https://arxiv.org/abs/2411.03082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03082">https://arxiv.org/pdf/2411.03082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03082]] Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data(https://arxiv.org/abs/2411.03082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.</li>
</ul>

<h3>Title: Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting</h3>
<ul>
<li><strong>Authors: </strong>Adrian B. Ch≈Çopowiec, Adam R. Ch≈Çopowiec, Krzysztof Galus, Wojciech Cebula, Martin Tabakov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03098">https://arxiv.org/abs/2411.03098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03098">https://arxiv.org/pdf/2411.03098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03098]] Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting(https://arxiv.org/abs/2411.03098)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model. We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical image processing techniques further enhances the results.</li>
</ul>

<h3>Title: Pre-trained Visual Dynamics Representations for Efficient Policy Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Luo, Bohan Zhou, Zongqing Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03169">https://arxiv.org/abs/2411.03169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03169">https://arxiv.org/pdf/2411.03169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03169]] Pre-trained Visual Dynamics Representations for Efficient Policy Learning(https://arxiv.org/abs/2411.03169)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem. Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training. To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning. By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations. The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos. This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation. We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning.</li>
</ul>

<h3>Title: On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tariq Berrada Ifriqi, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03177">https://arxiv.org/abs/2411.03177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03177">https://arxiv.org/pdf/2411.03177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03177]] On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models(https://arxiv.org/abs/2411.03177)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i)~the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii)~the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset -- with FID improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23% on 512 resolution.</li>
</ul>

<h3>Title: Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education</h3>
<ul>
<li><strong>Authors: </strong>Steve Goliath, Pitso Tsibolane, Dirk Snyman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03219">https://arxiv.org/abs/2411.03219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03219">https://arxiv.org/pdf/2411.03219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03219]] Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education(https://arxiv.org/abs/2411.03219)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.</li>
</ul>

<h3>Title: Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03223">https://arxiv.org/abs/2411.03223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03223">https://arxiv.org/pdf/2411.03223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03223]] Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation(https://arxiv.org/abs/2411.03223)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.</li>
</ul>

<h3>Title: Topograph: An efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03228">https://arxiv.org/abs/2411.03228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03228">https://arxiv.org/pdf/2411.03228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03228]] Topograph: An efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation(https://arxiv.org/abs/2411.03228)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets. Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.</li>
</ul>

<h3>Title: Formal Logic-guided Robust Federated Learning against Poisoning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Dung Thuy Nguyen, Ziyan An, Taylor T. Johnson, Meiyi Ma, Kevin Leach</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03231">https://arxiv.org/abs/2411.03231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03231">https://arxiv.org/pdf/2411.03231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03231]] Formal Logic-guided Robust Federated Learning against Poisoning Attacks(https://arxiv.org/abs/2411.03231)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27\% in the best-case scenario compared to the second-best baseline. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Enhancing Transformer Training Efficiency with Dynamic Dropout</h3>
<ul>
<li><strong>Authors: </strong>Hanrui Yan, Dan Shao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03236">https://arxiv.org/abs/2411.03236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03236">https://arxiv.org/pdf/2411.03236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03236]] Enhancing Transformer Training Efficiency with Dynamic Dropout(https://arxiv.org/abs/2411.03236)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Dynamic Dropout, a novel regularization technique designed to enhance the training efficiency of Transformer models by dynamically adjusting the dropout rate based on training epochs or validation loss improvements. This approach addresses the challenge of balancing regularization and model capacity, which is crucial for achieving fast convergence and high performance. Our method involves modifying the GPT model to accept a variable dropout rate and updating dropout layers during training using schedules such as linear decay, exponential decay, and validation loss-based adjustments. Extensive experiments on the Shakespeare\_char dataset demonstrate that Dynamic Dropout significantly accelerates training and improves inference efficiency compared to a baseline model with a fixed dropout rate. The validation loss-based adjustment schedule provided the best overall performance, highlighting the potential of Dynamic Dropout as a valuable technique for training large-scale Transformer models.</li>
</ul>

<h3>Title: DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03250">https://arxiv.org/abs/2411.03250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03250">https://arxiv.org/pdf/2411.03250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03250]] DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models(https://arxiv.org/abs/2411.03250)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases. The data and code will be publicly available upon completion of internal review.</li>
</ul>

<h3>Title: Proxy-informed Bayesian transfer learning with unknown sources</h3>
<ul>
<li><strong>Authors: </strong>Sabina J. Sloman, Julien Martinelli, Samuel Kaski</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03263">https://arxiv.org/abs/2411.03263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03263">https://arxiv.org/pdf/2411.03263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03263]] Proxy-informed Bayesian transfer learning with unknown sources(https://arxiv.org/abs/2411.03263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources. Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks. We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown). We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task. Instead, PROMPT relies on the availability of proxy information. PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks. We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings.</li>
</ul>

<h3>Title: Graph-Based Semi-Supervised Segregated Lipschitz Learning</h3>
<ul>
<li><strong>Authors: </strong>Farid Bozorgnia, Yassine Belkheiri, Abderrahim Elmoataz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03273">https://arxiv.org/abs/2411.03273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03273">https://arxiv.org/pdf/2411.03273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03273]] Graph-Based Semi-Supervised Segregated Lipschitz Learning(https://arxiv.org/abs/2411.03273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs. We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled. By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning. Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data.</li>
</ul>

<h3>Title: Oblivious Defense in ML Models: Backdoor Removal without Detection</h3>
<ul>
<li><strong>Authors: </strong>Shafi Goldwasser, Jonathan Shafer, Neekon Vafa, Vinod Vaikuntanathan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03279">https://arxiv.org/abs/2411.03279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03279">https://arxiv.org/pdf/2411.03279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03279]] Oblivious Defense in ML Models: Backdoor Removal without Detection(https://arxiv.org/abs/2411.03279)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>As society grows more reliant on machine learning, ensuring the security of machine learning systems against sophisticated attacks becomes a pressing concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022) shows that an adversary can plant undetectable backdoors in machine learning models, allowing the adversary to covertly control the model's behavior. Backdoors can be planted in such a way that the backdoored machine learning model is computationally indistinguishable from an honest model without backdoors. In this paper, we present strategies for defending against backdoors in ML models, even if they are undetectable. The key observation is that it is sometimes possible to provably mitigate or even remove backdoors without needing to detect them, using techniques inspired by the notion of random self-reducibility. This depends on properties of the ground-truth labels (chosen by nature), and not of the proposed ML model (which may be chosen by an attacker). We give formal definitions for secure backdoor mitigation, and proceed to show two types of results. First, we show a "global mitigation" technique, which removes all backdoors from a machine learning model under the assumption that the ground-truth labels are close to a Fourier-heavy function. Second, we consider distributions where the ground-truth labels are close to a linear or polynomial function in $\mathbb{R}^n$. Here, we show "local mitigation" techniques, which remove backdoors with high probability for every inputs of interest, and are computationally cheaper than global mitigation. All of our constructions are black-box, so our techniques work without needing access to the model's representation (i.e., its code or parameters). Along the way we prove a simple result for robust mean estimation.</li>
</ul>

<h3>Title: DiT4Edit: Diffusion Transformer for Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03286">https://arxiv.org/abs/2411.03286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03286">https://arxiv.org/pdf/2411.03286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03286]] DiT4Edit: Diffusion Transformer for Image Editing(https://arxiv.org/abs/2411.03286)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patches merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing.</li>
</ul>

<h3>Title: VERITAS: A Unified Approach to Reliability Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Rajkumar Ramamurthy, Meghana Arakkal Rajeev, Oliver Molenschot, James Zou, Nazneen Rajani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03300">https://arxiv.org/abs/2411.03300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03300">https://arxiv.org/pdf/2411.03300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03300]] VERITAS: A Unified Approach to Reliability Evaluation(https://arxiv.org/abs/2411.03300)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats. While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings. On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency. In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs. VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with $10\%$ increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting.</li>
</ul>

<h3>Title: LLMs for Domain Generation Algorithm Detection</h3>
<ul>
<li><strong>Authors: </strong>Reynier Leyva La O, Carlos A. Catania, Tatiana Parlanti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.03307">https://arxiv.org/abs/2411.03307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.03307">https://arxiv.org/pdf/2411.03307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.03307]] LLMs for Domain Generation Algorithm Detection(https://arxiv.org/abs/2411.03307)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
