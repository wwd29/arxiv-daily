<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-15</h1>
<h3>Title: PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human  Feedback and Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08702">https://arxiv.org/abs/2402.08702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08702">https://arxiv.org/pdf/2402.08702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08702]] PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human  Feedback and Preference Alignment(https://arxiv.org/abs/2402.08702)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent prompt and its associated feedback; we use a learned heuristic function that predicts prompt performance to efficiently sample from these candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across eight representative multi-step tasks (an average 27.7% and 28.2% improvement to current best methods on GPT-3.5 and GPT-4, respectively). We further show that the score function for tasks can be modified to better align with individual preferences. We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. Datasets and Codes are available at https://github.com/yongchao98/PROMST. Project Page is available at https://yongchao98.github.io/MIT-REALM-PROMST.</li>
</ul>

<h3>Title: PRDP: Proximal Reward Difference Prediction for Large-Scale Reward  Finetuning of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08714">https://arxiv.org/abs/2402.08714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08714">https://arxiv.org/pdf/2402.08714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08714]] PRDP: Proximal Reward Difference Prediction for Large-Scale Reward  Finetuning of Diffusion Models(https://arxiv.org/abs/2402.08714)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.</li>
</ul>

<h3>Title: Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs</h3>
<ul>
<li><strong>Authors: </strong>Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08733">https://arxiv.org/abs/2402.08733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08733">https://arxiv.org/pdf/2402.08733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08733]] Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs(https://arxiv.org/abs/2402.08733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.</li>
</ul>

<h3>Title: Towards the Detection of AI-Synthesized Human Face Images</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Lu, Touradj Ebrahimi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08750">https://arxiv.org/abs/2402.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08750">https://arxiv.org/pdf/2402.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08750]] Towards the Detection of AI-Synthesized Human Face Images(https://arxiv.org/abs/2402.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Over the past years, image generation and manipulation have achieved remarkable progress due to the rapid development of generative AI based on deep learning. Recent studies have devoted significant efforts to address the problem of face image manipulation caused by deepfake techniques. However, the problem of detecting purely synthesized face images has been explored to a lesser extent. In particular, the recent popular Diffusion Models (DMs) have shown remarkable success in image synthesis. Existing detectors struggle to generalize between synthesized images created by different generative models. In this work, a comprehensive benchmark including human face images produced by Generative Adversarial Networks (GANs) and a variety of DMs has been established to evaluate both the generalization ability and robustness of state-of-the-art detectors. Then, the forgery traces introduced by different generative models have been analyzed in the frequency domain to draw various insights. The paper further demonstrates that a detector trained with frequency representation can generalize well to other unseen generative models.</li>
</ul>

<h3>Title: Bayesian Strategic Classification</h3>
<ul>
<li><strong>Authors: </strong>Lee Cohen, Saeed Sharifi-Malvajerdi, Kevin Stangl, Ali Vakilian, Juba Ziani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08758">https://arxiv.org/abs/2402.08758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08758">https://arxiv.org/pdf/2402.08758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08758]] Bayesian Strategic Classification(https://arxiv.org/abs/2402.08758)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner's classifier. The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior. When reasoning about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner. This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks. We initiate the study of partial information release by the learner in strategic classification. We move away from the traditional assumption that agents have full knowledge of the classifier. Instead, we consider agents that have a common distributional prior on which classifier the learner is using. The learner in our model can reveal truthful, yet not necessarily complete, information about the deployed classifier to the agents. The learner's goal is to release just enough information about the classifier to maximize accuracy. We show how such partial information release can, counter-intuitively, benefit the learner's accuracy, despite increasing agents' abilities to manipulate. We show that while it is intractable to compute the best response of an agent in the general case, there exist oracle-efficient algorithms that can solve the best response of the agents when the learner's hypothesis class is the class of linear classifiers, or when the agents' cost function satisfies a natural notion of submodularity as we define. We then turn our attention to the learner's optimization problem and provide both positive and negative results on the algorithmic problem of how much information the learner should release about the classifier to maximize their expected accuracy.</li>
</ul>

<h3>Title: JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding  over Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang, Zaid Harchaoui, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08761">https://arxiv.org/abs/2402.08761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08761">https://arxiv.org/pdf/2402.08761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08761]] JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding  over Small Language Models(https://arxiv.org/abs/2402.08761)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger.</li>
</ul>

<h3>Title: Enhancing Robustness of Indoor Robotic Navigation with Free-Space  Segmentation Models Against Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan An, Christos Sevastopoulos, Fillia Makedon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08763">https://arxiv.org/abs/2402.08763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08763">https://arxiv.org/pdf/2402.08763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08763]] Enhancing Robustness of Indoor Robotic Navigation with Free-Space  Segmentation Models Against Adversarial Attacks(https://arxiv.org/abs/2402.08763)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Endeavors in indoor robotic navigation rely on the accuracy of segmentation models to identify free space in RGB images. However, deep learning models are vulnerable to adversarial attacks, posing a significant challenge to their real-world deployment. In this study, we identify vulnerabilities within the hidden layers of neural networks and introduce a practical approach to reinforce traditional adversarial training. Our method incorporates a novel distance loss function, minimizing the gap between hidden layers in clean and adversarial images. Experiments demonstrate satisfactory performance in improving the model's robustness against adversarial perturbations.</li>
</ul>

<h3>Title: FLASH: Federated Learning Across Simultaneous Heterogeneities</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Chang, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler, Ananthram Swami, Samet Oymak, Amit K. Roy-Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08769">https://arxiv.org/abs/2402.08769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08769">https://arxiv.org/pdf/2402.08769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08769]] FLASH: Federated Learning Across Simultaneous Heterogeneities(https://arxiv.org/abs/2402.08769)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The key premise of federated learning (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client's data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner. To do so, FLASH models the learning dynamics through contextual multi-armed bandits (CMAB) and dynamically selects the most promising clients. Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines -- as much as 10% in absolute accuracy -- thanks to its unified approach. Importantly, FLASH also outperforms federated aggregation methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them.</li>
</ul>

<h3>Title: LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Angus Fung, Beno Benhabib, Goldie Nejat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08774">https://arxiv.org/abs/2402.08774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08774">https://arxiv.org/pdf/2402.08774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08774]] LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models(https://arxiv.org/abs/2402.08774)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance.</li>
</ul>

<h3>Title: InstructGraph: Boosting Large Language Models via Graph-centric  Instruction Tuning and Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08785">https://arxiv.org/abs/2402.08785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08785">https://arxiv.org/pdf/2402.08785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08785]] InstructGraph: Boosting Large Language Models via Graph-centric  Instruction Tuning and Preference Alignment(https://arxiv.org/abs/2402.08785)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\% and 38\%, respectively.</li>
</ul>

<h3>Title: Rethinking Machine Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08787">https://arxiv.org/abs/2402.08787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08787">https://arxiv.org/pdf/2402.08787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08787]] Rethinking Machine Unlearning for Large Language Models(https://arxiv.org/abs/2402.08787)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.</li>
</ul>

<h3>Title: Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Aditya Malusare, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08790">https://arxiv.org/abs/2402.08790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08790">https://arxiv.org/pdf/2402.08790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08790]] Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model(https://arxiv.org/abs/2402.08790)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. K-DReAM outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.</li>
</ul>

<h3>Title: BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Omid Nejati Manzari, Javad Mirzapour Kaleybar, Hooman Saadat, Shahin Maleki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08793">https://arxiv.org/abs/2402.08793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08793">https://arxiv.org/pdf/2402.08793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08793]] BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image  Segmentation(https://arxiv.org/abs/2402.08793)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The accurate segmentation of medical images is critical for various healthcare applications. Convolutional neural networks (CNNs), especially Fully Convolutional Networks (FCNs) like U-Net, have shown remarkable success in medical image segmentation tasks. However, they have limitations in capturing global context and long-range relations, especially for objects with significant variations in shape, scale, and texture. While transformers have achieved state-of-the-art results in natural language processing and image recognition, they face challenges in medical image segmentation due to image locality and translational invariance issues. To address these challenges, this paper proposes an innovative U-shaped network called BEFUnet, which enhances the fusion of body and edge information for precise medical image segmentation. The BEFUnet comprises three main modules, including a novel Local Cross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF) module, and dual-branch encoder. The dual-branch encoder consists of an edge encoder and a body encoder. The edge encoder employs PDC blocks for effective edge information extraction, while the body encoder uses the Swin Transformer to capture semantic information with global attention. The LCAF module efficiently fuses edge and body features by selectively performing local cross-attention on features that are spatially close between the two modalities. This local approach significantly reduces computational complexity compared to global cross-attention while ensuring accurate feature matching. BEFUnet demonstrates superior performance over existing methods across various evaluation metrics on medical image segmentation datasets.</li>
</ul>

<h3>Title: eCeLLM: Generalizing Large Language Models for E-commerce from  Large-scale, High-quality Instruction Data</h3>
<ul>
<li><strong>Authors: </strong>Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08831">https://arxiv.org/abs/2402.08831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08831">https://arxiv.org/pdf/2402.08831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08831]] eCeLLM: Generalizing Large Language Models for E-commerce from  Large-scale, High-quality Instruction Data(https://arxiv.org/abs/2402.08831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM.</li>
</ul>

<h3>Title: Learning to Generate Context-Sensitive Backchannel Smiles for Embodied  AI Agents with Applications in Mental Health Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Maneesh Bilalpur, Mert Inan, Dorsa Zeinali, Jeffrey F. Cohn, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08837">https://arxiv.org/abs/2402.08837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08837">https://arxiv.org/pdf/2402.08837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08837]] Learning to Generate Context-Sensitive Backchannel Smiles for Embodied  AI Agents with Applications in Mental Health Dialogues(https://arxiv.org/abs/2402.08837)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem. Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.</li>
</ul>

<h3>Title: Feature Attribution with Necessity and Sufficiency via Dual-stage  Perturbation Test for Causal Explanation</h3>
<ul>
<li><strong>Authors: </strong>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08845">https://arxiv.org/abs/2402.08845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08845">https://arxiv.org/pdf/2402.08845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08845]] Feature Attribution with Necessity and Sufficiency via Dual-stage  Perturbation Test for Causal Explanation(https://arxiv.org/abs/2402.08845)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution.Finally, we combine FANS and gradient-based optimization to extract the subset with the largest PNS.We demonstrate that FANS outperforms existing feature attribution methods on six benchmarks.</li>
</ul>

<h3>Title: An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08846">https://arxiv.org/abs/2402.08846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08846">https://arxiv.org/pdf/2402.08846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08846]] An Embarrassingly Simple Approach for LLM with Strong ASR Capacity(https://arxiv.org/abs/2402.08846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.</li>
</ul>

<h3>Title: Hybrid Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, Sanjiban Choudhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08848">https://arxiv.org/abs/2402.08848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08848">https://arxiv.org/pdf/2402.08848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08848]] Hybrid Inverse Reinforcement Learning(https://arxiv.org/abs/2402.08848)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.</li>
</ul>

<h3>Title: Approximation of relation functions and attention mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Awni Altabaa, John Lafferty</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08856">https://arxiv.org/abs/2402.08856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08856">https://arxiv.org/pdf/2402.08856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08856]] Approximation of relation functions and attention mechanisms(https://arxiv.org/abs/2402.08856)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the attention mechanism underlying Transformers, showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations. This result uses the Debreu representation theorem in economics to represent preference relations in terms of utility functions.</li>
</ul>

<h3>Title: Tree-Based Hard Attention with Self-Motivation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Lin, Jiayu Ren, Guoxiu He, Zhuoren Jiang, Haiyan Yu, Xiaomin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08874">https://arxiv.org/abs/2402.08874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08874">https://arxiv.org/pdf/2402.08874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08874]] Tree-Based Hard Attention with Self-Motivation for Large Language Models(https://arxiv.org/abs/2402.08874)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representation of their relationship. Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped with a trainable adapter and a linear layer. The selected symbolic outcomes are integrated into another prompt, along with the predictive value of the task. We iteratively feed output values back into the prompt, enabling the trainable LLM to progressively approximate the golden truth. TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three benchmark datasets, showing its effectiveness in estimating task-specific properties. Through comprehensive experiments and analysis, we have validated the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences.</li>
</ul>

<h3>Title: Moving Object Proposals with Deep Learned Optical Flow for Video Object  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ge Shi, Zhili Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08882">https://arxiv.org/abs/2402.08882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08882">https://arxiv.org/pdf/2402.08882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08882]] Moving Object Proposals with Deep Learned Optical Flow for Video Object  Segmentation(https://arxiv.org/abs/2402.08882)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.</li>
</ul>

<h3>Title: Weakly Supervised Segmentation of Vertebral Bodies with Iterative  Slice-propagation</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08892">https://arxiv.org/abs/2402.08892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08892">https://arxiv.org/pdf/2402.08892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08892]] Weakly Supervised Segmentation of Vertebral Bodies with Iterative  Slice-propagation(https://arxiv.org/abs/2402.08892)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly supervised Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.</li>
</ul>

<h3>Title: Teamwork Makes TEE Work: Open and Resilient Remote Attestation on  Decentralized Trust</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Zhang, Kailun Qin, Shipei Qu, Tengfei Wang, Chi Zhang, Dawu Gu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08908">https://arxiv.org/abs/2402.08908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08908">https://arxiv.org/pdf/2402.08908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08908]] Teamwork Makes TEE Work: Open and Resilient Remote Attestation on  Decentralized Trust(https://arxiv.org/abs/2402.08908)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Remote Attestation (RA) enables the integrity and authenticity of applications in Trusted Execution Environment (TEE) to be verified. Existing TEE RA designs employ a centralized trust model where they rely on a single provisioned secret key and a centralized verifier to establish trust for remote parties. This model is however brittle and can be untrusted under advanced attacks nowadays. Besides, most designs only provide fixed functionalities once deployed, making them hard to adapt to different needs on availability, Quality of Service (QoS), etc. Therefore, we propose JANUS, an open and resilient TEE RA scheme. To decentralize trust, we, on one hand, introduce Physically Unclonable Function (PUF) as an intrinsic root of trust (RoT) in TEE to provide additional measurements and cryptographic enhancements. On the other hand, we use blockchain and smart contract to realize decentralized verification and result audit. Furthermore, we design an automated turnout mechanism that allows JANUS to remain resilient and offer flexible RA services under various situations. We provide a UC-based security proof and demonstrate the scalability and generality of JANUS by implementing an open-sourced prototype.</li>
</ul>

<h3>Title: Learning-based Bone Quality Classification Method for Spinal Metastasis</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08910">https://arxiv.org/abs/2402.08910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08910">https://arxiv.org/pdf/2402.08910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08910]] Learning-based Bone Quality Classification Method for Spinal Metastasis(https://arxiv.org/abs/2402.08910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spinal metastasis is the most common disease in bone metastasis and may cause pain, instability and neurological injuries. Early detection of spinal metastasis is critical for accurate staging and optimal treatment. The diagnosis is usually facilitated with Computed Tomography (CT) scans, which requires considerable efforts from well-trained radiologists. In this paper, we explore a learning-based automatic bone quality classification method for spinal metastasis based on CT images. We simultaneously take the posterolateral spine involvement classification task into account, and employ multi-task learning (MTL) technique to improve the performance. MTL acts as a form of inductive bias which helps the model generalize better on each task by sharing representations between related tasks. Based on the prior knowledge that the mixed type can be viewed as both blastic and lytic, we model the task of bone quality classification as two binary classification sub-tasks, i.e., whether blastic and whether lytic, and leverage a multiple layer perceptron to combine their predictions. In order to make the model more robust and generalize better, self-paced learning is adopted to gradually involve from easy to more complex samples into the training process. The proposed learning-based method is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our method significantly outperforms an 121-layer DenseNet classifier in sensitivities by $+12.54\%$, $+7.23\%$ and $+29.06\%$ for blastic, mixed and lytic lesions, respectively, meanwhile $+12.33\%$, $+23.21\%$ and $+34.25\%$ at vertebrae level.</li>
</ul>

<h3>Title: Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08919">https://arxiv.org/abs/2402.08919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08919">https://arxiv.org/pdf/2402.08919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08919]] Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding(https://arxiv.org/abs/2402.08919)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of "conceptual similarity" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate "explanations" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.</li>
</ul>

<h3>Title: The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes</h3>
<ul>
<li><strong>Authors: </strong>Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08922">https://arxiv.org/abs/2402.08922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08922">https://arxiv.org/pdf/2402.08922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08922]] The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes(https://arxiv.org/abs/2402.08922)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF.</li>
</ul>

<h3>Title: IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human  Pose Estimation with Transformer Architecture</h3>
<ul>
<li><strong>Authors: </strong>Varun Ramani, Hossein Khayemi, Yang Bai, Nakul Garg, Nirupam Roy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08923">https://arxiv.org/abs/2402.08923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08923">https://arxiv.org/pdf/2402.08923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08923]] IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human  Pose Estimation with Transformer Architecture(https://arxiv.org/abs/2402.08923)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.</li>
</ul>

<h3>Title: MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with  Diverse Human Preferences</h3>
<ul>
<li><strong>Authors: </strong>Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08925">https://arxiv.org/abs/2402.08925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08925">https://arxiv.org/pdf/2402.08925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08925]] MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with  Diverse Human Preferences(https://arxiv.org/abs/2402.08925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.</li>
</ul>

<h3>Title: Measuring Sharpness in Grokking</h3>
<ul>
<li><strong>Authors: </strong>Jack Miller, Patrick Gleeson, Charles O'Neill, Thang Bui, Noam Levi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08946">https://arxiv.org/abs/2402.08946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08946">https://arxiv.org/pdf/2402.08946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08946]] Measuring Sharpness in Grokking(https://arxiv.org/abs/2402.08946)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward explaining some trends and identify the need for further study to untangle the various mechanisms which influence the sharpness of grokking.</li>
</ul>

<h3>Title: Seagull: Privacy preserving network verification system</h3>
<ul>
<li><strong>Authors: </strong>Jaber Daneshamooz, Melody Yu, Sucheer Maddury</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08956">https://arxiv.org/abs/2402.08956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08956">https://arxiv.org/pdf/2402.08956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08956]] Seagull: Privacy preserving network verification system(https://arxiv.org/abs/2402.08956)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust</a></li>
<li><strong>Abstract: </strong>The current routing protocol used in the internet backbone is based on manual configuration, making it susceptible to errors. To mitigate these configuration-related issues, it becomes imperative to validate the accuracy and convergence of the algorithm, ensuring a seamless operation devoid of problems. However, the process of network verification faces challenges related to privacy and scalability. This paper addresses these challenges by introducing a novel approach: leveraging privacy-preserving computation, specifically multiparty computation (MPC), to verify the correctness of configurations in the internet backbone, governed by the BGP protocol. Not only does our proposed solution effectively address scalability concerns, but it also establishes a robust privacy framework. Through rigorous analysis, we demonstrate that our approach maintains privacy by not disclosing any information beyond the query result, thus providing a comprehensive and secure solution to the intricacies associated with routing protocol verification in large-scale networks.</li>
</ul>

<h3>Title: Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Junhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08958">https://arxiv.org/abs/2402.08958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08958">https://arxiv.org/pdf/2402.08958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08958]] Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers(https://arxiv.org/abs/2402.08958)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing Transformer models.</li>
</ul>

<h3>Title: Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</h3>
<ul>
<li><strong>Authors: </strong>Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08960">https://arxiv.org/abs/2402.08960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08960">https://arxiv.org/pdf/2402.08960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08960]] Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision(https://arxiv.org/abs/2402.08960)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.</li>
</ul>

<h3>Title: DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning</h3>
<ul>
<li><strong>Authors: </strong>Won-Seok Choi, Hyundo Lee, Dong-Sig Han, Junseok Park, Heeyeon Koo, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08963">https://arxiv.org/abs/2402.08963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08963">https://arxiv.org/pdf/2402.08963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08963]] DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning(https://arxiv.org/abs/2402.08963)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks. We also analyze the role of the DUEL policy in the training process through various metrics and visualizations.</li>
</ul>

<h3>Title: Predicting User Experience on Laptops from Hardware Specifications</h3>
<ul>
<li><strong>Authors: </strong>Saswat Padhi, Sunil K. Bhasin, Udaya K. Ammu, Alex Bergman, Allan Knies</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08964">https://arxiv.org/abs/2402.08964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08964">https://arxiv.org/pdf/2402.08964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08964]] Predicting User Experience on Laptops from Hardware Specifications(https://arxiv.org/abs/2402.08964)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Estimating the overall user experience (UX) on a device is a common challenge faced by manufacturers. Today, device makers primarily rely on microbenchmark scores, such as Geekbench, that stress test specific hardware components, such as CPU or RAM, but do not satisfactorily capture consumer workloads. System designers often rely on domain-specific heuristics and extensive testing of prototypes to reach a desired UX goal, and yet there is often a mismatch between the manufacturers' performance claims and the consumers' experience. We present our initial results on predicting real-life experience on laptops from their hardware specifications. We target web applications that run on Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience across applications and workloads. On 54 laptops, we track 9 UX metrics on common end-user workloads: web browsing, video playback and audio/video calls. We focus on a subset of high-level metrics exposed by the Chrome browser, that are part of the Web Vitals initiative for judging the UX on web applications. With a dataset of 100K UX data points, we train gradient boosted regression trees that predict the metric values from device specifications. Across our 9 metrics, we note a mean $R^2$ score (goodness-of-fit on our dataset) of 97.8% and a mean MAAPE (percentage error in prediction on unseen data) of 10.1%.</li>
</ul>

<h3>Title: Structured Language Generation Model for Robust Structure Prediction</h3>
<ul>
<li><strong>Authors: </strong>Minho Lee, Junghyun Min, Woochul Lee, Yeonsoo Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08971">https://arxiv.org/abs/2402.08971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08971">https://arxiv.org/pdf/2402.08971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08971]] Structured Language Generation Model for Robust Structure Prediction(https://arxiv.org/abs/2402.08971)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.</li>
</ul>

<h3>Title: Research and application of Transformer based anomaly detection model: A  literature review</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ma, Lansheng Han, Chunjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08975">https://arxiv.org/abs/2402.08975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08975">https://arxiv.org/pdf/2402.08975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08975]] Research and application of Transformer based anomaly detection model: A  literature review(https://arxiv.org/abs/2402.08975)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the best of our knowledge, this is the first comprehensive review that focuses on the research related to Transformer in the context of anomaly detection. We hope that this paper can provide detailed technical information to researchers interested in Transformer-based anomaly detection tasks.</li>
</ul>

<h3>Title: SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08983">https://arxiv.org/abs/2402.08983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08983">https://arxiv.org/pdf/2402.08983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08983]] SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware  Decoding(https://arxiv.org/abs/2402.08983)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five LLMs using six state-of-the-art jailbreak attacks and four benchmark datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.</li>
</ul>

<h3>Title: Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary  Statistics</h3>
<ul>
<li><strong>Authors: </strong>Wenwei Zhao, Xiaowen Li, Shangqing Zhao, Jie Xu, Yao Liu, Zhuo Lu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08986">https://arxiv.org/abs/2402.08986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08986">https://arxiv.org/pdf/2402.08986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08986]] Detecting Adversarial Spectrum Attacks via Distance to Decision Boundary  Statistics(https://arxiv.org/abs/2402.08986)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Machine learning has been adopted for efficient cooperative spectrum sensing. However, it incurs an additional security risk due to attacks leveraging adversarial machine learning to create malicious spectrum sensing values to deceive the fusion center, called adversarial spectrum attacks. In this paper, we propose an efficient framework for detecting adversarial spectrum attacks. Our design leverages the concept of the distance to the decision boundary (DDB) observed at the fusion center and compares the training and testing DDB distributions to identify adversarial spectrum attacks. We create a computationally efficient way to compute the DDB for machine learning based spectrum sensing systems. Experimental results based on realistic spectrum data show that our method, under typical settings, achieves a high detection rate of up to 99\% and maintains a low false alarm rate of less than 1\%. In addition, our method to compute the DDB based on spectrum data achieves 54\%--64\% improvements in computational efficiency over existing distance calculation methods. The proposed DDB-based detection framework offers a practical and efficient solution for identifying malicious sensing values created by adversarial spectrum attacks.</li>
</ul>

<h3>Title: CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08994">https://arxiv.org/abs/2402.08994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08994">https://arxiv.org/pdf/2402.08994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08994]] CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic  Decoding(https://arxiv.org/abs/2402.08994)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.</li>
</ul>

<h3>Title: Exploring Federated Deep Learning for Standardising Naming Conventions  in Radiotherapy Data</h3>
<ul>
<li><strong>Authors: </strong>Ali Haidar, Daniel Al Mouiee, Farhannah Aly, David Thwaites, Lois Holloway</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08999">https://arxiv.org/abs/2402.08999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08999">https://arxiv.org/pdf/2402.08999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08999]] Exploring Federated Deep Learning for Standardising Naming Conventions  in Radiotherapy Data(https://arxiv.org/abs/2402.08999)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres. This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task. Several machine learning-based methods have been proposed and evaluated to standardise nomenclature. However, no studies have considered that RT patient records are distributed across multiple data centres. This paper introduces a method that emulates real-world environments to establish standardised nomenclature. This is achieved by integrating decentralised real-time data and federated learning (FL). A multimodal deep artificial neural network was proposed to standardise RT data in federated settings. Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric. Simulated experiments were carried out to train the models across several scenarios including multiple data centres, input modalities, and aggregation strategies. The models were compared against models developed with single modalities in federated settings, in addition to models trained in centralised settings. Categorical classification accuracy was calculated on hold-out samples to inform the models performance. Our results highlight the need for fusing multiple modalities when training such models, with better performance reported with tabular-volumetric models. In addition, we report comparable accuracy compared to models built in centralised settings. This demonstrates the suitability of FL for handling the standardization task. Additional ablation analyses showed that the total number of samples in the data centres and the number of data centres highly affects the training process and should be carefully considered when building standardisation models.</li>
</ul>

<h3>Title: Multi-Query Focused Disaster Summarization via Instruction-Based  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Philipp Seeberger, Korbinian Riedhammer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09008">https://arxiv.org/abs/2402.09008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09008">https://arxiv.org/pdf/2402.09008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09008]] Multi-Query Focused Disaster Summarization via Instruction-Based  Prompting(https://arxiv.org/abs/2402.09008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic summarization of mass-emergency events plays a critical role in disaster management. The second edition of CrisisFACTS aims to advance disaster summarization based on multi-stream fact-finding with a focus on web sources such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to develop systems that can extract key facts from several disaster-related events, which ultimately serve as a summary. This paper describes our method to tackle this challenging task. We follow previous work and propose to use a combination of retrieval, reranking, and an embarrassingly simple instruction-following summarization. The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source Large Language Model (LLM) LLaMA-13b. For summarization, we explore a Question Answering (QA)-motivated prompting approach and find the evidence useful for extracting query-relevant facts. The automatic metrics and human evaluation show strong results but also highlight the gap between open-source and proprietary systems.</li>
</ul>

<h3>Title: Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications</h3>
<ul>
<li><strong>Authors: </strong>Negar Arabzadeh, Julia Kiseleva, Qingyun Wu, Chi Wang, Ahmed Awadallah, Victor Dibia, Adam Fourney, Charles Clarke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09015">https://arxiv.org/abs/2402.09015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09015">https://arxiv.org/pdf/2402.09015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09015]] Towards better Human-Agent Alignment: Assessing Task Utility in  LLM-Powered Applications(https://arxiv.org/abs/2402.09015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier's work.</li>
</ul>

<h3>Title: Pyramid Attention Network for Medical Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyuan Wang, Haiqiao Wang, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09016">https://arxiv.org/abs/2402.09016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09016">https://arxiv.org/pdf/2402.09016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09016]] Pyramid Attention Network for Medical Image Registration(https://arxiv.org/abs/2402.09016)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.The pure convolutional neural networks (CNNs) neglect feature enhancement, while current Transformer-based networks are susceptible to information redundancy.To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.Moreover, a multi-head local attention Transformer is introduced as decoder to analyze motion patterns and generate deformation fields.Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several CNN-based and Transformer-based registration networks.Our code is publicly available at https://github.com/JuliusWang-7/PAN.</li>
</ul>

<h3>Title: Review-Incorporated Model-Agnostic Profile Injection Attacks on  Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09023">https://arxiv.org/abs/2402.09023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09023">https://arxiv.org/pdf/2402.09023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09023]] Review-Incorporated Model-Agnostic Profile Injection Attacks on  Recommender Systems(https://arxiv.org/abs/2402.09023)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility.</li>
</ul>

<h3>Title: SLEB: Streamlining LLMs through Redundancy Verification and Elimination  of Transformer Blocks</h3>
<ul>
<li><strong>Authors: </strong>Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09025">https://arxiv.org/abs/2402.09025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09025">https://arxiv.org/pdf/2402.09025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09025]] SLEB: Streamlining LLMs through Redundancy Verification and Elimination  of Transformer Blocks(https://arxiv.org/abs/2402.09025)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB</li>
</ul>

<h3>Title: Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?</h3>
<ul>
<li><strong>Authors: </strong>Tiantian Feng, Daniel Yang, Digbalay Bose, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09036">https://arxiv.org/abs/2402.09036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09036">https://arxiv.org/pdf/2402.09036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09036]] Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?(https://arxiv.org/abs/2402.09036)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of text-to-image models to assist multi-modal learning. Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers. Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques.</li>
</ul>

<h3>Title: Under manipulations, are some AI models harder to audit?</h3>
<ul>
<li><strong>Authors: </strong>Augustin Godinot, Gilles Tredan, Erwan Le Merrer, Camilla Penzo, Francois Taïani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09043">https://arxiv.org/abs/2402.09043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09043">https://arxiv.org/pdf/2402.09043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09043]] Under manipulations, are some AI models harder to audit?(https://arxiv.org/abs/2402.09043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Auditors need robust methods to assess the compliance of web platforms with the law. However, since they hardly ever have access to the algorithm, implementation, or training data used by a platform, the problem is harder than a simple metric estimation. Within the recent framework of manipulation-proof auditing, we study in this paper the feasibility of robust audits in realistic settings, in which models exhibit large capacities. We first prove a constraining result: if a web platform uses models that may fit any data, no audit strategy -- whether active or not -- can outperform random sampling when estimating properties such as demographic parity. To better understand the conditions under which state-of-the-art auditing techniques may remain competitive, we then relate the manipulability of audits to the capacity of the targeted models, using the Rademacher complexity. We empirically validate these results on popular models of increasing capacities, thus confirming experimentally that large-capacity models, which are commonly used in practice, are particularly hard to audit robustly. These results refine the limits of the auditing problem, and open up enticing questions on the connection between model capacity and the ability of platforms to manipulate audit attempts.</li>
</ul>

<h3>Title: I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of  Transformers using Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Prajwal Panzade, Daniel Takabi, Zhipeng Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09059">https://arxiv.org/abs/2402.09059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09059">https://arxiv.org/pdf/2402.09059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09059]] I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of  Transformers using Fully Homomorphic Encryption(https://arxiv.org/abs/2402.09059)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, transformer</a></li>
<li><strong>Abstract: </strong>In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x over previous work in this domain.</li>
</ul>

<h3>Title: Soft Prompt Threats: Attacking Safety Alignment and Unlearning in  Open-Source LLMs through the Embedding Space</h3>
<ul>
<li><strong>Authors: </strong>Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, Stephan Gunnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09063">https://arxiv.org/abs/2402.09063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09063">https://arxiv.org/pdf/2402.09063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09063]] Soft Prompt Threats: Attacking Safety Alignment and Unlearning in  Open-Source LLMs through the Embedding Space(https://arxiv.org/abs/2402.09063)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. Trigger Warning: the appendix contains LLM-generated text with violence and harassment.</li>
</ul>

<h3>Title: Solid Waste Detection in Remote Sensing Images: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Piero Fraternali, Luca Morandini, Sergio Luis Herrera González</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09066">https://arxiv.org/abs/2402.09066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09066">https://arxiv.org/pdf/2402.09066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09066]] Solid Waste Detection in Remote Sensing Images: A Survey(https://arxiv.org/abs/2402.09066)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locations for new landfills. This review aims to provide a detailed illustration of the most relevant proposals for the detection and monitoring of solid waste sites by describing and comparing the approaches, the implemented techniques, and the employed data. Furthermore, since the data sources are of the utmost importance for developing an effective solid waste detection model, a comprehensive overview of the satellites and publicly available data sets is presented. Finally, this paper identifies the open issues in the state-of-the-art and discusses the relevant research directions for reducing the costs and improving the effectiveness of novel solid waste detection methods.</li>
</ul>

<h3>Title: Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Puccetti, Andrea Ceccarelli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09082">https://arxiv.org/abs/2402.09082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09082">https://arxiv.org/pdf/2402.09082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09082]] Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?(https://arxiv.org/abs/2402.09082)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things. Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors.</li>
</ul>

<h3>Title: Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit  Clues</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09091">https://arxiv.org/abs/2402.09091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09091">https://arxiv.org/pdf/2402.09091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09091]] Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit  Clues(https://arxiv.org/abs/2402.09091)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.</li>
</ul>

<h3>Title: Unity is Strength: Enhancing Precision in Reentrancy Vulnerability  Detection of Smart Contract Analysis Tools</h3>
<ul>
<li><strong>Authors: </strong>Zexu Wang, Jiachi Chen, Zibin Zheng, Peilin Zheng, Yu Zhang, Weizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09094">https://arxiv.org/abs/2402.09094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09094">https://arxiv.org/pdf/2402.09094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09094]] Unity is Strength: Enhancing Precision in Reentrancy Vulnerability  Detection of Smart Contract Analysis Tools(https://arxiv.org/abs/2402.09094)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Reentrancy is one of the most notorious vulnerabilities in smart contracts, resulting in significant digital asset losses. However, many previous works indicate that current Reentrancy detection tools suffer from high false positive rates. Even worse, recent years have witnessed the emergence of new Reentrancy attack patterns fueled by intricate and diverse vulnerability exploit mechanisms. Unfortunately, current tools face a significant limitation in their capacity to adapt and detect these evolving Reentrancy patterns. Consequently, ensuring precise and highly extensible Reentrancy vulnerability detection remains critical challenges for existing tools. To address this issue, we propose a tool named ReEP, designed to reduce the false positives for Reentrancy vulnerability detection. Additionally, ReEP can integrate multiple tools, expanding its capacity for vulnerability detection. It evaluates results from existing tools to verify vulnerability likelihood and reduce false positives. ReEP also offers excellent extensibility, enabling the integration of different detection tools to enhance precision and cover different vulnerability attack patterns. We perform ReEP to eight existing state-of-the-art Reentrancy detection tools. The average precision of these eight tools increased from the original 0.5% to 73% without sacrificing recall. Furthermore, ReEP exhibits robust extensibility. By integrating multiple tools, the precision further improved to a maximum of 83.6%. These results demonstrate that ReEP effectively unites the strengths of existing works, enhances the precision of Reentrancy vulnerability detection tools.</li>
</ul>

<h3>Title: FedSiKD: Clients Similarity and Knowledge Distillation: Addressing  Non-i.i.d. and Constraints in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yousef Alsenani, Rahul Mishra, Khaled R. Ahmed, Atta Ur Rahman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09095">https://arxiv.org/abs/2402.09095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09095">https://arxiv.org/pdf/2402.09095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09095]] FedSiKD: Clients Similarity and Knowledge Distillation: Addressing  Non-i.i.d. and Constraints in Federated Learning(https://arxiv.org/abs/2402.09095)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>In recent years, federated learning (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD) within a similarity-based federated learning framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring knowledge between teacher and student models and addressing device constraints. FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25\% and 18\% for highly skewed data at $\alpha = {0.1,0.5}$ on the HAR and MNIST datasets, respectively. Its faster convergence is illustrated by a 17\% and 20\% increase in accuracy within the first five rounds on the HAR and MNIST datasets, respectively, highlighting its early-stage learning proficiency. Code is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)</li>
</ul>

<h3>Title: Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09100">https://arxiv.org/abs/2402.09100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09100">https://arxiv.org/pdf/2402.09100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09100]] Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs(https://arxiv.org/abs/2402.09100)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative</a></li>
<li><strong>Abstract: </strong>Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.</li>
</ul>

<h3>Title: DolphCoder: Echo-Locating Code Large Language Models with Diverse and  Multi-Objective Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09136">https://arxiv.org/abs/2402.09136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09136">https://arxiv.org/pdf/2402.09136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09136]] DolphCoder: Echo-Locating Code Large Language Models with Diverse and  Multi-Objective Instruction Tuning(https://arxiv.org/abs/2402.09136)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.</li>
</ul>

<h3>Title: ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural  Networks</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Kashif, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09146">https://arxiv.org/abs/2402.09146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09146">https://arxiv.org/pdf/2402.09146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09146]] ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural  Networks(https://arxiv.org/abs/2402.09146)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications.</li>
</ul>

<h3>Title: Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for  Chinese Mental Health Text Analysis</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhai, Hongzhi Qi, Qing Zhao, Jianqiang Li, Ziqi Wang, Han Wang, Bing Xiang Yang, Guanghui Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09151">https://arxiv.org/abs/2402.09151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09151">https://arxiv.org/pdf/2402.09151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09151]] Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for  Chinese Mental Health Text Analysis(https://arxiv.org/abs/2402.09151)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT.</li>
</ul>

<h3>Title: Attacking Large Language Models with Projected Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09154">https://arxiv.org/abs/2402.09154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09154">https://arxiv.org/pdf/2402.09154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09154]] Attacking Large Language Models with Projected Gradient Descent(https://arxiv.org/abs/2402.09154)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.</li>
</ul>

<h3>Title: Less is More: Fewer Interpretable Region via Submodular Subset Selection</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09164">https://arxiv.org/abs/2402.09164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09164">https://arxiv.org/pdf/2402.09164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09164]] Less is More: Fewer Interpretable Region via Submodular Subset Selection(https://arxiv.org/abs/2402.09164)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.</li>
</ul>

<h3>Title: Unifying Invariance and Spuriousity for Graph Out-of-Distribution via  Probability of Necessity and Sufficiency</h3>
<ul>
<li><strong>Authors: </strong>Xuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang, Zhifeng Hao, Zijian Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09165">https://arxiv.org/abs/2402.09165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09165">https://arxiv.org/pdf/2402.09165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09165]] Unifying Invariance and Spuriousity for Graph Out-of-Distribution via  Probability of Necessity and Sufficiency(https://arxiv.org/abs/2402.09165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph Out-of-Distribution (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications. One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation. However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization. To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data. Specificially, we first consider the data generation process for graph data. Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an upper bound, which is built on the theoretical advance of probability of necessity and sufficiency. To further bridge the theory and algorithm, we devise the PNSIS model, which involves an invariant subgraph extractor for invariant graph learning as well invariant and spurious subgraph classifiers for generalization enhancement. Experimental results demonstrate that our \textbf{PNSIS} model outperforms the state-of-the-art techniques on graph OOD on several benchmarks, highlighting the effectiveness in real-world scenarios.</li>
</ul>

<h3>Title: Leveraging the Context through Multi-Round Interactions for Jailbreaking  Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09177">https://arxiv.org/abs/2402.09177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09177">https://arxiv.org/pdf/2402.09177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09177]] Leveraging the Context through Multi-Round Interactions for Jailbreaking  Attacks(https://arxiv.org/abs/2402.09177)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of this attack, which is black-box and can also transfer across LLMs. We believe this can lead to further developments and understanding of the context vector in LLMs.</li>
</ul>

<h3>Title: Generalized Portrait Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09178">https://arxiv.org/abs/2402.09178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09178">https://arxiv.org/pdf/2402.09178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09178]] Generalized Portrait Quality Assessment(https://arxiv.org/abs/2402.09178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 benchmark and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at https://github.com/DXOMARK-Research/PIQ2023.</li>
</ul>

<h3>Title: Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model  Customization</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09179">https://arxiv.org/abs/2402.09179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09179">https://arxiv.org/pdf/2402.09179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09179]] Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model  Customization(https://arxiv.org/abs/2402.09179)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 4 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose an instruction-ignoring defense mechanism and demonstrate its partial effectiveness in mitigating such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.</li>
</ul>

<h3>Title: Cyber Deception Reactive: TCP Stealth Redirection to On-Demand Honeypots</h3>
<ul>
<li><strong>Authors: </strong>Pedro Beltran Lopez, Pantaleone Nespoli, Manuel Gil Perez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, cs.PF, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09191">https://arxiv.org/abs/2402.09191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09191">https://arxiv.org/pdf/2402.09191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09191]] Cyber Deception Reactive: TCP Stealth Redirection to On-Demand Honeypots(https://arxiv.org/abs/2402.09191)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, steal</a></li>
<li><strong>Abstract: </strong>Cybersecurity is developing rapidly, and new methods of defence against attackers are appearing, such as Cyber Deception (CYDEC). CYDEC consists of deceiving the enemy who performs actions without realising that he/she is being deceived. This article proposes designing, implementing, and evaluating a deception mechanism based on the stealthy redirection of TCP communications to an on-demand honey server with the same characteristics as the victim asset, i.e., it is a clone. Such a mechanism ensures that the defender fools the attacker, thanks to stealth redirection. In this situation, the attacker will focus on attacking the honey server while enabling the recollection of relevant information to generate threat intelligence. The experiments in different scenarios show how the proposed solution can effectively redirect an attacker to a copied asset on demand, thus protecting the real asset. Finally, the results obtained by evaluating the latency times ensure that the redirection is undetectable by humans and very difficult to detect by a machine.</li>
</ul>

<h3>Title: (Ir)rationality and Cognitive Biases in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Olivia Macmillan-Scott, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09193">https://arxiv.org/abs/2402.09193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09193">https://arxiv.org/pdf/2402.09193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09193]] (Ir)rationality and Cognitive Biases in Large Language Models(https://arxiv.org/abs/2402.09193)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</li>
</ul>

<h3>Title: Implementing local-explainability in Gradient Boosting Trees: Feature  Contribution</h3>
<ul>
<li><strong>Authors: </strong>Ángel Delgado-Panadero, Beatriz Hernández-Lorca, María Teresa García-Ordás, José Alberto Benítez-Andrades</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09197">https://arxiv.org/abs/2402.09197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09197">https://arxiv.org/pdf/2402.09197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09197]] Implementing local-explainability in Gradient Boosting Trees: Feature  Contribution(https://arxiv.org/abs/2402.09197)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, explainability</a></li>
<li><strong>Abstract: </strong>Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles. Its nature makes GBDT a black-box model even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally. Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify. In this paper, a feature contribution method for GBDT is developed. The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node. This algorithm allows to calculate the sequence of node decisions given a prediction. Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs internal behavior. The proposal is aligned to the contribution of characteristics having impact in some artificial intelligence problems such as ethical analysis of Artificial Intelligence (AI) and comply with the new European laws such as the General Data Protection Regulation (GDPR) about the right to explain and nondiscrimination.</li>
</ul>

<h3>Title: Ten Words Only Still Help: Improving Black-Box AI-Generated Text  Detection via Proxy-Guided Efficient Re-Sampling</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09199">https://arxiv.org/abs/2402.09199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09199">https://arxiv.org/pdf/2402.09199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09199]] Ten Words Only Still Help: Improving Black-Box AI-Generated Text  Detection via Proxy-Guided Efficient Re-Sampling(https://arxiv.org/abs/2402.09199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LLMs show that POGER outperforms all baselines in macro F1 under black-box, partial white-box, and out-of-distribution settings and maintains lower re-sampling costs than its existing counterparts.</li>
</ul>

<h3>Title: Discovering Command and Control (C2) Channels on Tor and Public Networks  Using Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wang, Christopher Redino, Abdul Rahman, Ryan Clark, Daniel Radke, Tyler Cody, Dhruv Nandakumar, Edward Bowen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09200">https://arxiv.org/abs/2402.09200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09200">https://arxiv.org/pdf/2402.09200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09200]] Discovering Command and Control (C2) Channels on Tor and Public Networks  Using Reinforcement Learning(https://arxiv.org/abs/2402.09200)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Command and control (C2) channels are an essential component of many types of cyber attacks, as they enable attackers to remotely control their malware-infected machines and execute harmful actions, such as propagating malicious code across networks, exfiltrating confidential data, or initiating distributed denial of service (DDoS) attacks. Identifying these C2 channels is therefore crucial in helping to mitigate and prevent cyber attacks. However, identifying C2 channels typically involves a manual process, requiring deep knowledge and expertise in cyber operations. In this paper, we propose a reinforcement learning (RL) based approach to automatically emulate C2 attack campaigns using both the normal (public) and the Tor networks. In addition, payload size and network firewalls are configured to simulate real-world attack scenarios. Results on a typical network configuration show that the RL agent can automatically discover resilient C2 attack paths utilizing both Tor-based and conventional communication channels, while also bypassing network firewalls.</li>
</ul>

<h3>Title: Scaling the Authoring of AutoTutors with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sankalan Pal Chowdhury, Vilém Zouhar, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09216">https://arxiv.org/abs/2402.09216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09216">https://arxiv.org/pdf/2402.09216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09216]] Scaling the Authoring of AutoTutors with Large Language Models(https://arxiv.org/abs/2402.09216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches. Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow</li>
</ul>

<h3>Title: Is my Data in your AI Model? Membership Inference Test with Application  to Face Images</h3>
<ul>
<li><strong>Authors: </strong>Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, Javier Ortega-Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09225">https://arxiv.org/abs/2402.09225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09225">https://arxiv.org/pdf/2402.09225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09225]] Is my Data in your AI Model? Membership Inference Test with Application  to Face Images(https://arxiv.org/abs/2402.09225)</code><input type="text"></li>
<li><strong>Keywords: </strong>membership infer</a></li>
<li><strong>Abstract: </strong>This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.</li>
</ul>

<h3>Title: Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, Pradeep Ravikumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09236">https://arxiv.org/abs/2402.09236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09236">https://arxiv.org/pdf/2402.09236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09236]] Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models(https://arxiv.org/abs/2402.09236)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.</li>
</ul>

<h3>Title: Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yannis Kalantidis, Mert Bülent Sarıyıldız, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09237">https://arxiv.org/abs/2402.09237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09237">https://arxiv.org/pdf/2402.09237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09237]] Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency(https://arxiv.org/abs/2402.09237)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: https://europe.naverlabs.com/ret4loc</li>
</ul>

<h3>Title: Robust Training of Temporal GNNs using Nearest Neighbours based Hard  Negatives</h3>
<ul>
<li><strong>Authors: </strong>Shubham Gupta, Srikanta Bedathur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09239">https://arxiv.org/abs/2402.09239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09239">https://arxiv.org/pdf/2402.09239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09239]] Robust Training of Temporal GNNs using Nearest Neighbours based Hard  Negatives(https://arxiv.org/abs/2402.09239)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.</li>
</ul>

<h3>Title: Switch EMA: A Free Lunch for Better Flatness and Sharpness</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09240">https://arxiv.org/abs/2402.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09240">https://arxiv.org/pdf/2402.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09240]] Switch EMA: A Free Lunch for Better Flatness and Sharpness(https://arxiv.org/abs/2402.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.</li>
</ul>

<h3>Title: Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Zhou, Weiqing Min, Jiajun Song, Yang Zhang, Shuqiang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09242">https://arxiv.org/abs/2402.09242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09242">https://arxiv.org/pdf/2402.09242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09242]] Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection(https://arxiv.org/abs/2402.09242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.</li>
</ul>

<h3>Title: Momentum Approximation in Asynchronous Private Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Tao Yu, Congzheng Song, Jianyu Wang, Mona Chitnis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09247">https://arxiv.org/abs/2402.09247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09247">https://arxiv.org/pdf/2402.09247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09247]] Momentum Approximation in Asynchronous Private Federated Learning(https://arxiv.org/abs/2402.09247)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum approximation can achieve $1.15 \textrm{--}4\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum.</li>
</ul>

<h3>Title: TDViT: Temporal Dilated Video Transformer for Dense Video Tasks</h3>
<ul>
<li><strong>Authors: </strong>Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09257">https://arxiv.org/abs/2402.09257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09257">https://arxiv.org/pdf/2402.09257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09257]] TDViT: Temporal Dilated Video Transformer for Dense Video Tasks(https://arxiv.org/abs/2402.09257)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Deep video models, for example, 3D CNNs or video transformers, have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video Transformer (TDViT) that consists of carefully designed temporal dilated transformer blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video benchmarks, i.e., ImageNet VID for video object detection and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.</li>
</ul>

<h3>Title: SyntaxShap: Syntax-aware Explainability Method for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09259">https://arxiv.org/abs/2402.09259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09259">https://arxiv.org/pdf/2402.09259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09259]] SyntaxShap: Syntax-aware Explainability Method for Text Generation(https://arxiv.org/abs/2402.09259)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models.</li>
</ul>

<h3>Title: MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical  Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Corentin Royer, Bjoern Menze, Anjany Sekuboyina</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09262">https://arxiv.org/abs/2402.09262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09262">https://arxiv.org/pdf/2402.09262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09262]] MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical  Vision-Language Models(https://arxiv.org/abs/2402.09262)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively assesses the models' performance on a broad array of six multi-modal tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model's overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform benchmarking of future models.</li>
</ul>

<h3>Title: UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers</h3>
<ul>
<li><strong>Authors: </strong>Hong Jia, Young D. Kwon, Dong Ma, Nhat Pham, Lorena Qendro, Tam Vu, Cecilia Mascolo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09264">https://arxiv.org/abs/2402.09264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09264">https://arxiv.org/pdf/2402.09264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09264]] UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers(https://arxiv.org/abs/2402.09264)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection. In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different event models; (iii) optimize the deployment of the model and MCU library for system efficiency. We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets. Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance. UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications.</li>
</ul>

<h3>Title: Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via  Self-Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, Helen Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09267">https://arxiv.org/abs/2402.09267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09267">https://arxiv.org/pdf/2402.09267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09267]] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via  Self-Evaluation(https://arxiv.org/abs/2402.09267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.</li>
</ul>

<h3>Title: Transformers, parallel computation, and logarithmic depth</h3>
<ul>
<li><strong>Authors: </strong>Clayton Sanford, Daniel Hsu, Matus Telgarsky</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09268">https://arxiv.org/abs/2402.09268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09268">https://arxiv.org/pdf/2402.09268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09268]] Transformers, parallel computation, and logarithmic depth(https://arxiv.org/abs/2402.09268)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We show that a constant number of self-attention layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for transformers to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and sub-quadratic transformer approximations. We thus establish parallelism as a key distinguishing property of transformers.</li>
</ul>

<h3>Title: Personalized Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09269">https://arxiv.org/abs/2402.09269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09269">https://arxiv.org/pdf/2402.09269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09269]] Personalized Large Language Models(https://arxiv.org/abs/2402.09269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.</li>
</ul>

<h3>Title: Fast Window-Based Event Denoising with Spatiotemporal Correlation  Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Huachen Fang, Jinjian Wu, Qibin Hou, Weisheng Dong, Guangming Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09270">https://arxiv.org/abs/2402.09270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09270">https://arxiv.org/pdf/2402.09270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09270]] Fast Window-Based Event Denoising with Spatiotemporal Correlation  Enhancement(https://arxiv.org/abs/2402.09270)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events. In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned convolutional sparse coding to optimize the objective function. Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet. The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes. Extensive experimental results verify the effectiveness and robustness of our MSDNet. Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks.</li>
</ul>

<h3>Title: Hybrid Machine Learning techniques in the management of harmful algal  blooms impact</h3>
<ul>
<li><strong>Authors: </strong>Andres Molares-Ulloa, Daniel Rivero, Jesus Gil Ruiz, Enrique Fernandez-Blanco, Luis de-la-Fuente-Valentín</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09271">https://arxiv.org/abs/2402.09271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09271">https://arxiv.org/pdf/2402.09271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09271]] Hybrid Machine Learning techniques in the management of harmful algal  blooms impact(https://arxiv.org/abs/2402.09271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Harmful algal blooms (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit. This new option is the most similar to the actual functioning of the control of shellfish production areas. For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas. The study has been carried out in several estuaries with different levels of complexity in the episodes of algal blooms to demonstrate the generalization capacity of the models in bloom detection. As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Enhanced NLP Task Performance  through Knowledge Distillation and Optimized Training Strategies</h3>
<ul>
<li><strong>Authors: </strong>Yining Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09282">https://arxiv.org/abs/2402.09282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09282">https://arxiv.org/pdf/2402.09282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09282]] Leveraging Large Language Models for Enhanced NLP Task Performance  through Knowledge Distillation and Optimized Training Strategies(https://arxiv.org/abs/2402.09282)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) like GPT-4 into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) prompting technique to distill knowledge from GPT-4, subsequently applying it to improve the efficiency and effectiveness of a smaller model, BERT, on Named Entity Recognition (NER) tasks. Our method involves a two-phase training process: initially employing GPT-4 annotated data for pre-training and then refining the model with a combination of distilled and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The study also discusses the challenges encountered, such as LLM output variability and the tendency towards hallucinations, proposing future work directions to enhance prompt design and annotation selection. Our findings indicate a promising synergy between LLM insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications.</li>
</ul>

<h3>Title: Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09283">https://arxiv.org/abs/2402.09283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09283">https://arxiv.org/pdf/2402.09283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09283]] Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey(https://arxiv.org/abs/2402.09283)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.</li>
</ul>

<h3>Title: Learning Interpretable Policies in Hindsight-Observable POMDPs through  Partially Supervised Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, Yevgeniy Vorobeychik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09290">https://arxiv.org/abs/2402.09290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09290">https://arxiv.org/pdf/2402.09290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09290]] Learning Interpretable Policies in Hindsight-Observable POMDPs through  Partially Supervised Reinforcement Learning(https://arxiv.org/abs/2402.09290)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed.</li>
</ul>

<h3>Title: Few-Shot Object Detection with Sparse Context Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jie Mei, Mingyuan Jiu, Hichem Sahbi, Xiaoheng Jiang, Mingliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09315">https://arxiv.org/abs/2402.09315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09315">https://arxiv.org/pdf/2402.09315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09315]] Few-Shot Object Detection with Sparse Context Transformers(https://arxiv.org/abs/2402.09315)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data. One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain. However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.</li>
</ul>

<h3>Title: Only My Model On My Data: A Privacy Preserving Approach Protecting one  Model and Deceiving Unauthorized Black-Box Models</h3>
<ul>
<li><strong>Authors: </strong>Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09316">https://arxiv.org/abs/2402.09316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09316">https://arxiv.org/pdf/2402.09316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09316]] Only My Model On My Data: A Privacy Preserving Approach Protecting one  Model and Deceiving Unauthorized Black-Box Models(https://arxiv.org/abs/2402.09316)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Deep neural networks are extensively applied to real-world tasks, such as face recognition and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. Adversarial attack approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized black-box models of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</li>
</ul>

<h3>Title: ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09320">https://arxiv.org/abs/2402.09320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09320">https://arxiv.org/pdf/2402.09320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09320]] ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization(https://arxiv.org/abs/2402.09320)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.</li>
</ul>

<h3>Title: Stability and Multigroup Fairness in Ranking with Uncertain Predictions</h3>
<ul>
<li><strong>Authors: </strong>Siddartha Devic, Aleksandra Korolova, David Kempe, Vatsal Sharan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09326">https://arxiv.org/abs/2402.09326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09326">https://arxiv.org/pdf/2402.09326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09326]] Stability and Multigroup Fairness in Ranking with Uncertain Predictions(https://arxiv.org/abs/2402.09326)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but -- as we show -- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve multigroup fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.</li>
</ul>

<h3>Title: Mitigating Reward Hacking via Information-Theoretic Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09345">https://arxiv.org/abs/2402.09345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09345">https://arxiv.org/pdf/2402.09345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09345]] Mitigating Reward Hacking via Information-Theoretic Reward Modeling(https://arxiv.org/abs/2402.09345)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of RLHF. Code will be released upon acceptance.</li>
</ul>

<h3>Title: HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference</h3>
<ul>
<li><strong>Authors: </strong>Yashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09360">https://arxiv.org/abs/2402.09360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09360">https://arxiv.org/pdf/2402.09360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09360]] HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference(https://arxiv.org/abs/2402.09360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\times$ on a single TPUv5e device.</li>
</ul>

<h3>Title: Copyright Traps for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Meeus, Igor Shilov, Manuel Faysse, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09363">https://arxiv.org/abs/2402.09363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09363">https://arxiv.org/pdf/2402.09363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09363]] Copyright Traps for Large Language Models(https://arxiv.org/abs/2402.09363)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair, large language model</a></li>
<li><strong>Abstract: </strong>Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being very actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B LLM. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. We further improve these results by studying how the number of times a sequence is seen improves detectability, how sequences with higher perplexity tend to be memorized more, and how taking context into account further improves detectability.</li>
</ul>

<h3>Title: Magic-Me: Identity-Specific Video Customized Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09368">https://arxiv.org/abs/2402.09368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09368">https://arxiv.org/pdf/2402.09368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09368]] Magic-Me: Identity-Specific Video Customized Diffusion(https://arxiv.org/abs/2402.09368)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution. Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.</li>
</ul>

<h3>Title: Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Yi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09369">https://arxiv.org/abs/2402.09369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09369">https://arxiv.org/pdf/2402.09369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09369]] Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking(https://arxiv.org/abs/2402.09369)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.</li>
</ul>

<h3>Title: Pseudorandom Error-Correcting Codes</h3>
<ul>
<li><strong>Authors: </strong>Miranda Christ, Sam Gunn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09370">https://arxiv.org/abs/2402.09370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09370">https://arxiv.org/pdf/2402.09370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09370]] Pseudorandom Error-Correcting Codes(https://arxiv.org/abs/2402.09370)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, watermark</a></li>
<li><strong>Abstract: </strong>We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key. We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density. As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors. Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.</li>
</ul>

<h3>Title: Transformers Can Achieve Length Generalization But Not Robustly</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, Denny Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09371">https://arxiv.org/abs/2402.09371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09371">https://arxiv.org/pdf/2402.09371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09371]] Transformers Can Achieve Length Generalization But Not Robustly(https://arxiv.org/abs/2402.09371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.</li>
</ul>

<h3>Title: Loss Shaping Constraints for Long-Term Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Hounie, Javier Porras-Valenzuela, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09373">https://arxiv.org/abs/2402.09373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09373">https://arxiv.org/pdf/2402.09373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09373]] Loss Shaping Constraints for Long-Term Time Series Forecasting(https://arxiv.org/abs/2402.09373)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical Primal-Dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting benchmarks, while shaping the distribution of errors across the predicted window.</li>
</ul>

<h3>Title: GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly</h3>
<ul>
<li><strong>Authors: </strong>Ali Azizpour, Advait Balaji, Todd J. Treangen, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09381">https://arxiv.org/abs/2402.09381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09381">https://arxiv.org/pdf/2402.09381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09381]] GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly(https://arxiv.org/abs/2402.09381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes. In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection. We evaluate our method using simulated and synthetic metagenomic datasets. The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences. Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall.</li>
</ul>

<h3>Title: Introduction to Physically Unclonable Fuctions: Properties and  Applications</h3>
<ul>
<li><strong>Authors: </strong>M. Garcia-Bosque, G. Díez-Señorans, C. Sánchez-Azqueta, S. Celma</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09386">https://arxiv.org/abs/2402.09386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09386">https://arxiv.org/pdf/2402.09386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09386]] Introduction to Physically Unclonable Fuctions: Properties and  Applications(https://arxiv.org/abs/2402.09386)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>During the last years, Physically Unclonable Functions (PUFs) have become a very important research area in the field of hardware security due to their capability of generating volatile secret keys as well as providing a low-cost authentication. In this paper, an introduction to Physically Unclonable Functions is given, including their definition, properties and applications. Finally, as an example of how to design a PUF, the general structure of a ring oscillator PUF is presented.</li>
</ul>

<h3>Title: Long-form evaluation of model editing</h3>
<ul>
<li><strong>Authors: </strong>Domenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan, Yahya Kayani, Satya Deepika Chavatapalli, Frank Rudzicz, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09394">https://arxiv.org/abs/2402.09394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09394">https://arxiv.org/pdf/2402.09394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09394]] Long-form evaluation of model editing(https://arxiv.org/abs/2402.09394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.</li>
</ul>

<h3>Title: Get More with LESS: Synthesizing Recurrence with KV Cache Compression  for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09398">https://arxiv.org/abs/2402.09398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09398">https://arxiv.org/pdf/2402.09398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09398]] Get More with LESS: Synthesizing Recurrence with KV Cache Compression  for Efficient LLM Inference(https://arxiv.org/abs/2402.09398)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.</li>
</ul>

<h3>Title: Reinforcement Learning from Human Feedback with Active Queries</h3>
<ul>
<li><strong>Authors: </strong>Kaixuan Ji, Jiafan He, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09401">https://arxiv.org/abs/2402.09401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09401">https://arxiv.org/pdf/2402.09401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09401]] Reinforcement Learning from Human Feedback with Active Queries(https://arxiv.org/abs/2402.09401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.</li>
</ul>

<h3>Title: Auditing Private Prediction</h3>
<ul>
<li><strong>Authors: </strong>Karan Chadha, Matthew Jagielski, Nicolas Papernot, Christopher Choquette-Choo, Milad Nasr</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09403">https://arxiv.org/abs/2402.09403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09403">https://arxiv.org/pdf/2402.09403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09403]] Auditing Private Prediction(https://arxiv.org/abs/2402.09403)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of analgorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist forDP training algorithms. However machine learning can also be made private at inference. We propose thefirst framework for auditing private prediction where we instantiate adversaries with varying poisoningand query capabilities. This enables us to study the privacy leakage of four private prediction algorithms:PATE [Papernot et al., 2016], CaPC [Choquette-Choo et al., 2020], PromptPATE [Duan et al., 2023],and Private-kNN [Zhu et al., 2020]. To conduct our audit, we introduce novel techniques to empiricallyevaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis ofprivate prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacyleakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than thosewith full control.</li>
</ul>

<h3>Title: AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential  Reasoning Ability</h3>
<ul>
<li><strong>Authors: </strong>Siwei Yang, Bingchen Zhao, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09404">https://arxiv.org/abs/2402.09404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09404">https://arxiv.org/pdf/2402.09404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09404]] AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential  Reasoning Ability(https://arxiv.org/abs/2402.09404)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
