<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Pose Estimation and Tracking for ASIST. (arXiv:2311.18665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18665">http://arxiv.org/abs/2311.18665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18665]] Pose Estimation and Tracking for ASIST(http://arxiv.org/abs/2311.18665)</code></li>
<li>Summary: <p>Aircraft Ship Integrated Secure and Traverse (ASIST) is a system designed to
arrest helicopters safely and efficiently on ships. Originally, a precision
Helicopter Position Sensing Equipment (HPSE) tracked and monitored the position
of the helicopter relative to the Rapid Securing Device (RSD). However, using
the HPSE component was determined to be infeasible in the transition of the
ASIST system due to the hardware installation requirements. As a result,
sailors track the position of the helicopters with their eyes with no sensor or
artificially intelligent decision aid. Manually tracking the helicopter takes
additional time and makes recoveries more difficult, especially at high sea
states. Performing recoveries without the decision aid leads to higher
uncertainty and cognitive load. PETA (Pose Estimation and Tracking for ASIST)
is a research effort to create a helicopter tracking system prototype without
hardware installation requirements for ASIST system operators. Its overall goal
is to improve situational awareness and reduce operator uncertainty with
respect to the aircrafts position relative to the RSD, and consequently
increase the allowable landing area. The authors produced a prototype system
capable of tracking helicopters with respect to the RSD. The software included
a helicopter pose estimation component, camera pose estimation component, and a
user interface component. PETA demonstrated the potential for state-of-the-art
computer vision algorithms Faster R-CNN and HRNet (High-Resolution Network) to
be used to estimate the pose of helicopters in real-time, returning ASIST to
its originally intended capability. PETA also demonstrated that traditional
methods of encoder-decoders could be used to estimate the orientation of the
helicopter and could be used to confirm the output from HRNet.
</p></li>
</ul>

<h3>Title: AnonPSI: An Anonymity Assessment Framework for PSI. (arXiv:2311.18118v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18118">http://arxiv.org/abs/2311.18118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18118]] AnonPSI: An Anonymity Assessment Framework for PSI(http://arxiv.org/abs/2311.18118)</code></li>
<li>Summary: <p>Private Set Intersection (PSI) is a widely used protocol that enables two
parties to securely compute a function over the intersected part of their
shared datasets and has been a significant research focus over the years.
However, recent studies have highlighted its vulnerability to Set Membership
Inference Attacks (SMIA), where an adversary might deduce an individual's
membership by invoking multiple PSI protocols. This presents a considerable
risk, even in the most stringent versions of PSI, which only return the
cardinality of the intersection. This paper explores the evaluation of
anonymity within the PSI context. Initially, we highlight the reasons why
existing works fall short in measuring privacy leakage, and subsequently
propose two attack strategies that address these deficiencies. Furthermore, we
provide theoretical guarantees on the performance of our proposed methods. In
addition to these, we illustrate how the integration of auxiliary information,
such as the sum of payloads associated with members of the intersection
(PSI-SUM), can enhance attack efficiency. We conducted a comprehensive
performance evaluation of various attack strategies proposed utilizing two real
datasets. Our findings indicate that the methods we propose markedly enhance
attack efficiency when contrasted with previous research endeavors. {The
effective attacking implies that depending solely on existing PSI protocols may
not provide an adequate level of privacy assurance. It is recommended to
combine privacy-enhancing technologies synergistically to enhance privacy
protection even further.
</p></li>
</ul>

<h3>Title: Unclonable Cryptography with Unbounded Collusions. (arXiv:2311.18318v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18318">http://arxiv.org/abs/2311.18318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18318]] Unclonable Cryptography with Unbounded Collusions(http://arxiv.org/abs/2311.18318)</code></li>
<li>Summary: <p>Quantum no-cloning theorem gives rise to the intriguing possibility of
quantum copy protection where we encode a program in a quantum state such that
a user in possession of k such states cannot create k + 1 working copies.
Introduced by Aaronson (CCC 09) over a decade ago, copy protection has proven
to be notoriously hard to achieve.
</p>
<p>In this work, we construct public-key encryption and functional encryption
schemes whose secret keys are copy-protected against unbounded collusions in
the plain model (i.e. without any idealized oracles), assuming (post-quantum)
subexponentially secure iO, one-way functions and LWE. This resolves a
long-standing open question of constructing fully collusion-resistant
copy-protected functionalities raised by multiple previous works.
</p>
<p>Prior to our work, copy-protected functionalities were known only in
restricted collusion models where either an a-priori bound on the collusion
size was needed, in the plain model with the same assumptions as ours (Liu,
Liu, Qian, Zhandry [TCC 22]), or adversary was only prevented from doubling
their number of working programs, in a structured quantum oracle model
(Aaronson [CCC 09]).
</p>
<p>We obtain our results through a novel technique which uses identity-based
encryption to construct unbounded collusion resistant copy-protection schemes
from 1-to-2 secure schemes. This is analogous to the technique of using digital
signatures to construct full-fledged quantum money from single banknote
schemes1 (Lutomirski et al. [ICS 09], Farhi et al. [ITCS 12], Aaronson and
Christiano [STOC 12]). We believe our technique is of independent interest.
</p>
<p>Along the way, we also construct a puncturable functional encryption scheme
whose master secret key can be punctured at all functions f such that f (m0) !=
f (m1). This might also be of independent interest.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Leveraging a Randomized Key Matrix to Enhance the Security of Symmetric Substitution Ciphers. (arXiv:2311.18085v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18085">http://arxiv.org/abs/2311.18085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18085]] Leveraging a Randomized Key Matrix to Enhance the Security of Symmetric Substitution Ciphers(http://arxiv.org/abs/2311.18085)</code></li>
<li>Summary: <p>An innovative strategy to enhance the security of symmetric substitution
ciphers is presented, through the implementation of a randomized key matrix
suitable for various file formats, including but not limited to binary and text
files. Despite their historical relevance, symmetric substitution ciphers have
been limited by vulnerabilities to cryptanalytic methods like frequency
analysis and known plaintext attacks. The aim of our research is to mitigate
these vulnerabilities by employing a polyalphabetic substitution strategy that
incorporates a distinct randomized key matrix. This matrix plays a pivotal role
in generating a unique random key, comprising characters, encompassing both
uppercase and lowercase letters, numeric, and special characters, to derive the
corresponding ciphertext. The effectiveness of the proposed methodology in
enhancing the security of conventional substitution methods for file encryption
and decryption is supported by comprehensive testing and analysis, which
encompass computational speed, frequency analysis, keyspace examination,
Kasiski test, entropy analysis, and the utilization of a large language model.
</p></li>
</ul>

<h3>Title: The Role of Visual Features in Text-Based CAPTCHAs: An fNIRS Study for Usable Security. (arXiv:2311.18436v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18436">http://arxiv.org/abs/2311.18436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18436]] The Role of Visual Features in Text-Based CAPTCHAs: An fNIRS Study for Usable Security(http://arxiv.org/abs/2311.18436)</code></li>
<li>Summary: <p>To mitigate dictionary attacks or similar undesirable automated attacks to
information systems, developers mostly prefer using CAPTCHA challenges as Human
Interactive Proofs (HIPs) to distinguish between human users and scripts.
Appropriate use of CAPTCHA requires a setup that balances between robustness
and usability during the design of a challenge. The previous research reveals
that most usability studies have used accuracy and response time as measurement
criteria for quantitative analysis. The present study aims at applying optical
neuroimaging techniques for the analysis of CAPTCHA design. The functional
Near-Infrared Spectroscopy technique was used to explore the hemodynamic
responses in the prefrontal cortex elicited by CAPTCHA stimulus of varying
types. )e findings suggest that regions in the left and right dorsolateral and
right dorsomedial prefrontal cortex respond to the degrees of line occlusion,
rotation, and wave distortions present in a CAPTCHA. The systematic addition of
the visual effects introduced nonlinear effects on the behavioral and
prefrontal oxygenation measures, indicative of the emergence of Gestalt effects
that might have influenced the perception of the overall CAPTCHA figure.
</p></li>
</ul>

<h3>Title: Scalable and Lightweight Post-Quantum Authentication for Internet of Things. (arXiv:2311.18674v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18674">http://arxiv.org/abs/2311.18674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18674]] Scalable and Lightweight Post-Quantum Authentication for Internet of Things(http://arxiv.org/abs/2311.18674)</code></li>
<li>Summary: <p>Internet of Things (IoT) applications are composed of massive quantities of
resource-limited devices that collect sensitive data with long-term operational
and security requirements. With the threat of emerging quantum computers,
Post-Quantum Cryptography (PQC) is a critical requirement for IoTs. In
particular, digital signatures offer scalable authentication with
non-repudiation and are an essential tool for IoTs. However, as seen in NIST
PQC standardization, post-quantum signatures are extremely costly for
resource-limited IoTs. Hence, there is a significant need for quantum-safe
signatures that respect the processing, memory, and bandwidth limitations of
IoTs. In this paper, we created a new lightweight quantum-safe digital
signature referred to as INFinity-HORS (INF-HORS), which is (to the best of our
knowledge) the first signer-optimal hash-based signature with (polynomially)
unbounded signing capability. INF-HORS enables a verifier to non-interactively
construct one-time public keys from a master public key via encrypted function
evaluations. This strategy avoids the performance bottleneck of hash-based
standards (e.g., SPHINCS+) by eliminating hyper-tree structures. It also does
not require a trusted party or non-colliding servers to distribute public keys.
Our performance analysis confirms that INF-HORS is magnitudes of times more
signer computation efficient than selected NIST PQC schemes (e.g., SPHINCS+,
Dilithium, Falcon) with a small memory footprint.
</p></li>
</ul>

<h3>Title: Self-Supervised Learning for Large-Scale Preventive Security Constrained DC Optimal Power Flow. (arXiv:2311.18072v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18072">http://arxiv.org/abs/2311.18072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18072]] Self-Supervised Learning for Large-Scale Preventive Security Constrained DC Optimal Power Flow(http://arxiv.org/abs/2311.18072)</code></li>
<li>Summary: <p>Security-Constrained Optimal Power Flow (SCOPF) plays a crucial role in power
grid stability but becomes increasingly complex as systems grow. This paper
introduces PDL-SCOPF, a self-supervised end-to-end primal-dual learning
framework for producing near-optimal solutions to large-scale SCOPF problems in
milliseconds. Indeed, PDL-SCOPF remedies the limitations of supervised
counterparts that rely on training instances with their optimal solutions,
which becomes impractical for large-scale SCOPF problems. PDL-SCOPF mimics an
Augmented Lagrangian Method (ALM) for training primal and dual networks that
learn the primal solutions and the Lagrangian multipliers, respectively, to the
unconstrained optimizations. In addition, PDL-SCOPF incorporates a repair layer
to ensure the feasibility of the power balance in the nominal case, and a
binary search layer to compute, using the Automatic Primary Response (APR), the
generator dispatches in the contingencies. The resulting differentiable program
can then be trained end-to-end using the objective function of the SCOPF and
the power balance constraints of the contingencies. Experimental results
demonstrate that the PDL-SCOPF delivers accurate feasible solutions with
minimal optimality gaps. The framework underlying PDL-SCOPF aims at bridging
the gap between traditional optimization methods and machine learning,
highlighting the potential of self-supervised end-to-end primal-dual learning
for large-scale optimization tasks.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: LiDAR-based Outdoor Crowd Management for Smart Campus on the Edge. (arXiv:2311.18077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18077">http://arxiv.org/abs/2311.18077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18077]] LiDAR-based Outdoor Crowd Management for Smart Campus on the Edge(http://arxiv.org/abs/2311.18077)</code></li>
<li>Summary: <p>Crowd management is crucial for a smart campus. Popular methods are
camera-based. However, conventional camera-based approaches may leak users'
personally identifiable features, jeopardizing user's privacy, which limits its
application. In this work, we investigate using affordable light detection and
ranging (LiDAR) technology to perform outdoor crowd management leveraging edge
computing. Specifically, we aim to count the number of people on a walkway of a
university campus. Besides privacy protection, LiDAR sensors are superior to
cameras since their performance will not be compromised when the campus is not
well-illuminated. We deploy LiDAR sensors on light poles to collect data from
the crowd on the campus and leverage edge accelerators to process data locally.
We proposed two different methodologies in this work: 1) a non-convolutional
neural network (CNN)-based approach, using clustering and autoencoder, and 2) a
CNN-based approach that first projects point clouds to 2D planes and then
processes the projection with conventional CNNs. Our first approach relies on
careful feature engineering, whereas our second approach does not require such
effort. However, the CNN-based approach requires more computational power than
our non-CNN-based approach. We evaluate both approaches comprehensively with
our hand-labeled real-life data collected from campus. Our evaluation results
show that the first method achieves an accuracy of 85.4%, whereas the second
method achieves 95.8%. Our CNN-based method outperforms existing solutions
significantly. We also deploy our two models on an edge accelerator, TPU, to
measure the speedup, leveraging this specialized accelerator.
</p></li>
</ul>

<h3>Title: Toward the Tradeoffs between Privacy, Fairness and Utility in Federated Learning. (arXiv:2311.18190v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18190">http://arxiv.org/abs/2311.18190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18190]] Toward the Tradeoffs between Privacy, Fairness and Utility in Federated Learning(http://arxiv.org/abs/2311.18190)</code></li>
<li>Summary: <p>Federated Learning (FL) is a novel privacy-protection distributed machine
learning paradigm that guarantees user privacy and prevents the risk of data
leakage due to the advantage of the client's local training. Researchers have
struggled to design fair FL systems that ensure fairness of results. However,
the interplay between fairness and privacy has been less studied. Increasing
the fairness of FL systems can have an impact on user privacy, while an
increase in user privacy can affect fairness. In this work, on the client side,
we use fairness metrics, such as Demographic Parity (DemP), Equalized Odds
(EOs), and Disparate Impact (DI), to construct the local fair model. To protect
the privacy of the client model, we propose a privacy-protection fairness FL
method. The results show that the accuracy of the fair model with privacy
increases because privacy breaks the constraints of the fairness metrics. In
our experiments, we conclude the relationship between privacy, fairness and
utility, and there is a tradeoff between these.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: TIDE: Test Time Few Shot Object Detection. (arXiv:2311.18358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18358">http://arxiv.org/abs/2311.18358</a></li>
<li>Code URL: https://github.com/deku-0621/tide</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18358]] TIDE: Test Time Few Shot Object Detection(http://arxiv.org/abs/2311.18358)</code></li>
<li>Summary: <p>Few-shot object detection (FSOD) aims to extract semantic knowledge from
limited object instances of novel categories within a target domain. Recent
advances in FSOD focus on fine-tuning the base model based on a few objects via
meta-learning or data augmentation. Despite their success, the majority of them
are grounded with parametric readjustment to generalize on novel objects, which
face considerable challenges in Industry 5.0, such as (i) a certain amount of
fine-tuning time is required, and (ii) the parameters of the constructed model
being unavailable due to the privilege protection, making the fine-tuning fail.
Such constraints naturally limit its application in scenarios with real-time
configuration requirements or within black-box settings. To tackle the
challenges mentioned above, we formalize a novel FSOD task, referred to as Test
TIme Few Shot DEtection (TIDE), where the model is un-tuned in the
configuration procedure. To that end, we introduce an asymmetric architecture
for learning a support-instance-guided dynamic category classifier. Further, a
cross-attention module and a multi-scale resizer are provided to enhance the
model performance. Experimental results on multiple few-shot object detection
platforms reveal that the proposed TIDE significantly outperforms existing
contemporary methods. The implementation codes are available at
https://github.com/deku-0621/TIDE
</p></li>
</ul>

<h3>Title: Detecting Anomalous Network Communication Patterns Using Graph Convolutional Networks. (arXiv:2311.18525v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18525">http://arxiv.org/abs/2311.18525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18525]] Detecting Anomalous Network Communication Patterns Using Graph Convolutional Networks(http://arxiv.org/abs/2311.18525)</code></li>
<li>Summary: <p>To protect an organizations' endpoints from sophisticated cyberattacks,
advanced detection methods are required. In this research, we present
GCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder
(VAE) anomaly detector trained on data that include connection events among
internal and external machines. As input, the proposed GCN-based VAE model
receives two matrices: (i) the normalized adjacency matrix, which represents
the connections among the machines, and (ii) the feature matrix, which includes
various features (demographic, statistical, process-related, and Node2vec
structural features) that are used to profile the individual nodes/machines.
After training the model on data collected for a predefined time window, the
model is applied on the same data; the reconstruction score obtained by the
model for a given machine then serves as the machine's anomaly score.
GCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR
from a large financial organization's automated teller machines (ATMs) as well
as communication with Active Directory (AD) servers in two setups: unsupervised
and supervised. The results of our evaluation demonstrate GCNetOmaly's
effectiveness in detecting anomalous behavior of machines on unsupervised data.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations. (arXiv:2311.18403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18403">http://arxiv.org/abs/2311.18403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18403]] Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations(http://arxiv.org/abs/2311.18403)</code></li>
<li>Summary: <p>Unlearnable datasets lead to a drastic drop in the generalization performance
of models trained on them by introducing elaborate and imperceptible
perturbations into clean training sets. Many existing defenses, e.g., JPEG
compression and adversarial training, effectively counter UDs based on
norm-constrained additive noise. However, a fire-new type of convolution-based
UDs have been proposed and render existing defenses all ineffective, presenting
a greater challenge to defenders. To address this, we express the
convolution-based unlearnable sample as the result of multiplying a matrix by a
clean sample in a simplified scenario, and formalize the intra-class matrix
inconsistency as $\Theta_{imi}$, inter-class matrix consistency as
$\Theta_{imc}$ to investigate the working mechanism of the convolution-based
UDs. We conjecture that increasing both of these metrics will mitigate the
unlearnability effect. Through validation experiments that commendably support
our hypothesis, we further design a random matrix to boost both $\Theta_{imi}$
and $\Theta_{imc}$, achieving a notable degree of defense effect. Hence, by
building upon and extending these facts, we first propose a brand-new image
COrruption that employs randomly multiplicative transformation via
INterpolation operation to successfully defend against convolution-based UDs.
Our approach leverages global pixel random interpolations, effectively
suppressing the impact of multiplicative noise in convolution-based UDs.
Additionally, we have also designed two new forms of convolution-based UDs, and
find that our defense is the most effective against them.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: TeG-DG: Textually Guided Domain Generalization for Face Anti-Spoofing. (arXiv:2311.18420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18420">http://arxiv.org/abs/2311.18420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18420]] TeG-DG: Textually Guided Domain Generalization for Face Anti-Spoofing(http://arxiv.org/abs/2311.18420)</code></li>
<li>Summary: <p>Enhancing the domain generalization performance of Face Anti-Spoofing (FAS)
techniques has emerged as a research focus. Existing methods are dedicated to
extracting domain-invariant features from various training domains. Despite the
promising performance, the extracted features inevitably contain residual style
feature bias (e.g., illumination, capture device), resulting in inferior
generalization performance. In this paper, we propose an alternative and
effective solution, the Textually Guided Domain Generalization (TeG-DG)
framework, which can effectively leverage text information for cross-domain
alignment. Our core insight is that text, as a more abstract and universal form
of expression, can capture the commonalities and essential characteristics
across various attacks, bridging the gap between different image domains.
Contrary to existing vision-language models, the proposed framework is
elaborately designed to enhance the domain generalization ability of the FAS
task. Concretely, we first design a Hierarchical Attention Fusion (HAF) module
to enable adaptive aggregation of visual features at different levels; Then, a
Textual-Enhanced Visual Discriminator (TEVD) is proposed for not only better
alignment between the two modalities but also to regularize the classifier with
unbiased text features. TeG-DG significantly outperforms previous approaches,
especially in situations with extremely limited source domain data (~14% and
~12% improvements on HTER and AUC respectively), showcasing impressive few-shot
performance.
</p></li>
</ul>

<h3>Title: Bridging Both Worlds in Semantics and Time: Domain Knowledge Based Analysis and Correlation of Industrial Process. (arXiv:2311.18539v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18539">http://arxiv.org/abs/2311.18539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18539]] Bridging Both Worlds in Semantics and Time: Domain Knowledge Based Analysis and Correlation of Industrial Process(http://arxiv.org/abs/2311.18539)</code></li>
<li>Summary: <p>Modern industrial control systems (ICS) attacks infect supervisory control
and data acquisition (SCADA) hosts to stealthily alter industrial processes,
causing damage. To detect attacks with low false alarms, recent work detects
attacks in both SCADA and process data. Unfortunately, this led to the same
problem - disjointed (false) alerts, due to the semantic and time gap in SCADA
and process behavior, i.e., SCADA execution does not map to process dynamics
nor evolve at similar time scales. We propose BRIDGE to analyze and correlate
SCADA and industrial process attacks using domain knowledge to bridge their
unique semantic and time evolution. This enables operators to tie malicious
SCADA operations to their adverse process effects, which reduces false alarms
and improves attack understanding. BRIDGE (i) identifies process constraints
violations in SCADA by measuring actuation dependencies in SCADA
process-control, and (ii) detects malicious SCADA effects in processes via a
physics-informed neural network that embeds generic knowledge of inertial
process dynamics. BRIDGE then dynamically aligns both analysis (i and ii) in a
time-window that adjusts their time evolution based on process inertial delays.
We applied BRIDGE to 11 diverse real-world industrial processes, and adaptive
attacks inspired by past events. BRIDGE correlated 98.3% of attacks with 0.8%
false positives (FP), compared to 78.3% detection accuracy and 13.7% FP of
recent work.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem. (arXiv:2311.18107v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18107">http://arxiv.org/abs/2311.18107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18107]] A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem(http://arxiv.org/abs/2311.18107)</code></li>
<li>Summary: <p>Background: Pose estimation of rigid objects is a practical challenge in
optical metrology and computer vision. In this paper a novel
stochastic-geometrical modeling framework for object pose estimation is
presented based on observing multiple feature points. Methods: This
stochastic-geometrical framework utilizes mixture models for the feature point
densities in object space as well as for interpreting real measurements. Direct
advantages of this approach are the avoidance to resolve individual feature
correspondences and to incorporate correct stochastic dependencies in
multi-view applications. First, the general modeling framework is presented,
second, a general algorithm for pose estimation is derived, and third, two
example models for a camera setup as well as a lateration setup are presented.
Results: The numerical experiments show the effectiveness of this modeling and
general algorithm by investigating four simulation scenarios for three
different observation systems, including the dependence on measurement
resolution, object deformations as well as strong measurement noise. It can be
concluded that the probabilistic modeling of pose estimation based on mixture
models can lead to accurate and robust pose estimations.
</p></li>
</ul>

<h3>Title: Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features. (arXiv:2311.18113v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18113">http://arxiv.org/abs/2311.18113</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18113]] Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features(http://arxiv.org/abs/2311.18113)</code></li>
<li>Summary: <p>With the immense growth of dataset sizes and computing resources in recent
years, so-called foundation models have become popular in NLP and vision tasks.
In this work, we propose to explore foundation models for the task of keypoint
detection on 3D shapes. A unique characteristic of keypoint detection is that
it requires semantic and geometric awareness while demanding high localization
accuracy. To address this problem, we propose, first, to back-project features
from large pre-trained 2D vision models onto 3D shapes and employ them for this
task. We show that we obtain robust 3D features that contain rich semantic
information and analyze multiple candidate features stemming from different 2D
foundation models. Second, we employ a keypoint candidate optimization module
which aims to match the average observed distribution of keypoints on the shape
and is guided by the back-projected features. The resulting approach achieves a
new state of the art for few-shot keypoint detection on the KeyPointNet
dataset, almost doubling the performance of the previous best methods.
</p></li>
</ul>

<h3>Title: Anisotropic Neural Representation Learning for High-Quality Neural Rendering. (arXiv:2311.18311v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18311">http://arxiv.org/abs/2311.18311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18311]] Anisotropic Neural Representation Learning for High-Quality Neural Rendering(http://arxiv.org/abs/2311.18311)</code></li>
<li>Summary: <p>Neural radiance fields (NeRFs) have achieved impressive view synthesis
results by learning an implicit volumetric representation from multi-view
images. To project the implicit representation into an image, NeRF employs
volume rendering that approximates the continuous integrals of rays as an
accumulation of the colors and densities of the sampled points. Although this
approximation enables efficient rendering, it ignores the direction information
in point intervals, resulting in ambiguous features and limited reconstruction
quality. In this paper, we propose an anisotropic neural representation
learning method that utilizes learnable view-dependent features to improve
scene representation and reconstruction. We model the volumetric function as
spherical harmonic (SH)-guided anisotropic features, parameterized by
multilayer perceptrons, facilitating ambiguity elimination while preserving the
rendering efficiency. To achieve robust scene reconstruction without anisotropy
overfitting, we regularize the energy of the anisotropic features during
training. Our method is flexiable and can be plugged into NeRF-based
frameworks. Extensive experiments show that the proposed representation can
boost the rendering quality of various NeRFs and achieve state-of-the-art
rendering performance on both synthetic and real-world scenes.
</p></li>
</ul>

<h3>Title: DSeg: Direct Line Segments Detection. (arXiv:2311.18344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18344">http://arxiv.org/abs/2311.18344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18344]] DSeg: Direct Line Segments Detection(http://arxiv.org/abs/2311.18344)</code></li>
<li>Summary: <p>This paper presents a model-driven approach to detect image line segments.
The approach incrementally detects segments on the gradient image using a
linear Kalman filter that estimates the supporting line parameters and their
associated variances. The algorithm is fast and robust with respect to image
noise and illumination variations, it allows the detection of longer line
segments than data-driven approaches, and does not require any tedious
parameters tuning. An extension of the algorithm that exploits a pyramidal
approach to enhance the quality of results is proposed. Results with varying
scene illumination and comparisons to classic existing approaches are
presented.
</p></li>
</ul>

<h3>Title: On Exact Inversion of DPM-Solvers. (arXiv:2311.18387v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18387">http://arxiv.org/abs/2311.18387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18387]] On Exact Inversion of DPM-Solvers(http://arxiv.org/abs/2311.18387)</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) are a key component in modern
generative models. DPM-solvers have achieved reduced latency and enhanced
quality significantly, but have posed challenges to find the exact inverse
(i.e., finding the initial noise from the given image). Here we investigate the
exact inversions for DPM-solvers and propose algorithms to perform them when
samples are generated by the first-order as well as higher-order DPM-solvers.
For each explicit denoising step in DPM-solvers, we formulated the inversions
using implicit methods such as gradient descent or forward step method to
ensure the robustness to large classifier-free guidance unlike the prior
approach using fixed-point iteration. Experimental results demonstrated that
our proposed exact inversion methods significantly reduced the error of both
image and noise reconstructions, greatly enhanced the ability to distinguish
invisible watermarks and well prevented unintended background changes
consistently during image editing. Project page:
\url{https://smhongok.github.io/inv-dpm.html}.
</p></li>
</ul>

<h3>Title: E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning. (arXiv:2311.18433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18433">http://arxiv.org/abs/2311.18433</a></li>
<li>Code URL: https://github.com/xmu-qcj/e2pnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18433]] E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning(http://arxiv.org/abs/2311.18433)</code></li>
<li>Summary: <p>Event cameras have emerged as a promising vision sensor in recent years due
to their unparalleled temporal resolution and dynamic range. While registration
of 2D RGB images to 3D point clouds is a long-standing problem in computer
vision, no prior work studies 2D-3D registration for event cameras. To this
end, we propose E2PNet, the first learning-based method for event-to-point
cloud registration. The core of E2PNet is a novel feature representation
network called Event-Points-to-Tensor (EP2T), which encodes event data into a
2D grid-shaped feature tensor. This grid-shaped feature enables matured
RGB-based frameworks to be easily used for event-to-point cloud registration,
without changing hyper-parameters and the training procedure. EP2T treats the
event input as spatio-temporal point clouds. Unlike standard 3D learning
architectures that treat all dimensions of point clouds equally, the novel
sampling and information aggregation modules in EP2T are designed to handle the
inhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC
and VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and
other learning-based methods. Compared to RGB-based registration, E2PNet is
more robust to extreme illumination or fast motion due to the use of event
data. Beyond 2D-3D registration, we also show the potential of EP2T for other
vision tasks such as flow estimation, event-to-image reconstruction and object
recognition. The source code can be found at:
https://github.com/Xmu-qcj/E2PNet.
</p></li>
</ul>

<h3>Title: HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video. (arXiv:2311.18448v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18448">http://arxiv.org/abs/2311.18448</a></li>
<li>Code URL: https://github.com/zc-alexfan/hold</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18448]] HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video(http://arxiv.org/abs/2311.18448)</code></li>
<li>Summary: <p>Since humans interact with diverse objects every day, the holistic 3D capture
of these interactions is important to understand and model human behaviour.
However, most existing methods for hand-object reconstruction from RGB either
assume pre-scanned object templates or heavily rely on limited 3D hand-object
data, restricting their ability to scale and generalize to more unconstrained
interaction settings. To this end, we introduce HOLD -- the first
category-agnostic method that reconstructs an articulated hand and object
jointly from a monocular interaction video. We develop a compositional
articulated implicit model that can reconstruct disentangled 3D hand and object
from 2D images. We also further incorporate hand-object constraints to improve
hand-object poses and consequently the reconstruction quality. Our method does
not rely on 3D hand-object annotations while outperforming fully-supervised
baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we
qualitatively show its robustness in reconstructing from in-the-wild videos.
Code: https://github.com/zc-alexfan/hold
</p></li>
</ul>

<h3>Title: Fingerprint Matching with Localized Deep Representation. (arXiv:2311.18576v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18576">http://arxiv.org/abs/2311.18576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18576]] Fingerprint Matching with Localized Deep Representation(http://arxiv.org/abs/2311.18576)</code></li>
<li>Summary: <p>Compared to minutia-based fingerprint representations, fixed-length
representations are attractive due to simple and efficient matching. However,
fixed-length fingerprint representations are limited in accuracy when matching
fingerprints with different visible areas, which can occur due to different
finger poses or acquisition methods. To address this issue, we propose a
localized deep representation of fingerprint, named LDRF. By focusing on the
discriminative characteristics within local regions, LDRF provides a more
robust and accurate fixed-length representation for fingerprints with variable
visible areas. LDRF can be adapted to retain information within any valid area,
making it highly flexible. The matching scores produced by LDRF also exhibit
intuitive statistical characteristics, which led us to propose a matching score
normalization technique to mitigate the uncertainty in the cases of very small
overlapping area. With this new technique, we can maintain a high level of
accuracy and reliability in our fingerprint matching, even as the size of the
database grows rapidly. Our experimental results on 21 datasets containing over
140K fingerprints of various finger poses and impression types show that LDRF
outperforms other fixed-length representations and is robust to sensing
technologies and impression types. Besides, the proposed matching score
normalization effectively reduces the false match rate (FMR) in large-scale
identification experiments comprising over 5.11 million fingerprints.
Specifically, this technique results in a reduction of two orders of magnitude
compared to matching without matching score normalization and five orders of
magnitude compared to prior works.
</p></li>
</ul>

<h3>Title: DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image. (arXiv:2311.18610v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18610">http://arxiv.org/abs/2311.18610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18610]] DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and Alignment from an RGB Image(http://arxiv.org/abs/2311.18610)</code></li>
<li>Summary: <p>Perceiving 3D structures from RGB images based on CAD model primitives can
enable an effective, efficient 3D object-based representation of scenes.
However, current approaches rely on supervision from expensive annotations of
CAD models associated with real images, and encounter challenges due to the
inherent ambiguities in the task -- both in depth-scale ambiguity in monocular
perception, as well as inexact matches of CAD database models to real
observations. We thus propose DiffCAD, the first weakly-supervised
probabilistic approach to CAD retrieval and alignment from an RGB image. We
formulate this as a conditional generative task, leveraging diffusion to learn
implicit probabilistic models capturing the shape, pose, and scale of CAD
objects in an image. This enables multi-hypothesis generation of different
plausible CAD reconstructions, requiring only a few hypotheses to characterize
ambiguities in depth/scale and inexact shape matches. Our approach is trained
only on synthetic data, leveraging monocular depth and mask estimates to enable
robust zero-shot adaptation to various real target domains. Despite being
trained solely on synthetic data, our multi-hypothesis approach can even
surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8
hypotheses.
</p></li>
</ul>

<h3>Title: Simple Semantic-Aided Few-Shot Learning. (arXiv:2311.18649v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18649">http://arxiv.org/abs/2311.18649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18649]] Simple Semantic-Aided Few-Shot Learning(http://arxiv.org/abs/2311.18649)</code></li>
<li>Summary: <p>Learning from a limited amount of data, namely Few-Shot Learning, stands out
as a challenging computer vision task. Several works exploit semantics and
design complicated semantic fusion mechanisms to compensate for rare
representative features within restricted data. However, relying on naive
semantics such as class names introduces biases due to their brevity, while
acquiring extensive semantics from external knowledge takes a huge time and
effort. This limitation severely constrains the potential of semantics in
few-shot learning. In this paper, we design an automatic way called Semantic
Evolution to generate high-quality semantics. The incorporation of high-quality
semantics alleviates the need for complex network structures and learning
algorithms used in previous works. Hence, we employ a simple two-layer network
termed Semantic Alignment Network to transform semantics and visual features
into robust class prototypes with rich discriminative features for few-shot
classification. The experimental results show our framework outperforms all
previous methods on five benchmarks, demonstrating a simple network with
high-quality semantics can beat intricate multi-modal modules on few-shot
classification tasks.
</p></li>
</ul>

<h3>Title: ROBBIE: Robust Bias Evaluation of Large Generative Language Models. (arXiv:2311.18140v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18140">http://arxiv.org/abs/2311.18140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18140]] ROBBIE: Robust Bias Evaluation of Large Generative Language Models(http://arxiv.org/abs/2311.18140)</code></li>
<li>Summary: <p>As generative large language models (LLMs) grow more performant and
prevalent, we must develop comprehensive enough tools to measure and improve
their fairness. Different prompt-based datasets can be used to measure social
bias across multiple text domains and demographic axes, meaning that testing
LLMs on more datasets can potentially help us characterize their biases more
fully, and better ensure equal and equitable treatment of marginalized
demographic groups. In this work, our focus is two-fold:
</p>
<p>(1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity
metrics across 12 demographic axes and 5 families of generative LLMs. Out of
those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in
the paper. The comparison of those benchmarks gives us insights about the bias
and toxicity of the compared models. Therefore, we explore the frequency of
demographic terms in common LLM pre-training corpora and how this may relate to
model biases.
</p>
<p>(2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity
mitigation techniques perform across our suite of measurements. ROBBIE aims to
provide insights for practitioners while deploying a model, emphasizing the
need to not only measure potential harms, but also understand how they arise by
characterizing the data, mitigate harms once found, and balance any trade-offs.
We open-source our analysis code in hopes of encouraging broader measurements
of bias in future LLMs.
</p></li>
</ul>

<h3>Title: Decentralized Deepfake Detection Network using Blockchain Technology. (arXiv:2311.18545v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18545">http://arxiv.org/abs/2311.18545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18545]] Decentralized Deepfake Detection Network using Blockchain Technology(http://arxiv.org/abs/2311.18545)</code></li>
<li>Summary: <p>Deepfake technology is a major threat to the integrity of digital media. This
paper presents a comprehensive framework for a blockchain-based decentralized
system designed to tackle the escalating challenge of digital content
integrity. The proposed system integrates advanced deep learning algorithms
with the immutable and transparent nature of blockchain technology to create a
trustless environment where authenticity can be verified without relying on a
single centralized authority. Furthermore, the system utilizes smart contracts
for dynamic algorithm management and token-based incentives further enhances
the system's effectiveness and adaptability. The decentralized architecture of
the system democratizes the process of verifying digital content and introduces
a novel approach to combat deepfakes. The collaborative and adjustable nature
of this system sets a new benchmark for digital media integrity, offering a
more robust digital media environment.
</p></li>
</ul>

<h3>Title: SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation. (arXiv:2311.18206v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18206">http://arxiv.org/abs/2311.18206</a></li>
<li>Code URL: https://github.com/hakuhodo-technologies/scope-rl</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18206]] SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation(http://arxiv.org/abs/2311.18206)</code></li>
<li>Summary: <p>This paper introduces SCOPE-RL, a comprehensive open-source Python software
designed for offline reinforcement learning (offline RL), off-policy evaluation
(OPE), and selection (OPS). Unlike most existing libraries that focus solely on
either policy learning or evaluation, SCOPE-RL seamlessly integrates these two
key aspects, facilitating flexible and complete implementations of both offline
RL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,
offering a range of OPE estimators and robust evaluation-of-OPE protocols. This
approach enables more in-depth and reliable OPE compared to other packages. For
instance, SCOPE-RL enhances OPE by estimating the entire reward distribution
under a policy rather than its mere point-wise expected value. Additionally,
SCOPE-RL provides a more thorough evaluation-of-OPE by presenting the
risk-return tradeoff in OPE results, extending beyond mere accuracy evaluations
in existing OPE literature. SCOPE-RL is designed with user accessibility in
mind. Its user-friendly APIs, comprehensive documentation, and a variety of
easy-to-follow examples assist researchers and practitioners in efficiently
implementing and experimenting with various offline RL methods and OPE
estimators, tailored to their specific problem contexts. The documentation of
SCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.
</p></li>
</ul>

<h3>Title: Learning Robust Precipitation Forecaster by Temporal Frame Interpolation. (arXiv:2311.18341v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18341">http://arxiv.org/abs/2311.18341</a></li>
<li>Code URL: https://github.com/secilia-cxy/unettfi</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18341]] Learning Robust Precipitation Forecaster by Temporal Frame Interpolation(http://arxiv.org/abs/2311.18341)</code></li>
<li>Summary: <p>Recent advancements in deep learning have propelled the field of weather
prediction models to new heights. Despite their progress, these models often
struggle with real-world application due to their sensitivity to
spatial-temporal shifts, a vulnerability particularly pronounced in weather
prediction tasks where overfitting to local and temporal variations is common.
This paper presents an investigation into the development of a robust
precipitation forecasting model that stands resilient to such shifts. We
introduce Temporal Frame Interpolation (TFI), an innovative technique designed
to fortify forecasting models against spatial-temporal discrepancies. TFI
operates by generating synthetic samples through the interpolation of adjacent
frames from satellite imagery and ground radar data, thereby enriching the
training dataset and bolstering the model's defense against noise on frames.
Additionally, we integrate a novel multi-level dice loss, which exploits the
ordinal nature of rainfall intensities to further refine model performance.
These methodologies have collectively advanced our model's forecasting
precision, achieving \textit{1st place} on the transfer learning leaderboard in
the \textit{Weather4Cast'23 competition}.It not only demonstrates the efficacy
of our approaches but also sets a new benchmark for deep learning applications
in meteorological forecasting. Our code and weights have been public on
\url{https://github.com/Secilia-Cxy/UNetTFI}.
</p></li>
</ul>

<h3>Title: Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying Speed Conditions. (arXiv:2311.18547v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18547">http://arxiv.org/abs/2311.18547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18547]] Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying Speed Conditions(http://arxiv.org/abs/2311.18547)</code></li>
<li>Summary: <p>Detection of rolling-element bearing faults is crucial for implementing
proactive maintenance strategies and for minimizing the economic and
operational consequences of unexpected failures. However, many existing
techniques are developed and tested under strictly controlled conditions,
limiting their adaptability to the diverse and dynamic settings encountered in
practical applications. This paper presents an efficient real-time
convolutional neural network (CNN) for diagnosing multiple bearing faults under
various noise levels and time-varying rotational speeds. Additionally, we
propose a novel Fisher-based spectral separability analysis (SSA) method to
elucidate the effectiveness of the designed CNN model. We conducted experiments
on both healthy bearings and bearings afflicted with inner race, outer race,
and roller ball faults. The experimental results show the superiority of our
model over the current state-of-the-art approach in three folds: it achieves
substantial accuracy gains of up to 15.8%, it is robust to noise with high
performance across various signal-to-noise ratios, and it runs in real-time
with processing durations five times less than acquisition. Additionally, by
using the proposed SSA technique, we offer insights into the model's
performance and underscore its effectiveness in tackling real-world challenges.
</p></li>
</ul>

<h3>Title: Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations. (arXiv:2311.18575v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18575">http://arxiv.org/abs/2311.18575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18575]] Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations(http://arxiv.org/abs/2311.18575)</code></li>
<li>Summary: <p>Distribution shifts between training and deployment data often affect the
performance of machine learning models. In this paper, we explore a setting
where a hidden variable induces a shift in the distribution of classes. These
distribution shifts are particularly challenging for zero-shot classifiers, as
they rely on representations learned from training classes, but are deployed on
new, unseen ones. We introduce an algorithm to learn data representations that
are robust to such class distribution shifts in zero-shot verification tasks.
We show that our approach, which combines hierarchical data sampling with
out-of-distribution generalization techniques, improves generalization to
diverse class distributions in both simulations and real-world datasets.
</p></li>
</ul>

<h3>Title: Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural Networks. (arXiv:2311.18587v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18587">http://arxiv.org/abs/2311.18587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18587]] Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural Networks(http://arxiv.org/abs/2311.18587)</code></li>
<li>Summary: <p>In the field of deep learning, the prevalence of models initially trained
with 32-bit precision is a testament to its robustness and accuracy. However,
the continuous evolution of these models often demands further training, which
can be resource-intensive. This study introduces a novel approach where we
continue the training of these pre-existing 32-bit models using 16-bit
precision. This technique not only caters to the need for efficiency in
computational resources but also significantly improves the speed of additional
training phases. By adopting 16-bit precision for ongoing training, we are able
to substantially decrease memory requirements and computational burden, thereby
accelerating the training process in a resource-limited setting. Our
experiments show that this method maintains the high standards of accuracy set
by the original 32-bit training while providing a much-needed boost in training
speed. This approach is especially pertinent in today's context, where most
models are initially trained in 32-bit and require periodic updates and
refinements. The findings from our research suggest that this strategy of
16-bit continuation training can be a key solution for sustainable and
efficient deep learning, offering a practical way to enhance pre-trained models
rapidly and in a resource-conscious manner.
</p></li>
</ul>

<h3>Title: Generalisable Agents for Neural Network Optimisation. (arXiv:2311.18598v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18598">http://arxiv.org/abs/2311.18598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18598]] Generalisable Agents for Neural Network Optimisation(http://arxiv.org/abs/2311.18598)</code></li>
<li>Summary: <p>Optimising deep neural networks is a challenging task due to complex training
dynamics, high computational requirements, and long training times. To address
this difficulty, we propose the framework of Generalisable Agents for Neural
Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL)
approach that learns to improve neural network optimisation by dynamically and
responsively scheduling hyperparameters during training. GANNO utilises an
agent per layer that observes localised network dynamics and accordingly takes
actions to adjust these dynamics at a layerwise level to collectively improve
global performance. In this paper, we use GANNO to control the layerwise
learning rate and show that the framework can yield useful and responsive
schedules that are competitive with handcrafted heuristics. Furthermore, GANNO
is shown to perform robustly across a wide variety of unseen initial
conditions, and can successfully generalise to harder problems than it was
trained on. Our work presents an overview of the opportunities that this
paradigm offers for training neural networks, along with key challenges that
remain to be overcome.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Multilevel Saliency-Guided Self-Supervised Learning for Image Anomaly Detection. (arXiv:2311.18332v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18332">http://arxiv.org/abs/2311.18332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18332]] Multilevel Saliency-Guided Self-Supervised Learning for Image Anomaly Detection(http://arxiv.org/abs/2311.18332)</code></li>
<li>Summary: <p>Anomaly detection (AD) is a fundamental task in computer vision. It aims to
identify incorrect image data patterns which deviate from the normal ones.
Conventional methods generally address AD by preparing augmented negative
samples to enforce self-supervised learning. However, these techniques
typically do not consider semantics during augmentation, leading to the
generation of unrealistic or invalid negative samples. Consequently, the
feature extraction network can be hindered from embedding critical features. In
this study, inspired by visual attention learning approaches, we propose
CutSwap, which leverages saliency guidance to incorporate semantic cues for
augmentation. Specifically, we first employ LayerCAM to extract multilevel
image features as saliency maps and then perform clustering to obtain multiple
centroids. To fully exploit saliency guidance, on each map, we select a pixel
pair from the cluster with the highest centroid saliency to form a patch pair.
Such a patch pair includes highly similar context information with dense
semantic correlations. The resulting negative sample is created by swapping the
locations of the patch pair. Compared to prior augmentation methods, CutSwap
generates more subtle yet realistic negative samples to facilitate quality
feature learning. Extensive experimental and ablative evaluations demonstrate
that our method achieves state-of-the-art AD performance on two mainstream AD
benchmark datasets.
</p></li>
</ul>

<h3>Title: ESG Accountability Made Easy: DocQA at Your Service. (arXiv:2311.18481v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18481">http://arxiv.org/abs/2311.18481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18481]] ESG Accountability Made Easy: DocQA at Your Service(http://arxiv.org/abs/2311.18481)</code></li>
<li>Summary: <p>We present Deep Search DocQA. This application enables information extraction
from documents via a question-answering conversational assistant. The system
integrates several technologies from different AI disciplines consisting of
document conversion to machine-readable format (via computer vision), finding
relevant data (via natural language processing), and formulating an eloquent
response (via large language models). Users can explore over 10,000
Environmental, Social, and Governance (ESG) disclosure reports from over 2000
corporations. The Deep Search platform can be accessed at:
https://ds4sd.github.io.
</p></li>
</ul>

<h3>Title: PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing. (arXiv:2311.18494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18494">http://arxiv.org/abs/2311.18494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18494]] PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing(http://arxiv.org/abs/2311.18494)</code></li>
<li>Summary: <p>Surface reconstruction with preservation of geometric features is a
challenging computer vision task. Despite significant progress in implicit
shape reconstruction, state-of-the-art mesh extraction methods often produce
aliased, perceptually distorted surfaces and lack scalability to
high-resolution 3D shapes. We present a data-driven approach for automatic
feature detection and remeshing that requires only a coarse, aliased mesh as
input and scales to arbitrary resolution reconstructions. We define and learn a
collection of surface-based fields to (1) capture sharp geometric features in
the shape with an implicit vertexwise model and (2) approximate improvements in
normals alignment obtained by applying edge-flips with an edgewise model. To
support scaling to arbitrary complexity shapes, we learn our fields using local
triangulated patches, fusing estimates on complete surface meshes. Our feature
remeshing algorithm integrates the learned fields as sharp feature priors and
optimizes vertex placement and mesh connectivity for maximum expected surface
improvement. On a challenging collection of high-resolution shape
reconstructions in the ABC dataset, our algorithm improves over
state-of-the-art by 26% normals F-score and 42% perceptual
$\text{RMSE}_{\text{v}}$.
</p></li>
</ul>

<h3>Title: Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy. (arXiv:2311.18664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18664">http://arxiv.org/abs/2311.18664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18664]] Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy(http://arxiv.org/abs/2311.18664)</code></li>
<li>Summary: <p>Colonoscopy screening is the gold standard procedure for assessing
abnormalities in the colon and rectum, such as ulcers and cancerous polyps.
Measuring the abnormal mucosal area and its 3D reconstruction can help quantify
the surveyed area and objectively evaluate disease burden. However, due to the
complex topology of these organs and variable physical conditions, for example,
lighting, large homogeneous texture, and image modality estimating distance
from the camera aka depth) is highly challenging. Moreover, most colonoscopic
video acquisition is monocular, making the depth estimation a non-trivial
problem. While methods in computer vision for depth estimation have been
proposed and advanced on natural scene datasets, the efficacy of these
techniques has not been widely quantified on colonoscopy datasets. As the
colonic mucosa has several low-texture regions that are not well pronounced,
learning representations from an auxiliary task can improve salient feature
extraction, allowing estimation of accurate camera depths. In this work, we
propose to develop a novel multi-task learning (MTL) approach with a shared
encoder and two decoders, namely a surface normal decoder and a depth estimator
decoder. Our depth estimator incorporates attention mechanisms to enhance
global context awareness. We leverage the surface normal prediction to improve
geometric feature extraction. Also, we apply a cross-task consistency loss
among the two geometrically related tasks, surface normal and camera depth. We
demonstrate an improvement of 14.17% on relative error and 10.4% improvement on
$\delta_{1}$ accuracy over the most accurate baseline state-of-the-art BTS
approach. All experiments are conducted on a recently released C3VD dataset;
thus, we provide a first benchmark of state-of-the-art methods.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FediOS: Decoupling Orthogonal Subspaces for Personalization in Feature-skew Federated Learning. (arXiv:2311.18559v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18559">http://arxiv.org/abs/2311.18559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18559]] FediOS: Decoupling Orthogonal Subspaces for Personalization in Feature-skew Federated Learning(http://arxiv.org/abs/2311.18559)</code></li>
<li>Summary: <p>Personalized federated learning (pFL) enables collaborative training among
multiple clients to enhance the capability of customized local models. In pFL,
clients may have heterogeneous (also known as non-IID) data, which poses a key
challenge in how to decouple the data knowledge into generic knowledge for
global sharing and personalized knowledge for preserving local personalization.
A typical way of pFL focuses on label distribution skew, and they adopt a
decoupling scheme where the model is split into a common feature extractor and
two prediction heads (generic and personalized). However, such a decoupling
scheme cannot solve the essential problem of feature skew heterogeneity,
because a common feature extractor cannot decouple the generic and personalized
features. Therefore, in this paper, we rethink the architecture decoupling
design for feature-skew pFL and propose an effective pFL method called FediOS.
In FediOS, we reformulate the decoupling into two feature extractors (generic
and personalized) and one shared prediction head. Orthogonal projections are
used for clients to map the generic features into one common subspace and
scatter the personalized features into different subspaces to achieve
decoupling for them. In addition, a shared prediction head is trained to
balance the importance of generic and personalized features during inference.
Extensive experiments on four vision datasets demonstrate our method reaches
state-of-the-art pFL performances under feature skew heterogeneity.
</p></li>
</ul>

<h3>Title: Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum. (arXiv:2311.18578v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18578">http://arxiv.org/abs/2311.18578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18578]] Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum(http://arxiv.org/abs/2311.18578)</code></li>
<li>Summary: <p>Federated Learning (FL) is the state-of-the-art approach for learning from
decentralized data in privacy-constrained scenarios. As the current literature
reports, the main problems associated with FL refer to system and statistical
challenges: the former ones demand for efficient learning from edge devices,
including lowering communication bandwidth and frequency, while the latter
require algorithms robust to non-iidness. State-of-art approaches either
guarantee convergence at increased communication cost or are not sufficiently
robust to handle extreme heterogeneous local distributions. In this work we
propose a novel generalization of the heavy-ball momentum, and present FedHBM
to effectively address statistical heterogeneity in FL without introducing any
communication overhead. We conduct extensive experimentation on common FL
vision and NLP datasets, showing that our FedHBM algorithm empirically yields
better model quality and higher convergence speed w.r.t. the state-of-art,
especially in pathological non-iid scenarios. While being designed for
cross-silo settings, we show how FedHBM is applicable in moderate-to-high
cross-device scenarios, and how good model initializations (e.g. pre-training)
can be exploited for prompt acceleration. Extended experimentation on
large-scale real-world federated datasets further corroborates the
effectiveness of our approach for real-world FL applications.
</p></li>
</ul>

<h3>Title: Data-Agnostic Model Poisoning against Federated Learning: A Graph Autoencoder Approach. (arXiv:2311.18498v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18498">http://arxiv.org/abs/2311.18498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18498]] Data-Agnostic Model Poisoning against Federated Learning: A Graph Autoencoder Approach(http://arxiv.org/abs/2311.18498)</code></li>
<li>Summary: <p>This paper proposes a novel, data-agnostic, model poisoning attack on
Federated Learning (FL), by designing a new adversarial graph autoencoder
(GAE)-based framework. The attack requires no knowledge of FL training data and
achieves both effectiveness and undetectability. By listening to the benign
local models and the global model, the attacker extracts the graph structural
correlations among the benign local models and the training data features
substantiating the models. The attacker then adversarially regenerates the
graph structural correlations while maximizing the FL training loss, and
subsequently generates malicious local models using the adversarial graph
structure and the training data features of the benign ones. A new algorithm is
designed to iteratively train the malicious local models using GAE and
sub-gradient descent. The convergence of FL under attack is rigorously proved,
with a considerably large optimality gap. Experiments show that the FL accuracy
drops gradually under the proposed attack and existing defense mechanisms fail
to detect it. The attack can give rise to an infection across all benign
devices, making it a serious threat to FL.
</p></li>
</ul>

<h3>Title: CommunityAI: Towards Community-based Federated Learning. (arXiv:2311.17958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17958">http://arxiv.org/abs/2311.17958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17958]] CommunityAI: Towards Community-based Federated Learning(http://arxiv.org/abs/2311.17958)</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as a promising paradigm to train machine
learning models collaboratively while preserving data privacy. However, its
widespread adoption faces several challenges, including scalability,
heterogeneous data and devices, resource constraints, and security concerns.
Despite its promise, FL has not been specifically adapted for community
domains, primarily due to the wide-ranging differences in data types and
context, devices and operational conditions, environmental factors, and
stakeholders. In response to these challenges, we present a novel framework for
Community-based Federated Learning called CommunityAI. CommunityAI enables
participants to be organized into communities based on their shared interests,
expertise, or data characteristics. Community participants collectively
contribute to training and refining learning models while maintaining data and
participant privacy within their respective groups. Within this paper, we
discuss the conceptual architecture, system requirements, processes, and future
challenges that must be solved. Finally, our goal within this paper is to
present our vision regarding enabling a collaborative learning process within
various communities.
</p></li>
</ul>

<h3>Title: Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices. (arXiv:2311.18129v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18129">http://arxiv.org/abs/2311.18129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18129]] Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices(http://arxiv.org/abs/2311.18129)</code></li>
<li>Summary: <p>While federated learning (FL) systems often utilize quantization to battle
communication and computational bottlenecks, they have heretofore been limited
to deploying fixed-precision quantization schemes. Meanwhile, the concept of
mixed-precision quantization (MPQ), where different layers of a deep learning
model are assigned varying bit-width, remains unexplored in the FL settings. We
present a novel FL algorithm, FedMPQ, which introduces mixed-precision
quantization to resource-heterogeneous FL systems. Specifically, local models,
quantized so as to satisfy bit-width constraint, are trained by optimizing an
objective function that includes a regularization term which promotes reduction
of precision in some of the layers without significant performance degradation.
The server collects local model updates, de-quantizes them into full-precision
models, and then aggregates them into a global model. To initialize the next
round of local training, the server relies on the information learned in the
previous training round to customize bit-width assignments of the models
delivered to different clients. In extensive benchmarking experiments on
several model architectures and different datasets in both iid and non-iid
settings, FedMPQ outperformed the baseline FL schemes that utilize
fixed-precision quantization while incurring only a minor computational
overhead on the participating devices.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity. (arXiv:2311.18580v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18580">http://arxiv.org/abs/2311.18580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18580]] FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity(http://arxiv.org/abs/2311.18580)</code></li>
<li>Summary: <p>The widespread of generative artificial intelligence has heightened concerns
about the potential harms posed by AI-generated texts, primarily stemming from
factoid, unfair, and toxic content. Previous researchers have invested much
effort in assessing the harmlessness of generative language models. However,
existing benchmarks are struggling in the era of large language models (LLMs),
due to the stronger language generation and instruction following capabilities,
as well as wider applications. In this paper, we propose FFT, a new benchmark
with 2116 elaborated-designed instances, for LLM harmlessness evaluation with
factuality, fairness, and toxicity. To investigate the potential harms of LLMs,
we evaluate 9 representative LLMs covering various parameter scales, training
stages, and creators. Experiments show that the harmlessness of LLMs is still
under-satisfactory, and extensive analysis derives some insightful findings
that could inspire future research for harmless LLM research.
</p></li>
</ul>

<h3>Title: Towards Comparable Active Learning. (arXiv:2311.18356v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18356">http://arxiv.org/abs/2311.18356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18356]] Towards Comparable Active Learning(http://arxiv.org/abs/2311.18356)</code></li>
<li>Summary: <p>Active Learning has received significant attention in the field of machine
learning for its potential in selecting the most informative samples for
labeling, thereby reducing data annotation costs. However, we show that the
reported lifts in recent literature generalize poorly to other domains leading
to an inconclusive landscape in Active Learning research. Furthermore, we
highlight overlooked problems for reproducing AL experiments that can lead to
unfair comparisons and increased variance in the results. This paper addresses
these issues by providing an Active Learning framework for a fair comparison of
algorithms across different tasks and domains, as well as a fast and performant
oracle algorithm for evaluation. To the best of our knowledge, we propose the
first AL benchmark that tests algorithms in 3 major domains: Tabular, Image,
and Text. We report empirical results for 6 widely used algorithms on 7
real-world and 2 synthetic datasets and aggregate them into a domain-specific
ranking of AL algorithms.
</p></li>
</ul>

<h3>Title: Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework. (arXiv:2311.18460v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18460">http://arxiv.org/abs/2311.18460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18460]] Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework(http://arxiv.org/abs/2311.18460)</code></li>
<li>Summary: <p>Fairness for machine learning predictions is widely required in practice for
legal, ethical, and societal reasons. Existing work typically focuses on
settings without unobserved confounding, even though unobserved confounding can
lead to severe violations of causal fairness and, thus, unfair predictions. In
this work, we analyze the sensitivity of causal fairness to unobserved
confounding. Our contributions are three-fold. First, we derive bounds for
causal fairness metrics under different sources of unobserved confounding. This
enables practitioners to examine the sensitivity of their machine learning
models to unobserved confounding in fairness-critical applications. Second, we
propose a novel neural framework for learning fair predictions, which allows us
to offer worst-case guarantees of the extent to which causal fairness can be
violated due to unobserved confounding. Third, we demonstrate the effectiveness
of our framework in a series of experiments, including a real-world case study
about predicting prison sentences. To the best of our knowledge, ours is the
first work to study causal fairness under unobserved confounding. To this end,
our work is of direct practical value as a refutation strategy to ensure the
fairness of predictions in high-stakes applications.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition. (arXiv:2311.18254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18254">http://arxiv.org/abs/2311.18254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18254]] Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition(http://arxiv.org/abs/2311.18254)</code></li>
<li>Summary: <p>With the recent surge in the use of touchscreen devices, free-hand sketching
has emerged as a promising modality for human-computer interaction. While
previous research has focused on tasks such as recognition, retrieval, and
generation of familiar everyday objects, this study aims to create a Sketch
Input Method Editor (SketchIME) specifically designed for a professional C4I
system. Within this system, sketches are utilized as low-fidelity prototypes
for recommending standardized symbols in the creation of comprehensive
situation maps. This paper also presents a systematic dataset comprising 374
specialized sketch types, and proposes a simultaneous recognition and
segmentation architecture with multilevel supervision between recognition and
segmentation to improve performance and enhance interpretability. By
incorporating few-shot domain adaptation and class-incremental learning, the
network's ability to adapt to new users and extend to new task-specific classes
is significantly enhanced. Results from experiments conducted on both the
proposed dataset and the SPG dataset illustrate the superior performance of the
proposed architecture. Our dataset and code are publicly available at
https://github.com/Anony517/SketchIME.
</p></li>
</ul>

<h3>Title: Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18296">http://arxiv.org/abs/2311.18296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18296]] Perceptual Group Tokenizer: Building Perception with Iterative Grouping(http://arxiv.org/abs/2311.18296)</code></li>
<li>Summary: <p>Human visual recognition system shows astonishing capability of compressing
visual information into a set of tokens containing rich representations without
label supervision. One critical driving principle behind it is perceptual
grouping. Despite being widely used in computer vision in the early 2010s, it
remains a mystery whether perceptual grouping can be leveraged to derive a
neural visual recognition backbone that generates as powerful representations.
In this paper, we propose the Perceptual Group Tokenizer, a model that entirely
relies on grouping operations to extract visual features and perform
self-supervised representation learning, where a series of grouping operations
are used to iteratively hypothesize the context for pixels or superpixels to
refine feature representations. We show that the proposed model can achieve
competitive performance compared to state-of-the-art vision architectures, and
inherits desirable properties including adaptive computation without
re-training, and interpretability. Specifically, Perceptual Group Tokenizer
achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear
probe evaluation, marking a new progress under this paradigm.
</p></li>
</ul>

<h3>Title: Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings. (arXiv:2311.18034v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18034">http://arxiv.org/abs/2311.18034</a></li>
<li>Code URL: https://github.com/andreawwenyi/hyperpolyglot</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18034]] Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings(http://arxiv.org/abs/2311.18034)</code></li>
<li>Summary: <p>Cross-lingual transfer learning is an important property of multilingual
large language models (LLMs). But how do LLMs represent relationships between
languages? Every language model has an input layer that maps tokens to vectors.
This ubiquitous layer of language models is often overlooked. We find that
similarities between these input embeddings are highly interpretable and that
the geometry of these embeddings differs between model families. In one case
(XLM-RoBERTa), embeddings encode language: tokens in different writing systems
can be linearly separated with an average of 99.2% accuracy. Another family
(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors
for any token represent an average of 7.61 writing systems, and are frequently
translations. This result is surprising given that there is no explicit
parallel cross-lingual training corpora and no explicit incentive for
translations in pre-training objectives. Our research opens the door for
investigations in 1) The effect of pre-training and model architectures on
representations of languages and 2) The applications of cross-lingual
representations embedded in language models.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: TrustMark: Universal Watermarking for Arbitrary Resolution Images. (arXiv:2311.18297v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18297">http://arxiv.org/abs/2311.18297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18297]] TrustMark: Universal Watermarking for Arbitrary Resolution Images(http://arxiv.org/abs/2311.18297)</code></li>
<li>Summary: <p>Imperceptible digital watermarking is important in copyright protection,
misinformation prevention, and responsible generative AI. We propose TrustMark
- a GAN-based watermarking method with novel design in architecture and
spatio-spectra losses to balance the trade-off between watermarked image
quality with the watermark recovery accuracy. Our model is trained with
robustness in mind, withstanding various in- and out-place perturbations on the
encoded image. Additionally, we introduce TrustMark-RM - a watermark remover
method useful for re-watermarking. Our methods achieve state-of-art performance
on 3 benchmarks comprising arbitrary resolution images.
</p></li>
</ul>

<h3>Title: I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text. (arXiv:2311.18054v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18054">http://arxiv.org/abs/2311.18054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18054]] I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text(http://arxiv.org/abs/2311.18054)</code></li>
<li>Summary: <p>Potential harms of Large Language Models such as mass misinformation and
plagiarism can be partially mitigated if there exists a reliable way to detect
machine generated text. In this paper, we propose a new watermarking method to
detect machine-generated texts. Our method embeds a unique pattern within the
generated text, ensuring that while the content remains coherent and natural to
human readers, it carries distinct markers that can be identified
algorithmically. Specifically, we intervene with the token sampling process in
a way which enables us to trace back our token choices during the detection
phase. We show how watermarking affects textual quality and compare our
proposed method with a state-of-the-art watermarking method in terms of
robustness and detectability. Through extensive experiments, we demonstrate the
effectiveness of our watermarking scheme in distinguishing between watermarked
and non-watermarked text, achieving high detection rates while maintaining
textual quality.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Unlocking Spatial Comprehension in Text-to-Image Diffusion Models. (arXiv:2311.17937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17937">http://arxiv.org/abs/2311.17937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17937]] Unlocking Spatial Comprehension in Text-to-Image Diffusion Models(http://arxiv.org/abs/2311.17937)</code></li>
<li>Summary: <p>We propose CompFuser, an image generation pipeline that enhances spatial
comprehension and attribute assignment in text-to-image generative models. Our
pipeline enables the interpretation of instructions defining spatial
relationships between objects in a scene, such as `An image of a gray cat on
the left of an orange dog', and generate corresponding images. This is
especially important in order to provide more control to the user. CompFuser
overcomes the limitation of existing text-to-image diffusion models by decoding
the generation of multiple objects into iterative steps: first generating a
single object and then editing the image by placing additional objects in their
designated positions. To create training data for spatial comprehension and
attribute assignment we introduce a synthetic data generation process, that
leverages a frozen large language model and a frozen layout-based diffusion
model for object placement. We compare our approach to strong baselines and
show that our model outperforms state-of-the-art image generation models in
spatial comprehension and attribute assignment, despite being 3x to 5x smaller
in parameters.
</p></li>
</ul>

<h3>Title: DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback. (arXiv:2311.17946v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17946">http://arxiv.org/abs/2311.17946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17946]] DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback(http://arxiv.org/abs/2311.17946)</code></li>
<li>Summary: <p>Despite their wide-spread success, Text-to-Image models (T2I) still struggle
to produce images that are both aesthetically pleasing and faithful to the
user's input text. We introduce DreamSync, a model-agnostic training algorithm
by design that improves T2I models to be faithful to the text input. DreamSync
builds off a recent insight from TIFA's evaluation framework -- that large
vision-language models (VLMs) can effectively identify the fine-grained
discrepancies between generated images and the text inputs. DreamSync uses this
insight to train T2I models without any labeled data; it improves T2I models
using its own generations. First, it prompts the model to generate several
candidate images for a given input text. Then, it uses two VLMs to select the
best generation: a Visual Question Answering model that measures the alignment
of generated images to the text, and another that measures the generation's
aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I
model to guide its generation towards the selected best generations. DreamSync
does not need any additional human annotation. model architecture changes, or
reinforcement learning. Despite its simplicity, DreamSync improves both the
semantic alignment and aesthetic appeal of two diffusion-based T2I models,
evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA
aesthetic) and human evaluation.
</p></li>
</ul>

<h3>Title: PEAN: A Diffusion-based Prior-Enhanced Attention Network for Scene Text Image Super-Resolution. (arXiv:2311.17955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17955">http://arxiv.org/abs/2311.17955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17955]] PEAN: A Diffusion-based Prior-Enhanced Attention Network for Scene Text Image Super-Resolution(http://arxiv.org/abs/2311.17955)</code></li>
<li>Summary: <p>Scene text image super-resolution (STISR) aims at simultaneously increasing
the resolution and readability of low-resolution scene text images, thus
boosting the performance of the downstream recognition task. Two factors in
scene text images, semantic information and visual structure, affect the
recognition performance significantly. To mitigate the effects from these
factors, this paper proposes a Prior-Enhanced Attention Network (PEAN).
Specifically, a diffusion-based module is developed to enhance the text prior,
hence offering better guidance for the SR network to generate SR images with
higher semantic accuracy. Meanwhile, the proposed PEAN leverages an
attention-based modulation module to understand scene text images by neatly
perceiving the local and global dependence of images, despite the shape of the
text. A multi-task learning paradigm is employed to optimize the network,
enabling the model to generate legible SR images. As a result, PEAN establishes
new SOTA results on the TextZoom benchmark. Experiments are also conducted to
analyze the importance of the enhanced text prior as a means of improving the
performance of the SR network. Code will be made available at
https://github.com/jdfxzzy/PEAN.
</p></li>
</ul>

<h3>Title: HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting. (arXiv:2311.17957v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17957">http://arxiv.org/abs/2311.17957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17957]] HandRefiner: Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting(http://arxiv.org/abs/2311.17957)</code></li>
<li>Summary: <p>Diffusion models have achieved remarkable success in generating realistic
images but suffer from generating accurate human hands, such as incorrect
finger counts or irregular shapes. This difficulty arises from the complex task
of learning the physical structure and pose of hands from training images,
which involves extensive deformations and occlusions. For correct hand
generation, our paper introduces a lightweight post-processing solution called
$\textbf{HandRefiner}$. HandRefiner employs a conditional inpainting approach
to rectify malformed hands while leaving other parts of the image untouched. We
leverage the hand mesh reconstruction model that consistently adheres to the
correct number of fingers and hand shape, while also being capable of fitting
the desired hand pose in the generated image. Given a generated failed image
due to malformed hands, we utilize ControlNet modules to re-inject such correct
hand information. Additionally, we uncover a phase transition phenomenon within
ControlNet as we vary the control strength. It enables us to take advantage of
more readily available synthetic data without suffering from the domain gap
between realistic and synthetic hands. Experiments demonstrate that HandRefiner
can significantly improve the generation quality quantitatively and
qualitatively. The code is available at
https://github.com/wenquanlu/HandRefiner .
</p></li>
</ul>

<h3>Title: ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model. (arXiv:2311.17963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17963">http://arxiv.org/abs/2311.17963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17963]] ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model(http://arxiv.org/abs/2311.17963)</code></li>
<li>Summary: <p>As the capabilities of Large-Language Models (LLMs) become widely recognized,
there is an increasing demand for human-machine chat applications. Human
interaction with text often inherently invokes mental imagery, an aspect that
existing LLM-based chatbots like GPT-4 do not currently emulate, as they are
confined to generating text-only content. To bridge this gap, we introduce
ChatIllusion, an advanced Generative multimodal large language model (MLLM)
that combines the capabilities of LLM with not only visual comprehension but
also creativity. Specifically, ChatIllusion integrates Stable Diffusion XL and
Llama, which have been fine-tuned on modest image-caption data, to facilitate
multiple rounds of illustrated chats. The central component of ChatIllusion is
the "GenAdapter," an efficient approach that equips the multimodal language
model with capabilities for visual representation, without necessitating
modifications to the foundational model. Extensive experiments validate the
efficacy of our approach, showcasing its ability to produce diverse and
superior-quality image outputs Simultaneously, it preserves semantic
consistency and control over the dialogue, significantly enhancing the overall
user's quality of experience (QoE). The code is available at
https://github.com/litwellchi/ChatIllusion.
</p></li>
</ul>

<h3>Title: GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation. (arXiv:2311.17971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17971">http://arxiv.org/abs/2311.17971</a></li>
<li>Code URL: https://github.com/baaivision/uni3d</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17971]] GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation(http://arxiv.org/abs/2311.17971)</code></li>
<li>Summary: <p>Text-to-3D generation by distilling pretrained large-scale text-to-image
diffusion models has shown great promise but still suffers from inconsistent 3D
geometric structures (Janus problems) and severe artifacts. The aforementioned
problems mainly stem from 2D diffusion models lacking 3D awareness during the
lifting. In this work, we present GeoDream, a novel method that incorporates
explicit generalized 3D priors with 2D diffusion priors to enhance the
capability of obtaining unambiguous 3D consistent geometric structures without
sacrificing diversity or fidelity. Specifically, we first utilize a multi-view
diffusion model to generate posed images and then construct cost volume from
the predicted image, which serves as native 3D geometric priors, ensuring
spatial consistency in 3D space. Subsequently, we further propose to harness 3D
geometric priors to unlock the great potential of 3D awareness in 2D diffusion
priors via a disentangled design. Notably, disentangling 2D and 3D priors
allows us to refine 3D geometric priors further. We justify that the refined 3D
geometric priors aid in the 3D-aware capability of 2D diffusion priors, which
in turn provides superior guidance for the refinement of 3D geometric priors.
Our numerical and visual comparisons demonstrate that GeoDream generates more
3D consistent textured meshes with high-resolution realistic renderings (i.e.,
1024 $\times$ 1024) and adheres more closely to semantic coherence.
</p></li>
</ul>

<h3>Title: 4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling. (arXiv:2311.17984v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17984">http://arxiv.org/abs/2311.17984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17984]] 4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling(http://arxiv.org/abs/2311.17984)</code></li>
<li>Summary: <p>Recent breakthroughs in text-to-4D generation rely on pre-trained
text-to-image and text-to-video models to generate dynamic 3D scenes. However,
current text-to-4D methods face a three-way tradeoff between the quality of
scene appearance, 3D structure, and motion. For example, text-to-image models
and their 3D-aware variants are trained on internet-scale image datasets and
can be used to produce scenes with realistic appearance and 3D structure -- but
no motion. Text-to-video models are trained on relatively smaller video
datasets and can produce scenes with motion, but poorer appearance and 3D
structure. While these models have complementary strengths, they also have
opposing weaknesses, making it difficult to combine them in a way that
alleviates this three-way tradeoff. Here, we introduce hybrid score
distillation sampling, an alternating optimization procedure that blends
supervision signals from multiple pre-trained diffusion models and incorporates
benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we
demonstrate synthesis of 4D scenes with compelling appearance, 3D structure,
and motion.
</p></li>
</ul>

<h3>Title: Turn Down the Noise: Leveraging Diffusion Models for Test-time Adaptation via Pseudo-label Ensembling. (arXiv:2311.18071v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18071">http://arxiv.org/abs/2311.18071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18071]] Turn Down the Noise: Leveraging Diffusion Models for Test-time Adaptation via Pseudo-label Ensembling(http://arxiv.org/abs/2311.18071)</code></li>
<li>Summary: <p>The goal of test-time adaptation is to adapt a source-pretrained model to a
continuously changing target domain without relying on any source data.
Typically, this is either done by updating the parameters of the model (model
adaptation) using inputs from the target domain or by modifying the inputs
themselves (input adaptation). However, methods that modify the model suffer
from the issue of compounding noisy updates whereas methods that modify the
input need to adapt to every new data point from scratch while also struggling
with certain domain shifts. We introduce an approach that leverages a
pre-trained diffusion model to project the target domain images closer to the
source domain and iteratively updates the model via pseudo-label ensembling.
Our method combines the advantages of model and input adaptations while
mitigating their shortcomings. Our experiments on CIFAR-10C demonstrate the
superiority of our approach, outperforming the strongest baseline by an average
of 1.7% across 15 diverse corruptions and surpassing the strongest input
adaptation baseline by an average of 18%.
</p></li>
</ul>

<h3>Title: Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing. (arXiv:2311.18082v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18082">http://arxiv.org/abs/2311.18082</a></li>
<li>Code URL: https://github.com/allenai/satlas-super-resolution</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18082]] Zooming Out on Zooming In: Advancing Super-Resolution for Remote Sensing(http://arxiv.org/abs/2311.18082)</code></li>
<li>Summary: <p>Super-Resolution for remote sensing has the potential for huge impact on
planet monitoring by producing accurate and realistic high resolution imagery
on a frequent basis and a global scale. Despite a lot of attention, several
inconsistencies and challenges have prevented it from being deployed in
practice. These include the lack of effective metrics, fragmented and
relatively small-scale datasets for training, insufficient comparisons across a
suite of methods, and unclear evidence for the use of super-resolution outputs
for machine consumption. This work presents a new metric for super-resolution,
CLIPScore, that corresponds far better with human judgments than previous
metrics on an extensive study. We use CLIPScore to evaluate four standard
methods on a new large-scale dataset, S2-NAIP, and three existing benchmark
datasets, and find that generative adversarial networks easily outperform more
traditional L2 loss-based models and are more semantically accurate than modern
diffusion models. We also find that using CLIPScore as an auxiliary loss can
speed up the training of GANs by 18x and lead to improved outputs, resulting in
an effective model in diverse geographies across the world which we will
release publicly. The dataset, pre-trained model weights, and code are
available at https://github.com/allenai/satlas-super-resolution/.
</p></li>
</ul>

<h3>Title: HiPA: Enabling One-Step Text-to-Image Diffusion Models via High-Frequency-Promoting Adaptation. (arXiv:2311.18158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18158">http://arxiv.org/abs/2311.18158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18158]] HiPA: Enabling One-Step Text-to-Image Diffusion Models via High-Frequency-Promoting Adaptation(http://arxiv.org/abs/2311.18158)</code></li>
<li>Summary: <p>Diffusion models have revolutionized text-to-image generation, but their
real-world applications are hampered by the extensive time needed for hundreds
of diffusion steps. Although progressive distillation has been proposed to
speed up diffusion sampling to 2-8 steps, it still falls short in one-step
generation, and necessitates training multiple student models, which is highly
parameter-extensive and time-consuming. To overcome these limitations, we
introduce High-frequency-Promoting Adaptation (HiPA), a parameter-efficient
approach to enable one-step text-to-image diffusion. Grounded in the insight
that high-frequency information is essential but highly lacking in one-step
diffusion, HiPA focuses on training one-step, low-rank adaptors to specifically
enhance the under-represented high-frequency abilities of advanced diffusion
models. The learned adaptors empower these diffusion models to generate
high-quality images in just a single step. Compared with progressive
distillation, HiPA achieves much better performance in one-step text-to-image
generation (37.3 $\rightarrow$ 23.8 in FID-5k on MS-COCO 2017) and 28.6x
training speed-up (108.8 $\rightarrow$ 3.8 A100 GPU days), requiring only 0.04%
training parameters (7,740 million $\rightarrow$ 3.3 million). We also
demonstrate HiPA's effectiveness in text-guided image editing, inpainting and
super-resolution tasks, where our adapted models consistently deliver
high-quality outputs in just one diffusion step. The source code will be
released.
</p></li>
</ul>

<h3>Title: SMaRt: Improving GANs with Score Matching Regularity. (arXiv:2311.18208v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18208">http://arxiv.org/abs/2311.18208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18208]] SMaRt: Improving GANs with Score Matching Regularity(http://arxiv.org/abs/2311.18208)</code></li>
<li>Summary: <p>Generative adversarial networks (GANs) usually struggle in learning from
highly diverse data, whose underlying manifold is complex. In this work, we
revisit the mathematical foundations of GANs, and theoretically reveal that the
native adversarial loss for GAN training is insufficient to fix the problem of
subsets with positive Lebesgue measure of the generated data manifold lying out
of the real data manifold. Instead, we find that score matching serves as a
valid solution to this issue thanks to its capability of persistently pushing
the generated data points towards the real data manifold. We thereby propose to
improve the optimization of GANs with score matching regularity (SMaRt).
Regarding the empirical evidences, we first design a toy example to show that
training GANs by the aid of a ground-truth score function can help reproduce
the real data distribution more accurately, and then confirm that our approach
can consistently boost the synthesis performance of various state-of-the-art
GANs on real-world datasets with pre-trained diffusion models acting as the
approximate score function. For instance, when training Aurora on the ImageNet
64x64 dataset, we manage to improve FID from 8.87 to 7.11, on par with the
performance of one-step consistency model. The source code will be made public.
</p></li>
</ul>

<h3>Title: Diffusion Models Without Attention. (arXiv:2311.18257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18257">http://arxiv.org/abs/2311.18257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18257]] Diffusion Models Without Attention(http://arxiv.org/abs/2311.18257)</code></li>
<li>Summary: <p>In recent advancements in high-fidelity image generation, Denoising Diffusion
Probabilistic Models (DDPMs) have emerged as a key player. However, their
application at high resolutions presents significant computational challenges.
Current methods, such as patchifying, expedite processes in UNet and
Transformer architectures but at the expense of representational capacity.
Addressing this, we introduce the Diffusion State Space Model (DiffuSSM), an
architecture that supplants attention mechanisms with a more scalable state
space model backbone. This approach effectively handles higher resolutions
without resorting to global compression, thus preserving detailed image
representation throughout the diffusion process. Our focus on FLOP-efficient
architectures in diffusion training marks a significant step forward.
Comprehensive evaluations on both ImageNet and LSUN datasets at two resolutions
demonstrate that DiffuSSMs are on par or even outperform existing diffusion
models with attention modules in FID and Inception Score metrics while
significantly reducing total FLOP usage.
</p></li>
</ul>

<h3>Title: Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning. (arXiv:2311.18266v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18266">http://arxiv.org/abs/2311.18266</a></li>
<li>Code URL: https://github.com/kerrydrx/escort</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18266]] Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning(http://arxiv.org/abs/2311.18266)</code></li>
<li>Summary: <p>Replay-based methods in class-incremental learning (CIL) have attained
remarkable success, as replaying the exemplars of old classes can significantly
mitigate catastrophic forgetting. Despite their effectiveness, the inherent
memory restrictions of CIL result in saving a limited number of exemplars with
poor diversity, leading to data imbalance and overfitting issues. In this
paper, we introduce a novel exemplar super-compression and regeneration method,
ESCORT, which substantially increases the quantity and enhances the diversity
of exemplars. Rather than storing past images, we compress images into visual
and textual prompts, e.g., edge maps and class tags, and save the prompts
instead, reducing the memory usage of each exemplar to 1/24 of the original
size. In subsequent learning phases, diverse high-resolution exemplars are
generated from the prompts by a pre-trained diffusion model, e.g., ControlNet.
To minimize the domain gap between generated exemplars and real images, we
propose partial compression and diffusion-based data augmentation, allowing us
to utilize an off-the-shelf diffusion model without fine-tuning it on the
target dataset. Therefore, the same diffusion model can be downloaded whenever
it is needed, incurring no memory consumption. Comprehensive experiments
demonstrate that our method significantly improves model performance across
multiple CIL benchmarks, e.g., 5.0 percentage points higher than the previous
state-of-the-art on 10-phase Caltech-256 dataset.
</p></li>
</ul>

<h3>Title: CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model. (arXiv:2311.18405v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18405">http://arxiv.org/abs/2311.18405</a></li>
<li>Code URL: https://github.com/zengjianhao/cat-dm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18405]] CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model(http://arxiv.org/abs/2311.18405)</code></li>
<li>Summary: <p>Image-based virtual try-on enables users to virtually try on different
garments by altering original clothes in their photographs. Generative
Adversarial Networks (GANs) dominate the research field in image-based virtual
try-on, but have not resolved problems such as unnatural deformation of
garments and the blurry generation quality. Recently, diffusion models have
emerged with surprising performance across various image generation tasks.
While the generative quality of diffusion models is impressive, achieving
controllability poses a significant challenge when applying it to virtual
try-on tasks and multiple denoising iterations limit its potential for
real-time applications. In this paper, we propose Controllable Accelerated
virtual Try-on with Diffusion Model called CAT-DM. To enhance the
controllability, a basic diffusion-based virtual try-on network is designed,
which utilizes ControlNet to introduce additional control conditions and
improves the feature extraction of garment images. In terms of acceleration,
CAT-DM initiates a reverse denoising process with an implicit distribution
generated by a pre-trained GAN-based model. Compared with previous try-on
methods based on diffusion models, CAT-DM not only retains the pattern and
texture details of the in-shop garment but also reduces the sampling steps
without compromising generation quality. Extensive experiments demonstrate the
superiority of CAT-DM against both GAN-based and diffusion-based methods in
producing more realistic images and accurately reproducing garment patterns.
Our code and models will be publicly released.
</p></li>
</ul>

<h3>Title: Layered Rendering Diffusion Model for Zero-Shot Guided Image Synthesis. (arXiv:2311.18435v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18435">http://arxiv.org/abs/2311.18435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18435]] Layered Rendering Diffusion Model for Zero-Shot Guided Image Synthesis(http://arxiv.org/abs/2311.18435)</code></li>
<li>Summary: <p>This paper introduces innovative solutions to enhance spatial controllability
in diffusion models reliant on text queries. We present two key innovations:
Vision Guidance and the Layered Rendering Diffusion (LRDiff) framework. Vision
Guidance, a spatial layout condition, acts as a clue in the perturbed
distribution, greatly narrowing down the search space, to focus on the image
sampling process adhering to the spatial layout condition. The LRDiff framework
constructs an image-rendering process with multiple layers, each of which
applies the vision guidance to instructively estimate the denoising direction
for a single object. Such a layered rendering strategy effectively prevents
issues like unintended conceptual blending or mismatches, while allowing for
more coherent and contextually accurate image synthesis. The proposed method
provides a more efficient and accurate means of synthesising images that align
with specific spatial and contextual requirements. We demonstrate through our
experiments that our method provides better results than existing techniques
both quantitatively and qualitatively. We apply our method to three practical
applications: bounding box-to-image, semantic mask-to-image and image editing.
</p></li>
</ul>

<h3>Title: Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing. (arXiv:2311.18608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18608">http://arxiv.org/abs/2311.18608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18608]] Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing(http://arxiv.org/abs/2311.18608)</code></li>
<li>Summary: <p>With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.
</p></li>
</ul>

<h3>Title: DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars. (arXiv:2311.18635v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18635">http://arxiv.org/abs/2311.18635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18635]] DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars(http://arxiv.org/abs/2311.18635)</code></li>
<li>Summary: <p>DiffusionAvatars synthesizes a high-fidelity 3D head avatar of a person,
offering intuitive control over both pose and expression. We propose a
diffusion-based neural renderer that leverages generic 2D priors to produce
compelling images of faces. For coarse guidance of the expression and head
pose, we render a neural parametric head model (NPHM) from the target
viewpoint, which acts as a proxy geometry of the person. Additionally, to
enhance the modeling of intricate facial expressions, we condition
DiffusionAvatars directly on the expression codes obtained from NPHM via
cross-attention. Finally, to synthesize consistent surface details across
different viewpoints and expressions, we rig learnable spatial features to the
head's surface via TriPlane lookup in NPHM's canonical space. We train
DiffusionAvatars on RGB videos and corresponding tracked NPHM meshes of a
person and test the obtained avatars in both self-reenactment and animation
scenarios. Our experiments demonstrate that DiffusionAvatars generates
temporally consistent and visually appealing videos for novel poses and
expressions of a person, outperforming existing approaches.
</p></li>
</ul>

<h3>Title: Detailed Human-Centric Text Description-Driven Large Scene Synthesis. (arXiv:2311.18654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18654">http://arxiv.org/abs/2311.18654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18654]] Detailed Human-Centric Text Description-Driven Large Scene Synthesis(http://arxiv.org/abs/2311.18654)</code></li>
<li>Summary: <p>Text-driven large scene image synthesis has made significant progress with
diffusion models, but controlling it is challenging. While using additional
spatial controls with corresponding texts has improved the controllability of
large scene synthesis, it is still challenging to faithfully reflect detailed
text descriptions without user-provided controls. Here, we propose
DetText2Scene, a novel text-driven large-scale image synthesis with high
faithfulness, controllability, and naturalness in a global context for the
detailed human-centric text description. Our DetText2Scene consists of 1)
hierarchical keypoint-box layout generation from the detailed description by
leveraging large language model (LLM), 2) view-wise conditioned joint diffusion
process to synthesize a large scene from the given detailed text with
LLM-generated grounded keypoint-box layout and 3) pixel perturbation-based
pyramidal interpolation to progressively refine the large scene for global
coherence. Our DetText2Scene significantly outperforms prior arts in
text-to-large scene synthesis qualitatively and quantitatively, demonstrating
strong faithfulness with detailed descriptions, superior controllability, and
excellent naturalness in a global context.
</p></li>
</ul>

<h3>Title: C3Net: Compound Conditioned ControlNet for Multimodal Content Generation. (arXiv:2311.17951v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17951">http://arxiv.org/abs/2311.17951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17951]] C3Net: Compound Conditioned ControlNet for Multimodal Content Generation(http://arxiv.org/abs/2311.17951)</code></li>
<li>Summary: <p>We present Compound Conditioned ControlNet, C3Net, a novel generative neural
architecture taking conditions from multiple modalities and synthesizing
multimodal contents simultaneously (e.g., image, text, audio). C3Net adapts the
ControlNet architecture to jointly train and make inferences on a
production-ready diffusion model and its trainable copies. Specifically, C3Net
first aligns the conditions from multi-modalities to the same semantic latent
space using modality-specific encoders based on contrastive training. Then, it
generates multimodal outputs based on the aligned latent space, whose semantic
information is combined using a ControlNet-like architecture called Control
C3-UNet. Correspondingly, with this system design, our model offers an improved
solution for joint-modality generation through learning and explaining
multimodal conditions instead of simply taking linear interpolations on the
latent space. Meanwhile, as we align conditions to a unified latent space,
C3Net only requires one trainable Control C3-UNet to work on multimodal
semantic information. Furthermore, our model employs unimodal pretraining on
the condition alignment stage, outperforming the non-pretrained alignment even
on relatively scarce training data and thus demonstrating high-quality compound
condition generation. We contribute the first high-quality tri-modal validation
set to validate quantitatively that C3Net outperforms or is on par with first
and contemporary state-of-the-art multimodal generation. Our codes and
tri-modal dataset will be released.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Contrastive Vision-Language Alignment Makes Efficient Instruction Learner. (arXiv:2311.17945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17945">http://arxiv.org/abs/2311.17945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17945]] Contrastive Vision-Language Alignment Makes Efficient Instruction Learner(http://arxiv.org/abs/2311.17945)</code></li>
<li>Summary: <p>We study the task of extending the large language model (LLM) into a
vision-language instruction-following model. This task is crucial but
challenging since the LLM is trained on text modality only, making it hard to
effectively digest the visual modality. To address this, existing methods
typically train a visual adapter to align the representation between a
pre-trained vision transformer (ViT) and the LLM by a generative image
captioning loss. However, we find that the generative objective can only
produce weak alignment for vision and language, making the aligned
vision-language model very hungry for the instruction fine-tuning data. In this
paper, we propose CG-VLM that applies both Contrastive and Generative alignment
objectives to effectively align the representation of ViT and LLM. Different
from image level and sentence level alignment in common contrastive learning
settings, CG-VLM aligns the image-patch level features and text-token level
embeddings, which, however, is very hard to achieve as no explicit grounding
patch-token relation provided in standard image captioning datasets. To address
this issue, we propose to maximize the averaged similarity between pooled
image-patch features and text-token embeddings. Extensive experiments
demonstrate that the proposed CG-VLM produces strong vision-language alignment
and is an efficient instruction learner. For example, using only 10%
instruction tuning data, we reach 95% performance of state-of-the-art method
LLaVA [29] on the zero-shot ScienceQA-Image benchmark.
</p></li>
</ul>

<h3>Title: Transformer-empowered Multi-modal Item Embedding for Enhanced Image Search in E-Commerce. (arXiv:2311.17954v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17954">http://arxiv.org/abs/2311.17954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17954]] Transformer-empowered Multi-modal Item Embedding for Enhanced Image Search in E-Commerce(http://arxiv.org/abs/2311.17954)</code></li>
<li>Summary: <p>Over the past decade, significant advances have been made in the field of
image search for e-commerce applications. Traditional image-to-image retrieval
models, which focus solely on image details such as texture, tend to overlook
useful semantic information contained within the images. As a result, the
retrieved products might possess similar image details, but fail to fulfil the
user's search goals. Moreover, the use of image-to-image retrieval models for
products containing multiple images results in significant online product
feature storage overhead and complex mapping implementations. In this paper, we
report the design and deployment of the proposed Multi-modal Item Embedding
Model (MIEM) to address these limitations. It is capable of utilizing both
textual information and multiple images about a product to construct meaningful
product features. By leveraging semantic information from images, MIEM
effectively supplements the image search process, improving the overall
accuracy of retrieval results. MIEM has become an integral part of the Shopee
image search platform. Since its deployment in March 2023, it has achieved a
remarkable 9.90% increase in terms of clicks per user and a 4.23% boost in
terms of orders per user for the image search feature on the Shopee e-commerce
platform.
</p></li>
</ul>

<h3>Title: QuadraNet: Improving High-Order Neural Interaction Efficiency with Hardware-Aware Quadratic Neural Networks. (arXiv:2311.17956v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17956">http://arxiv.org/abs/2311.17956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17956]] QuadraNet: Improving High-Order Neural Interaction Efficiency with Hardware-Aware Quadratic Neural Networks(http://arxiv.org/abs/2311.17956)</code></li>
<li>Summary: <p>Recent progress in computer vision-oriented neural network designs is mostly
driven by capturing high-order neural interactions among inputs and features.
And there emerged a variety of approaches to accomplish this, such as
Transformers and its variants. However, these interactions generate a large
amount of intermediate state and/or strong data dependency, leading to
considerable memory consumption and computing cost, and therefore compromising
the overall runtime performance. To address this challenge, we rethink the
high-order interactive neural network design with a quadratic computing
approach. Specifically, we propose QuadraNet -- a comprehensive model design
methodology from neuron reconstruction to structural block and eventually to
the overall neural network implementation. Leveraging quadratic neurons'
intrinsic high-order advantages and dedicated computation optimization schemes,
QuadraNet could effectively achieve optimal cognition and computation
performance. Incorporating state-of-the-art hardware-aware neural architecture
search and system integration techniques, QuadraNet could also be well
generalized in different hardware constraint settings and deployment scenarios.
The experiment shows thatQuadraNet achieves up to 1.5$\times$ throughput, 30%
less memory footprint, and similar cognition performance, compared with the
state-of-the-art high-order approaches.
</p></li>
</ul>

<h3>Title: GeoDeformer: Geometric Deformable Transformer for Action Recognition. (arXiv:2311.17975v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17975">http://arxiv.org/abs/2311.17975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17975]] GeoDeformer: Geometric Deformable Transformer for Action Recognition(http://arxiv.org/abs/2311.17975)</code></li>
<li>Summary: <p>Vision transformers have recently emerged as an effective alternative to
convolutional networks for action recognition. However, vision transformers
still struggle with geometric variations prevalent in video data. This paper
proposes a novel approach, GeoDeformer, designed to capture the variations
inherent in action video by integrating geometric comprehension directly into
the ViT architecture. Specifically, at the core of GeoDeformer is the Geometric
Deformation Predictor, a module designed to identify and quantify potential
spatial and temporal geometric deformations within the given video. Spatial
deformations adjust the geometry within individual frames, while temporal
deformations capture the cross-frame geometric dynamics, reflecting motion and
temporal progression. To demonstrate the effectiveness of our approach, we
incorporate it into the established MViTv2 framework, replacing the standard
self-attention blocks with GeoDeformer blocks. Our experiments at UCF101,
HMDB51, and Mini-K200 achieve significant increases in both Top-1 and Top-5
accuracy, establishing new state-of-the-art results with only a marginal
increase in computational cost. Additionally, visualizations affirm that
GeoDeformer effectively manifests explicit geometric deformations and minimizes
geometric variations. Codes and checkpoints will be released.
</p></li>
</ul>

<h3>Title: Improving Faithfulness for Vision Transformers. (arXiv:2311.17983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17983">http://arxiv.org/abs/2311.17983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17983]] Improving Faithfulness for Vision Transformers(http://arxiv.org/abs/2311.17983)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have achieved state-of-the-art performance for
various vision tasks. One reason behind the success lies in their ability to
provide plausible innate explanations for the behavior of neural architectures.
However, ViTs suffer from issues with explanation faithfulness, as their focal
points are fragile to adversarial attacks and can be easily changed with even
slight perturbations on the input image. In this paper, we propose a rigorous
approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly
speaking, an FViT should have the following two properties: (1) The top-$k$
indices of its self-attention vector should remain mostly unchanged under input
perturbation, indicating stable explanations; (2) The prediction distribution
should be robust to perturbations. To achieve this, we propose a new method
called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing
and diffusion-based denoising. We theoretically prove that processing ViTs
directly with DDS can turn them into FViTs. We also show that Gaussian noise is
nearly optimal for both $\ell_2$ and $\ell_\infty$-norm cases. Finally, we
demonstrate the effectiveness of our approach through comprehensive experiments
and evaluations. Specifically, we compare our FViTs with other baselines
through visual interpretation and robustness accuracy under adversarial
attacks. Results show that FViTs are more robust against adversarial attacks
while maintaining the explainability of attention, indicating higher
faithfulness.
</p></li>
</ul>

<h3>Title: LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News. (arXiv:2311.18241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18241">http://arxiv.org/abs/2311.18241</a></li>
<li>Code URL: https://github.com/joshzyj/llvms4protest</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18241]] LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News(http://arxiv.org/abs/2311.18241)</code></li>
<li>Summary: <p>Large language and vision models have transformed how social movements
scholars identify protest and extract key protest attributes from multi-modal
data such as texts, images, and videos. This article documents how we
fine-tuned two large pretrained transformer models, including longformer and
swin-transformer v2, to infer potential protests in news articles using textual
and imagery data. First, the longformer model was fine-tuned using the Dynamic
of Collective Action (DoCA) Corpus. We matched the New York Times articles with
the DoCA database to obtain a training dataset for downstream tasks. Second,
the swin-transformer v2 models was trained on UCLA-protest imagery data.
UCLA-protest project contains labeled imagery data with information such as
protest, violence, and sign. Both fine-tuned models will be available via
\url{https://github.com/Joshzyj/llvms4protest}. We release this short technical
report for social movement scholars who are interested in using LLVMs to infer
protests in textual and imagery data.
</p></li>
</ul>

<h3>Title: OmniMotionGPT: Animal Motion Generation with Limited Data. (arXiv:2311.18303v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18303">http://arxiv.org/abs/2311.18303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18303]] OmniMotionGPT: Animal Motion Generation with Limited Data(http://arxiv.org/abs/2311.18303)</code></li>
<li>Summary: <p>Our paper aims to generate diverse and realistic animal motion sequences from
textual descriptions, without a large-scale animal text-motion dataset. While
the task of text-driven human motion synthesis is already extensively studied
and benchmarked, it remains challenging to transfer this success to other
skeleton structures with limited data. In this work, we design a model
architecture that imitates Generative Pretraining Transformer (GPT), utilizing
prior knowledge learned from human data to the animal domain. We jointly train
motion autoencoders for both animal and human motions and at the same time
optimize through the similarity scores among human motion encoding, animal
motion encoding, and text CLIP embedding. Presenting the first solution to this
problem, we are able to generate animal motions with high diversity and
fidelity, quantitatively and qualitatively outperforming the results of
training human motion generation baselines on animal data. Additionally, we
introduce AnimalML3D, the first text-animal motion dataset with 1240 animation
sequences spanning 36 different animal identities. We hope this dataset would
mediate the data scarcity problem in text-driven animal motion generation,
providing a new playground for the research community.
</p></li>
</ul>

<h3>Title: Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent. (arXiv:2311.18307v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18307">http://arxiv.org/abs/2311.18307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18307]] Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent(http://arxiv.org/abs/2311.18307)</code></li>
<li>Summary: <p>Adept traffic models are critical to both planning and closed-loop simulation
for autonomous vehicles (AV), and key design objectives include accuracy,
diverse multimodal behaviors, interpretability, and downstream compatibility.
Recently, with the advent of large language models (LLMs), an additional
desirable feature for traffic models is LLM compatibility. We present
Categorical Traffic Transformer (CTT), a traffic model that outputs both
continuous trajectory predictions and tokenized categorical predictions (lane
modes, homotopies, etc.). The most outstanding feature of CTT is its fully
interpretable latent space, which enables direct supervision of the latent
variable from the ground truth during training and avoids mode collapse
completely. As a result, CTT can generate diverse behaviors conditioned on
different latent modes with semantic meanings while beating SOTA on prediction
accuracy. In addition, CTT's ability to input and output tokens enables
integration with LLMs for common-sense reasoning and zero-shot generalization.
</p></li>
</ul>

<h3>Title: MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation. (arXiv:2311.18537v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18537">http://arxiv.org/abs/2311.18537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18537]] MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation(http://arxiv.org/abs/2311.18537)</code></li>
<li>Summary: <p>Video panoptic segmentation requires consistently segmenting (for both
`thing' and `stuff' classes) and tracking objects in a video over time. In this
work, we present MaXTron, a general framework that exploits Mask XFormer with
Trajectory Attention to tackle the task. MaXTron enriches an off-the-shelf mask
transformer by leveraging trajectory attention. The deployed mask transformer
takes as input a short clip consisting of only a few frames and predicts the
clip-level segmentation. To enhance the temporal consistency, MaXTron employs
within-clip and cross-clip tracking modules, efficiently utilizing trajectory
attention. Originally designed for video classification, trajectory attention
learns to model the temporal correspondences between neighboring frames and
aggregates information along the estimated motion paths. However, it is
nontrivial to directly extend trajectory attention to the per-pixel dense
prediction tasks due to its quadratic dependency on input size. To alleviate
the issue, we propose to adapt the trajectory attention for both the dense
pixel features and object queries, aiming to improve the short-term and
long-term tracking results, respectively. Particularly, in our within-clip
tracking module, we propose axial-trajectory attention that effectively
computes the trajectory attention for tracking dense pixels sequentially along
the height- and width-axes. The axial decomposition significantly reduces the
computational complexity for dense pixel features. In our cross-clip tracking
module, since the object queries in mask transformer are learned to encode the
object information, we are able to capture the long-term temporal connections
by applying trajectory attention to object queries, which learns to track each
object across different clips. Without bells and whistles, MaXTron demonstrates
state-of-the-art performances on video segmentation benchmarks.
</p></li>
</ul>

<h3>Title: Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models. (arXiv:2311.18592v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18592">http://arxiv.org/abs/2311.18592</a></li>
<li>Code URL: https://github.com/event-ahu/safe_largevlm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18592]] Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models(http://arxiv.org/abs/2311.18592)</code></li>
<li>Summary: <p>Pattern recognition through the fusion of RGB frames and Event streams has
emerged as a novel research area in recent years. Current methods typically
employ backbone networks to individually extract the features of RGB frames and
event streams, and subsequently fuse these features for pattern recognition.
However, we posit that these methods may suffer from key issues like sematic
gaps and small-scale backbone networks. In this study, we introduce a novel
pattern recognition framework that consolidates the semantic labels, RGB
frames, and event streams, leveraging pre-trained large-scale vision-language
models. Specifically, given the input RGB frames, event streams, and all the
predefined semantic labels, we employ a pre-trained large-scale vision model
(CLIP vision encoder) to extract the RGB and event features. To handle the
semantic labels, we initially convert them into language descriptions through
prompt engineering, and then obtain the semantic features using the pre-trained
large-scale language model (CLIP text encoder). Subsequently, we integrate the
RGB/Event features and semantic features using multimodal Transformer networks.
The resulting frame and event tokens are further amplified using self-attention
layers. Concurrently, we propose to enhance the interactions between text
tokens and RGB/Event tokens via cross-attention. Finally, we consolidate all
three modalities using self-attention and feed-forward layers for recognition.
Comprehensive experiments on the HARDVS and PokerEvent datasets fully
substantiate the efficacy of our proposed SAFE model. The source code will be
made available at https://github.com/Event-AHU/SAFE_LargeVLM.
</p></li>
</ul>

<h3>Title: Stochastic Vision Transformers with Wasserstein Distance-Aware Attention. (arXiv:2311.18645v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18645">http://arxiv.org/abs/2311.18645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18645]] Stochastic Vision Transformers with Wasserstein Distance-Aware Attention(http://arxiv.org/abs/2311.18645)</code></li>
<li>Summary: <p>Self-supervised learning is one of the most promising approaches to acquiring
knowledge from limited labeled data. Despite the substantial advancements made
in recent years, self-supervised models have posed a challenge to
practitioners, as they do not readily provide insight into the model's
confidence and uncertainty. Tackling this issue is no simple feat, primarily
due to the complexity involved in implementing techniques that can make use of
the latent representations learned during pre-training without relying on
explicit labels. Motivated by this, we introduce a new stochastic vision
transformer that integrates uncertainty and distance awareness into
self-supervised learning (SSL) pipelines. Instead of the conventional
deterministic vector embedding, our novel stochastic vision transformer encodes
image patches into elliptical Gaussian distributional embeddings. Notably, the
attention matrices of these stochastic representational embeddings are computed
using Wasserstein distance-based attention, effectively capitalizing on the
distributional nature of these embeddings. Additionally, we propose a
regularization term based on Wasserstein distance for both pre-training and
fine-tuning processes, thereby incorporating distance awareness into latent
representations. We perform extensive experiments across different tasks such
as in-distribution generalization, out-of-distribution detection, dataset
corruption, semi-supervised settings, and transfer learning to other datasets
and tasks. Our proposed method achieves superior accuracy and calibration,
surpassing the self-supervised baseline in a wide range of experiments on a
variety of datasets.
</p></li>
</ul>

<h3>Title: Uncertainty Guided Global Memory Improves Multi-Hop Question Answering. (arXiv:2311.18151v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18151">http://arxiv.org/abs/2311.18151</a></li>
<li>Code URL: https://github.com/aloriosa/gemformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18151]] Uncertainty Guided Global Memory Improves Multi-Hop Question Answering(http://arxiv.org/abs/2311.18151)</code></li>
<li>Summary: <p>Transformers have become the gold standard for many natural language
processing tasks and, in particular, for multi-hop question answering (MHQA).
This task includes processing a long document and reasoning over the multiple
parts of it. The landscape of MHQA approaches can be classified into two
primary categories. The first group focuses on extracting supporting evidence,
thereby constraining the QA model's context to predicted facts. Conversely, the
second group relies on the attention mechanism of the long input encoding model
to facilitate multi-hop reasoning. However, attention-based token
representations lack explicit global contextual information to connect
reasoning steps. To address these issues, we propose GEMFormer, a two-stage
method that first collects relevant information over the entire document to the
memory and then combines it with local context to solve the task. Our
experimental results show that fine-tuning a pre-trained model with
memory-augmented input, including the most certain global elements, improves
the model's performance on three MHQA datasets compared to the baseline. We
also found that the global explicit memory contains information from supporting
facts required for the correct answer.
</p></li>
</ul>

<h3>Title: Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes. (arXiv:2311.18194v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18194">http://arxiv.org/abs/2311.18194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18194]] Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes(http://arxiv.org/abs/2311.18194)</code></li>
<li>Summary: <p>In-context learning (ICL) refers to the ability of a model to condition on a
few in-context demonstrations (input-output examples of the underlying task) to
generate the answer for a new query input, without updating parameters. Despite
the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is
sensitive to input demonstrations and limited to short context lengths. To
understand the limitations and principles for successful ICL, we conduct an
investigation with ICL linear regression of transformers. We characterize
several Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL
failures and compare transformers with DeepSet, a simple yet powerful
architecture for ICL. Surprisingly, DeepSet outperforms transformers across a
variety of distribution shifts, implying that preserving permutation invariance
symmetry to input demonstrations is crucial for OOD ICL. The phenomenon
specifies a fundamental requirement by ICL, which we termed as ICL invariance.
Nevertheless, the positional encodings in LLMs will break ICL invariance. To
this end, we further evaluate transformers with identical positional encodings
and find preserving ICL invariance in transformers achieves state-of-the-art
performance across various ICL distribution shifts
</p></li>
</ul>

<h3>Title: LayerCollapse: Adaptive compression of neural networks. (arXiv:2311.17943v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17943">http://arxiv.org/abs/2311.17943</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17943]] LayerCollapse: Adaptive compression of neural networks(http://arxiv.org/abs/2311.17943)</code></li>
<li>Summary: <p>Handling the ever-increasing scale of contemporary deep learning and
transformer-based models poses a significant challenge. Although great strides
have been made in optimizing model compression techniques such as model
architecture search and knowledge distillation, the availability of data and
computational resources remains a considerable hurdle for these optimizations.
This paper introduces LayerCollapse, a novel alternative adaptive model
compression methodology. LayerCollapse works by eliminating non-linearities
within the network and collapsing two consecutive fully connected layers into a
single linear transformation. This approach simultaneously reduces both the
number of layers and the parameter count, thereby enhancing model efficiency.
We also introduce a compression aware regularizer, which compresses the model
in alignment with the dataset quality and model expressiveness, consequently
reducing overfitting across tasks. Our results demonstrate LayerCollapse's
effective compression and regularization capabilities in multiple fine-grained
classification benchmarks, achieving up to 74% post training compression with
minimal accuracy loss. We compare this method with knowledge distillation on
the same target network, showcasing a five-fold increase in computational
efficiency and 8% improvement in overall accuracy on the ImageNet dataset.
</p></li>
</ul>

<h3>Title: Transformer Based Model for Predicting Rapid Impact Compaction Outcomes: A Case Study of Utapao International Airport. (arXiv:2311.17959v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17959">http://arxiv.org/abs/2311.17959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17959]] Transformer Based Model for Predicting Rapid Impact Compaction Outcomes: A Case Study of Utapao International Airport(http://arxiv.org/abs/2311.17959)</code></li>
<li>Summary: <p>This paper introduces a novel deep learning approach to predict the
engineering properties of the ground improved by Rapid Impact Compaction (RIC),
which is a ground improvement technique that uses a drop hammer to compact the
soil and fill layers. The proposed approach uses transformer-based neural
networks to capture the complex nonlinear relationships between the input
features, such as the hammer energy, drop height, and number of blows, and the
output variables, such as the cone resistance. The approach is applied to a
real-world dataset from a trial test section for the new apron construction of
the Utapao International Airport in Thailand. The results show that the
proposed approach outperforms the existing methods in terms of prediction
accuracy and efficiency and provides interpretable attention maps that reveal
the importance of different features for RIC prediction. The paper also
discusses the limitations and future directions of applying deep learning
methods to RIC prediction.
</p></li>
</ul>

<h3>Title: TransOpt: Transformer-based Representation Learning for Optimization Problem Classification. (arXiv:2311.18035v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18035">http://arxiv.org/abs/2311.18035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18035]] TransOpt: Transformer-based Representation Learning for Optimization Problem Classification(http://arxiv.org/abs/2311.18035)</code></li>
<li>Summary: <p>We propose a representation of optimization problem instances using a
transformer-based neural network architecture trained for the task of problem
classification of the 24 problem classes from the Black-box Optimization
Benchmarking (BBOB) benchmark. We show that transformer-based methods can be
trained to recognize problem classes with accuracies in the range of 70\%-80\%
for different problem dimensions, suggesting the possible application of
transformer architectures in acquiring representations for black-box
optimization problems.
</p></li>
</ul>

<h3>Title: TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection. (arXiv:2311.18061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18061">http://arxiv.org/abs/2311.18061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18061]] TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection(http://arxiv.org/abs/2311.18061)</code></li>
<li>Summary: <p>The surge in real-time data collection across various industries has
underscored the need for advanced anomaly detection in both univariate and
multivariate time series data. Traditional methods, while comprehensive, often
struggle to capture the complex interdependencies in such data. This paper
introduces TransNAS-TSAD, a novel framework that synergizes transformer
architecture with neural architecture search (NAS), enhanced through NSGA-II
algorithm optimization. This innovative approach effectively tackles the
complexities of both univariate and multivariate time series, balancing
computational efficiency with detection accuracy. Our evaluation reveals that
TransNAS-TSAD surpasses conventional anomaly detection models, demonstrating
marked improvements in diverse data scenarios. We also propose the
Efficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model
performance, emphasizing the crucial balance between accuracy and computational
resources. TransNAS-TSAD sets a new benchmark in time series anomaly detection,
offering a versatile, efficient solution for complex real-world applications.
This research paves the way for future developments in the field, highlighting
its potential in a wide range of industry applications.
</p></li>
</ul>

<h3>Title: Exploring the Temperature-Dependent Phase Transition in Modern Hopfield Networks. (arXiv:2311.18434v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18434">http://arxiv.org/abs/2311.18434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18434]] Exploring the Temperature-Dependent Phase Transition in Modern Hopfield Networks(http://arxiv.org/abs/2311.18434)</code></li>
<li>Summary: <p>The recent discovery of a connection between Transformers and Modern Hopfield
Networks (MHNs) has reignited the study of neural networks from a physical
energy-based perspective. This paper focuses on the pivotal effect of the
inverse temperature hyperparameter $\beta$ on the distribution of energy minima
of the MHN. To achieve this, the distribution of energy minima is tracked in a
simplified MHN in which equidistant normalised patterns are stored. This
network demonstrates a phase transition at a critical temperature
$\beta_{\text{c}}$, from a single global attractor towards highly pattern
specific minima as $\beta$ is increased. Importantly, the dynamics are not
solely governed by the hyperparameter $\beta$ but are instead determined by an
effective inverse temperature $\beta_{\text{eff}}$ which also depends on the
distribution and size of the stored patterns. Recognizing the role of
hyperparameters in the MHN could, in the future, aid researchers in the domain
of Transformers to optimise their initial choices, potentially reducing the
necessity for time and energy expensive hyperparameter fine-tuning.
</p></li>
</ul>

<h3>Title: HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers. (arXiv:2311.18526v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18526">http://arxiv.org/abs/2311.18526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18526]] HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers(http://arxiv.org/abs/2311.18526)</code></li>
<li>Summary: <p>Many graph representation learning (GRL) problems are dynamic, with millions
of edges added or removed per second. A fundamental workload in this setting is
dynamic link prediction: using a history of graph updates to predict whether a
given pair of vertices will become connected. Recent schemes for link
prediction in such dynamic settings employ Transformers, modeling individual
graph updates as single tokens. In this work, we propose HOT: a model that
enhances this line of works by harnessing higher-order (HO) graph structures;
specifically, k-hop neighbors and more general subgraphs containing a given
pair of vertices. Harnessing such HO structures by encoding them into the
attention matrix of the underlying Transformer results in higher accuracy of
link prediction outcomes, but at the expense of increased memory pressure. To
alleviate this, we resort to a recent class of schemes that impose hierarchy on
the attention matrix, significantly reducing memory footprint. The final design
offers a sweetspot between high accuracy and low memory utilization. HOT
outperforms other dynamic GRL schemes, for example achieving 9%, 7%, and 15%
higher accuracy than - respectively - DyGFormer, TGN, and GraphMixer, for the
MOOC dataset. Our design can be seamlessly extended towards other dynamic GRL
workloads.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Rethinking Image Editing Detection in the Era of Generative AI Revolution. (arXiv:2311.17953v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17953">http://arxiv.org/abs/2311.17953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17953]] Rethinking Image Editing Detection in the Era of Generative AI Revolution(http://arxiv.org/abs/2311.17953)</code></li>
<li>Summary: <p>The accelerated advancement of generative AI significantly enhance the
viability and effectiveness of generative regional editing methods. This
evolution render the image manipulation more accessible, thereby intensifying
the risk of altering the conveyed information within original images and even
propagating misinformation. Consequently, there exists a critical demand for
robust capable of detecting the edited images. However, the lack of
comprehensive dataset containing images edited with abundant and advanced
generative regional editing methods poses a substantial obstacle to the
advancement of corresponding detection methods.
</p>
<p>We endeavor to fill the vacancy by constructing the GRE dataset, a
large-scale generative regional editing dataset with the following advantages:
1) Collection of real-world original images, focusing on two frequently edited
scenarios. 2) Integration of a logical and simulated editing pipeline,
leveraging multiple large models in various modalities. 3) Inclusion of various
editing approaches with distinct architectures. 4) Provision of comprehensive
analysis tasks. We perform comprehensive experiments with proposed three tasks:
edited image classification, edited method attribution and edited region
localization, providing analysis of distinct editing methods and evaluation of
detection methods in related fields. We expect that the GRE dataset can promote
further research and exploration in the field of generative region editing
detection.
</p></li>
</ul>

<h3>Title: VBench: Comprehensive Benchmark Suite for Video Generative Models. (arXiv:2311.17982v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17982">http://arxiv.org/abs/2311.17982</a></li>
<li>Code URL: https://github.com/vchitect/vbench</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17982]] VBench: Comprehensive Benchmark Suite for Video Generative Models(http://arxiv.org/abs/2311.17982)</code></li>
<li>Summary: <p>Video generation has witnessed significant advancements, yet evaluating these
models remains a challenge. A comprehensive evaluation benchmark for video
generation is indispensable for two reasons: 1) Existing metrics do not fully
align with human perceptions; 2) An ideal evaluation system should provide
insights to inform future developments of video generation. To this end, we
present VBench, a comprehensive benchmark suite that dissects "video generation
quality" into specific, hierarchical, and disentangled dimensions, each with
tailored prompts and evaluation methods. VBench has three appealing properties:
1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation
(e.g., subject identity inconsistency, motion smoothness, temporal flickering,
and spatial relationship, etc). The evaluation metrics with fine-grained levels
reveal individual models' strengths and weaknesses. 2) Human Alignment: We also
provide a dataset of human preference annotations to validate our benchmarks'
alignment with human perception, for each evaluation dimension respectively. 3)
Valuable Insights: We look into current models' ability across various
evaluation dimensions, and various content types. We also investigate the gaps
between video and image generation models. We will open-source VBench,
including all prompts, evaluation methods, generated videos, and human
preference annotations, and also include more video generation models in VBench
to drive forward the field of video generation.
</p></li>
</ul>

<h3>Title: GELDA: A generative language annotation framework to reveal visual biases in datasets. (arXiv:2311.18064v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18064">http://arxiv.org/abs/2311.18064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18064]] GELDA: A generative language annotation framework to reveal visual biases in datasets(http://arxiv.org/abs/2311.18064)</code></li>
<li>Summary: <p>Bias analysis is a crucial step in the process of creating fair datasets for
training and evaluating computer vision models. The bottleneck in dataset
analysis is annotation, which typically requires: (1) specifying a list of
attributes relevant to the dataset domain, and (2) classifying each
image-attribute pair. While the second step has made rapid progress in
automation, the first has remained human-centered, requiring an experimenter to
compile lists of in-domain attributes. However, an experimenter may have
limited foresight leading to annotation "blind spots," which in turn can lead
to flawed downstream dataset analyses. To combat this, we propose GELDA, a
nearly automatic framework that leverages large generative language models
(LLMs) to propose and label various attributes for a domain. GELDA takes a
user-defined domain caption (e.g., "a photo of a bird," "a photo of a living
room") and uses an LLM to hierarchically generate attributes. In addition,
GELDA uses the LLM to decide which of a set of vision-language models (VLMs) to
use to classify each attribute in images. Results on real datasets show that
GELDA can generate accurate and diverse visual attribute suggestions, and
uncover biases such as confounding between class labels and background
features. Results on synthetic datasets demonstrate that GELDA can be used to
evaluate the biases of text-to-image diffusion models and generative
adversarial networks. Overall, we show that while GELDA is not accurate enough
to replace human annotators, it can serve as a complementary tool to help
humans analyze datasets in a cheap, low-effort, and flexible manner.
</p></li>
</ul>

<h3>Title: Few-shot Image Generation via Style Adaptation and Content Preservation. (arXiv:2311.18169v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18169">http://arxiv.org/abs/2311.18169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18169]] Few-shot Image Generation via Style Adaptation and Content Preservation(http://arxiv.org/abs/2311.18169)</code></li>
<li>Summary: <p>Training a generative model with limited data (e.g., 10) is a very
challenging task. Many works propose to fine-tune a pre-trained GAN model.
However, this can easily result in overfitting. In other words, they manage to
adapt the style but fail to preserve the content, where \textit{style} denotes
the specific properties that defines a domain while \textit{content} denotes
the domain-irrelevant information that represents diversity. Recent works try
to maintain a pre-defined correspondence to preserve the content, however, the
diversity is still not enough and it may affect style adaptation. In this work,
we propose a paired image reconstruction approach for content preservation. We
propose to introduce an image translation module to GAN transferring, where the
module teaches the generator to separate style and content, and the generator
provides training data to the translation module in return. Qualitative and
quantitative experiments show that our method consistently surpasses the
state-of-the-art methods in few shot setting.
</p></li>
</ul>

<h3>Title: Combining deep generative models with extreme value theory for synthetic hazard simulation: a multivariate and spatially coherent approach. (arXiv:2311.18521v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18521">http://arxiv.org/abs/2311.18521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18521]] Combining deep generative models with extreme value theory for synthetic hazard simulation: a multivariate and spatially coherent approach(http://arxiv.org/abs/2311.18521)</code></li>
<li>Summary: <p>Climate hazards can cause major disasters when they occur simultaneously as
compound hazards. To understand the distribution of climate risk and inform
adaptation policies, scientists need to simulate a large number of physically
realistic and spatially coherent events. Current methods are limited by
computational constraints and the probabilistic spatial distribution of
compound events is not given sufficient attention. The bottleneck in current
approaches lies in modelling the dependence structure between variables, as
inference on parametric models suffers from the curse of dimensionality.
Generative adversarial networks (GANs) are well-suited to such a problem due to
their ability to implicitly learn the distribution of data in high-dimensional
settings. We employ a GAN to model the dependence structure for daily maximum
wind speed, significant wave height, and total precipitation over the Bay of
Bengal, combining this with traditional extreme value theory for controlled
extrapolation of the tails. Once trained, the model can be used to efficiently
generate thousands of realistic compound hazard events, which can inform
climate risk assessments for climate adaptation and disaster preparedness. The
method developed is flexible and transferable to other multivariate and spatial
climate datasets.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Understanding and Improving In-Context Learning on Vision-language Models. (arXiv:2311.18021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18021">http://arxiv.org/abs/2311.18021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18021]] Understanding and Improving In-Context Learning on Vision-language Models(http://arxiv.org/abs/2311.18021)</code></li>
<li>Summary: <p>Recently, in-context learning (ICL) on large language models (LLMs) has
received great attention, and this technique can also be applied to
vision-language models (VLMs) built upon LLMs. These VLMs can respond to
queries by conditioning responses on a series of multimodal demonstrations,
which comprise images, queries, and answers. Though ICL has been extensively
studied on LLMs, its research on VLMs remains limited. The inclusion of
additional visual information in the demonstrations motivates the following
research questions: which of the two modalities in the demonstration is more
significant? How can we select effective multimodal demonstrations to enhance
ICL performance? This study investigates the significance of both visual and
language information. Our findings indicate that ICL in VLMs is predominantly
driven by the textual information in the demonstrations whereas the visual
information in the demonstrations barely affects the ICL performance.
Subsequently, we provide an understanding of the findings by analyzing the
model information flow and comparing model inner states given different ICL
settings. Motivated by our analysis, we propose a simple yet effective
approach, termed Mixed Modality In-Context Example Selection (MMICES), which
considers both visual and language modalities when selecting demonstrations and
shows better ICL performance. Extensive experiments are conducted to support
our findings, understanding, and improvement of the ICL performance of VLMs.
</p></li>
</ul>

<h3>Title: TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers. (arXiv:2311.18291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18291">http://arxiv.org/abs/2311.18291</a></li>
<li>Code URL: https://github.com/tmlabonte/last-layer-retraining</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18291]] TLDR: Text Based Last-layer Retraining for Debiasing Image Classifiers(http://arxiv.org/abs/2311.18291)</code></li>
<li>Summary: <p>A classifier may depend on incidental features stemming from a strong
correlation between the feature and the classification target in the training
dataset. Recently, Last Layer Retraining (LLR) with group-balanced datasets is
known to be efficient in mitigating the spurious correlation of classifiers.
However, the acquisition of group-balanced datasets is costly, which hinders
the applicability of the LLR method. In this work, we propose to perform LLR
based on text datasets built with large language models for a general image
classifier. We demonstrate that text can be a proxy for its corresponding image
beyond the image-text joint embedding space, such as CLIP. Based on this, we
use generated texts to train the final layer in the embedding space of the
arbitrary image classifier. In addition, we propose a method of filtering the
generated words to get rid of noisy, imprecise words, which reduces the effort
of inspecting each word. We dub these procedures as TLDR (\textbf{T}ext-based
\textbf{L}ast layer retraining for \textbf{D}ebiasing image
classifie\textbf{R}s) and show our method achieves the performance that is
comparable to those of the LLR methods that also utilize group-balanced image
dataset for retraining. Furthermore, TLDR outperforms other baselines that
involve training the last linear layer without a group annotated dataset.
</p></li>
</ul>

<h3>Title: VTimeLLM: Empower LLM to Grasp Video Moments. (arXiv:2311.18445v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18445">http://arxiv.org/abs/2311.18445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18445]] VTimeLLM: Empower LLM to Grasp Video Moments(http://arxiv.org/abs/2311.18445)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable text understanding
capabilities, which have been extended as Video LLMs to handle video data for
comprehending visual details. However, existing Video LLMs can only provide a
coarse description of the entire video, failing to capture the precise start
and end time boundary of specific events. In this paper, we solve this issue
via proposing VTimeLLM, a novel Video LLM designed for fine-grained video
moment understanding and reasoning with respect to time boundary. Specifically,
our VTimeLLM adopts a boundary-aware three-stage training strategy, which
respectively utilizes image-text pairs for feature alignment, multiple-event
videos to increase temporal-boundary awareness, and high-quality
video-instruction tuning to further improve temporal understanding ability as
well as align with human intents. Extensive experiments demonstrate that in
fine-grained time-related comprehension tasks for videos such as Temporal Video
Grounding and Dense Video Captioning, VTimeLLM significantly outperforms
existing Video LLMs. Besides, benefits from the fine-grained temporal
understanding of the videos further enable VTimeLLM to beat existing Video LLMs
in video dialogue benchmark, showing its superior cross-modal understanding and
reasoning abilities.
</p></li>
</ul>

<h3>Title: Zero-shot Conversational Summarization Evaluations with small Large Language Models. (arXiv:2311.18041v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18041">http://arxiv.org/abs/2311.18041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18041]] Zero-shot Conversational Summarization Evaluations with small Large Language Models(http://arxiv.org/abs/2311.18041)</code></li>
<li>Summary: <p>Large Language Models (LLMs) exhibit powerful summarization abilities.
However, their capabilities on conversational summarization remains under
explored. In this work we evaluate LLMs (approx. 10 billion parameters) on
conversational summarization and showcase their performance on various prompts.
We show that the summaries generated by models depend on the instructions and
the performance of LLMs vary with different instructions sometimes resulting
steep drop in ROUGE scores if prompts are not selected carefully. We also
evaluate the models with human evaluations and discuss the limitations of the
models on conversational summarization
</p></li>
</ul>

<h3>Title: TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis. (arXiv:2311.18063v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18063">http://arxiv.org/abs/2311.18063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18063]] TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis(http://arxiv.org/abs/2311.18063)</code></li>
<li>Summary: <p>Turkish is one of the most popular languages in the world. Wide us of this
language on social media platforms such as Twitter, Instagram, or Tiktok and
strategic position of the country in the world politics makes it appealing for
the social network researchers and industry. To address this need, we introduce
TurkishBERTweet, the first large scale pre-trained language model for Turkish
social media built using almost 900 million tweets. The model shares the same
architecture as base BERT model with smaller input length, making
TurkishBERTweet lighter than BERTurk and can have significantly lower inference
time. We trained our model using the same approach for RoBERTa model and
evaluated on two text classification tasks: Sentiment Classification and Hate
Speech Detection. We demonstrate that TurkishBERTweet outperforms the other
available alternatives on generalizability and its lower inference time gives
significant advantage to process large-scale datasets. We also compared our
models with the commercial OpenAI solutions in terms of cost and performance to
demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of
our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the
mentioned tasks under the MIT License to facilitate future research and
applications on Turkish social media. Our TurkishBERTweet model is available
at: https://github.com/ViralLab/TurkishBERTweet
</p></li>
</ul>

<h3>Title: DisCGen: A Framework for Discourse-Informed Counterspeech Generation. (arXiv:2311.18147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18147">http://arxiv.org/abs/2311.18147</a></li>
<li>Code URL: https://github.com/sabithsn/discgen</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18147]] DisCGen: A Framework for Discourse-Informed Counterspeech Generation(http://arxiv.org/abs/2311.18147)</code></li>
<li>Summary: <p>Counterspeech can be an effective method for battling hateful content on
social media. Automated counterspeech generation can aid in this process.
Generated counterspeech, however, can be viable only when grounded in the
context of topic, audience and sensitivity as these factors influence both the
efficacy and appropriateness. In this work, we propose a novel framework based
on theories of discourse to study the inferential links that connect counter
speeches to the hateful comment. Within this framework, we propose: i) a
taxonomy of counterspeech derived from discourse frameworks, and ii)
discourse-informed prompting strategies for generating contextually-grounded
counterspeech. To construct and validate this framework, we present a process
for collecting an in-the-wild dataset of counterspeech from Reddit. Using this
process, we manually annotate a dataset of 3.9k Reddit comment pairs for the
presence of hatespeech and counterspeech. The positive pairs are annotated for
10 classes in our proposed taxonomy. We annotate these pairs with paraphrased
counterparts to remove offensiveness and first-person references. We show that
by using our dataset and framework, large language models can generate
contextually-grounded counterspeech informed by theories of discourse.
According to our human evaluation, our approaches can act as a safeguard
against critical failures of discourse-agnostic models.
</p></li>
</ul>

<h3>Title: COVID-19 Vaccine Misinformation in Middle Income Countries. (arXiv:2311.18195v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18195">http://arxiv.org/abs/2311.18195</a></li>
<li>Code URL: https://github.com/zzoliman/covid-vaccine-misinfo-mic</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18195]] COVID-19 Vaccine Misinformation in Middle Income Countries(http://arxiv.org/abs/2311.18195)</code></li>
<li>Summary: <p>This paper introduces a multilingual dataset of COVID-19 vaccine
misinformation, consisting of annotated tweets from three middle-income
countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset
includes annotations for 5,952 tweets, assessing their relevance to COVID-19
vaccines, presence of misinformation, and the themes of the misinformation. To
address challenges posed by domain specificity, the low-resource setting, and
data imbalance, we adopt two approaches for developing COVID-19 vaccine
misinformation detection models: domain-specific pre-training and text
augmentation using a large language model. Our best misinformation detection
models demonstrate improvements ranging from 2.7 to 15.9 percentage points in
macro F1-score compared to the baseline models. Additionally, we apply our
misinformation detection models in a large-scale study of 19 million unlabeled
tweets from the three countries between 2020 and 2022, showcasing the practical
application of our dataset and models for detecting and analyzing vaccine
misinformation in multiple countries and languages. Our analysis indicates that
percentage changes in the number of new COVID-19 cases are positively
associated with COVID-19 vaccine misinformation rates in a staggered manner for
Brazil and Indonesia, and there are significant positive associations between
the misinformation rates across the three countries.
</p></li>
</ul>

<h3>Title: Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models. (arXiv:2311.18215v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18215">http://arxiv.org/abs/2311.18215</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18215]] Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models(http://arxiv.org/abs/2311.18215)</code></li>
<li>Summary: <p>Caution: this paper may include material that could be offensive or
distressing.
</p>
<p>The advent of Large Language Models (LLMs) necessitates the development of
training approaches that mitigate the generation of unethical language and
aptly manage toxic user queries. Given the challenges related to human labor
and the scarcity of data, we present KoTox, comprising 39K unethical
instruction-output pairs. This collection of automatically generated toxic
instructions refines the training of LLMs and establishes a foundational
framework for improving LLMs' ethical awareness and response to various toxic
inputs, promoting more secure and responsible interactions in Natural Language
Processing (NLP) applications.
</p></li>
</ul>

<h3>Title: LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models. (arXiv:2311.18232v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18232">http://arxiv.org/abs/2311.18232</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18232]] LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models(http://arxiv.org/abs/2311.18232)</code></li>
<li>Summary: <p>Large language models (LLMs) provide excellent text-generation capabilities,
but standard prompting and generation methods generally do not lead to
intentional or goal-directed agents and might necessitate considerable prompt
tuning. This becomes particularly apparent in multi-turn conversations: even
the best current LLMs rarely ask clarifying questions, engage in explicit
information gathering, or take actions now that lead to better decisions after
multiple turns. Reinforcement learning has the potential to leverage the
powerful modeling capabilities of LLMs, as well as their internal
representation of textual interactions, to create capable goal-directed
language agents. This can enable intentional and temporally extended
interactions, such as with humans, through coordinated persuasion and carefully
crafted questions, or in goal-directed play through text games to bring about
desired final outcomes. However, enabling this requires the community to
develop stable and reliable reinforcement learning algorithms that can
effectively train LLMs. Developing such algorithms requires tasks that can
gauge progress on algorithm design, provide accessible and reproducible
evaluations for multi-turn interactions, and cover a range of task properties
and challenges in improving reinforcement learning algorithms. Our paper
introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,
together with an open-source research framework containing a basic toolkit for
getting started on multi-turn RL with offline value-based and policy-based RL
methods. Our benchmark consists of 8 different language tasks, which require
multiple rounds of language interaction and cover a range of tasks in
open-ended dialogue and text games.
</p></li>
</ul>

<h3>Title: Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension. (arXiv:2311.18353v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18353">http://arxiv.org/abs/2311.18353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18353]] Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension(http://arxiv.org/abs/2311.18353)</code></li>
<li>Summary: <p>To precisely evaluate a language model's capability for logical reading
comprehension, we present a dataset for testing the understanding of the
rationale behind critical reasoning. For questions taken from an existing
multiplechoice logical reading comprehension dataset, we crowdsource rationale
texts that explain why we should select or eliminate answer options, resulting
in 3,003 multiple-choice subquestions that are associated with 943 main
questions. Experiments on our dataset show that recent large language models
(e.g., InstructGPT) struggle to answer the subquestions even if they are able
to answer the main questions correctly. We find that the models perform
particularly poorly in answering subquestions written for the incorrect options
of the main questions, implying that the models have a limited capability for
explaining why incorrect alternatives should be eliminated. These results
suggest that our dataset encourages further investigation into the critical
reasoning ability of language models while focusing on the elimination process
of relevant alternatives.
</p></li>
</ul>

<h3>Title: IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions. (arXiv:2311.18397v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18397">http://arxiv.org/abs/2311.18397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18397]] IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions(http://arxiv.org/abs/2311.18397)</code></li>
<li>Summary: <p>Retrieval-Augmented Generation (RAG), by incorporating external knowledge
with parametric memory of language models, has become the state-of-the-art
architecture for open-domain QA tasks. However, common knowledge bases are
inherently constrained by limited coverage and noisy information, making
retrieval-based approaches inadequate to answer implicit reasoning questions.
In this paper, we propose an Induction-Augmented Generation (IAG) framework
that utilizes inductive knowledge along with the retrieved documents for
implicit reasoning. We leverage large language models (LLMs) for deriving such
knowledge via a novel prompting method based on inductive reasoning patterns.
On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,
respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for
answer prediction, while IAG-Student gets rid of dependencies on GPT service at
inference time by incorporating a student inductor model. The inductor is
firstly trained via knowledge distillation and further optimized by
back-propagating the generator feedback via differentiable beam scores.
Experimental results show that IAG outperforms RAG baselines as well as ChatGPT
on two Open-Domain QA tasks. Notably, our best models have won the first place
in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA
(since Jan 8, 2023).
</p></li>
</ul>

<h3>Title: ArthModel: Enhance Arithmetic Skills to Large Language Model. (arXiv:2311.18609v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18609">http://arxiv.org/abs/2311.18609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18609]] ArthModel: Enhance Arithmetic Skills to Large Language Model(http://arxiv.org/abs/2311.18609)</code></li>
<li>Summary: <p>With the great success of ChatGPT, the research of large language models has
become increasingly popular. However, the models have several limitations, such
as toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have
some potential abilities that have yet to be exploited. In this paper, we
choose a different way to enhance the arithmetic ability of LLM. We propose to
train LLM to generate a postfix expression related to the arithmetic problem
and incorporate it with small pretrained models. Moreover, this small model
transfers the token embeddings into real dense numbers and invokes native
functions of a deep learning platform to get the correct answer. To generate
the final result, we propose prompt injection for adding the result outputs by
the small model to LLM. This work provides different ways of thinking, training
and using a language model. The codes and models will be released at
\url{https://github.com/eteced/arithmetic_finetuning_v1}.
</p></li>
</ul>

<h3>Title: ArcMMLU: A Library and Information Science Benchmark for Large Language Models. (arXiv:2311.18658v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18658">http://arxiv.org/abs/2311.18658</a></li>
<li>Code URL: https://github.com/stzhang-patrick/arcmmlu</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18658]] ArcMMLU: A Library and Information Science Benchmark for Large Language Models(http://arxiv.org/abs/2311.18658)</code></li>
<li>Summary: <p>In light of the rapidly evolving capabilities of large language models
(LLMs), it becomes imperative to develop rigorous domain-specific evaluation
benchmarks to accurately assess their capabilities. In response to this need,
this paper introduces ArcMMLU, a specialized benchmark tailored for the Library
&amp; Information Science (LIS) domain in Chinese. This benchmark aims to measure
the knowledge and reasoning capability of LLMs within four key sub-domains:
Archival Science, Data Science, Library Science, and Information Science.
Following the format of MMLU/CMMLU, we collected over 6,000 high-quality
questions for the compilation of ArcMMLU. This extensive compilation can
reflect the diverse nature of the LIS domain and offer a robust foundation for
LLM evaluation. Our comprehensive evaluation reveals that while most mainstream
LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a
notable performance gap, suggesting substantial headroom for refinement in LLM
capabilities within the LIS domain. Further analysis explores the effectiveness
of few-shot examples on model performance and highlights challenging questions
where models consistently underperform, providing valuable insights for
targeted improvements. ArcMMLU fills a critical gap in LLM evaluations within
the Chinese LIS domain and paves the way for future development of LLMs
tailored to this specialized area.
</p></li>
</ul>

<h3>Title: CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation. (arXiv:2311.18702v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18702">http://arxiv.org/abs/2311.18702</a></li>
<li>Code URL: https://github.com/thu-coai/critiquellm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18702]] CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation(http://arxiv.org/abs/2311.18702)</code></li>
<li>Summary: <p>Since the natural language processing (NLP) community started to make large
language models (LLMs), such as GPT-4, act as a critic to evaluate the quality
of generated texts, most of them only train a critique generation model of a
specific scale on specific datasets. We argue that a comprehensive
investigation on the key factor of LLM-based evaluation models, such as scaling
properties, is lacking, so that it is still inconclusive whether these models
have potential to replace GPT-4's evaluation in practical scenarios. In this
paper, we propose a new critique generation model called CritiqueLLM, which
includes a dialogue-based prompting method for high-quality referenced /
reference-free evaluation data. Experimental results show that our model can
achieve comparable evaluation performance to GPT-4 especially in system-level
correlations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging
reference-free setting. We conduct detailed analysis to show promising scaling
properties of our model in the quality of generated critiques. We also
demonstrate that our generated critiques can act as scalable feedback to
directly improve the generation quality of LLMs.
</p></li>
</ul>

<h3>Title: Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation. (arXiv:2311.18062v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18062">http://arxiv.org/abs/2311.18062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18062]] Understanding Your Agent: Leveraging Large Language Models for Behavior Explanation(http://arxiv.org/abs/2311.18062)</code></li>
<li>Summary: <p>Intelligent agents such as robots are increasingly deployed in real-world,
safety-critical settings. It is vital that these agents are able to explain the
reasoning behind their decisions to human counterparts; however, their behavior
is often produced by uninterpretable models such as deep neural networks. We
propose an approach to generate natural language explanations for an agent's
behavior based only on observations of states and actions, thus making our
method independent from the underlying model's representation. For such models,
we first learn a behavior representation and subsequently use it to produce
plausible explanations with minimal hallucination while affording user
interaction with a pre-trained large language model. We evaluate our method in
a multi-agent search-and-rescue environment and demonstrate the effectiveness
of our explanations for agents executing various behaviors. Through user
studies and empirical experiments, we show that our approach generates
explanations as helpful as those produced by a human domain expert while
enabling beneficial interactions such as clarification and counterfactual
queries.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation. (arXiv:2311.17952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17952">http://arxiv.org/abs/2311.17952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17952]] Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation(http://arxiv.org/abs/2311.17952)</code></li>
<li>Summary: <p>Referring Image Segmentation (RIS) aims to segment target objects expressed
in natural language within a scene at the pixel level. Various recent RIS
models have achieved state-of-the-art performance by generating contextual
tokens to model multimodal features from pretrained encoders and effectively
fusing them using transformer-based cross-modal attention. While these methods
match language features with image features to effectively identify likely
target objects, they often struggle to correctly understand contextual
information in complex and ambiguous sentences and scenes. To address this
issue, we propose a novel bidirectional token-masking autoencoder (BTMAE)
inspired by the masked autoencoder (MAE). The proposed model learns the context
of image-to-language and language-to-image by reconstructing missing features
in both image and language features at the token level. In other words, this
approach involves mutually complementing across the features of images and
language, with a focus on enabling the network to understand interconnected
deep contextual information between the two modalities. This learning method
enhances the robustness of RIS performance in complex sentences and scenes. Our
BTMAE achieves state-of-the-art performance on three popular datasets, and we
demonstrate the effectiveness of the proposed method through various ablation
studies.
</p></li>
</ul>

<h3>Title: Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images. (arXiv:2311.17960v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17960">http://arxiv.org/abs/2311.17960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17960]] Guided Prompting in SAM for Weakly Supervised Cell Segmentation in Histopathological Images(http://arxiv.org/abs/2311.17960)</code></li>
<li>Summary: <p>Cell segmentation in histopathological images plays a crucial role in
understanding, diagnosing, and treating many diseases. However, data annotation
for this is expensive since there can be a large number of cells per image, and
expert pathologists are needed for labelling images. Instead, our paper focuses
on using weak supervision -- annotation from related tasks -- to induce a
segmenter. Recent foundation models, such as Segment Anything (SAM), can use
prompts to leverage additional supervision during inference. SAM has performed
remarkably well in natural image segmentation tasks; however, its applicability
to cell segmentation has not been explored.
</p>
<p>In response, we investigate guiding the prompting procedure in SAM for weakly
supervised cell segmentation when only bounding box supervision is available.
We develop two workflows: (1) an object detector's output as a test-time prompt
to SAM (D-SAM), and (2) SAM as pseudo mask generator over training data to
train a standalone segmentation model (SAM-S). On finding that both workflows
have some complementary strengths, we develop an integer programming-based
approach to reconcile the two sets of segmentation masks, achieving yet higher
performance. We experiment on three publicly available cell segmentation
datasets namely, ConSep, MoNuSeg, and TNBC, and find that all SAM-based
solutions hugely outperform existing weakly supervised image segmentation
models, obtaining 9-15 pt Dice gains.
</p></li>
</ul>

<h3>Title: ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction. (arXiv:2311.18068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18068">http://arxiv.org/abs/2311.18068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18068]] ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction(http://arxiv.org/abs/2311.18068)</code></li>
<li>Summary: <p>We propose an online 3D semantic segmentation method that incrementally
reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline
methods, ours is directly applicable to scenarios with real-time constraints,
such as robotics or mixed reality. To overcome the inherent challenges of
online methods, we make two main contributions. First, to effectively extract
information from the input RGB-D video stream, we jointly estimate geometry and
semantic labels per frame in 3D. A key focus of our approach is to reason about
semantic entities both in the 2D input and the local 3D domain to leverage
differences in spatial context and network architectures. Our method predicts
2D features using an off-the-shelf segmentation network. The extracted 2D
features are refined by a lightweight 3D network to enable reasoning about the
local 3D structure. Second, to efficiently deal with an infinite stream of
input RGB-D frames, a subsequent network serves as a temporal expert predicting
the incremental scene updates by leveraging 2D, 3D, and past information in a
learned manner. These updates are then integrated into a global scene
representation. Using these main contributions, our method can enable scenarios
with real-time constraints and can scale to arbitrary scene sizes by processing
and updating the scene only in a local region defined by the new measurement.
Our experiments demonstrate improved results compared to existing online
methods that purely operate in local regions and show that complementary
sources of information can boost the performance. We provide a thorough
ablation study on the benefits of different architectural as well as
algorithmic design decisions. Our method yields competitive results on the
popular ScanNet benchmark and SceneNN dataset.
</p></li>
</ul>

<h3>Title: Beyond Entropy: Style Transfer Guided Single Image Continual Test-Time Adaptation. (arXiv:2311.18270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18270">http://arxiv.org/abs/2311.18270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18270]] Beyond Entropy: Style Transfer Guided Single Image Continual Test-Time Adaptation(http://arxiv.org/abs/2311.18270)</code></li>
<li>Summary: <p>Continual test-time adaptation (cTTA) methods are designed to facilitate the
continual adaptation of models to dynamically changing real-world environments
where computational resources are limited. Due to this inherent limitation,
existing approaches fail to simultaneously achieve accuracy and efficiency. In
detail, when using a single image, the instability caused by batch
normalization layers and entropy loss significantly destabilizes many existing
methods in real-world cTTA scenarios. To overcome these challenges, we present
BESTTA, a novel single image continual test-time adaptation method guided by
style transfer, which enables stable and efficient adaptation to the target
environment by transferring the style of the input image to the source style.
To implement the proposed method, we devise BeIN, a simple yet powerful
normalization method, along with the style-guided losses. We demonstrate that
BESTTA effectively adapts to the continually changing target environment,
leveraging only a single image on both semantic segmentation and image
classification tasks. Remarkably, despite training only two parameters in a
BeIN layer consuming the least memory, BESTTA outperforms existing
state-of-the-art methods in terms of performance.
</p></li>
</ul>

<h3>Title: SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation. (arXiv:2311.18286v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18286">http://arxiv.org/abs/2311.18286</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18286]] SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation(http://arxiv.org/abs/2311.18286)</code></li>
<li>Summary: <p>Unsupervised video object segmentation (UVOS) aims at detecting the primary
objects in a given video sequence without any human interposing. Most existing
methods rely on two-stream architectures that separately encode the appearance
and motion information before fusing them to identify the target and generate
object masks. However, this pipeline is computationally expensive and can lead
to suboptimal performance due to the difficulty of fusing the two modalities
properly. In this paper, we propose a novel UVOS model called SimulFlow that
simultaneously performs feature extraction and target identification, enabling
efficient and effective unsupervised video object segmentation. Concretely, we
design a novel SimulFlow Attention mechanism to bridege the image and motion by
utilizing the flexibility of attention operation, where coarse masks predicted
from fused feature at each stage are used to constrain the attention operation
within the mask area and exclude the impact of noise. Because of the
bidirectional information flow between visual and optical flow features in
SimulFlow Attention, no extra hand-designed fusing module is required and we
only adopt a light decoder to obtain the final prediction. We evaluate our
method on several benchmark datasets and achieve state-of-the-art results. Our
proposed approach not only outperforms existing methods but also addresses the
computational complexity and fusion difficulties caused by two-stream
architectures. Our models achieve 87.4% J &amp; F on DAVIS-16 with the highest
speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow
also obtains competitive results on video salient object detection datasets.
</p></li>
</ul>

<h3>Title: MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation. (arXiv:2311.18331v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18331">http://arxiv.org/abs/2311.18331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18331]] MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation(http://arxiv.org/abs/2311.18331)</code></li>
<li>Summary: <p>Deep neural networks have shown exemplary performance on semantic scene
understanding tasks on source domains, but due to the absence of style
diversity during training, enhancing performance on unseen target domains using
only single source domain data remains a challenging task. Generation of
simulated data is a feasible alternative to retrieving large style-diverse
real-world datasets as it is a cumbersome and budget-intensive process.
However, the large domain-specific inconsistencies between simulated and
real-world data pose a significant generalization challenge in semantic
segmentation. In this work, to alleviate this problem, we propose a novel
MultiResolution Feature Perturbation (MRFP) technique to randomize
domain-specific fine-grained features and perturb style of coarse features. Our
experimental results on various urban-scene segmentation datasets clearly
indicate that, along with the perturbation of style-information, perturbation
of fine-feature components is paramount to learn domain invariant robust
feature maps for semantic segmentation models. MRFP is a simple and
computationally efficient, transferable module with no additional learnable
parameters or objective functions, that helps state-of-the-art deep neural
networks to learn robust domain invariant features for simulation-to-real
semantic segmentation.
</p></li>
</ul>

<h3>Title: Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation. (arXiv:2311.18363v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18363">http://arxiv.org/abs/2311.18363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18363]] Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation(http://arxiv.org/abs/2311.18363)</code></li>
<li>Summary: <p>Distribution shift widely exists in medical images acquired from different
medical centres and poses a significant obstacle to deploying the pre-trained
semantic segmentation model in real-world applications. Test-time adaptation
has proven its effectiveness in tackling the cross-domain distribution shift
during inference. However, most existing methods achieve adaptation by updating
the pre-trained models, rendering them susceptible to error accumulation and
catastrophic forgetting when encountering a series of distribution shifts
(i.e., under the continual test-time adaptation setup). To overcome these
challenges caused by updating the models, in this paper, we freeze the
pre-trained model and propose the Visual Prompt-based Test-Time Adaptation
(VPTTA) method to train a specific prompt for each test image to align the
statistics in the batch normalization layers. Specifically, we present the
low-frequency prompt, which is lightweight with only a few parameters and can
be effectively trained in a single iteration. To enhance prompt initialization,
we equip VPTTA with a memory bank to benefit the current prompt from previous
ones. Additionally, we design a warm-up mechanism, which mixes source and
target statistics to construct warm-up statistics, thereby facilitating the
training process. Extensive experiments demonstrate the superiority of our
VPTTA over other state-of-the-art methods on two medical image segmentation
benchmark tasks. The code and weights of pre-trained source models are
available at https://github.com/Chen-Ziyang/VPTTA.
</p></li>
</ul>

<h3>Title: A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends. (arXiv:2311.18373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18373">http://arxiv.org/abs/2311.18373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18373]] A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges and Future Trends(http://arxiv.org/abs/2311.18373)</code></li>
<li>Summary: <p>Early detection and assessment of polyps play a crucial role in the
prevention and treatment of colorectal cancer (CRC). Polyp segmentation
provides an effective solution to assist clinicians in accurately locating and
segmenting polyp regions. In the past, people often relied on manually
extracted lower-level features such as color, texture, and shape, which often
had issues capturing global context and lacked robustness to complex scenarios.
With the advent of deep learning, more and more outstanding medical image
segmentation algorithms based on deep learning networks have emerged, making
significant progress in this field. This paper provides a comprehensive review
of polyp segmentation algorithms. We first review some traditional algorithms
based on manually extracted features and deep segmentation algorithms, then
detail benchmark datasets related to the topic. Specifically, we carry out a
comprehensive evaluation of recent deep learning models and results based on
polyp sizes, considering the pain points of research topics and differences in
network structures. Finally, we discuss the challenges of polyp segmentation
and future trends in this field. The models, benchmark datasets, and source
code links we collected are all published at
https://github.com/taozh2017/Awesome-Polyp-Segmentation.
</p></li>
</ul>

<h3>Title: Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding. (arXiv:2311.18482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18482">http://arxiv.org/abs/2311.18482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18482]] Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding(http://arxiv.org/abs/2311.18482)</code></li>
<li>Summary: <p>Open-vocabulary querying in 3D space is challenging but essential for scene
understanding tasks such as object localization and segmentation.
Language-embedded scene representations have made progress by incorporating
language features into 3D spaces. However, their efficacy heavily depends on
neural networks that are resource-intensive in training and rendering. Although
recent 3D Gaussians offer efficient and high-quality novel view synthesis,
directly embedding language features in them leads to prohibitive memory usage
and decreased performance. In this work, we introduce Language Embedded 3D
Gaussians, a novel scene representation for open-vocabulary query tasks.
Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we
propose a dedicated quantization scheme that drastically alleviates the memory
requirement, and a novel embedding procedure that achieves smoother yet high
accuracy query, countering the multi-view feature inconsistencies and the
high-frequency inductive bias in point-based representations. Our comprehensive
experiments show that our representation achieves the best visual quality and
language querying accuracy across current language-embedded representations,
while maintaining real-time rendering frame rates on a single desktop GPU.
</p></li>
</ul>

<h3>Title: Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels by Noise-Aware Learning. (arXiv:2311.18496v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18496">http://arxiv.org/abs/2311.18496</a></li>
<li>Code URL: https://github.com/wwwtttjjj/mpnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18496]] Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels by Noise-Aware Learning(http://arxiv.org/abs/2311.18496)</code></li>
<li>Summary: <p>Optic disc and cup segmentation play a crucial role in automating the
screening and diagnosis of optic glaucoma. While data-driven convolutional
neural networks (CNNs) show promise in this area, the inherent ambiguity of
segmenting object and background boundaries in the task of optic disc and cup
segmentation leads to noisy annotations that impact model performance. To
address this, we propose an innovative label-denoising method of Multiple
Pseudo-labels Noise-aware Network (MPNN) for accurate optic disc and cup
segmentation. Specifically, the Multiple Pseudo-labels Generation and Guided
Denoising (MPGGD) module generates pseudo-labels by multiple different
initialization networks trained on true labels, and the pixel-level consensus
information extracted from these pseudo-labels guides to differentiate clean
pixels from noisy pixels. The training framework of the MPNN is constructed by
a teacher-student architecture to learn segmentation from clean pixels and
noisy pixels. Particularly, such a framework adeptly leverages (i) reliable and
fundamental insights from clean pixels and (ii) the supplementary knowledge
within noisy pixels via multiple perturbation-based unsupervised consistency.
Compared to other label-denoising methods, comprehensive experimental results
on the RIGA dataset demonstrate our method's excellent performance and
significant denoising ability.
</p></li>
</ul>

<h3>Title: Revisiting Proposal-based Object Detection. (arXiv:2311.18512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18512">http://arxiv.org/abs/2311.18512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18512]] Revisiting Proposal-based Object Detection(http://arxiv.org/abs/2311.18512)</code></li>
<li>Summary: <p>This paper revisits the pipeline for detecting objects in images with
proposals. For any object detector, the obtained box proposals or queries need
to be classified and regressed towards ground truth boxes. The common solution
for the final predictions is to directly maximize the overlap between each
proposal and the ground truth box, followed by a winner-takes-all ranking or
non-maximum suppression. In this work, we propose a simple yet effective
alternative. For proposal regression, we solve a simpler problem where we
regress to the area of intersection between proposal and ground truth. In this
way, each proposal only specifies which part contains the object, avoiding a
blind inpainting problem where proposals need to be regressed beyond their
visual scope. In turn, we replace the winner-takes-all strategy and obtain the
final prediction by taking the union over the regressed intersections of a
proposal group surrounding an object. Our revisited approach comes with minimal
changes to the detection pipeline and can be plugged into any existing method.
We show that our approach directly improves canonical object detection and
instance segmentation architectures, highlighting the utility of
intersection-based regression and grouping.
</p></li>
</ul>

<h3>Title: Anatomy and Physiology of Artificial Intelligence in PET Imaging. (arXiv:2311.18614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18614">http://arxiv.org/abs/2311.18614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18614]] Anatomy and Physiology of Artificial Intelligence in PET Imaging(http://arxiv.org/abs/2311.18614)</code></li>
<li>Summary: <p>The influence of artificial intelligence (AI) within the field of nuclear
medicine has been rapidly growing. Many researchers and clinicians are seeking
to apply AI within PET, and clinicians will soon find themselves engaging with
AI-based applications all along the chain of molecular imaging, from image
reconstruction to enhanced reporting. This expanding presence of AI in PET
imaging will result in greater demand for educational resources for those
unfamiliar with AI. The objective of this article to is provide an illustrated
guide to the core principles of modern AI, with specific focus on aspects that
are most likely to be encountered in PET imaging. We describe convolutional
neural networks, algorithm training, and explain the components of the commonly
used U-Net for segmentation and image synthesis.
</p></li>
</ul>

<h3>Title: JPPF: Multi-task Fusion for Consistent Panoptic-Part Segmentation. (arXiv:2311.18618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18618">http://arxiv.org/abs/2311.18618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18618]] JPPF: Multi-task Fusion for Consistent Panoptic-Part Segmentation(http://arxiv.org/abs/2311.18618)</code></li>
<li>Summary: <p>Part-aware panoptic segmentation is a problem of computer vision that aims to
provide a semantic understanding of the scene at multiple levels of
granularity. More precisely, semantic areas, object instances, and semantic
parts are predicted simultaneously. In this paper, we present our Joint
Panoptic Part Fusion (JPPF) that combines the three individual segmentations
effectively to obtain a panoptic-part segmentation. Two aspects are of utmost
importance for this: First, a unified model for the three problems is desired
that allows for mutually improved and consistent representation learning.
Second, balancing the combination so that it gives equal importance to all
individual results during fusion. Our proposed JPPF is parameter-free and
dynamically balances its input. The method is evaluated and compared on the
Cityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets in
terms of PartPQ and Part-Whole Quality (PWQ). In extensive experiments, we
verify the importance of our fair fusion, highlight its most significant impact
for areas that can be further segmented into parts, and demonstrate the
generalization capabilities of our design without fine-tuning on 5 additional
datasets.
</p></li>
</ul>

<h3>Title: A Lightweight Clustering Framework for Unsupervised Semantic Segmentation. (arXiv:2311.18628v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18628">http://arxiv.org/abs/2311.18628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18628]] A Lightweight Clustering Framework for Unsupervised Semantic Segmentation(http://arxiv.org/abs/2311.18628)</code></li>
<li>Summary: <p>Unsupervised semantic segmentation aims to label each pixel of an image to a
corresponding class without the use of annotated data. It is a widely
researched area as obtaining labeled datasets are expensive. While previous
works in the field demonstrated a gradual improvement in segmentation
performance, most of them required neural network training. This made
segmentation equally expensive, especially when dealing with large-scale
datasets. We thereby propose a lightweight clustering framework for
unsupervised semantic segmentation. Attention features of the self-supervised
vision transformer exhibit strong foreground-background differentiability. By
clustering these features into a small number of clusters, we could separate
foreground and background image patches into distinct groupings. In our
clustering framework, we first obtain attention features from the
self-supervised vision transformer. Then we extract Dataset-level,
Category-level and Image-level masks by clustering features within the same
dataset, category and image. We further ensure multilevel clustering
consistency across the three levels and this allows us to extract patch-level
binary pseudo-masks. Finally, the pseudo-mask is upsampled, refined and class
assignment is performed according to the CLS token of object regions. Our
framework demonstrates great promise in unsupervised semantic segmentation and
achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.
</p></li>
</ul>

<h3>Title: Learning Part Segmentation from Synthetic Animals. (arXiv:2311.18661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18661">http://arxiv.org/abs/2311.18661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18661]] Learning Part Segmentation from Synthetic Animals(http://arxiv.org/abs/2311.18661)</code></li>
<li>Summary: <p>Semantic part segmentation provides an intricate and interpretable
understanding of an object, thereby benefiting numerous downstream tasks.
However, the need for exhaustive annotations impedes its usage across diverse
object types. This paper focuses on learning part segmentation from synthetic
animals, leveraging the Skinned Multi-Animal Linear (SMAL) models to scale up
existing synthetic data generated by computer-aided design (CAD) animal models.
Compared to CAD models, SMAL models generate data with a wider range of poses
observed in real-world scenarios. As a result, our first contribution is to
construct a synthetic animal dataset of tigers and horses with more pose
diversity, termed Synthetic Animal Parts (SAP). We then benchmark Syn-to-Real
animal part segmentation from SAP to PartImageNet, namely SynRealPart, with
existing semantic segmentation domain adaptation methods and further improve
them as our second contribution. Concretely, we examine three Syn-to-Real
adaptation methods but observe relative performance drop due to the innate
difference between the two tasks. To address this, we propose a simple yet
effective method called Class-Balanced Fourier Data Mixing (CB-FDM). Fourier
Data Mixing aligns the spectral amplitudes of synthetic images with real
images, thereby making the mixed images have more similar frequency content to
real images. We further use Class-Balanced Pseudo-Label Re-Weighting to
alleviate the imbalanced class distribution. We demonstrate the efficacy of
CB-FDM on SynRealPart over previous methods with significant performance
improvements. Remarkably, our third contribution is to reveal that the learned
parts from synthetic tiger and horse are transferable across all quadrupeds in
PartImageNet, further underscoring the utility and potential applications of
animal part segmentation.
</p></li>
</ul>

<h3>Title: Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction. (arXiv:2311.18695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18695">http://arxiv.org/abs/2311.18695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18695]] Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction(http://arxiv.org/abs/2311.18695)</code></li>
<li>Summary: <p>State-of-the-art single-view 360-degree room layout reconstruction methods
formulate the problem as a high-level 1D (per-column) regression task. On the
other hand, traditional low-level 2D layout segmentation is simpler to learn
and can represent occluded regions, but it requires complex post-processing for
the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to
render 1D layout depth regression from the 2D segmentation map in a
differentiable and occlusion-aware way, marrying the merits of both sides.
Specifically, our model predicts floor-plan density for the input
equirectangular 360-degree image. Formulating the 2D layout representation as a
density field enables us to employ `flattened' volume rendering to form 1D
layout depth regression. In addition, we propose a novel 3D warping
augmentation on layout to improve generalization. Finally, we re-implement
recent room layout reconstruction methods into our codebase for benchmarking
and explore modern backbones and training techniques to serve as the strong
baseline. Our model significantly outperforms previous arts. The code will be
made available upon publication.
</p></li>
</ul>

<h3>Title: Filtered Semi-Markov CRF. (arXiv:2311.18028v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.18028">http://arxiv.org/abs/2311.18028</a></li>
<li>Code URL: https://github.com/urchade/filtered-semi-markov-crf</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.18028]] Filtered Semi-Markov CRF(http://arxiv.org/abs/2311.18028)</code></li>
<li>Summary: <p>Semi-Markov CRF has been proposed as an alternative to the traditional Linear
Chain CRF for text segmentation tasks such as Named Entity Recognition (NER).
Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF
considers segments as the basic unit, making it more expressive. However,
Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over
sequence length, as it operates on every span of the input sequence, and (2)
inferior performance compared to CRF for sequence labeling tasks like NER. In
this paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that
addresses these issues by incorporating a filtering step to eliminate
irrelevant segments, reducing complexity and search space. Our approach is
evaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF
while being significantly faster. The implementation of our method is available
on \href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
