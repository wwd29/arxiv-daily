<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-03</h1>
<h3>Title: Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset</h3>
<ul>
<li><strong>Authors: </strong>Leroy Z. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01219">https://arxiv.org/abs/2510.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01219">https://arxiv.org/pdf/2510.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01219]] Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset(https://arxiv.org/abs/2510.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a dataset of concept learning tasks that helps uncover implicit biases in large language models. Using in-context concept learning experiments, we found that language models may have a bias toward upward monotonicity in quantifiers; such bias is less apparent when the model is tested by direct prompting without concept learning components. This demonstrates that in-context concept learning can be an effective way to discover hidden biases in language models.</li>
</ul>

<h3>Title: Towards Open-Ended Discovery for Low-Resource NLP</h3>
<ul>
<li><strong>Authors: </strong>Bonaventure F. P. Dossou, Henri AÃ¯dasso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01220">https://arxiv.org/abs/2510.01220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01220">https://arxiv.org/pdf/2510.01220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01220]] Towards Open-Ended Discovery for Low-Resource NLP(https://arxiv.org/abs/2510.01220)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural Language Processing (NLP) for low-resource languages remains fundamentally constrained by the lack of textual corpora, standardized orthographies, and scalable annotation pipelines. While recent advances in large language models have improved cross-lingual transfer, they remain inaccessible to underrepresented communities due to their reliance on massive, pre-collected data and centralized infrastructure. In this position paper, we argue for a paradigm shift toward open-ended, interactive language discovery, where AI systems learn new languages dynamically through dialogue rather than static datasets. We contend that the future of language technology, particularly for low-resource and under-documented languages, must move beyond static data collection pipelines toward interactive, uncertainty-driven discovery, where learning emerges dynamically from human-machine collaboration instead of being limited to pre-existing datasets. We propose a framework grounded in joint human-machine uncertainty, combining epistemic uncertainty from the model with hesitation cues and confidence signals from human speakers to guide interaction, query selection, and memory retention. This paper is a call to action: we advocate a rethinking of how AI engages with human knowledge in under-documented languages, moving from extractive data collection toward participatory, co-adaptive learning processes that respect and empower communities while discovering and preserving the world's linguistic diversity. This vision aligns with principles of human-centered AI, emphasizing interactive, cooperative model building between AI systems and speakers.</li>
</ul>

<h3>Title: Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bertrand Kian Hassani, Yacoub Bahini, Rizwan Mushtaq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01222">https://arxiv.org/abs/2510.01222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01222">https://arxiv.org/pdf/2510.01222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01222]] Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs(https://arxiv.org/abs/2510.01222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Climate change has increased demands for transparent and comparable corporate climate disclosures, yet imitation and symbolic reporting often undermine their value. This paper develops a multidimensional framework to assess disclosure maturity among 828 this http URL firms using large language models (LLMs) fine-tuned for climate communication. Four classifiers-sentiment, commitment, specificity, and target ambition-extract narrative indicators from sustainability and annual reports, which are linked to firm attributes such as emissions, market capitalization, and sector. Analyses reveal three insights: (1) risk-focused narratives often align with explicit commitments, but quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2) larger and higher-emitting firms disclose more commitments and actions than peers, though inconsistently with quantitative targets; and (3) widespread similarity in disclosure styles suggests mimetic behavior, reducing differentiation and decision usefulness. These results highlight the value of LLMs for ESG narrative analysis and the need for stronger regulation to connect commitments with verifiable transition strategies.</li>
</ul>

<h3>Title: Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Hui Dou, Ning Xu, Yiwen Zhang, Kaibin Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01223">https://arxiv.org/abs/2510.01223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01223">https://arxiv.org/pdf/2510.01223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01223]] Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge(https://arxiv.org/abs/2510.01223)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.</li>
</ul>

<h3>Title: Context Matters: Comparison of commercial large language tools in veterinary medicine</h3>
<ul>
<li><strong>Authors: </strong>Tyler J Poore, Christopher J Pinard, Aleena Shabbir, Andrew Lagree, Andre Telfer, Kuan-Chuen Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01224">https://arxiv.org/abs/2510.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01224">https://arxiv.org/pdf/2510.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01224]] Context Matters: Comparison of commercial large language tools in veterinary medicine(https://arxiv.org/abs/2510.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in clinical settings, yet their performance in veterinary medicine remains underexplored. We evaluated three commercially available veterinary-focused LLM summarization tools (Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework, summaries were scored across five domains: Factual Accuracy, Completeness, Chronological Order, Clinical Relevance, and Organization. Product 1 achieved the highest overall performance, with a median average score of 4.61 (IQR: 0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for Product 3. It also received perfect median scores in Factual Accuracy and Chronological Order. To assess the internal consistency of the grading framework itself, we repeated the evaluation across three independent runs. The LLM grader demonstrated high reproducibility, with Average Score standard deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3). These findings highlight the importance of veterinary-specific commercial LLM tools and demonstrate that LLM-as-a-judge evaluation is a scalable and reproducible method for assessing clinical NLP summarization in veterinary medicine.</li>
</ul>

<h3>Title: EEFSUVA: A New Mathematical Olympiad Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Nicole N Khatibi, Daniil A. Radamovich, Michael P. Brenner</a></li>
<li><strong>Subjects: </strong>cs.CL, math.HO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01227">https://arxiv.org/abs/2510.01227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01227">https://arxiv.org/pdf/2510.01227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01227]] EEFSUVA: A New Mathematical Olympiad Benchmark(https://arxiv.org/abs/2510.01227)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs have spurred claims that large language models (LLMs) match gold medal Olympiad to graduate level proficiency on mathematics benchmarks. In this work, we examine these claims in detail and assess the extent to which current benchmarks capture genuine LLM mathematical reasoning. The composition of these benchmarks, primarily drawing from the International Mathematics Olympiad (IMO) and related competitions, may overstate models reasoning ability due to potential data contamination and a narrow focus on familiar problem types. To enable a more holistic assessment of mathematical understanding, we introduce EEFSUVA, a novel benchmark curated from under circulated regional and national Olympiads of Eastern Europe and the countries from the former Soviet Union. These contests feature problems of comparable difficulty to the IMO and are renowned for demanding nonstandard problem-solving techniques, yet their problems are far less prevalent in online corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a notable performance decline on EEFSUVA relative to other Olympiad-style benchmarks. These findings also suggest the potential importance of broader evaluation datasets for a fuller assessment of mathematical reasoning and for guiding future model development.</li>
</ul>

<h3>Title: Who is In Charge? Dissecting Role Conflicts in Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Siqi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01228">https://arxiv.org/abs/2510.01228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01228">https://arxiv.org/pdf/2510.01228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01228]] Who is In Charge? Dissecting Role Conflicts in Instruction Following(https://arxiv.org/abs/2510.01228)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models should follow hierarchical instructions where system prompts override user inputs, yet recent work shows they often ignore this rule while strongly obeying social cues such as authority or consensus. We extend these behavioral findings with mechanistic interpretations on a large-scale dataset. Linear probing shows conflict-decision signals are encoded early, with system-user and social conflicts forming distinct subspaces. Direct Logit Attribution reveals stronger internal conflict detection in system-user cases but consistent resolution only for social cues. Steering experiments show that, despite using social cues, the vectors surprisingly amplify instruction following in a role-agnostic way. Together, these results explain fragile system obedience and underscore the need for lightweight hierarchy-sensitive alignment methods.</li>
</ul>

<h3>Title: Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision</h3>
<ul>
<li><strong>Authors: </strong>Dimitar Peshevski, Kiril Blazhevski, Martin Popovski, Gjorgji Madjarov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01229">https://arxiv.org/abs/2510.01229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01229">https://arxiv.org/pdf/2510.01229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01229]] Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision(https://arxiv.org/abs/2510.01229)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Effective document reranking is essential for improving search relevance across diverse applications. While Large Language Models (LLMs) excel at reranking due to their deep semantic understanding and reasoning, their high computational cost makes them impractical for many real-world deployments. Fine-tuning smaller, task-specific models is a more efficient alternative but typically depends on scarce, manually labeled data. To overcome this, we propose a novel pipeline that eliminates the need for human-labeled query-document pairs. Our method uses LLMs to generate synthetic queries from domain-specific corpora and employs an LLM-based classifier to label positive and hard-negative pairs. This synthetic dataset is then used to fine-tune a smaller transformer model with contrastive learning using Localized Contrastive Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our approach significantly boosts in-domain performance and generalizes well to out-of-domain tasks. By using LLMs for data generation and supervision rather than inference, we reduce computational costs while maintaining strong reranking capabilities.</li>
</ul>

<h3>Title: Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuaidong Pan, Di Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01231">https://arxiv.org/abs/2510.01231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01231">https://arxiv.org/pdf/2510.01231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01231]] Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models(https://arxiv.org/abs/2510.01231)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study addresses the reliability of automatic summarization in high-risk scenarios and proposes a large language model framework that integrates uncertainty quantification and risk-aware mechanisms. Starting from the demands of information overload and high-risk decision-making, a conditional generation-based summarization model is constructed, and Bayesian inference is introduced during generation to model uncertainty in the parameter space, which helps avoid overconfident predictions. The uncertainty level of the generated content is measured using predictive distribution entropy, and a joint optimization of entropy regularization and risk-aware loss is applied to ensure that key information is preserved and risk attributes are explicitly expressed during information compression. On this basis, the model incorporates risk scoring and regulation modules, allowing summaries to cover the core content accurately while enhancing trustworthiness through explicit risk-level prompts. Comparative experiments and sensitivity analyses verify that the proposed method significantly improves the robustness and reliability of summarization in high-risk applications while maintaining fluency and semantic integrity. This research provides a systematic solution for trustworthy summarization and demonstrates both scalability and practical value at the methodological level.</li>
</ul>

<h3>Title: Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Dongjun Kim, Gyuho Shim, Yongchan Chun, Minhyuk Kim, Chanjun Park, Heuiseok Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01232">https://arxiv.org/abs/2510.01232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01232">https://arxiv.org/pdf/2510.01232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01232]] Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks(https://arxiv.org/abs/2510.01232)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models are commonly judged by their scores on standard benchmarks, yet such scores often overstate real capability since they mask the mix of skills a task actually demands. For example, ARC is assumed to test reasoning, while HellaSwag is designed to evaluate commonsense. However, we lack a systematic way to verify if these benchmarks actually measure these labels. We introduce Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS) that quantifies how much each ability contributes to a model's success on a given benchmark. Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and thus show only modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance. Benchmark Profiling therefore explains why performance gains do not always translate into user-perceived competence and offers a transparent tool for benchmark audit and model interpretability.</li>
</ul>

<h3>Title: LLMRank: Understanding LLM Strengths for Model Routing</h3>
<ul>
<li><strong>Authors: </strong>Shubham Agrawal, Prasang Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01234">https://arxiv.org/abs/2510.01234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01234">https://arxiv.org/pdf/2510.01234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01234]] LLMRank: Understanding LLM Strengths for Model Routing(https://arxiv.org/abs/2510.01234)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of large language models (LLMs) with diverse capabilities, latency and computational costs presents a critical deployment challenge: selecting the most suitable model for each prompt to optimize the trade-off between performance and efficiency. We introduce LLMRank, a prompt-aware routing framework that leverages rich, human-readable features extracted from prompts, including task type, reasoning patterns, complexity indicators, syntactic cues, and signals from a lightweight proxy solver. Unlike prior one-shot routers that rely solely on latent embeddings, LLMRank predicts per-model utility using a neural ranking model trained on RouterBench, comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs, from small efficient models to large frontier systems. Our approach achieves up to 89.2% of oracle utility, while providing interpretable feature attributions that explain routing decisions. Extensive studies demonstrate the importance of multifaceted feature extraction and the hybrid ranking objective, highlighting the potential of feature-driven routing for efficient and transparent LLM deployment.</li>
</ul>

<h3>Title: Automated Extraction of Material Properties using LLM-based AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Subham Ghosh, Abhishek Tewari</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01235">https://arxiv.org/abs/2510.01235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01235">https://arxiv.org/pdf/2510.01235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01235]] Automated Extraction of Material Properties using LLM-based AI Agents(https://arxiv.org/abs/2510.01235)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The rapid discovery of materials is constrained by the lack of large, machine-readable datasets that couple performance metrics with structural context. Existing databases are either small, manually curated, or biased toward first principles results, leaving experimental literature underexploited. We present an agentic, large language model (LLM)-driven workflow that autonomously extracts thermoelectric and structural-properties from about 10,000 full-text scientific articles. The pipeline integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost. Benchmarking on 50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91 for thermoelectric properties and 0.82 for structural fields), while GPT-4.1 Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction of the cost, enabling practical large scale deployment. Applying this workflow, we curated 27,822 temperature resolved property records with normalized units, spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity, power factor, and thermal conductivity, together with structural attributes such as crystal class, space group, and doping strategy. Dataset analysis reproduces known thermoelectric trends, such as the superior performance of alloys over oxides and the advantage of p-type doping, while also surfacing broader structure-property correlations. To facilitate community access, we release an interactive web explorer with semantic filters, numeric queries, and CSV export. This study delivers the largest LLM-curated thermoelectric dataset to date, provides a reproducible and cost-profiled extraction pipeline, and establishes a foundation for scalable, data-driven materials discovery beyond thermoelectrics.</li>
</ul>

<h3>Title: Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Nandakishor M</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01237">https://arxiv.org/abs/2510.01237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01237">https://arxiv.org/pdf/2510.01237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01237]] Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation(https://arxiv.org/abs/2510.01237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models suffer from hallucination, generating plausible yet factually incorrect content. Current mitigation strategies focus on post-generation correction, which is computationally expensive and fails to prevent unreliable content generation. We propose a confidence-aware routing system that proactively assesses model uncertainty before generation and redirects queries based on estimated reliability. Our approach combines three complementary signals: semantic alignment between internal representations and reference embeddings, internal convergence analysis across model layers, and learned confidence estimation. The unified confidence score determines routing to four pathways: local generation for high confidence, retrieval-augmented generation for medium confidence, larger models for low confidence, and human review for very low confidence. Evaluation on knowledge-intensive QA benchmarks demonstrates significant improvements in hallucination detection (0.74 vs. 0.42 baseline) while reducing computational costs by 40% compared to post-hoc methods. The F1 score improves from 0.61 to 0.82 with low false positive rates (0.09). This paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.</li>
</ul>

<h3>Title: Silent Tokens, Loud Effects: Padding in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rom Himelstein, Amit LeVi, Yonatan Belinkov, Avi Mendelson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01238">https://arxiv.org/abs/2510.01238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01238">https://arxiv.org/pdf/2510.01238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01238]] Silent Tokens, Loud Effects: Padding in LLMs(https://arxiv.org/abs/2510.01238)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Padding tokens are widely used in large language models (LLMs) to equalize sequence lengths during batched inference. While they should be fully masked, implementation errors can cause them to influence computation, and the extent of this influence is not well understood. We systematically study this effect across three open-source model families (Llama, Gemma, Qwen), inserting controlled amounts of padding and evaluating outcomes along four axes: activations, generation quality, bias, and safety. Even small amounts of padding shift hidden representations, degrade quality in smaller models, alter bias in unpredictable ways, and weaken safety guardrails. These findings demonstrate that padding is not a harmless detail but a robustness risk that must be carefully handled in deployment.</li>
</ul>

<h3>Title: CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM</h3>
<ul>
<li><strong>Authors: </strong>Juntae Lee, Jihwan Bang, Seunghan Yang, Simyung Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01239">https://arxiv.org/abs/2510.01239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01239">https://arxiv.org/pdf/2510.01239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01239]] CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM(https://arxiv.org/abs/2510.01239)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which is a novel execution system for efficient sub-task handling in multi-turn interactions with a single on-device large language model (LLM). As LLMs become increasingly capable, a single model is expected to handle diverse sub-tasks that more effectively and comprehensively support answering user requests. Naive approach reprocesses the entire conversation context when switching between main and sub-tasks (e.g., query rewriting, summarization), incurring significant computational overhead. CIFLEX mitigates this overhead by reusing the key-value (KV) cache from the main task and injecting only task-specific instructions into isolated side paths. After sub-task execution, the model rolls back to the main path via cached context, thereby avoiding redundant prefill computation. To support sub-task selection, we also develop a hierarchical classification strategy tailored for small-scale models, decomposing multi-choice decisions into binary ones. Experiments show that CIFLEX significantly reduces computational costs without degrading task performance, enabling scalable and efficient multi-task dialogue on-device.</li>
</ul>

<h3>Title: RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zukang Xu, Xing Hu, Qiang Wu, Dawei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01240">https://arxiv.org/abs/2510.01240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01240">https://arxiv.org/pdf/2510.01240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01240]] RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models(https://arxiv.org/abs/2510.01240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.</li>
</ul>

<h3>Title: SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Hu Wei, Ze Xu, Boyu Yang, Linlin Miao, Weiqi Zhai, Yihan Li, Zixuan Li, Zhijun Wang, Boya Wang, Jianwei Yu, Jialing Yuan, Xiaoyue Zhang, Cheng He, Minglei Chen, Zifan Zhang, Qianhui Li, Wei Wang, Xiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01241">https://arxiv.org/abs/2510.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01241">https://arxiv.org/pdf/2510.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01241]] SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation(https://arxiv.org/abs/2510.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.</li>
</ul>

<h3>Title: Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Seyma Yaman Kayadibi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01242">https://arxiv.org/abs/2510.01242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01242">https://arxiv.org/pdf/2510.01242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01242]] Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI(https://arxiv.org/abs/2510.01242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence is observed to age not through chronological time but through structural asymmetries in memory performance. In large language models, semantic cues such as the name of the day often remain stable across sessions, while episodic details like the sequential progression of experiment numbers tend to collapse when conversational context is reset. To capture this phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled, entropy-informed metric of memory aging derived from observable recall behavior. The score is formally proven to be well-defined, bounded, and monotonic under mild and model-agnostic assumptions, making it applicable across various tasks and domains. In its Redundancy-as-Masking formulation, the score interprets redundancy as overlapping information that reduces the penalized mass. However, in the present study, redundancy is not explicitly estimated; all reported values assume a redundancy-neutral setting (R = 0), yielding conservative upper bounds. The AAS framework was tested over a 25-day bilingual study involving ChatGPT-5, structured into stateless and persistent interaction phases. During persistent sessions, the model consistently recalled both semantic and episodic details, driving the AAS toward its theoretical minimum, indicative of structural youth. In contrast, when sessions were reset, the model preserved semantic consistency but failed to maintain episodic continuity, causing a sharp increase in the AAS and signaling structural memory aging. These findings support the utility of AAS as a theoretically grounded, task-independent diagnostic tool for evaluating memory degradation in artificial systems. The study builds on foundational concepts from von Neumann's work on automata, Shannon's theories of information and redundancy, and Turing's behavioral approach to intelligence.</li>
</ul>

<h3>Title: Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing</h3>
<ul>
<li><strong>Authors: </strong>Yisong Xiao, Aishan Liu, Siyuan Liang, Zonghao Ying, Xianglong Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01243">https://arxiv.org/abs/2510.01243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01243">https://arxiv.org/pdf/2510.01243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01243]] Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing(https://arxiv.org/abs/2510.01243)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance across various tasks, yet they remain vulnerable to generating toxic content, necessitating detoxification strategies to ensure safe and responsible deployment. Test-time detoxification methods, which typically introduce static or dynamic interventions into LLM representations, offer a promising solution due to their flexibility and minimal invasiveness. However, current approaches often suffer from imprecise interventions, primarily due to their insufficient exploration of the transition space between toxic and non-toxic outputs. To address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward \textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel test-time detoxification framework that explicitly models toxicity transitions within the latent representation space, enabling stable and precise reward-guided editing. ARGRE identifies non-toxic semantic directions and interpolates between toxic and non-toxic representations to reveal fine-grained transition trajectories. These trajectories transform sparse toxicity annotations into dense training signals, enabling the construction of an autoregressive reward model that delivers stable and precise editing guidance. At inference, the reward model guides an adaptive two-step editing process to obtain detoxified representations: it first performs directional steering based on expected reward gaps to shift representations toward non-toxic regions, followed by lightweight gradient-based refinements. Extensive experiments across 8 widely used LLMs show that ARGRE significantly outperforms leading baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference time), while preserving the core capabilities of the original model with minimal degradation. Our code is available at the website.</li>
</ul>

<h3>Title: Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Hyeoneui Kim, Jeongha Kim, Huijing Xu, Jinsun Jung, Sunghoon Kang, Sun Joo Jang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01244">https://arxiv.org/abs/2510.01244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01244">https://arxiv.org/pdf/2510.01244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01244]] Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model(https://arxiv.org/abs/2510.01244)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Stress, arising from the dynamic interaction between external stressors, individual appraisals, and physiological or psychological responses, significantly impacts health yet is often underreported and inconsistently documented, typically captured as unstructured free-text in electronic health records. Ambient AI technologies offer promise in reducing documentation burden, but predominantly generate unstructured narratives, limiting downstream clinical utility. This study aimed to develop an ontology for mental stress and evaluate the feasibility of using a Large Language Model (LLM) to extract ontology-guided stress-related information from narrative text. The Mental Stress Ontology (MeSO) was developed by integrating theoretical models like the Transactional Model of Stress with concepts from 11 validated stress assessment tools. MeSO's structure and content were refined using Ontology Pitfall Scanner! and expert validation. Using MeSO, six categories of stress-related information--stressor, stress response, coping strategy, duration, onset, and temporal profile--were extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated accuracy and ontology coverage. The final ontology included 181 concepts across eight top-level classes. Of 220 extractable stress-related items, the LLM correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21 (9.5%). All correctly extracted items were accurately mapped to MeSO, although 24 relevant concepts were not yet represented in the ontology. This study demonstrates the feasibility of using an ontology-guided LLM for structured extraction of stress-related information, offering potential to enhance the consistency and utility of stress documentation in ambient AI systems. Future work should involve clinical dialogue data and comparison across LLMs.</li>
</ul>

<h3>Title: Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports</h3>
<ul>
<li><strong>Authors: </strong>Punit Kumar Singh, Nishant Kumar, Akash Ghosh, Kunal Pasad, Khushi Soni, Manisha Jaishwal, Sriparna Saha, Syukron Abu Ishaq Alfarozi, Asres Temam Abagissa, Kitsuchart Pasupa, Haiqin Yang, Jose G Moreno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01247">https://arxiv.org/abs/2510.01247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01247">https://arxiv.org/pdf/2510.01247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01247]] Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports(https://arxiv.org/abs/2510.01247)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) are primarily evaluated on globally popular sports, often overlooking regional and indigenous sporting traditions. To address this gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess LMs' understanding of traditional sports across 60 countries and 6 continents, encompassing four distinct cultural categories. The dataset features 33,000 multiple-choice questions (MCQs) across text and image modalities, each of which is categorized into three key types: history-based, rule-based, and scenario-based. To evaluate model performance, we employ zero-shot, few-shot, and chain-of-thought (CoT) prompting across a diverse set of Large Language Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language Models (MLMs). By providing a comprehensive multilingual and multicultural sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for assessing AI's ability to understand and reason about traditional sports.</li>
</ul>

<h3>Title: SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01248">https://arxiv.org/abs/2510.01248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01248">https://arxiv.org/pdf/2510.01248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01248]] SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs(https://arxiv.org/abs/2510.01248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.</li>
</ul>

<h3>Title: LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning</h3>
<ul>
<li><strong>Authors: </strong>You-Le Fang, Dong-Shan Jian, Xiang Li, Ce Meng, Ling-Shi Meng, Chen-Xu Yan, Zhi-Zhang Bian, Yan-Qing Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01249">https://arxiv.org/abs/2510.01249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01249">https://arxiv.org/pdf/2510.01249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01249]] LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning(https://arxiv.org/abs/2510.01249)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel in general domains, their reliability often falls short in scientific problem-solving. The advancement of scientific AI depends on large-scale, high-quality corpora. However, existing scientific question-answering (QA) datasets suffer from high error rates, frequently resulting from logical leaps and implicit reasoning within the answers. To address this issue, we introduce LOCA (Logical Chain Augmentation), a novel framework for automatically cleaning scientific corpora, implemented through an augment-and-review loop. At its core, LOCA enhances raw answers by completing missing logical steps and explicitly separating the underlying scientific principle from its subsequent derivation. By applying LOCA to challenging scientific corpora, we demonstrate that it can automatically filter noisy datasets, typically reducing the error rate from as high as 20\% to below 2\%. LOCA provides a scalable and effective methodology for creating high-quality scientific corpora, paving the way for more reliable training and evaluation of scientific AI.</li>
</ul>

<h3>Title: GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Trung Duc Anh Dang, Ferdinando Pio D'Elia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01250">https://arxiv.org/abs/2510.01250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01250">https://arxiv.org/pdf/2510.01250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01250]] GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages(https://arxiv.org/abs/2510.01250)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As social-media platforms emerge and evolve faster than the regulations meant to oversee them, automated detoxification might serve as a timely tool for moderators to enforce safe discourse at scale. We here describe our submission to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites toxic single-sentence inputs into neutral paraphrases across 15 typologically diverse languages. Building on a 12B-parameter Gemma-3 multilingual transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting techniques like few-shot and Chain-of-Thought. Our multilingual training corpus combines 3,600 human-authored parallel pairs, 21,600 machine-translated synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At inference, inputs are enriched with three LaBSE-retrieved neighbors and explicit toxic-span annotations. Evaluated via Style Transfer Accuracy, LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first on high-resource and low-resource languages. Ablations show +0.081 joint score increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA analysis identifies language resource status as the strongest predictor of performance ($\eta^2$ = 0.667, p < 0.01).</li>
</ul>

<h3>Title: Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Carlo Bono, Federico Belotti, Matteo Palmonari</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01251">https://arxiv.org/abs/2510.01251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01251">https://arxiv.org/pdf/2510.01251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01251]] Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data(https://arxiv.org/abs/2510.01251)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Linking textual values in tabular data to their corresponding entities in a Knowledge Base is a core task across a variety of data integration and enrichment applications. Although Large Language Models (LLMs) have shown State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in real-world scenarios requires not only accurate predictions but also reliable uncertainty estimates, which require resource-demanding multi-shot inference, posing serious limits to their actual applicability. As a more efficient alternative, we investigate a self-supervised approach for estimating uncertainty from single-shot LLM outputs using token-level features, reducing the need for multiple generations. Evaluation is performed on an EL task on tabular data across multiple LLMs, showing that the resulting uncertainty estimates are highly effective in detecting low-accuracy outputs. This is achieved at a fraction of the computational cost, ultimately supporting a cost-effective integration of uncertainty measures into LLM-based EL workflows. The method offers a practical way to incorporate uncertainty estimation into EL workflows with limited computational overhead.</li>
</ul>

<h3>Title: GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mariam Mahran, Katharina Simbeck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01252">https://arxiv.org/abs/2510.01252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01252">https://arxiv.org/pdf/2510.01252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01252]] GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models(https://arxiv.org/abs/2510.01252)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) are increasingly trained on massive, uncurated corpora, understanding both model representations and the data they internalize has become a major challenge. In this work, we show that pairing LLMs with sparse autoencoders (SAEs) enables interpretation not only of model behavior but also of the deeper structures, themes, and biases embedded in the training data. We train a GPT-style transformer model exclusively on the novels of Jane Austen, a corpus rich in social constructs and narrative patterns. We then apply SAEs to hidden states across multiple layers, uncovering sparse, interpretable features that reflect the key narratives and concepts present in the corpus, including gender, class, and societal duty. Our findings demonstrate that LLMs combined with SAEs can act as scalable probes into complex datasets, offering a new path for corpus exploration, bias discovery, and model interpretability at scale.</li>
</ul>

<h3>Title: Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs</h3>
<ul>
<li><strong>Authors: </strong>Shree Harsha Bokkahalli Satish, Gustav Eje Henter, Ãva SzÃ©kely</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01254">https://arxiv.org/abs/2510.01254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01254">https://arxiv.org/pdf/2510.01254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01254]] Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs(https://arxiv.org/abs/2510.01254)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent work in benchmarking bias and fairness in speech large language models (SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA) formats. The model is tasked to choose between stereotypical, anti-stereotypical, or neutral/irrelevant answers given an input speech prompt and an optional text prompt. Such MCQA benchmarks implicitly assume that model performance is consistent across other MCQA tasks, voices, and other task formats such as more realistic, long-form evaluations. In this paper, we probe that assumption. We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA behaviours: preference for stereotypical, anti-stereotypical, or neutral/uncertain answers. We then evaluate whether these behaviours generalise to another, distinct MCQA benchmark, and more critically to long-form, creative generation tasks. Our results show that performance on MCQA bias benchmarks fails to reliably predict performances across other MCQA benchmarks, and more importantly across long-form tasks. We conclude that current MCQA bias benchmarks show limited evidence of cross-task generalisation in the speech domain, and also propose an evaluation suite for measuring behaviour transferability in future models and benchmarks.</li>
</ul>

<h3>Title: Longitudinal Monitoring of LLM Content Moderation of Social Issues</h3>
<ul>
<li><strong>Authors: </strong>Yunlang Dai, Emma Lurie, DanaÃ© Metaxa, Sorelle A. Friedler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01255">https://arxiv.org/abs/2510.01255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01255">https://arxiv.org/pdf/2510.01255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01255]] Longitudinal Monitoring of LLM Content Moderation of Social Issues(https://arxiv.org/abs/2510.01255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models' (LLMs') outputs are shaped by opaque and frequently-changing company content moderation policies and practices. LLM moderation often takes the form of refusal; models' refusal to produce text about certain topics both reflects company policy and subtly shapes public discourse. We introduce AI Watchman, a longitudinal auditing system to publicly measure and track LLM refusals over time, to provide transparency into an important and black-box aspect of LLMs. Using a dataset of over 400 social issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and DeepSeek (both in English and Chinese). We find evidence that changes in company policies, even those not publicly announced, can be detected by AI Watchman, and identify company- and model-specific differences in content moderation. We also qualitatively analyze and categorize different forms of refusal. This work contributes evidence for the value of longitudinal auditing of LLMs, and AI Watchman, one system for doing so.</li>
</ul>

<h3>Title: RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Can Lin, Zhengwang Jiang, Ling Zheng, Qi Zhao, Yuhang Zhang, Qi Song, Wangqiu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01257">https://arxiv.org/abs/2510.01257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01257">https://arxiv.org/pdf/2510.01257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01257]] RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs(https://arxiv.org/abs/2510.01257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs. Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs. To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements.</li>
</ul>

<h3>Title: Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse</h3>
<ul>
<li><strong>Authors: </strong>Nathan Junzi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01258">https://arxiv.org/abs/2510.01258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01258">https://arxiv.org/pdf/2510.01258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01258]] Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse(https://arxiv.org/abs/2510.01258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information mediums. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague the novel technology. This paper employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing an aforementioned bias evaluation metric. Results show an amplified liberal-authoritarian alignment across all six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on a region's pre-existing socio-political structures.</li>
</ul>

<h3>Title: Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks</h3>
<ul>
<li><strong>Authors: </strong>Vedant Palit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01261">https://arxiv.org/abs/2510.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01261">https://arxiv.org/pdf/2510.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01261]] Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks(https://arxiv.org/abs/2510.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is vulnerable to poisoning and backdoor attacks under partial observability. We formulate defence as a partially observable sequential decision problem and introduce a trust-aware Deep Q-Network that integrates multi-signal evidence into client trust updates while optimizing a long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a baseline showing steadily improving accuracy, (ii) show through a Dirichlet sweep that increased client overlap consistently improves accuracy and reduces ASR with stable detection, and (iii) demonstrate in a signal-budget study that accuracy remains steady while ASR increases and ROC-AUC declines as observability is reduced, which highlights that sequential belief updates mitigate weaker signals. Finally, a comparison with random, linear-Q, and policy gradient controllers confirms that DQN achieves the best robustness--accuracy trade-off.</li>
</ul>

<h3>Title: Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Yaron Meirovitch, Fuming Yang, Jeff Lichtman, Nir Shavit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01263">https://arxiv.org/abs/2510.01263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01263">https://arxiv.org/pdf/2510.01263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01263]] Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency(https://arxiv.org/abs/2510.01263)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget (the product of its long-term on-rate $a_i$ and fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, $\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path toward learning more diverse and efficient representations.</li>
</ul>

<h3>Title: A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</h3>
<ul>
<li><strong>Authors: </strong>Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01264">https://arxiv.org/abs/2510.01264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01264">https://arxiv.org/pdf/2510.01264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01264]] A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab(https://arxiv.org/abs/2510.01264)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: this https URL .</li>
</ul>

<h3>Title: AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Zhou, Jin Zhu, Pingfan Su, Kai Ye, Ying Yang, Shakeel A O B Gavioli-Akilagun, Chengchun Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01268">https://arxiv.org/abs/2510.01268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01268">https://arxiv.org/pdf/2510.01268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01268]] AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees(https://arxiv.org/abs/2510.01268)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 58%. A python implementation of our method is available at this https URL.</li>
</ul>

<h3>Title: Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection</h3>
<ul>
<li><strong>Authors: </strong>Hoang Phan, Victor Li, Qi Lei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01270">https://arxiv.org/abs/2510.01270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01270">https://arxiv.org/pdf/2510.01270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01270]] Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection(https://arxiv.org/abs/2510.01270)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing with their ability to generate coherent and contextually relevant text. However, their deployment raises significant concerns about the potential for generating harmful or inappropriate content. In this paper, we introduce Progressive Self-Reflection (PSR), a novel inference-time technique that empowers LLMs to self-monitor and correct their outputs dynamically. Experimental results demonstrate that applying our proposed method to Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\% to 3.8\%, without additional training, while maintaining their original performance on benign tasks. Our approach acts as a test-time scaling method, where additional self-reflection rounds enhance safety at the cost of inference overhead. To balance safety with computational efficiency, we introduce a lightweight self-reflection predictor that estimates the optimal number of reflection rounds based on input complexity. This adaptive mechanism prevents unnecessary self-assessment on benign inputs while ensuring thorough evaluation when encountering potentially harmful content. Our findings suggest that Progressive Self-Reflection serves as a scalable test-time approach, enhancing LLM safety by dynamically allocating computational resources in proportion to the input's risk profile.</li>
</ul>

<h3>Title: Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations</h3>
<ul>
<li><strong>Authors: </strong>Arend Hintze, Asadullah Najam, Jory Schossau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01271">https://arxiv.org/abs/2510.01271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01271">https://arxiv.org/pdf/2510.01271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01271]] Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations(https://arxiv.org/abs/2510.01271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is crucial for advancing their interpretability and improving their design. This study introduces an innovative information-theoretic method to identify and analyze information-transfer nodes within RNNs, which we refer to as \textit{information relays}. By quantifying the mutual information between input and output vectors across nodes, our approach pinpoints critical pathways through which information flows during network operations. We apply this methodology to both synthetic and real-world time series classification tasks, employing various RNN architectures, including Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns of information relay across different architectures, offering insights into how information is processed and maintained over time. Additionally, we conduct node knockout experiments to assess the functional importance of identified nodes, significantly contributing to explainable artificial intelligence by elucidating how specific nodes influence overall network behavior. This study not only enhances our understanding of the complex mechanisms driving RNNs but also provides a valuable tool for designing more robust and interpretable neural networks.</li>
</ul>

<h3>Title: TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shenxu Chang, Junchi Yu, Weixing Wang, Yongqiang Chen, Jialin Yu, Philip Torr, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01274">https://arxiv.org/abs/2510.01274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01274">https://arxiv.org/pdf/2510.01274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01274]] TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models(https://arxiv.org/abs/2510.01274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (D-LLMs) have recently emerged as a promising alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination problem in D-LLMs remains underexplored, limiting their reliability in real-world applications. Existing hallucination detection methods are designed for AR-LLMs and rely on signals from single-step generation, making them ill-suited for D-LLMs where hallucination signals often emerge throughout the multi-step denoising process. To bridge this gap, we propose TraceDet, a novel framework that explicitly leverages the intermediate denoising steps of D-LLMs for hallucination detection. TraceDet models the denoising process as an action trace, with each action defined as the model's prediction over the cleaned response, conditioned on the previous intermediate output. By identifying the sub-trace that is maximally informative to the hallucinated responses, TraceDet leverages the key hallucination signals in the multi-step denoising process of D-LLMs for hallucination detection. Extensive experiments on various open source D-LLMs demonstrate that TraceDet consistently improves hallucination detection, achieving an average gain in AUROC of 15.2% compared to baselines.</li>
</ul>

<h3>Title: LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews</h3>
<ul>
<li><strong>Authors: </strong>Sumaiya Tabassum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01276">https://arxiv.org/abs/2510.01276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01276">https://arxiv.org/pdf/2510.01276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01276]] LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews(https://arxiv.org/abs/2510.01276)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Sentiment analysis is an essential part of text analysis, which is a larger field that includes determining and evaluating the author's emotional state. This method is essential since it makes it easier to comprehend consumers' feelings, viewpoints, and preferences holistically. The introduction of large language models (LLMs), such as Llama, has greatly increased the availability of cutting-edge model applications, such as sentiment analysis. However, accurate sentiment analysis is hampered by the intricacy of written language and the diversity of languages used in evaluations. The viability of using transformer-based BERT models and other LLMs for sentiment analysis from Bangladesh e commerce reviews is investigated in this paper. A subset of 4000 samples from the original dataset of Bangla and English customer reviews was utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1, DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy, precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes how parameter efficient fine-tuning methods (LoRA and PEFT) can lower computational overhead and make it appropriate for contexts with limited resources. The results show how LLMs can</li>
</ul>

<h3>Title: Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning</h3>
<ul>
<li><strong>Authors: </strong>Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, Wenzhe Jiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01278">https://arxiv.org/abs/2510.01278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01278">https://arxiv.org/pdf/2510.01278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01278]] Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning(https://arxiv.org/abs/2510.01278)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Positive-Unlabeled (PU) learning aims to train a binary classifier (positive vs. negative) where only limited positive data and abundant unlabeled data are available. While widely applicable, state-of-the-art PU learning methods substantially underperform their supervised counterparts on complex datasets, especially without auxiliary negatives or pre-estimated parameters (e.g., a 14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the challenge of learning discriminative representations under unreliable supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU learning framework that requires no auxiliary information. NcPU combines a noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns intra-class representations despite unreliable supervision, with a phantom label disambiguation (PLD) scheme that supplies conservative negative supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can iteratively benefit each other from the perspective of the Expectation-Maximization framework. Empirically, extensive experiments demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive performance; and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging datasets on post-disaster building damage mapping, highlighting its promise for real-world applications. Code: Code will be open-sourced after review.</li>
</ul>

<h3>Title: TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Chen, Jiefeng Chen, Rui Meng, Ji Yin, Na Li, Chuchu Fan, Chi Wang, Tomas Pfister, Jinsung Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01279">https://arxiv.org/abs/2510.01279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01279">https://arxiv.org/pdf/2510.01279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01279]] TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture(https://arxiv.org/abs/2510.01279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.</li>
</ul>

<h3>Title: Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing</h3>
<ul>
<li><strong>Authors: </strong>Israel Abebe Azime, Tadesse Destaw Belay, Atnafu Lambebo Tonja</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01283">https://arxiv.org/abs/2510.01283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01283">https://arxiv.org/pdf/2510.01283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01283]] Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing(https://arxiv.org/abs/2510.01283)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) powered with argentic capabilities are able to do knowledge-intensive tasks without human involvement. A prime example of this tool is Deep research with the capability to browse the web, extract information and generate multi-page reports. In this work, we introduce an evaluation sheet that can be used for assessing the capability of Deep Research tools. In addition, we selected academic survey writing as a use case task and evaluated output reports based on the evaluation sheet we introduced. Our findings show the need to have carefully crafted evaluation standards. The evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating an academic survey showed the huge gap between search engines and standalone Deep Research tools, the shortcoming in representing the targeted area.</li>
</ul>

<h3>Title: Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours</h3>
<ul>
<li><strong>Authors: </strong>Rui Melo, Rui Abreu, Corina S. Pasareanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01288">https://arxiv.org/abs/2510.01288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01288">https://arxiv.org/pdf/2510.01288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01288]] Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours(https://arxiv.org/abs/2510.01288)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>We draw inspiration from microsaccades, tiny involuntary eye movements that reveal hidden dynamics of human perception, to propose an analogous probing method for large language models (LLMs). Just as microsaccades expose subtle but informative shifts in vision, we show that lightweight position encoding perturbations elicit latent signals that indicate model misbehaviour. Our method requires no fine-tuning or task-specific supervision, yet detects failures across diverse settings including factuality, safety, toxicity, and backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate that these perturbation-based probes surface misbehaviours while remaining computationally efficient. These findings suggest that pretrained LLMs already encode the internal evidence needed to flag their own failures, and that microsaccade-inspired interventions provide a pathway for detecting and mitigating undesirable behaviours.</li>
</ul>

<h3>Title: Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Ma, Hyunsoo Noh, James Tokishi, Ryan Hatch</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01292">https://arxiv.org/abs/2510.01292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01292">https://arxiv.org/pdf/2510.01292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01292]] Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections(https://arxiv.org/abs/2510.01292)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate vehicle delay estimation is essential for evaluating the performance of signalized intersections and informing traffic management strategies. Delay reflects congestion levels and affects travel time reliability, fuel use, and emissions. Machine learning (ML) offers a scalable, cost-effective alternative; However, conventional models typically assume that training and testing data follow the same distribution, an assumption that is rarely satisfied in real-world applications. Variations in road geometry, signal timing, and driver behavior across intersections often lead to poor generalization and reduced model accuracy. To address this issue, this study introduces a domain adaptation (DA) framework for estimating vehicle delays across diverse intersections. The framework separates data into source and target domains, extracts key traffic features, and fine-tunes the model using a small, labeled subset from the target domain. A novel DA model, Gradient Boosting with Balanced Weighting (GBBW), reweights source data based on similarity to the target domain, improving adaptability. The framework is tested using data from 57 heterogeneous intersections in Pima County, Arizona. Performance is evaluated against eight state-of-the-art ML regression models and seven instance-based DA methods. Results demonstrate that the GBBW framework provides more accurate and robust delay estimates. This approach supports more reliable traffic signal optimization, congestion management, and performance-based planning. By enhancing model transferability, the framework facilitates broader deployment of machine learning techniques in real-world transportation systems.</li>
</ul>

<h3>Title: From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review</h3>
<ul>
<li><strong>Authors: </strong>Emma McMillian, Abhirup Banerjee, Alfonso Bueno-Orovio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01296">https://arxiv.org/abs/2510.01296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01296">https://arxiv.org/pdf/2510.01296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01296]] From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review(https://arxiv.org/abs/2510.01296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning-based 3-dimensional (3D) shape reconstruction from 2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly important in medical disease diagnosis, treatment planning, and computational modeling. This review surveys the methodological landscape of 3D MRI reconstruction, focusing on 4 primary approaches: point cloud, mesh-based, shape-aware, and volumetric models. For each category, we analyze the current state-of-the-art techniques, their methodological foundation, limitations, and applications across anatomical structures. We provide an extensive overview ranging from cardiac to neurological to lung imaging. We also focus on the clinical applicability of models to diseased anatomy, and the influence of their training and testing data. We examine publicly available datasets, computational demands, and evaluation metrics. Finally, we highlight the emerging research directions including multimodal integration and cross-modality frameworks. This review aims to provide researchers with a structured overview of current 3D reconstruction methodologies to identify opportunities for advancing deep learning towards more robust, generalizable, and clinically impactful solutions.</li>
</ul>

<h3>Title: LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Alessio Spagnoletti, AndrÃ©s Almansa, Marcelo Pereyra</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01339">https://arxiv.org/abs/2510.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01339">https://arxiv.org/pdf/2510.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01339]] LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration(https://arxiv.org/abs/2510.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.</li>
</ul>

<h3>Title: Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach</h3>
<ul>
<li><strong>Authors: </strong>Xiangfang Li, Yu Wang, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01342">https://arxiv.org/abs/2510.01342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01342">https://arxiv.org/pdf/2510.01342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01342]] Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach(https://arxiv.org/abs/2510.01342)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models (LLMs), ensuring their safe use becomes increasingly critical. Fine-tuning is a widely used method for adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks. However, most existing studies focus on overly simplified attack scenarios, limiting their practical relevance to real-world defense settings. To make this risk concrete, we present a three-pronged jailbreak attack and evaluate it against provider defenses under a dataset-only black-box fine-tuning interface. In this setting, the attacker can only submit fine-tuning data to the provider, while the provider may deploy defenses across stages: (1) pre-upload data filtering, (2) training-time defensive fine-tuning, and (3) post-training safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism, enabling the model to learn harmful behaviors while individual datapoints appear innocuous. Extensive experiments demonstrate the effectiveness of our approach. In real-world deployment, our method successfully jailbreaks GPT-4.1 and GPT-4o on the OpenAI platform with attack success rates above 97% for both models. Our code is available at this https URL.</li>
</ul>

<h3>Title: Image Generation Based on Image Style Extraction</h3>
<ul>
<li><strong>Authors: </strong>Shuochen Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01347">https://arxiv.org/abs/2510.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01347">https://arxiv.org/pdf/2510.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01347]] Image Generation Based on Image Style Extraction(https://arxiv.org/abs/2510.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.</li>
</ul>

<h3>Title: Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Faheemur Rahman, Wayne Burleson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR, cs.ET, cs.NE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01350">https://arxiv.org/abs/2510.01350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01350">https://arxiv.org/pdf/2510.01350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01350]] Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays(https://arxiv.org/abs/2510.01350)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Memristive crossbar arrays enable in-memory computing by performing parallel analog computations directly within memory, making them well-suited for machine learning, neural networks, and neuromorphic systems. However, despite their advantages, non-volatile memristors are vulnerable to security threats (such as adversarial extraction of stored weights when the hardware is compromised. Protecting these weights is essential since they represent valuable intellectual property resulting from lengthy and costly training processes using large, often proprietary, datasets. As a solution we propose two security mechanisms: Keyed Permutor and Watermark Protection Columns; where both safeguard critical weights and establish verifiable ownership (even in cases of data leakage). Our approach integrates efficiently with existing memristive crossbar architectures without significant design modifications. Simulations across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and a large RF dataset, show that both mechanisms offer robust protection with under 10% overhead in area, delay and power. We also present initial experiments employing the widely known MNIST dataset; further highlighting the feasibility of securing memristive in-memory computing systems with minimal performance trade-offs.</li>
</ul>

<h3>Title: WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Liu, Ruohan Xu, Xilong Wang, Yuqi Jia, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01354">https://arxiv.org/abs/2510.01354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01354">https://arxiv.org/pdf/2510.01354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01354]] WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents(https://arxiv.org/abs/2510.01354)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: this https URL.</li>
</ul>

<h3>Title: Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks</h3>
<ul>
<li><strong>Authors: </strong>Shoumik Saha, Jifan Chen, Sam Mayers, Sanjay Krishna Gouda, Zijian Wang, Varun Kumar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01359">https://arxiv.org/abs/2510.01359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01359">https://arxiv.org/pdf/2510.01359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01359]] Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks(https://arxiv.org/abs/2510.01359)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Code-capable large language model (LLM) agents are increasingly embedded into software engineering workflows where they can read, write, and execute code, raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only settings. Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. We present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three escalating workspace regimes that mirror attacker capability: empty (JAWS-0), single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability, moving beyond refusal to measure deployable harm. Using seven LLMs from five families as backends, we find that under prompt-only conditions in JAWS-0, code agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27% run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~ 100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly deployable attack code. Across models, wrapping an LLM in an agent substantially increases vulnerability -- ASR raises by 1.6x -- because initial refusals are frequently overturned during later planning/tool-use steps. Category-level analyses identify which attack classes are most vulnerable and most readily deployable, while others exhibit large execution gaps. These findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms that preserve refusal decisions throughout the agent's multi-step reasoning and tool use.</li>
</ul>

<h3>Title: RheOFormer: A generative transformer model for simulation of complex fluids and flows</h3>
<ul>
<li><strong>Authors: </strong>Maedeh Saberi, Amir Barati Farimani, Safa Jamali</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01365">https://arxiv.org/abs/2510.01365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01365">https://arxiv.org/pdf/2510.01365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01365]] RheOFormer: A generative transformer model for simulation of complex fluids and flows(https://arxiv.org/abs/2510.01365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>The ability to model mechanics of soft materials under flowing conditions is key in designing and engineering processes and materials with targeted properties. This generally requires solution of internal stress tensor, related to the deformation tensor through nonlinear and history-dependent constitutive models. Traditional numerical methods for non-Newtonian fluid dynamics often suffer from prohibitive computational demands and poor scalability to new problem instances. Developments in data-driven methods have mitigated some limitations but still require retraining across varied physical conditions. In this work, we introduce Rheological Operator Transformer (RheOFormer), a generative operator learning method leveraging self-attention to efficiently learn different spatial interactions and features of complex fluid flows. We benchmark RheOFormer across a range of different viscometric and non-viscometric flows with different types of viscoelastic and elastoviscoplastic mechanics in complex domains against ground truth solutions. Our results demonstrate that RheOFormer can accurately learn both scalar and tensorial nonlinear mechanics of different complex fluids and predict the spatio-temporal evolution of their flows, even when trained on limited datasets. Its strong generalization capabilities and computational efficiency establish RheOFormer as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across a wide range of applications.</li>
</ul>

<h3>Title: SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs</h3>
<ul>
<li><strong>Authors: </strong>Abu Bucker Siddik, Diane Oyen, Alexander Most, Michal Kucer, Ayan Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01370">https://arxiv.org/abs/2510.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01370">https://arxiv.org/pdf/2510.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01370]] SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs(https://arxiv.org/abs/2510.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.</li>
</ul>

<h3>Title: Selective Underfitting in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiwhan Song, Jaeyeon Kim, Sitan Chen, Yilun Du, Sham Kakade, Vincent Sitzmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01378">https://arxiv.org/abs/2510.01378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01378">https://arxiv.org/pdf/2510.01378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01378]] Selective Underfitting in Diffusion Models(https://arxiv.org/abs/2510.01378)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. They raise a basic yet unsolved question: which score do they actually learn? In principle, a diffusion model that matches the empirical score in the entire data space would simply reproduce the training data, failing to generate novel samples. Recent work addresses this question by arguing that diffusion models underfit the empirical score due to training-time inductive biases. In this work, we refine this perspective, introducing the notion of selective underfitting: instead of underfitting the score everywhere, better diffusion models more accurately approximate the score in certain regions of input space, while underfitting it in others. We characterize these regions and design empirical interventions to validate our perspective. Our results establish that selective underfitting is essential for understanding diffusion models, yielding new, testable insights into their generalization and generative performance.</li>
</ul>

<h3>Title: Fine-Tuning Masked Diffusion for Provable Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Seunggeun Kim, Taekyun Lee, David Z. Pan, Hyeji Kim, Sham Kakade, Sitan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01384">https://arxiv.org/abs/2510.01384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01384">https://arxiv.org/pdf/2510.01384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01384]] Fine-Tuning Masked Diffusion for Provable Self-Correction(https://arxiv.org/abs/2510.01384)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).</li>
</ul>

<h3>Title: TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies</h3>
<ul>
<li><strong>Authors: </strong>Maithili Kadam, Francis Ferraro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01391">https://arxiv.org/abs/2510.01391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01391">https://arxiv.org/pdf/2510.01391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01391]] TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies(https://arxiv.org/abs/2510.01391)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at general language tasks but often struggle with event-based questions-especially those requiring causal or temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question Answering), a prompting framework that injects causal event graphs into LLM inputs by converting structured relations into natural-language statements. TAG-EQA spans nine prompting configurations, combining three strategies (zero-shot, few-shot, chain-of-thought) with three input modalities (text-only, graph-only, text+graph), enabling a systematic analysis of when and how structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA improves accuracy by 5% on average over text-only baselines, with gains up to 12% in zero-shot settings and 18% when graph-augmented CoT prompting is effective. While performance varies by model and configuration, our findings show that causal graphs can enhance event reasoning in LLMs without fine-tuning, offering a flexible way to encode structure in prompt-based QA.</li>
</ul>

<h3>Title: E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing</h3>
<ul>
<li><strong>Authors: </strong>Davide Rusconi, Osama Yousef, Mirco Picca, Flavio Toffalini, Andrea Lanzi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01393">https://arxiv.org/abs/2510.01393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01393">https://arxiv.org/pdf/2510.01393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01393]] E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing(https://arxiv.org/abs/2510.01393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted towards improving the throughput of fuzzing campaigns in contexts where scalability is unavailable. E-FuzzEdge addresses the inefficiencies of hardware-in-the-loop fuzzing for microcontrollers by optimizing execution speed. We evaluated our system against state-of-the-art benchmarks, demonstrating significant performance improvements. A key advantage of E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing techniques that perform on device testing instead of firmware emulation. This means that the broader embedded fuzzing community can integrate E-FuzzEdge into their workflows to enhance overall testing efficiency.</li>
</ul>

<h3>Title: Optimal Stopping vs Best-of-$N$ for Inference Time Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Kalayci, Vinod Raman, Shaddin Dughmi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01394">https://arxiv.org/abs/2510.01394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01394">https://arxiv.org/pdf/2510.01394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01394]] Optimal Stopping vs Best-of-$N$ for Inference Time Optimization(https://arxiv.org/abs/2510.01394)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly "box" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35 percent fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.</li>
</ul>

<h3>Title: DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation</h3>
<ul>
<li><strong>Authors: </strong>Shubhankar Borse, Farzad Farhadzadeh, Munawar Hayat, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01399">https://arxiv.org/abs/2510.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01399">https://arxiv.org/pdf/2510.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01399]] DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation(https://arxiv.org/abs/2510.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts - duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread - surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.</li>
</ul>

<h3>Title: Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Abou Ali, Fadi Dornaika</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01439">https://arxiv.org/abs/2510.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01439">https://arxiv.org/pdf/2510.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01439]] Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons(https://arxiv.org/abs/2510.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Edge Artificial Intelligence (Edge AI) embeds intelligence directly into devices at the network edge, enabling real-time processing with improved privacy and reduced latency by processing data close to its source. This review systematically examines the evolution, current landscape, and future directions of Edge AI through a multi-dimensional taxonomy including deployment location, processing capabilities such as TinyML and federated learning, application domains, and hardware types. Following PRISMA guidelines, the analysis traces the field from early content delivery networks and fog computing to modern on-device intelligence. Core enabling technologies such as specialized hardware accelerators, optimized software, and communication protocols are explored. Challenges including resource limitations, security, model management, power consumption, and connectivity are critically assessed. Emerging opportunities in neuromorphic hardware, continual learning algorithms, edge-cloud collaboration, and trustworthiness integration are highlighted, providing a comprehensive framework for researchers and practitioners.</li>
</ul>

<h3>Title: Securing IoT Devices in Smart Cities: A Review of Proposed Solutions</h3>
<ul>
<li><strong>Authors: </strong>AndrÃ©s F. Betancur-LÃ³pez</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01445">https://arxiv.org/abs/2510.01445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01445">https://arxiv.org/pdf/2510.01445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01445]] Securing IoT Devices in Smart Cities: A Review of Proposed Solutions(https://arxiv.org/abs/2510.01445)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Privacy and security in Smart Cities remain at constant risk due to the vulnerabilities introduced by Internet of Things (IoT) devices. The limited computational resources of these devices make them especially susceptible to attacks, while their widespread adoption increases the potential impact of security breaches. This article presents a review of security proposals aimed at protecting IoT devices in Smart City environments. The review was conducted by analyzing recent literature on device-level security, with particular emphasis on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions. Findings highlight both the strengths and limitations of current approaches, as well as the need for more practical, scalable, and resource-efficient mechanisms to ensure user privacy and data protection in IoT ecosystems.</li>
</ul>

<h3>Title: SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training</h3>
<ul>
<li><strong>Authors: </strong>Dorsa Soleymani, Ali Dadsetan, Frank Rudzicz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01447">https://arxiv.org/abs/2510.01447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01447">https://arxiv.org/pdf/2510.01447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01447]] SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training(https://arxiv.org/abs/2510.01447)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, fair</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) provides strong protection for sensitive data, but often reduces model performance and fairness, especially for underrepresented groups. One major reason is gradient clipping in DP-SGD, which can disproportionately suppress learning signals for minority subpopulations. Although adaptive clipping can enhance utility, it still relies on uniform hard clipping, which may restrict fairness. To address this, we introduce SoftAdaClip, a differentially private training method that replaces hard clipping with a smooth, tanh-based transformation to preserve relative gradient magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48% compared to Adaptive-DPSGD, and these reductions in subgroup disparities are statistically significant. These findings underscore the importance of integrating smooth transformations with adaptive mechanisms to achieve fair and private model training.</li>
</ul>

<h3>Title: GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Angel Daruna, Nicholas Meegan, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01448">https://arxiv.org/abs/2510.01448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01448">https://arxiv.org/pdf/2510.01448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01448]] GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings(https://arxiv.org/abs/2510.01448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Worldwide visual geo-localization seeks to determine the geographic location of an image anywhere on Earth using only its visual content. Learned representations of geography for visual geo-localization remain an active research topic despite much progress. We formulate geo-localization as aligning the visual representation of the query image with a learned geographic representation. Our novel geographic representation explicitly models the world as a hierarchy of geographic embeddings. Additionally, we introduce an approach to efficiently fuse the appearance features of the query image with its semantic segmentation map, forming a robust visual representation. Our main experiments demonstrate improved all-time bests in 22 out of 25 metrics measured across five benchmark datasets compared to prior state-of-the-art (SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional ablation studies support the claim that these gains are primarily driven by the combination of geographic and visual representations.</li>
</ul>

<h3>Title: Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zuo, Yutong Yin, Zhichen Zeng, Ang Li, Banghua Zhu, Zhaoran Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01450">https://arxiv.org/abs/2510.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01450">https://arxiv.org/pdf/2510.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01450]] Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression(https://arxiv.org/abs/2510.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and $\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at this https URL.</li>
</ul>

<h3>Title: Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Nilay Naharas, Dang Nguyen, Nesihan Bulut, Mohammadhossein Bateni, Vahab Mirrokni, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01454">https://arxiv.org/abs/2510.01454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01454">https://arxiv.org/pdf/2510.01454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01454]] Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories(https://arxiv.org/abs/2510.01454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The project's website can be found at this https URL.</li>
</ul>

<h3>Title: SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Brett Barkley, Preston Culbertson, David Fridovich-Keil</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01456">https://arxiv.org/abs/2510.01456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01456">https://arxiv.org/pdf/2510.01456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01456]] SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion(https://arxiv.org/abs/2510.01456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is essential for reliable deployment of machine learning systems in vision, robotics, reinforcement learning, and beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion (SCOPED), a fast and general-purpose OOD detection method for diffusion models that reduces the number of forward passes on the trained model by an order of magnitude compared to prior methods, outperforming most diffusion-based baselines and closely approaching the accuracy of the strongest ones. SCOPED is computed from a single diffusion model trained once on a diverse dataset, and combines the Jacobian trace and squared norm of the model's score function into a single test statistic. Rather than thresholding on a fixed value, we estimate the in-distribution density of SCOPED scores using kernel density estimation, enabling a flexible, unsupervised test that, in the simplest case, only requires a single forward pass and one Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator. On four vision benchmarks, SCOPED achieves competitive or state-of-the-art precision-recall scores despite its low computational cost. The same method generalizes to robotic control tasks with shared state and action spaces, identifying distribution shifts across reward functions and training regimes. These results position SCOPED as a practical foundation for fast and reliable OOD detection in real-world domains, including perceptual artifacts in vision, outlier detection in autoregressive models, exploration in reinforcement learning, and dataset curation for unsupervised training.</li>
</ul>

<h3>Title: How Well Can Preference Optimization Generalize Under Noisy Feedback?</h3>
<ul>
<li><strong>Authors: </strong>Shawn Im, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01458">https://arxiv.org/abs/2510.01458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01458">https://arxiv.org/pdf/2510.01458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01458]] How Well Can Preference Optimization Generalize Under Noisy Feedback?(https://arxiv.org/abs/2510.01458)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance their capabilities, aligning these models with human preferences has become crucial. Preference optimization, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for aligning LLMs. However, most existing works assume noise-free feedback, which is unrealistic due to the inherent errors and inconsistencies in human judgments. This paper addresses the impact of noisy feedback on preference optimization, providing generalization guarantees under these conditions. In particular, we consider noise models that correspond to common real-world sources of noise, such as mislabeling and uncertainty. Unlike traditional analyses that assume convergence, our work focuses on finite-step preference optimization, offering new insights that are more aligned with practical LLM training. We describe how generalization decays with different types of noise across levels of noise rates based on the preference data distribution and number of samples. Our analysis for noisy preference learning applies to a broad family of preference optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on contemporary LLMs confirms the practical relevance of our findings, offering valuable insights for developing AI systems that align with human preferences.</li>
</ul>

<h3>Title: LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weizhe Chen, Sven Koenig, Bistra Dilkina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01459">https://arxiv.org/abs/2510.01459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01459">https://arxiv.org/pdf/2510.01459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01459]] LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning(https://arxiv.org/abs/2510.01459)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research.</li>
</ul>

<h3>Title: A-VERT: Agnostic Verification with Embedding Ranking Targets</h3>
<ul>
<li><strong>Authors: </strong>NicolÃ¡s Aguirre, Ramiro Caso, Ramiro RodrÃ­guez Colmeiro, Mauro Santelli, JoaquÃ­n Toranzo CalderÃ³n</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01469">https://arxiv.org/abs/2510.01469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01469">https://arxiv.org/pdf/2510.01469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01469]] A-VERT: Agnostic Verification with Embedding Ranking Targets(https://arxiv.org/abs/2510.01469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The automatic evaluation of Language Model (LM) responses is a critical piece in the development of benchmarks and metrics, both for model training and quality assessment of production model endpoints. The current approaches to response classification relies on methods that are too expensive (i.e. LLM-as-a-Judge) or that are far from real-world conditions (string-matching, logprob). In this paper, a structure-free evaluation method is presented. The method makes use of semantic embedding distances to match target candidates with arbitrary LM-generated text, resulting in a robust classification of the response at a relatively low compute cost (embedding models of less than $10B$ parameters). The results show a regression score of ~0.97 and an accuracy of ~96% against human annotators, tested over 3 data sets and 3 different LM architectures.</li>
</ul>

<h3>Title: PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Hengyi Zhu, Grace Li Zhang, Shaoyi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01472">https://arxiv.org/abs/2510.01472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01472">https://arxiv.org/pdf/2510.01472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01472]] PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search(https://arxiv.org/abs/2510.01472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hardware-Aware Neural Architecture Search (HW-NAS) requires joint optimization of accuracy and latency under device constraints. Traditional supernet-based methods require multiple GPU days per dataset. Large Language Model (LLM)-driven approaches avoid training a large supernet and can provide quick feedback, but we observe an exploration bias: the LLM repeatedly proposes neural network designs within limited search space and fails to discover architectures across different latency ranges in the entire search space. To address this issue, we propose PEL-NAS: a search space Partitioned, architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search that can generate neural networks with high accuracy and low latency with reduced search cost. Our proposed PEL-NAS has three key components: 1) a complexity-driven partitioning engine that divides the search space by complexity to enforce diversity and mitigate exploration bias; 2) an LLM-powered architecture prompt co-evolution operator, in which the LLM first updates a knowledge base of design heuristics based on results from the previous round, then performs a guided evolution algorithm on architectures with prompts that incorporate this knowledge base. Prompts and designs improve together across rounds which avoids random guesswork and improve efficiency; 3) a zero-cost predictor to avoid training a large number of candidates from scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve overall higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Meanwhile, the search cost drops from days to minutes compared with traditional supernet baselines.</li>
</ul>

<h3>Title: Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets</h3>
<ul>
<li><strong>Authors: </strong>Shriram Karpoora Sundara Pandian, Ali Baheri</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01479">https://arxiv.org/abs/2510.01479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01479">https://arxiv.org/pdf/2510.01479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01479]] Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets(https://arxiv.org/abs/2510.01479)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) enables policy optimization from fixed datasets, making it suitable for safety-critical applications where online exploration is infeasible. However, these datasets are often contaminated by adversarial poisoning, system errors, or low-quality samples, leading to degraded policy performance in standard behavioral cloning (BC) and offline RL methods. This paper introduces Density-Ratio Weighted Behavioral Cloning (Weighted BC), a robust imitation learning approach that uses a small, verified clean reference set to estimate trajectory-level density ratios via a binary discriminator. These ratios are clipped and used as weights in the BC objective to prioritize clean expert behavior while down-weighting or discarding corrupted data, without requiring knowledge of the contamination mechanism. We establish theoretical guarantees showing convergence to the clean expert policy with finite-sample bounds that are independent of the contamination rate. A comprehensive evaluation framework is established, which incorporates various poisoning protocols (reward, state, transition, and action) on continuous control benchmarks. Experiments demonstrate that Weighted BC maintains near-optimal performance even at high contamination ratios outperforming baselines such as traditional BC, batch-constrained Q-learning (BCQ) and behavior regularized actor-critic (BRAC).</li>
</ul>

<h3>Title: Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed</h3>
<ul>
<li><strong>Authors: </strong>Isha Gupta, Rylan Schaeffer, Joshua Kazdan, Ken Liu, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01494">https://arxiv.org/abs/2510.01494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01494">https://arxiv.org/pdf/2510.01494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01494]] Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed(https://arxiv.org/abs/2510.01494)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks \emph{can} transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus models' unique representation spaces - a critical insight for building more robust models.</li>
</ul>

<h3>Title: AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Ou, Ning Bi, Jiazhen Pan, Jiancheng Yang, Boliang Yu, Usama Zidan, Regent Lee, Vicente Grau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01498">https://arxiv.org/abs/2510.01498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01498">https://arxiv.org/pdf/2510.01498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01498]] AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging(https://arxiv.org/abs/2510.01498)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information</h3>
<ul>
<li><strong>Authors: </strong>Rui Ai, Yuqi Pan, David Simchi-Levi, Milind Tambe, Haifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01499">https://arxiv.org/abs/2510.01499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01499">https://arxiv.org/pdf/2510.01499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01499]] Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information(https://arxiv.org/abs/2510.01499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid progress of multi-agent large language model (LLM) reasoning, how to effectively aggregate answers from multiple LLMs has emerged as a fundamental challenge. Standard majority voting treats all answers equally, failing to consider latent heterogeneity and correlation across models. In this work, we design two new aggregation algorithms called Optimal Weight (OW) and Inverse Surprising Popularity (ISP), leveraging both first-order and second-order information. Our theoretical analysis shows these methods provably mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions. We empirically validate our algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all cases, our methods consistently outperform majority voting, offering both practical performance gains and conceptual insights for the design of robust multi-agent LLM pipelines.</li>
</ul>

<h3>Title: Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control</h3>
<ul>
<li><strong>Authors: </strong>Will Y. Zou, Jean Feng, Alexandre Kalimouttou, Jennifer Yuntong Zhang, Christopher W. Seymour, Romain Pirracchio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01508">https://arxiv.org/abs/2510.01508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01508">https://arxiv.org/pdf/2510.01508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01508]] Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control(https://arxiv.org/abs/2510.01508)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) applications in Clinical Decision Support Systems (CDSS) frequently encounter skepticism from practitioners regarding inoperable dosing decisions. We address this challenge with an end-to-end approach for learning optimal drug dosing and control policies for dual vasopressor administration in intensive care unit (ICU) patients with septic shock. For realistic drug dosing, we apply action space design that accommodates discrete, continuous, and directional dosing strategies in a system that combines offline conservative Q-learning with a novel recurrent modeling in a replay buffer to capture temporal dependencies in ICU time-series data. Our comparative analysis of norepinephrine dosing strategies across different action space formulations reveals that the designed action spaces improve interpretability and facilitate clinical adoption while preserving efficacy. Empirical results1 on eICU and MIMIC demonstrate that action space design profoundly influences learned behavioral policies. The proposed methods achieve improved patient outcomes of over 15% in survival improvement probability, while aligning with established clinical protocols.</li>
</ul>

<h3>Title: Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties</h3>
<ul>
<li><strong>Authors: </strong>Hossein Sholehrasa, Xuan Xu, Doina Caragea, Jim E. Riviere, Majid Jaberi-Douraki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01520">https://arxiv.org/abs/2510.01520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01520">https://arxiv.org/pdf/2510.01520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01520]] Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties(https://arxiv.org/abs/2510.01520)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The safe use of pharmaceuticals in food-producing animals is vital to protect animal welfare and human food safety. Adverse events (AEs) may signal unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of violative residues in the food chain. This study introduces a predictive framework for classifying outcomes (Death vs. Recovery) using ~1.28 million reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary Medicine. A preprocessing pipeline merged relational tables and standardized AEs through VeDDRA ontologies. Data were normalized, missing values imputed, and high-cardinality features reduced; physicochemical drug properties were integrated to capture chemical-residue links. We evaluated supervised models, including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as undersampling and oversampling, with a focus on prioritizing recall for fatal outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best, achieving precision, recall, and F1-scores of 0.95. Incorporating Average Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved minority-class detection, particularly in ExcelFormer and XGBoost. Interpretability via SHAP identified biologically plausible predictors, including lung, heart, and bronchial disorders, animal demographics, and drug physicochemical properties. These features were strongly linked to fatal outcomes. Overall, the framework shows that combining rigorous data engineering, advanced machine learning, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes. The approach supports FARAD's mission by enabling early detection of high-risk drug-event profiles, strengthening residue risk assessment, and informing regulatory and clinical decision-making.</li>
</ul>

<h3>Title: WALT: Web Agents that Learn Tools</h3>
<ul>
<li><strong>Authors: </strong>Viraj Prabhu, Yutong Dai, Matthew Fernandez, Jing Gu, Krithika Ramakrishnan, Yanqi Luo, Silvio Savarese, Caiming Xiong, Junnan Li, Zeyuan Chen, Ran Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01524">https://arxiv.org/abs/2510.01524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01524">https://arxiv.org/pdf/2510.01524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01524]] WALT: Web Agents that Learn Tools(https://arxiv.org/abs/2510.01524)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Web agents promise to automate complex browser tasks, but current methods remain brittle -- relying on step-by-step UI interactions and heavy LLM reasoning that break under dynamic layouts and long horizons. Humans, by contrast, exploit website-provided functionality through high-level operations like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools), a framework that reverse-engineers latent website functionality into reusable invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust implementations of automations already designed into websites -- spanning discovery (search, filter, sort), communication (post, comment, upvote), and content management (create, edit, delete). Tools abstract away low-level execution: instead of reasoning about how to click and type, agents simply call search(query) or create(listing). This shifts the computational burden from fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning, establishing a robust and generalizable paradigm for browser automation.</li>
</ul>

<h3>Title: On Integer Programming for the Binarized Neural Network Verification Problem</h3>
<ul>
<li><strong>Authors: </strong>Woojin Kim, James R. Luedtke</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01525">https://arxiv.org/abs/2510.01525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01525">https://arxiv.org/pdf/2510.01525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01525]] On Integer Programming for the Binarized Neural Network Verification Problem(https://arxiv.org/abs/2510.01525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Binarized neural networks (BNNs) are feedforward neural networks with binary weights and activation functions. In the context of using a BNN for classification, the verification problem seeks to determine whether a small perturbation of a given input can lead it to be misclassified by the BNN, and the robustness of the BNN can be measured by solving the verification problem over multiple inputs. The BNN verification problem can be formulated as an integer programming (IP) problem. However, the natural IP formulation is often challenging to solve due to a large integrality gap induced by big-$M$ constraints. We present two techniques to improve the IP formulation. First, we introduce a new method for obtaining a linear objective for the multi-class setting. Second, we introduce a new technique for generating valid inequalities for the IP formulation that exploits the recursive structure of BNNs. We find that our techniques enable verifying BNNs against a higher range of input perturbation than existing IP approaches within a limited time.</li>
</ul>

<h3>Title: One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Wang, Sotirios Sabanis, Miguel de Carvalho, Shay B. Cohen, Tiejun Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01526">https://arxiv.org/abs/2510.01526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01526">https://arxiv.org/pdf/2510.01526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01526]] One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning(https://arxiv.org/abs/2510.01526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Domain-specific quantitative reasoning remains a major challenge for large language models (LLMs), especially in fields requiring expert knowledge and complex question answering (QA). In this work, we propose Expert Question Decomposition (EQD), an approach designed to balance the use of domain knowledge with computational efficiency. EQD is built on a two-step fine-tuning framework and guided by a reward function that measures the effectiveness of generated sub-questions in improving QA outcomes. It requires only a few thousand training examples and a single A100 GPU for fine-tuning, with inference time comparable to zero-shot prompting. Beyond its efficiency, EQD outperforms state-of-the-art domain-tuned models and advanced prompting strategies. We evaluate EQD in the financial domain, characterized by specialized knowledge and complex quantitative reasoning, across four benchmark datasets. Our method consistently improves QA performance by 0.6% to 10.5% across different LLMs. Our analysis reveals an important insight: in domain-specific QA, a single supporting question often provides greater benefit than detailed guidance steps.</li>
</ul>

<h3>Title: Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lecheng Kong, Xiyuan Wang, Yixin Chen, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01527">https://arxiv.org/abs/2510.01527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01527">https://arxiv.org/pdf/2510.01527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01527]] Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs(https://arxiv.org/abs/2510.01527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are emerging as versatile foundation models for computational chemistry, handling bidirectional tasks like reaction prediction and retrosynthesis. However, these models often lack round-trip consistency. For instance, a state-of-the-art chemical LLM may successfully caption a molecule, yet be unable to accurately reconstruct the original structure from its own generated text. This inconsistency suggests that models are learning unidirectional memorization rather than flexible mastery. Indeed, recent work has demonstrated a strong correlation between a model's round-trip consistency and its performance on the primary tasks. This strong correlation reframes consistency into a direct target for model improvement. We therefore introduce Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model to improve its consistency by using the success of a round-trip transformation as a reward signal. We further propose an iterative variant where forward and reverse mappings alternately train each other in a self-improvement loop, a process that is highly data-efficient and notably effective with the massive amount of unlabelled data common in chemistry. Experiments demonstrate that RTRL significantly \textbf{boosts performance and consistency} over strong baselines across supervised, self-supervised, and synthetic data regimes. This work shows that round-trip consistency is not just a desirable property but a trainable objective, offering a new path toward more robust and reliable foundation models.</li>
</ul>

<h3>Title: Bypassing Prompt Guards in Production with Controlled-Release Prompting</h3>
<ul>
<li><strong>Authors: </strong>Jaiden Fairoze, Sanjam Garg, Keewoo Lee, Mingyuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01529">https://arxiv.org/abs/2510.01529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01529">https://arxiv.org/pdf/2510.01529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01529]] Bypassing Prompt Guards in Production with Controlled-Release Prompting(https://arxiv.org/abs/2510.01529)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance, ensuring AI safety and alignment is paramount. One popular approach is prompt guards, lightweight mechanisms designed to filter malicious queries while being easy to implement and update. In this work, we introduce a new attack that circumvents such prompt guards, highlighting their limitations. Our method consistently jailbreaks production models while maintaining response quality, even under the highly protected chat interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry between the prompt guard and the main LLM, encoding a jailbreak prompt that lightweight guards cannot decode but the main model can. This reveals an attack surface inherent to lightweight prompt guards in modern LLM architectures and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs. We additionally identify other critical alignment issues, such as copyrighted data extraction, training data extraction, and malicious response leakage during thinking.</li>
</ul>

<h3>Title: MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Meilong Xu, Xiaoling Hu, Shahira Abousamra, Chen Li, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01532">https://arxiv.org/abs/2510.01532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01532">https://arxiv.org/pdf/2510.01532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01532]] MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation(https://arxiv.org/abs/2510.01532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In semi-supervised segmentation, capturing meaningful semantic structures from unlabeled data is essential. This is particularly challenging in histopathology image analysis, where objects are densely distributed. To address this issue, we propose a semi-supervised segmentation framework designed to robustly identify and preserve relevant topological features. Our method leverages multiple perturbed predictions obtained through stochastic dropouts and temporal training snapshots, enforcing topological consistency across these varied outputs. This consistency mechanism helps distinguish biologically meaningful structures from transient and noisy artifacts. A key challenge in this process is to accurately match the corresponding topological features across the predictions in the absence of ground truth. To overcome this, we introduce a novel matching strategy that integrates spatial overlap with global structural alignment, minimizing discrepancies among predictions. Extensive experiments demonstrate that our approach effectively reduces topological errors, resulting in more robust and accurate segmentations essential for reliable downstream analysis. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: NVIDIA AI Aerial: AI-Native Wireless Communications</h3>
<ul>
<li><strong>Authors: </strong>Kobi Cohen-Arazi, Michael Roe, Zhen Hu, Rohan Chavan, Anna Ptasznik, Joanna Lin, Joao Morais, Joseph Boccuzzi, Tommaso Balercia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01533">https://arxiv.org/abs/2510.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01533">https://arxiv.org/pdf/2510.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01533]] NVIDIA AI Aerial: AI-Native Wireless Communications(https://arxiv.org/abs/2510.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>6G brings a paradigm shift towards AI-native wireless systems, necessitating the seamless integration of digital signal processing (DSP) and machine learning (ML) within the software stacks of cellular networks. This transformation brings the life cycle of modern networks closer to AI systems, where models and algorithms are iteratively trained, simulated, and deployed across adjacent environments. In this work, we propose a robust framework that compiles Python-based algorithms into GPU-runnable blobs. The result is a unified approach that ensures efficiency, flexibility, and the highest possible performance on NVIDIA GPUs. As an example of the capabilities of the framework, we demonstrate the efficacy of performing the channel estimation function in the PUSCH receiver through a convolutional neural network (CNN) trained in Python. This is done in a digital twin first, and subsequently in a real-time testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform, lays the foundation for scalable integration of AI/ML models into next-generation cellular systems, and is essential for realizing the vision of natively intelligent 6G networks.</li>
</ul>

<h3>Title: Towards Better Optimization For Listwise Preference in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01540">https://arxiv.org/abs/2510.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01540">https://arxiv.org/pdf/2510.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01540]] Towards Better Optimization For Listwise Preference in Diffusion Models(https://arxiv.org/abs/2510.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.</li>
</ul>

<h3>Title: Growing Visual Generative Capacity for Pre-Trained MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Wang, Jiaming Han, Ziyan Yang, Qi Zhao, Shanchuan Lin, Xiangyu Yue, Abhinav Shrivastava, Zhenheng Yang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01546">https://arxiv.org/abs/2510.01546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01546">https://arxiv.org/pdf/2510.01546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01546]] Growing Visual Generative Capacity for Pre-Trained MLLMs(https://arxiv.org/abs/2510.01546)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.</li>
</ul>

<h3>Title: Robust Classification of Oral Cancer with Limited Training Data</h3>
<ul>
<li><strong>Authors: </strong>Akshay Bhagwan Sonawane, Lena D. Swamikannan, Lakshman Tamil</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01547">https://arxiv.org/abs/2510.01547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01547">https://arxiv.org/pdf/2510.01547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01547]] Robust Classification of Oral Cancer with Limited Training Data(https://arxiv.org/abs/2510.01547)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Oral cancer ranks among the most prevalent cancers globally, with a particularly high mortality rate in regions lacking adequate healthcare access. Early diagnosis is crucial for reducing mortality; however, challenges persist due to limited oral health programs, inadequate infrastructure, and a shortage of healthcare practitioners. Conventional deep learning models, while promising, often rely on point estimates, leading to overconfidence and reduced reliability. Critically, these models require large datasets to mitigate overfitting and ensure generalizability, an unrealistic demand in settings with limited training data. To address these issues, we propose a hybrid model that combines a convolutional neural network (CNN) with Bayesian deep learning for oral cancer classification using small training sets. This approach employs variational inference to enhance reliability through uncertainty quantification. The model was trained on photographic color images captured by smartphones and evaluated on three distinct test datasets. The proposed method achieved 94% accuracy on a test dataset with a distribution similar to that of the training data, comparable to traditional CNN performance. Notably, for real-world photographic image data, despite limitations and variations differing from the training dataset, the proposed model demonstrated superior generalizability, achieving 88% accuracy on diverse datasets compared to 72.94% for traditional CNNs, even with a smaller dataset. Confidence analysis revealed that the model exhibits low uncertainty (high confidence) for correctly classified samples and high uncertainty (low confidence) for misclassified samples. These results underscore the effectiveness of Bayesian inference in data-scarce environments in enhancing early oral cancer diagnosis by improving model reliability and generalizability.</li>
</ul>

<h3>Title: MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhai, Utsav Singh, Anirudh Thatipelli, Souradip Chakraborty, Anit Kumar Sahu, Furong Huang, Amrit Singh Bedi, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01549">https://arxiv.org/abs/2510.01549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01549">https://arxiv.org/pdf/2510.01549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01549]] MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models(https://arxiv.org/abs/2510.01549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at generating images conditioned on text prompts, but the resulting images often do not satisfy user-specific criteria measured by scalar rewards such as Aesthetic Scores. This alignment typically requires fine-tuning, which is computationally demanding. Recently, inference-time alignment via noise optimization has emerged as an efficient alternative, modifying initial input noise to steer the diffusion denoising process towards generating high-reward images. However, this approach suffers from reward hacking, where the model produces images that score highly, yet deviate significantly from the original prompt. We show that noise-space regularization is insufficient and that preventing reward hacking requires an explicit image-space constraint. To this end, we propose MIRA (MItigating Reward hAcking), a training-free, inference-time alignment method. MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution so reward can increase without off-distribution drift (reward hacking). We derive a tractable approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g., Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while preserving prompt adherence; mechanism plots show reward gains with near-zero drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO, mapping preference optimization to inference time with a frozen backbone, extending MIRA to non-differentiable rewards without fine-tuning.</li>
</ul>

<h3>Title: POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment</h3>
<ul>
<li><strong>Authors: </strong>Luoxi Tang, Yuqiao Meng, Ankita Patra, Weicheng Ma, Muchao Ye, Zhaohan Xi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01552">https://arxiv.org/abs/2510.01552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01552">https://arxiv.org/pdf/2510.01552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01552]] POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment(https://arxiv.org/abs/2510.01552)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.</li>
</ul>

<h3>Title: Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01555">https://arxiv.org/pdf/2510.01555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01555]] Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization(https://arxiv.org/abs/2510.01555)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value estimation-a practice that overlooks the term's functional role as an optimization loss. To analyze this issue, we establish a unified framework that connects two seemingly distinct implementation styles: using the mathematical term $k_n$ as a detached coefficient for the policy's score function ('$k_n$ in reward') or as a direct loss function through which gradients are propagated ('$k_n$ as loss'). We show that the latter can always be analyzed via an equivalent gradient coefficient in the former, unifying the two perspectives. Through this framework, we prove that the conventional '$k_1$ in reward' (like in PPO) is the principled loss for Reverse KL (RKL) regularization. We further establish a key finding: under on-policy conditions, the '$k_2$ as loss' formulation is, in fact, gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our work, identifies both as the theoretically sound implementations of the RKL objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like in GRPO) is merely a first-order, biased approximation of the principled loss. Furthermore, we argue that common off-policy implementations of '$k_n$ as loss' methods are biased due to neglected importance sampling, and we propose a principled correction. Our findings provide a comprehensive, gradient-based rationale for choosing and correctly implementing KL regularization, paving the way for more robust and effective RLHF systems.</li>
</ul>

<h3>Title: Consistent Assistant Domains Transformer for Source-free Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Renrong Shao, Wei Zhang, Kangyang Luo, Qin Li, and Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01559">https://arxiv.org/abs/2510.01559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01559">https://arxiv.org/pdf/2510.01559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01559]] Consistent Assistant Domains Transformer for Source-free Domain Adaptation(https://arxiv.org/abs/2510.01559)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Source-free domain adaptation (SFDA) aims to address the challenge of adapting to a target domain without accessing the source domain directly. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current mainstream methods primarily focus on evaluating invariant features in the target domain that closely resemble those in the source domain, subsequently aligning the target domain with the source domain. However, these methods are susceptible to hard samples and influenced by domain bias. In this paper, we propose a Consistent Assistant Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue by constructing invariable feature representations of domain consistency. Concretely, we develop an assistant domain module for CADTrans to obtain diversified representations from the intermediate aggregated global attentions, which addresses the limitation of existing methods in adequately representing diversity. Based on assistant and target domains, invariable feature representations are obtained by multiple consistent strategies, which can be used to distinguish easy and hard samples. Finally, to align the hard samples to the corresponding easy samples, we construct a conditional multi-kernel max mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same category and those of different categories. Extensive experiments are conducted on various benchmarks such as Office-31, Office-Home, VISDA-C, and DomainNet-126, proving the significant performance improvements achieved by our proposed approaches. Code is available at this https URL.</li>
</ul>

<h3>Title: Large-Scale Bayesian Causal Discovery with Interventional Data</h3>
<ul>
<li><strong>Authors: </strong>Seong Woo Han, Daniel Duy Vo, Brielin C. Brown</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01562">https://arxiv.org/abs/2510.01562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01562">https://arxiv.org/pdf/2510.01562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01562]] Large-Scale Bayesian Causal Discovery with Interventional Data(https://arxiv.org/abs/2510.01562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inferring the causal relationships among a set of variables in the form of a directed acyclic graph (DAG) is an important but notoriously challenging problem. Recently, advancements in high-throughput genomic perturbation screens have inspired development of methods that leverage interventional data to improve model identification. However, existing methods still suffer poor performance on large-scale tasks and fail to quantify uncertainty. Here, we propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian framework for causal discovery with interventional data. Our approach models the likelihood of the matrix of total causal effects, which can be approximated by a matrix normal distribution, rather than the full data matrix. We place a spike-and-slab horseshoe prior on the edges and separately learn data-driven weights for scale-free and ErdÅs-RÃ©nyi structures from observational data, treating each edge as a latent variable to enable uncertainty-aware inference. Through extensive simulation, we show that IBCD achieves superior structure recovery compared to existing baselines. We apply IBCD to CRISPR perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior inclusion probabilities enable identification of robust graph structures.</li>
</ul>

<h3>Title: TetriServe: Efficient DiT Serving for Heterogeneous Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Runyu Lu, Shiqi He, Wenxuan Tan, Shenggui Li, Ruofan Wu, Jeff J. Ma, Ang Chen, Mosharaf Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01565">https://arxiv.org/abs/2510.01565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01565">https://arxiv.org/pdf/2510.01565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01565]] TetriServe: Efficient DiT Serving for Heterogeneous Image Generation(https://arxiv.org/abs/2510.01565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment. In this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.</li>
</ul>

<h3>Title: Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01576">https://arxiv.org/abs/2510.01576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01576">https://arxiv.org/pdf/2510.01576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01576]] Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations(https://arxiv.org/abs/2510.01576)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at this https URL .</li>
</ul>

<h3>Title: Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control</h3>
<ul>
<li><strong>Authors: </strong>Haochen You, Baojing Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01578">https://arxiv.org/abs/2510.01578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01578">https://arxiv.org/pdf/2510.01578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01578]] Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control(https://arxiv.org/abs/2510.01578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gradient clipping is widely used to stabilize deep network training, but its formulation as a hard, fixed threshold limits flexibility and ignores gradient distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive Modulation and Projection), a unified framework that generalizes clipping into smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner. This perspective recasts clipping and warmup as dual mechanisms for controlling the effective update scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics. Extensive experiments across image and language tasks demonstrate that SPAMP improves stability, convergence, and robustness over existing methods.</li>
</ul>

<h3>Title: ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Krishna Teja Chitty-Venkata, Murali Emani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01582">https://arxiv.org/abs/2510.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01582">https://arxiv.org/pdf/2510.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01582]] ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models(https://arxiv.org/abs/2510.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the development of Vision Language Models (VLMs) with explicit reasoning capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset, providing structured thinking tokens and corresponding answers. Our synthetic dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of thinking-answer sequences, creating a resource for training and evaluating multimodal reasoning models. We capture the step-by-step reasoning process of VLMs and the final descriptive answers. Our goal with this dataset is to enable the development of more robust VLMs while contributing to the broader understanding of multimodal reasoning mechanisms. The dataset and evaluation benchmarks will be publicly available to aid research in reasoning/thinking multimodal VLMs.</li>
</ul>

<h3>Title: ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haochen You, Baojing Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01585">https://arxiv.org/abs/2510.01585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01585">https://arxiv.org/pdf/2510.01585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01585]] ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning(https://arxiv.org/abs/2510.01585)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in long-context reasoning, computational efficiency, and structural generalization - largely due to rigid layer stacking, dense attention, and reliance on positional encodings. We present ReSSFormer, a Recursive Sparse Structured Transformer that integrates three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient and focused context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction. ReSSFormer replaces conventional depth stacking with recurrent inference, substitutes full attention with token- and expert-level sparsity, and models latent token topology directly from content. Across language modeling, multi-hop QA, and structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines under comparable FLOPs and parameter budgets, highlighting its scalability, efficiency, and structural flexibility.</li>
</ul>

<h3>Title: Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, Hairong Lv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01588">https://arxiv.org/abs/2510.01588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01588">https://arxiv.org/pdf/2510.01588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01588]] Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation(https://arxiv.org/abs/2510.01588)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is one of the most common neurodegenerative disorder. PD telemonitoring emerges as a novel assessment modality enabling self-administered at-home tests of Unified Parkinson's Disease Rating Scale (UPDRS) scores, enhancing accessibility for PD patients. However, three types of noise would occur during measurements: (1) patient-induced measurement inaccuracies, (2) environmental noise, and (3) data packet loss during transmission, resulting in higher prediction errors. To address these challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First, the original speech features are grouped into ordered bins, based on the continuous values of a selected feature, to construct contrastive pairs. Second, the contrastive pairs are employed to train a multilayer perceptron encoder for generating noise-robust features. Finally, these features are concatenated with the original features as the augmented features, which are then fed into the UPDRS prediction models. Notably, we further introduces a novel evaluation approach with customizable noise injection module, and extensive experiments show that NoRo can successfully enhance the noise robustness of UPDRS prediction across various downstream prediction models under different noisy environments.</li>
</ul>

<h3>Title: CLUE: Non-parametric Verification from Experience via Hidden-State Clustering</h3>
<ul>
<li><strong>Authors: </strong>Zhenwen Liang, Ruosen Li, Yujun Zhou, Linfeng Song, Dian Yu, Xinya Du, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01591">https://arxiv.org/abs/2510.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01591">https://arxiv.org/pdf/2510.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01591]] CLUE: Non-parametric Verification from Experience via Hidden-State Clustering(https://arxiv.org/abs/2510.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).</li>
</ul>

<h3>Title: Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness</h3>
<ul>
<li><strong>Authors: </strong>Youwei Bao, Shuhan Yang, Hyunsoo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01598">https://arxiv.org/abs/2510.01598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01598">https://arxiv.org/pdf/2510.01598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01598]] Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness(https://arxiv.org/abs/2510.01598)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Deterministic pseudo random number generators (PRNGs) used in generative artificial intelligence (GAI) models produce predictable patterns vulnerable to exploitation by attackers. Conventional defences against the vulnerabilities often come with significant energy and latency overhead. Here, we embed hardware-generated true random bits from spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address the challenges. A highly parallel, FPGA-assisted prototype computing system delivers megabit-per-second true random numbers, passing NIST randomness tests after in-situ operations with minimal overhead. Integrating the hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to 18.6 times compared to the low-quality random number generators (RNG) baseline. With nanosecond switching speed, high energy efficiency, and established scalability, our STT-MTJ-based system holds the potential to scale beyond 106 parallel cells, achieving gigabit-per-second throughput suitable for large language model sampling. This advancement highlights spintronic RNGs as practical security components for next-generation GAI systems.</li>
</ul>

<h3>Title: A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Neal Gregory Lawton, Alfy Samuel, Anoop Kumar, Daben Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01600">https://arxiv.org/abs/2510.01600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01600">https://arxiv.org/pdf/2510.01600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01600]] A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation(https://arxiv.org/abs/2510.01600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel, Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP 2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0 Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and compare strategies for fine-tuning Retrieval Augmented Generation (RAG) pipelines, including independent fine-tuning, joint fine-tuning, and two-phase fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular framework for question answering that is powered by two large language models (LLMs): an embedding model that retrieves context documents from a database that are relevant to a given question, and a generator model that uses the retrieved context to generate an answer to the question. Both the embedding and generator models can be fine-tuned to increase performance of a RAG pipeline on a new task, but multiple fine-tuning strategies exist with different costs and benefits. In this paper, we evaluate and compare several RAG fine-tuning strategies, including independent, joint, and two-phase fine-tuning. In our experiments, we observe that all of these strategies achieve about equal improvement in EM and F1 generation quality metrics, although they have significantly different computational costs. We conclude the optimal fine-tuning strategy to use depends on whether the training dataset includes context labels and whether a grid search over the learning rates for the embedding and generator models is required.</li>
</ul>

<h3>Title: NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Roman Jacome, Romario GualdrÃ³n-Hurtado, Leon Suarez, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01608">https://arxiv.org/abs/2510.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01608">https://arxiv.org/pdf/2510.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01608]] NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems(https://arxiv.org/abs/2510.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.</li>
</ul>

<h3>Title: Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO</h3>
<ul>
<li><strong>Authors: </strong>Yu-Cheng Chih, Ming-Tao Duan, Yong-Hao Hou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01616">https://arxiv.org/abs/2510.01616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01616">https://arxiv.org/pdf/2510.01616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01616]] Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO(https://arxiv.org/abs/2510.01616)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Small Language Models (SLMs) enable cost-effective, on-device and latency-sensitive AI applications, yet their deployment in Traditional Chinese (TC) remains hindered by token-level instability - models unpredictably emit non-TC characters or code-switch into other languages. We address this practical reliability gap by creating PureTC-1B, a three-stage stabilization pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model released by Meta) using parameter-efficient LoRA adapters. Our method combines Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning (SFT) with instruction data, and Direct Preference Optimization (DPO) using TC-adherence preferences to improve monolingual robustness without full-model retraining. On a benchmark designed to simulate real-world usage, PureTC-1B achieves a 51.3% relative reduction (micro-average) in non-TC output tokens versus the base model. On a Named Entity Translation (NET) task, PureTC-1B further reduces incorrect-language tokens by 77.2% relative to Llama-3B and 57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable even at the 1B scale. The pipeline is reproducible, adapter-only, and hardware-friendly, offering practitioners a practical recipe to enhance language stability for TC and potentially other non-English languages.</li>
</ul>

<h3>Title: AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System</h3>
<ul>
<li><strong>Authors: </strong>Hui Yi Leong, Yuheng Li, Yuqing Wu, Wenwen Ouyang, Wei Zhu, Jiechao Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01617">https://arxiv.org/abs/2510.01617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01617">https://arxiv.org/pdf/2510.01617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01617]] AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System(https://arxiv.org/abs/2510.01617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.</li>
</ul>

<h3>Title: Posterior Collapse as a Phase Transition in Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Fan Zhang, Zheng Zhang, Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01621">https://arxiv.org/abs/2510.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01621">https://arxiv.org/pdf/2510.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01621]] Posterior Collapse as a Phase Transition in Variational Autoencoders(https://arxiv.org/abs/2510.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. Our results demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.</li>
</ul>

<h3>Title: VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01623">https://arxiv.org/abs/2510.01623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01623">https://arxiv.org/pdf/2510.01623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01623]] VLA-R1: Enhancing Reasoning in Vision-Language-Action Models(https://arxiv.org/abs/2510.01623)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: this https URL. Website: this https URL.</li>
</ul>

<h3>Title: Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Kang, Michael Kuchnik, Karthik Padthe, Marin Vlastelica, Ruoxi Jia, Carole-Jean Wu, Newsha Ardalani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01624">https://arxiv.org/abs/2510.01624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01624">https://arxiv.org/pdf/2510.01624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01624]] Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead(https://arxiv.org/abs/2510.01624)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.</li>
</ul>

<h3>Title: Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Kang, Newsha Ardalani, Michael Kuchnik, Youssef Emad, Mostafa Elhoushi, Shubhabrata Sengupta, Shang-Wen Li, Ramya Raghavendra, Ruoxi Jia, Carole-Jean Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01631">https://arxiv.org/abs/2510.01631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01631">https://arxiv.org/pdf/2510.01631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01631]] Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls(https://arxiv.org/abs/2510.01631)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations. We conduct a large-scale empirical investigation (>1000 LLMs with >100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data \textit{alone} is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data \textit{alone} results in notably higher loss on many downstream domains especially at small data budgets. "Good" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on "model collapse" during large-scale single-round (n=1) model training on synthetic data--training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by "model collapse". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance.</li>
</ul>

<h3>Title: CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning</h3>
<ul>
<li><strong>Authors: </strong>Ryan Y. Lin, Siddhartha Ojha, Nicholas Bai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01634">https://arxiv.org/abs/2510.01634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01634">https://arxiv.org/pdf/2510.01634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01634]] CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning(https://arxiv.org/abs/2510.01634)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers achieve strong performance across diverse domains but implicitly assume Euclidean geometry in their attention mechanisms, limiting their effectiveness on data with non-Euclidean structure. While recent extensions to hyperbolic and spherical spaces show promise for hierarchical and cyclical patterns, respectively, they require committing to a single geometry a priori, reducing flexibility when data exhibits mixed geometric properties. We introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism. Unlike fixed-geometry approaches, CAT enables adaptive geometric specialization, routing tokens to the appropriate curvature based on their local relational structure. The routing network provides interpretable curvature preferences while each branch employs geometry-specific operations optimized for its respective manifold. On knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines with minimal overhead (5% parameter increase, comparable inference time). These results demonstrate that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across language, vision, and multimodal domains.</li>
</ul>

<h3>Title: Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Liyan Xie, Muhammad Siddeek, Mohamed Seif, Andrea J. Goldsmith, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01637">https://arxiv.org/abs/2510.01637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01637">https://arxiv.org/pdf/2510.01637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01637]] Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking(https://arxiv.org/abs/2510.01637)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, watermark</a></li>
<li><strong>Abstract: </strong>Watermarking has become a key technique for proprietary language models, enabling the distinction between AI-generated and human-written text. However, in many real-world scenarios, LLM-generated content may undergo post-generation edits, such as human revisions or even spoofing attacks, making it critical to detect and localize such modifications. In this work, we introduce a new task: detecting post-generation edits locally made to watermarked LLM outputs. To this end, we propose a combinatorial pattern-based watermarking framework, which partitions the vocabulary into disjoint subsets and embeds the watermark by enforcing a deterministic combinatorial pattern over these subsets during generation. We accompany the combinatorial watermark with a global statistic that can be used to detect the watermark. Furthermore, we design lightweight local statistics to flag and localize potential edits. We introduce two task-specific evaluation metrics, Type-I error rate and detection accuracy, and evaluate our method on open-source LLMs across a variety of editing scenarios, demonstrating strong empirical performance in edit localization.</li>
</ul>

<h3>Title: FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Liu, Zhengyan Zhou, Zihang Xu, Jiezhang Cao, Zheng Chen, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01641">https://arxiv.org/abs/2510.01641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01641">https://arxiv.org/pdf/2510.01641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01641]] FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring(https://arxiv.org/abs/2510.01641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at this https URL.</li>
</ul>

<h3>Title: Support Basis: Fast Attention Beyond Bounded Entries</h3>
<ul>
<li><strong>Authors: </strong>Maryam Aliakbarpour, Vladimir Braverman, Junze Yin, Haochen Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01643">https://arxiv.org/abs/2510.01643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01643">https://arxiv.org/pdf/2510.01643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01643]] Support Basis: Fast Attention Beyond Bounded Entries(https://arxiv.org/abs/2510.01643)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive bounded-entry assumption. Since this assumption rarely holds in practice, its applicability to modern LLMs is limited. In this paper, we introduce support-basis decomposition, a new framework for efficient attention approximation beyond bounded entries. We empirically demonstrate that the entries of the query and key matrices exhibit sub-Gaussian behavior. Our approach uses this property to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. We establish rigorous theoretical guarantees, proving a sub-quadratic runtime, and extend the method to a multi-threshold setting that eliminates all distributional assumptions. Furthermore, we provide the first theoretical justification for the empirical success of polynomial attention [Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be closely approximated by a combination of multiple polynomial attentions with sketching.</li>
</ul>

<h3>Title: NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT</h3>
<ul>
<li><strong>Authors: </strong>John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01644">https://arxiv.org/abs/2510.01644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01644">https://arxiv.org/pdf/2510.01644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01644]] NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT(https://arxiv.org/abs/2510.01644)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.</li>
</ul>

<h3>Title: Position: Privacy Is Not Just Memorization!</h3>
<ul>
<li><strong>Authors: </strong>Niloofar Mireshghallah, Tianshi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01645">https://arxiv.org/abs/2510.01645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01645">https://arxiv.org/pdf/2510.01645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01645]] Position: Privacy Is Not Just Memorization!(https://arxiv.org/abs/2510.01645)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.</li>
</ul>

<h3>Title: Source-Free Cross-Domain Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Tanzil Furqon, Mahardhika Pratama, Igor Å krjanc, Lin Liu, Habibullah Habibullah, Kutluyil Dogancay</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01649">https://arxiv.org/abs/2510.01649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01649">https://arxiv.org/pdf/2510.01649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01649]] Source-Free Cross-Domain Continual Learning(https://arxiv.org/abs/2510.01649)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Although existing cross-domain continual learning approaches successfully address many streaming tasks having domain shifts, they call for a fully labeled source domain hindering their feasibility in the privacy constrained environments. This paper goes one step ahead with the problem of source-free cross-domain continual learning where the use of source-domain samples are completely prohibited. We propose the idea of rehearsal-free frequency-aware dynamic prompt collaborations (REFEREE) to cope with the absence of labeled source-domain samples in realm of cross-domain continual learning. REFEREE is built upon a synergy between a source-pre-trained model and a large-scale vision-language model, thus overcoming the problem of sub-optimal generalizations when relying only on a source pre-trained model. The domain shift problem between the source domain and the target domain is handled by a frequency-aware prompting technique encouraging low-frequency components while suppressing high-frequency components. This strategy generates frequency-aware augmented samples, robust against noisy pseudo labels. The noisy pseudo-label problem is further addressed with the uncertainty-aware weighting strategy where the mean and covariance matrix are weighted by prediction uncertainties, thus mitigating the adverse effects of the noisy pseudo label. Besides, the issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant analysis (KLDA) where the backbone network is frozen while the classification is performed using the linear discriminant analysis approach guided by the random kernel method. Our rigorous numerical studies confirm the advantage of our approach where it beats prior arts having access to source domain samples with significant margins.</li>
</ul>

<h3>Title: The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM</h3>
<ul>
<li><strong>Authors: </strong>Kwanhee Lee, Hyeondo Jang, Dongyeop Lee, Dan Alistarh, Namhoon Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01650">https://arxiv.org/abs/2510.01650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01650">https://arxiv.org/pdf/2510.01650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01650]] The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM(https://arxiv.org/abs/2510.01650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.</li>
</ul>

<h3>Title: LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition</h3>
<ul>
<li><strong>Authors: </strong>Rixin Zhou, Peiqiang Qiu, Qian Zhang, Chuntao Li, Xi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01651">https://arxiv.org/abs/2510.01651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01651">https://arxiv.org/pdf/2510.01651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01651]] LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition(https://arxiv.org/abs/2510.01651)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial stage of early Chinese writing and provide indispensable evidence for archaeological and historical studies. However, automatic BI recognition remains difficult due to severe visual degradation, multi-domain variability across photographs, rubbings, and tracings, and an extremely long-tailed character distribution. To address these challenges, we curate a large-scale BI dataset comprising 22454 full-page images and 198598 annotated characters spanning 6658 unique categories, enabling robust cross-domain evaluation. Building on this resource, we develop a two-stage detection-recognition pipeline that first localizes inscriptions and then transcribes individual characters. To handle heterogeneous domains and rare classes, we equip the pipeline with LadderMoE, which augments a pretrained CLIP encoder with ladder-style MoE adapters, enabling dynamic expert specialization and stronger robustness. Comprehensive experiments on single-character and full-page recognition tasks demonstrate that our method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities. These results establish a strong foundation for bronze inscription recognition and downstream archaeological analysis.</li>
</ul>

<h3>Title: Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxin Feng, Jianfei Ma, Emmanuele Chersoni, Xiaojing Zhao, Xiaoyi Bao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01652">https://arxiv.org/abs/2510.01652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01652">https://arxiv.org/pdf/2510.01652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01652]] Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention(https://arxiv.org/abs/2510.01652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive Large Language Models (LLMs) demonstrate exceptional performance in language understanding and generation. However, their application in text embedding tasks has been relatively slow, along with the analysis of their semantic representation in probing tasks, due to the constraints of the unidirectional attention mechanism. This paper aims to explore whether such constraints can be overcome by enabling bidirectional attention in LLMs. We tested different variants of the Llama architecture through additional training steps, progressively enabling bidirectional attention and unsupervised/supervised contrastive learning.</li>
</ul>

<h3>Title: SoK: Measuring What Matters for Closed-Loop Security Agents</h3>
<ul>
<li><strong>Authors: </strong>Mudita Khurana, Raunak Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01654">https://arxiv.org/abs/2510.01654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01654">https://arxiv.org/pdf/2510.01654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01654]] SoK: Measuring What Matters for Closed-Loop Security Agents(https://arxiv.org/abs/2510.01654)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Cybersecurity is a relentless arms race, with AI driven offensive systems evolving faster than traditional defenses can adapt. Research and tooling remain fragmented across isolated defensive functions, creating blind spots that adversaries exploit. Autonomous agents capable of integrating, exploit confirmation, remediation, and validation into a single closed loop offer promise, but the field lacks three essentials: a framework defining the agentic capabilities of security systems across security life cycle, a principled method for evaluating closed loop agents, and a benchmark for measuring their performance in practice. We introduce CLASP: the Closed-Loop Autonomous Security Performance framework which aligns the security lifecycle (reconnaissance, exploitation, root cause analysis, patch synthesis, validation) with core agentic capabilities (planning, tool use, memory, reasoning, reflection & perception) providing a common vocabulary and rubric for assessing agentic capabilities in security tasks. By applying CLASP to 21 representative works, we map where systems demonstrate strengths, and where capability gaps persist. We then define the Closed-Loop Capability (CLC) Score, a composite metric quantifying both degree of loop closure and operational effectiveness, and outline the requirements for a closed loop benchmark. Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and measurements needed to advance both function level performance and measure closed loop security agents.</li>
</ul>

<h3>Title: Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jiashun Liu, Johan Obando-Ceron, Han Lu, Yancheng He, Weixun Wang, Wenbo Su, Bo Zheng, Pablo Samuel Castro, Aaron Courville, Ling Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01656">https://arxiv.org/abs/2510.01656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01656">https://arxiv.org/pdf/2510.01656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01656]] Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning(https://arxiv.org/abs/2510.01656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under sparse rewards and long reasoning horizons. We revisit this bottleneck from an architectural perspective and introduce Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable framework that restores the critics role while remaining efficient in large-model settings. AsyPPO employs a set of lightweight mini-critics, each trained on disjoint prompt shards. This design encourages diversity while preserving calibration, reducing value-estimation bias. Beyond robust estimation, AsyPPO leverages inter-critic uncertainty to refine the policy update: (i) masking advantages in states where critics agree and gradients add little learning signal, and (ii) filtering high-divergence states from entropy regularization, suppressing spurious exploration. After training on open-source data with only 5,000 samples, AsyPPO consistently improves learning stability and performance across multiple benchmarks over strong baselines, such as GRPO, achieving performance gains of more than six percent on Qwen3-4b-Base and about three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without additional tricks. These results highlight the importance of architectural innovations for scalable, efficient algorithms.</li>
</ul>

<h3>Title: MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization</h3>
<ul>
<li><strong>Authors: </strong>Yinhong Liu, Jianfeng He, Hang Su, Ruixue Lian, Yi Nian, Jake Vincent, Srikanth Vishnubhotla, Robinson Piramuthu, Saab Mansour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01659">https://arxiv.org/abs/2510.01659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01659">https://arxiv.org/pdf/2510.01659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01659]] MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization(https://arxiv.org/abs/2510.01659)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging applications. To support the development of effective MDS models, robust automatic evaluation methods are essential for reducing both cost and human effort. However, such methods require a strong meta-evaluation benchmark grounded in human annotations. In this work, we introduce MDSEval, the first meta-evaluation benchmark for MDS, consisting image-sharing dialogues, corresponding summaries, and human judgments across eight well-defined quality aspects. To ensure data quality and richfulness, we propose a novel filtering framework leveraging Mutually Exclusive Key Information (MEKI) across modalities. Our work is the first to identify and formalize key evaluation dimensions specific to MDS. We benchmark state-of-the-art modal evaluation methods, revealing their limitations in distinguishing summaries from advanced MLLMs and their susceptibility to various bias.</li>
</ul>

<h3>Title: Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value</h3>
<ul>
<li><strong>Authors: </strong>Wangxuan Fan, Ching Wang, Siqi Li, Nan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01663">https://arxiv.org/abs/2510.01663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01663">https://arxiv.org/pdf/2510.01663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01663]] Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value(https://arxiv.org/abs/2510.01663)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov--Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.</li>
</ul>

<h3>Title: Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</h3>
<ul>
<li><strong>Authors: </strong>Yongbo Chen, Yanhao Zhang, Shaifali Parashar, Liang Zhao, Shoudong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01665">https://arxiv.org/abs/2510.01665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01665">https://arxiv.org/pdf/2510.01665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01665]] Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale(https://arxiv.org/abs/2510.01665)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention. We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset. Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. Unlike existing methods that rely on strict assumptions, such as locally planar surfaces or locally linear deformations, and fail to recover the conformal scale, our method eliminates these constraints and accurately computes the local conformal scale. Additionally, our framework decouples constraints on depth and conformal scale, which are inseparable in other approaches, enabling more precise depth estimation. To address the sensitivity of the formulated problem, we employ a parallel separable iterative optimization strategy. Furthermore, a self-supervised learning framework, utilizing an encoder-decoder network, is incorporated to generate dense 3D point clouds with texture. Simulation and experimental results using both synthetic and real datasets demonstrate that our method surpasses existing approaches in terms of reconstruction accuracy and robustness. The code for the proposed method will be made publicly available on the project website: this https URL.</li>
</ul>

<h3>Title: UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01669">https://arxiv.org/abs/2510.01669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01669">https://arxiv.org/pdf/2510.01669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01669]] UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction(https://arxiv.org/abs/2510.01669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene this http URL, these methods rely heavily on dense observations for robustly optimizing model this http URL address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization this http URL this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored this http URL with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image this http URL experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: this https URL</li>
</ul>

<h3>Title: Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Milad Nasr, Yanick Fratantonio, Luca Invernizzi, Ange Albertini, Loua Farah, Alex Petit-Bianco, Andreas Terzis, Kurt Thomas, Elie Bursztein, Nicholas Carlini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01676">https://arxiv.org/abs/2510.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01676">https://arxiv.org/pdf/2510.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01676]] Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks(https://arxiv.org/abs/2510.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>As deep learning models become widely deployed as components within larger production systems, their individual shortcomings can create system-level vulnerabilities with real-world impact. This paper studies how adversarial attacks targeting an ML component can degrade or bypass an entire production-grade malware detection system, performing a case study analysis of Gmail's pipeline where file-type identification relies on a ML model. The malware detection pipeline in use by Gmail contains a machine learning model that routes each potential malware sample to a specialized malware classifier to improve accuracy and performance. This model, called Magika, has been open sourced. By designing adversarial examples that fool Magika, we can cause the production malware service to incorrectly route malware to an unsuitable malware detector thereby increasing our chance of evading detection. Specifically, by changing just 13 bytes of a malware sample, we can successfully evade Magika in 90% of cases and thereby allow us to send malware files over Gmail. We then turn our attention to defenses, and develop an approach to mitigate the severity of these types of attacks. For our defended production model, a highly resourced adversary requires 50 bytes to achieve just a 20% attack success rate. We implement this defense, and, thanks to a collaboration with Google engineers, it has already been deployed in production for the Gmail classifier.</li>
</ul>

<h3>Title: Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Han Wu, Yanming Sun, Yunhe Yang, Derek F. Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01677">https://arxiv.org/abs/2510.01677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01677">https://arxiv.org/pdf/2510.01677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01677]] Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis(https://arxiv.org/abs/2510.01677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal sentiment analysis (MSA) leverages information fusion from diverse modalities (e.g., text, audio, visual) to enhance sentiment prediction. However, simple fusion techniques often fail to account for variations in modality quality, such as those that are noisy, missing, or semantically conflicting. This oversight leads to suboptimal performance, especially in discerning subtle emotional nuances. To mitigate this limitation, we introduce a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion \textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion mechanism based on information entropy and modality importance. This mechanism mitigates the influence of noisy modalities and prioritizes informative cues following unimodal encoding and cross-modal interaction. Experiments on CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong baselines in accuracy, effectively discerning subtle emotions with robust performance. Visualization analysis of feature representations demonstrates that AGFN enhances generalization by learning from a broader feature distribution, achieved by reducing the correlation between feature location and prediction error, thereby decreasing reliance on specific locations and creating more robust multimodal feature representations.</li>
</ul>

<h3>Title: An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution</h3>
<ul>
<li><strong>Authors: </strong>Ke Jia, Ji Zhou, Hanxin Li, Zhigan Zhou, Haojie Chu, Xiaojie Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01678">https://arxiv.org/abs/2510.01678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01678">https://arxiv.org/pdf/2510.01678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01678]] An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution(https://arxiv.org/abs/2510.01678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In industrial inspection and component alignment tasks, template matching requires efficient estimation of a target's position and geometric state (rotation and scaling) under complex backgrounds to support precise downstream operations. Traditional methods rely on exhaustive enumeration of angles and scales, leading to low efficiency under compound transformations. Meanwhile, most deep learning-based approaches only estimate similarity scores without explicitly modeling geometric pose, making them inadequate for real-world deployment. To overcome these limitations, we propose a lightweight end-to-end framework that reformulates template matching as joint localization and geometric regression, outputting the center coordinates, rotation angle, and independent horizontal and vertical scales. A Template-Aware Dynamic Convolution Module (TDCM) dynamically injects template features at inference to guide generalizable matching. The compact network integrates depthwise separable convolutions and pixel shuffle for efficient matching. To enable geometric-annotation-free training, we introduce a rotation-shear-based augmentation strategy with structure-aware pseudo labels. A lightweight refinement module further improves angle and scale precision via local optimization. Experiments show our 3.07M model achieves high precision and 14ms inference under compound transformations. It also demonstrates strong robustness in small-template and multi-object scenarios, making it highly suitable for deployment in real-time industrial applications. The code is available at:this https URL.</li>
</ul>

<h3>Title: Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring</h3>
<ul>
<li><strong>Authors: </strong>Han-Jay Shu, Wei-Ning Chiu, Shun-Ting Chang, Meng-Ping Huang, Takeshi Tohyama, Ahram Han, Po-Chih Kuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01683">https://arxiv.org/abs/2510.01683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01683">https://arxiv.org/pdf/2510.01683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01683]] Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring(https://arxiv.org/abs/2510.01683)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Deep learning models achieve strong performance in chest radiograph (CXR) interpretation, yet fairness and reliability concerns persist. Models often show uneven accuracy across patient subgroups, leading to hidden failures not reflected in aggregate metrics. Existing error detection approaches -- based on confidence calibration or out-of-distribution (OOD) detection -- struggle with subtle within-distribution errors, while image- and representation-level consistency-based methods remain underexplored in medical imaging. We propose an augmentation-sensitivity risk scoring (ASRS) framework to identify error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm 15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO encoder. Sensitivity scores stratify samples into stability quartiles, where highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$) despite high AUROC and confidence. ASRS provides a label-free means for selective prediction and clinician review, improving fairness and safety in medical AI.</li>
</ul>

<h3>Title: How Do Language Models Compose Functions?</h3>
<ul>
<li><strong>Authors: </strong>Apoorv Khandelwal, Ellie Pavlick</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01685">https://arxiv.org/abs/2510.01685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01685">https://arxiv.org/pdf/2510.01685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01685]] How Do Language Models Compose Functions?(https://arxiv.org/abs/2510.01685)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the "compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: this https URL .</li>
</ul>

<h3>Title: Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation</h3>
<ul>
<li><strong>Authors: </strong>Seungseop Lim, Gibaeg Kim, Wooseok Han, Jean Seo, Hyunkyung Lee, Jaehyo Yoo, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01688">https://arxiv.org/abs/2510.01688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01688">https://arxiv.org/pdf/2510.01688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01688]] Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation(https://arxiv.org/abs/2510.01688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have brought significant improvements to various service domains, including chatbots and medical pre-consultation applications. In the healthcare domain, the most common approach for adapting LLMs to multi-turn dialogue generation is Supervised Fine-Tuning (SFT). However, datasets for SFT in tasks like medical pre-consultation typically exhibit a skewed turn-count distribution. Training on such data induces a novel failure mechanism we term **Format Inertia**, where models tend to generate repetitive, format-correct, but diagnostically uninformative questions in long medical dialogues. To mitigate this observed failure mechanism, we adopt a simple, data-centric method that rebalances the turn-count distribution of the training dataset. Experimental results show that our approach substantially alleviates Format Inertia in medical pre-consultation.</li>
</ul>

<h3>Title: MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiyao Liu, Jinjie Wei, Wanying Qu, Chenglong Ma, Junzhi Ning, Yunheng Li, Ying Chen, Xinzhe Luo, Pengcheng Chen, Xin Gao, Ming Hu, Huihui Xu, Xin Wang, Shujian Gao, Dingkang Yang, Zhongying Deng, Jin Ye, Lihao Liu, Junjun He, Ningsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01691">https://arxiv.org/abs/2510.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01691">https://arxiv.org/pdf/2510.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01691]] MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs(https://arxiv.org/abs/2510.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.</li>
</ul>

<h3>Title: Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Linying Xue, Dongdong Lin, Qiushi Li, Hui Tian, Hongxia Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01699">https://arxiv.org/abs/2510.01699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01699">https://arxiv.org/pdf/2510.01699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01699]] Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations(https://arxiv.org/abs/2510.01699)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, generative</a></li>
<li><strong>Abstract: </strong>With the flourishing prosperity of generative models, manipulated facial images have become increasingly accessible, raising concerns regarding privacy infringement and societal trust. In response, proactive defense strategies embed adversarial perturbations into facial images to counter deepfake manipulation. However, existing methods often face a tradeoff between imperceptibility and defense effectiveness-strong perturbations may disrupt forgeries but degrade visual fidelity. Recent studies have attempted to address this issue by introducing additional visual loss constraints, yet often overlook the underlying gradient conflicts among losses, ultimately weakening defense performance. To bridge the gap, we propose a gradient-projection-based adversarial proactive defense (GRASP) method that effectively counters facial deepfakes while minimizing perceptual degradation. GRASP is the first approach to successfully integrate both structural similarity loss and low-frequency loss to enhance perturbation imperceptibility. By analyzing gradient conflicts between defense effectiveness loss and visual quality losses, GRASP pioneers the design of the gradient-projection mechanism to mitigate these conflicts, enabling balanced optimization that preserves image fidelity without sacrificing defensive performance. Extensive experiments validate the efficacy of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense success rate against facial attribute manipulations, significantly outperforming existing approaches in visual quality.</li>
</ul>

<h3>Title: Holistic Order Prediction in Natural Scenes</h3>
<ul>
<li><strong>Authors: </strong>Pierre Musacchio, Hyunmin Lee, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01704">https://arxiv.org/abs/2510.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01704">https://arxiv.org/pdf/2510.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01704]] Holistic Order Prediction in Natural Scenes(https://arxiv.org/abs/2510.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Even in controlled settings, understanding instance-wise geometries is a challenging task for a wide range of visual models. Although specialized systems exist, modern arts rely on expensive input formats (category labels, binary segmentation masks) and inference costs (a quadratic amount of forward passes). We mitigate these limitations by proposing InstaFormer, a network capable of holistic order prediction. That is, solely given an input RGB image, InstaFormer returns the full occlusion and depth orderings for all the instances in the scene in a single forward pass. At its core, InstaFormer relies on interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information. We comprehensively benchmark and ablate our approach to highlight its effectiveness. Our code and models are open-source and available at this URL: this https URL.</li>
</ul>

<h3>Title: Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Shaan Shah, Meenakshi Khosla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01706">https://arxiv.org/abs/2510.01706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01706">https://arxiv.org/pdf/2510.01706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01706]] Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport(https://arxiv.org/abs/2510.01706)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences. We propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution. We evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.</li>
</ul>

<h3>Title: ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning</h3>
<ul>
<li><strong>Authors: </strong>Aidan Acquah, Shing Chan, Aiden Doherty</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01712">https://arxiv.org/abs/2510.01712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01712">https://arxiv.org/pdf/2510.01712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01712]] ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning(https://arxiv.org/abs/2510.01712)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The use of reliable and accurate human activity recognition (HAR) models on passively collected wrist-accelerometer data is essential in large-scale epidemiological studies that investigate the association between physical activity and health outcomes. While the use of self-supervised learning has generated considerable excitement in improving HAR, it remains unknown the extent to which these models, coupled with hidden Markov models (HMMs), would make a tangible improvement to classification performance, and the effect this may have on the predicted daily activity intensity compositions. Using 151 CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised, 18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM) smoothing to classify labels of activity intensity. The performance of this model, evaluated using 5-fold stratified group cross-validation, was then compared to a baseline random forest (RF) + HMM, established in existing literature. Differences in performance and classification outputs were compared with different subgroups of age and sex within the Capture-24 population. The ActiNet model was able to distinguish labels of activity intensity with a mean macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the performance of the RF + HMM, trained and validated on the same dataset, with mean scores of 0.77 and 0.81, respectively. These findings were consistent across subgroups of age and sex. These findings encourage the use of ActiNet for the extraction of activity intensity labels from wrist-accelerometer data in future epidemiological studies.</li>
</ul>

<h3>Title: PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Raahul Krishna Durairaju (1), K. Saruladha (2) ((1) California State University, Fullerton, (2) Puducherry Technological University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01715">https://arxiv.org/abs/2510.01715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01715">https://arxiv.org/pdf/2510.01715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01715]] PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning(https://arxiv.org/abs/2510.01715)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.</li>
</ul>

<h3>Title: Latency-aware Multimodal Federated Learning over UAV Networks</h3>
<ul>
<li><strong>Authors: </strong>Shaba Shaon, Dinh C. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01717">https://arxiv.org/abs/2510.01717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01717">https://arxiv.org/pdf/2510.01717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01717]] Latency-aware Multimodal Federated Learning over UAV Networks(https://arxiv.org/abs/2510.01717)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This paper investigates federated multimodal learning (FML) assisted by unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and providing convergence analysis. In this framework, UAVs are distributed throughout the network to collect data, participate in model training, and collaborate with a base station (BS) to build a global model. By utilizing multimodal sensing, the UAVs overcome the limitations of unimodal systems, enhancing model accuracy, generalization, and offering a more comprehensive understanding of the environment. The primary objective is to optimize FML system latency in UAV networks by jointly addressing UAV sensing scheduling, power control, trajectory planning, resource allocation, and BS resource management. To address the computational complexity of our latency minimization problem, we propose an efficient iterative optimization algorithm combining block coordinate descent and successive convex approximation techniques, which provides high-quality approximate solutions. We also present a theoretical convergence analysis for the UAV-assisted FML framework under a non-convex loss function. Numerical experiments demonstrate that our FML framework outperforms existing approaches in terms of system latency and model training performance under different data settings.</li>
</ul>

<h3>Title: Accelerating Attention with Basis Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Jialin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01718">https://arxiv.org/abs/2510.01718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01718">https://arxiv.org/pdf/2510.01718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01718]] Accelerating Attention with Basis Decomposition(https://arxiv.org/abs/2510.01718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attention is a core operation in large language models (LLMs) and vision-language models (VLMs). We present BD Attention (BDA), the first lossless algorithmic reformulation of attention. BDA is enabled by a simple matrix identity from Basis Decomposition (BD), which restructures multi-head projections into a compact form while preserving exact outputs. Unlike I/O-aware system optimizations such as FlashAttention, BDA provides a mathematically guaranteed acceleration that is architecture-agnostic. On DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with no retraining required and, on modern GPUs, achieves 32% faster key/value projections and 25% smaller weights, while increasing end-to-end perplexity (PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model performance. These results position BDA as the first theoretically exact method for lossless attention acceleration that is complementary to existing engineering-level optimizations. Our code is available at this https URL.</li>
</ul>

<h3>Title: What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?</h3>
<ul>
<li><strong>Authors: </strong>Jiwan Chung, Neel Joshi, Pratyusha Sharma, Youngjae Yu, Vibhav Vineet</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01719">https://arxiv.org/abs/2510.01719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01719">https://arxiv.org/pdf/2510.01719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01719]] What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?(https://arxiv.org/abs/2510.01719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal reasoning models have recently shown promise on challenging domains such as olympiad-level geometry, yet their evaluation remains dominated by aggregate accuracy, a single score that obscures where and how models are improving. We introduce MathLens, a benchmark designed to disentangle the subskills of multimodal reasoning while preserving the complexity of textbook-style geometry problems. The benchmark separates performance into three components: Perception: extracting information from raw inputs, Reasoning: operating on available information, and Integration: selecting relevant perceptual evidence and applying it within reasoning. To support each test, we provide annotations: visual diagrams, textual descriptions to evaluate reasoning in isolation, controlled questions that require both modalities, and probes for fine-grained perceptual skills, all derived from symbolic specifications of the problems to ensure consistency and robustness. Our analysis reveals that different training approaches have uneven effects: First, reinforcement learning chiefly strengthens perception, especially when supported by textual supervision, while textual SFT indirectly improves perception through reflective reasoning. Second, reasoning improves only in tandem with perception. Third, integration remains the weakest capacity, with residual errors concentrated there once other skills advance. Finally, robustness diverges: RL improves consistency under diagram variation, whereas multimodal SFT reduces it through overfitting. We will release all data and experimental logs.</li>
</ul>

<h3>Title: Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Mandal, Yashaswini Murthy, R. Srikant</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01721">https://arxiv.org/abs/2510.01721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01721">https://arxiv.org/pdf/2510.01721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01721]] Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation(https://arxiv.org/abs/2510.01721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Distributionally robust reinforcement learning (DRRL) focuses on designing policies that achieve good performance under model uncertainties. In particular, we are interested in maximizing the worst-case long-term discounted reward, where the data for RL comes from a nominal model while the deployed environment can deviate from the nominal model within a prescribed uncertainty set. Existing convergence guarantees for robust temporal-difference (TD) learning for policy evaluation are limited to tabular MDPs or are dependent on restrictive discount-factor assumptions when function approximation is used. We present the first robust TD learning with linear function approximation, where robustness is measured with respect to the total-variation distance and Wasserstein-l distance uncertainty set. Additionally, our algorithm is both model-free and does not require generative access to the MDP. Our algorithm combines a two-time-scale stochastic-approximation update with an outer-loop target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample complexity to obtain an $\epsilon$-accurate value estimate. Our results close a key gap between the empirical success of robust RL algorithms and the non-asymptotic guarantees enjoyed by their non-robust counterparts. The key ideas in the paper also extend in a relatively straightforward fashion to robust Q-learning with function approximation.</li>
</ul>

<h3>Title: Workplace Location Choice Model based on Deep Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Tanay Rastogi, Anders KarlstrÃ¶m</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01723">https://arxiv.org/abs/2510.01723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01723">https://arxiv.org/pdf/2510.01723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01723]] Workplace Location Choice Model based on Deep Neural Network(https://arxiv.org/abs/2510.01723)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Discrete choice models (DCMs) have long been used to analyze workplace location decisions, but they face challenges in accurately mirroring individual decision-making processes. This paper presents a deep neural network (DNN) method for modeling workplace location choices, which aims to better understand complex decision patterns and provides better results than traditional discrete choice models (DCMs). The study demonstrates that DNNs show significant potential as a robust alternative to DCMs in this domain. While both models effectively replicate the impact of job opportunities on workplace location choices, the DNN outperforms the DCM in certain aspects. However, the DCM better aligns with data when assessing the influence of individual attributes on workplace distance. Notably, DCMs excel at shorter distances, while DNNs perform comparably to both data and DCMs for longer distances. These findings underscore the importance of selecting the appropriate model based on specific application requirements in workplace location choice analysis.</li>
</ul>

<h3>Title: Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD</h3>
<ul>
<li><strong>Authors: </strong>Lea Demelius, Dominik Kowald, Simone Kopeinik, Roman Kern, Andreas TrÃ¼gler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01744">https://arxiv.org/abs/2510.01744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01744">https://arxiv.org/pdf/2510.01744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01744]] Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD(https://arxiv.org/abs/2510.01744)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, fair</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) is a prominent method for protecting information about individuals during data analysis. Training neural networks with differentially private stochastic gradient descent (DPSGD) influences the model's learning dynamics and, consequently, its output. This can affect the model's performance and fairness. While the majority of studies on the topic report a negative impact on fairness, it has recently been suggested that fairness levels comparable to non-private models can be achieved by optimizing hyperparameters for performance directly on differentially private models (rather than re-using hyperparameters from non-private models, as is common practice). In this work, we analyze the generalizability of this claim by 1) comparing the disparate impact of DPSGD on different performance metrics, and 2) analyzing it over a wide range of hyperparameter settings. We highlight that a disparate impact on one metric does not necessarily imply a disparate impact on another. Most importantly, we show that while optimizing hyperparameters directly on differentially private models does not mitigate the disparate impact of DPSGD reliably, it can still lead to improved utility-fairness trade-offs compared to re-using hyperparameters from non-private models. We stress, however, that any form of hyperparameter tuning entails additional privacy leakage, calling for careful considerations of how to balance privacy, utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a variant of DPSGD designed to mitigate the disparate impact on accuracy, and conclude that this alternative may not be a robust solution with respect to hyperparameter choice.</li>
</ul>

<h3>Title: Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Bruno Corcuera, Carlos Eiras-Franco, Brais Cancela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01758">https://arxiv.org/abs/2510.01758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01758">https://arxiv.org/pdf/2510.01758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01758]] Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks(https://arxiv.org/abs/2510.01758)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.</li>
</ul>

<h3>Title: Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX</h3>
<ul>
<li><strong>Authors: </strong>Waris Radji, Thomas Michel, Hector Piteau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01764">https://arxiv.org/abs/2510.01764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01764">https://arxiv.org/pdf/2510.01764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01764]] Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX(https://arxiv.org/abs/2510.01764)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) research requires diverse, challenging environments that are both tractable and scalable. While modern video games may offer rich dynamics, they are computationally expensive and poorly suited for large-scale experimentation due to their CPU-bound execution. We introduce Octax, a high-performance suite of classic arcade game environments implemented in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely adopted as a benchmark in RL research. Octax provides the JAX community with a long-awaited end-to-end GPU alternative to the Atari benchmark, offering image-based environments, spanning puzzle, action, and strategy genres, all executable at massive scale on modern GPUs. Our JAX-based implementation achieves orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to the original game mechanics. We demonstrate Octax's capabilities by training RL agents across multiple games, showing significant improvements in training speed and scalability compared to existing solutions. The environment's modular design enables researchers to easily extend the suite with new games or generate novel environments using large language models, making it an ideal platform for large-scale RL experimentation.</li>
</ul>

<h3>Title: Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP</h3>
<ul>
<li><strong>Authors: </strong>Aueaphum Aueawatthanaphisut</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01780">https://arxiv.org/abs/2510.01780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01780">https://arxiv.org/pdf/2510.01780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01780]] Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP(https://arxiv.org/abs/2510.01780)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Secure and interoperable integration of heterogeneous medical data remains a grand challenge in digital health. Current federated learning (FL) frameworks offer privacy-preserving model training but lack standardized mechanisms to orchestrate multi-modal data fusion across distributed and resource-constrained environments. This study introduces a novel framework that leverages the Model Context Protocol (MCP) as an interoperability layer for secure, cross-agent communication in multi-modal federated healthcare systems. The proposed architecture unifies three pillars: (i) multi-modal feature alignment for clinical imaging, electronic medical records, and wearable IoT data; (ii) secure aggregation with differential privacy to protect patient-sensitive updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile clients. By employing MCP as a schema-driven interface, the framework enables adaptive orchestration of AI agents and toolchains while ensuring compliance with privacy regulations. Experimental evaluation on benchmark datasets and pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic accuracy compared with baseline FL, a 54\% reduction in client dropout rates, and clinically acceptable privacy--utility trade-offs. These results highlight MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward equitable, next-generation federated health infrastructures.</li>
</ul>

<h3>Title: Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Pan, Jie Xu, Qiguang Chen, Junhao Dong, Libo Qin, Xinfeng Li, Haining Yu, Xiaohua Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01782">https://arxiv.org/abs/2510.01782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01782">https://arxiv.org/pdf/2510.01782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01782]] Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks(https://arxiv.org/abs/2510.01782)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) should refuse to answer questions beyond their knowledge. This capability, which we term knowledge-aware refusal, is crucial for factual reliability. However, existing metrics fail to faithfully measure this ability. On the one hand, simple refusal-based metrics are biased by refusal rates and yield inconsistent scores when models exhibit different refusal tendencies. On the other hand, existing calibration metrics are proxy-based, capturing the performance of auxiliary calibration processes rather than the model's actual refusal behavior. In this work, we propose the Refusal Index (RI), a principled metric that measures how accurately LLMs refuse questions they do not know. We define RI as Spearman's rank correlation between refusal probability and error probability. To make RI practically measurable, we design a lightweight two-pass evaluation method that efficiently estimates RI from observed refusal rates across two standard evaluation runs. Extensive experiments across 16 models and 5 datasets demonstrate that RI accurately quantifies a model's intrinsic knowledge-aware refusal capability in factual tasks. Notably, RI remains stable across different refusal rates and provides consistent model rankings independent of a model's overall accuracy and refusal rates. More importantly, RI provides insight into an important but previously overlooked aspect of LLM factuality: while LLMs achieve high accuracy on factual tasks, their refusal behavior can be unreliable and fragile. This finding highlights the need to complement traditional accuracy metrics with the Refusal Index for comprehensive factuality evaluation.</li>
</ul>

<h3>Title: Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ivan Leonidovich Litvak, Anton Kostin, Fedor Lashkin, Tatiana Maksiyan, Sergey Lagutin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01792">https://arxiv.org/abs/2510.01792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01792">https://arxiv.org/pdf/2510.01792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01792]] Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction(https://arxiv.org/abs/2510.01792)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The rapid advancement of artificial intelligence in legal natural language processing demands scalable methods for evaluating text extraction from judicial decisions. This study evaluates 16 unsupervised metrics, including novel formulations, to assess the quality of extracting seven semantic blocks from 1,000 anonymized Russian judicial decisions, validated against 7,168 expert reviews on a 1--5 Likert scale. These metrics, spanning document-based, semantic, structural, pseudo-ground truth, and legal-specific categories, operate without pre-annotated ground truth. Bootstrapped correlations, Lin's concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE = 0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC = 0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density (Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These findings highlight that unsupervised metrics, including LLM-based approaches, enable scalable screening but, with moderate correlations and low CCC values, cannot fully replace human judgment in high-stakes legal contexts. This work advances legal NLP by providing annotation-free evaluation tools, with implications for judicial analytics and ethical AI deployment.</li>
</ul>

<h3>Title: Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Adil Koeken, Alexander Ziller, Moritz Knolle, Daniel Rueckert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01793">https://arxiv.org/abs/2510.01793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01793">https://arxiv.org/pdf/2510.01793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01793]] Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation(https://arxiv.org/abs/2510.01793)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The generation of privacy-preserving synthetic datasets is a promising avenue for overcoming data scarcity in medical AI research. Post-hoc privacy filtering techniques, designed to remove samples containing personally identifiable information, have recently been proposed as a solution. However, their effectiveness remains largely unverified. This work presents a rigorous evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary to claims from the original publications, our results demonstrate that current filters exhibit limited specificity and consistency, achieving high sensitivity only for real images while failing to reliably detect near-duplicates generated from training data. These results demonstrate a critical limitation of post-hoc filtering: rather than effectively safeguarding patient privacy, these methods may provide a false sense of security while leaving unacceptable levels of patient information exposed. We conclude that substantial advances in filter design are needed before these methods can be confidently deployed in sensitive applications.</li>
</ul>

<h3>Title: Rethinking the shape convention of an MLP</h3>
<ul>
<li><strong>Authors: </strong>Meng-Hsi Chen, Yu-Ang Lee, Feng-Ting Liao, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01796">https://arxiv.org/abs/2510.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01796">https://arxiv.org/pdf/2510.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01796]] Rethinking the shape convention of an MLP(https://arxiv.org/abs/2510.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.</li>
</ul>

<h3>Title: Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Xin Liu, Rongwu Xu, Xinyi Jia, Jason Liao, Jiao Sun, Ling Huang, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01801">https://arxiv.org/abs/2510.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01801">https://arxiv.org/pdf/2510.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01801]] Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network(https://arxiv.org/abs/2510.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rise of large language models (LLMs) has enabled the generation of highly persuasive spam reviews that closely mimic human writing. These reviews pose significant challenges for existing detection systems and threaten the credibility of online platforms. In this work, we first create three realistic LLM-generated spam review datasets using three distinct LLMs, each guided by product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm the high persuasion and deceptive potential of these reviews. To address this threat, we propose FraudSquad, a hybrid detection model that integrates text embeddings from a pre-trained language model with a gated graph transformer for spam node classification. FraudSquad captures both semantic and behavioral signals without relying on manual feature engineering or massive training resources. Experiments show that FraudSquad outperforms state-of-the-art baselines by up to 44.22% in precision and 43.01% in recall on three LLM-generated datasets, while also achieving promising results on two human-written spam datasets. Furthermore, FraudSquad maintains a modest model size and requires minimal labeled training data, making it a practical solution for real-world applications. Our contributions include new synthetic datasets, a practical detection framework, and empirical evidence highlighting the urgency of adapting spam detection to the LLM era. Our code and datasets are available at: this https URL.</li>
</ul>

<h3>Title: Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction</h3>
<ul>
<li><strong>Authors: </strong>Adam Filipek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01817">https://arxiv.org/abs/2510.01817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01817">https://arxiv.org/pdf/2510.01817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01817]] Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction(https://arxiv.org/abs/2510.01817)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models</li>
</ul>

<h3>Title: Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Olivier Goudet, Quentin Suire, Adrien GoÃ«ffon, FrÃ©dÃ©ric Saubion, Sylvain Lamprier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01824">https://arxiv.org/abs/2510.01824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01824">https://arxiv.org/pdf/2510.01824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01824]] Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning(https://arxiv.org/abs/2510.01824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training - a form of information-preserving dropout - the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.</li>
</ul>

<h3>Title: Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors</h3>
<ul>
<li><strong>Authors: </strong>Dane Williamson, Yangfeng Ji, Matthew Dwyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01831">https://arxiv.org/abs/2510.01831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01831">https://arxiv.org/pdf/2510.01831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01831]] Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors(https://arxiv.org/abs/2510.01831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities but frequently fail on problems that deviate syntactically from their training distribution. We identify a systematic failure mode, syntactic blind spots, in which models misapply familiar reasoning strategies to problems that are semantically straightforward but phrased in unfamiliar ways. These errors are not due to gaps in mathematical competence, but rather reflect a brittle coupling between surface form and internal representation. To test this, we rephrase incorrectly answered questions using syntactic templates drawn from correct examples. These rephrasings, which preserve semantics while reducing structural complexity, often lead to correct answers. We quantify syntactic complexity using a metric based on Dependency Locality Theory (DLT), and show that higher DLT scores are associated with increased failure rates across multiple datasets. Our findings suggest that many reasoning errors stem from structural misalignment rather than conceptual difficulty, and that syntax-aware interventions can reveal and mitigate these inductive failures.</li>
</ul>

<h3>Title: SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shicheng Liu, Kai Sun, Lisheng Fu, Xilun Chen, Xinyuan Zhang, Zhaojiang Lin, Rulin Shao, Yue Liu, Anuj Kumar, Wen-tau Yih, Xin Luna Dong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01832">https://arxiv.org/abs/2510.01832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01832">https://arxiv.org/pdf/2510.01832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01832]] SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning(https://arxiv.org/abs/2510.01832)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Semi-structured content in HTML tables, lists, and infoboxes accounts for a substantial share of factual data on the web, yet the formatting complicates usage, and reliably extracting structured information from them remains challenging. Existing methods either lack generalization or are resource-intensive due to per-page LLM inference. In this paper, we introduce SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel reinforcement learning framework that leverages layout similarity across webpages within the same site as a reward signal. Instead of processing each page individually, SCRIBES generates reusable extraction scripts that can be applied to groups of structurally similar webpages. Our approach further improves by iteratively training on synthetic annotations from in-the-wild CommonCrawl data. Experiments show that our approach outperforms strong baselines by over 13% in script quality and boosts downstream question answering accuracy by more than 4% for GPT-4o, enabling scalable and resource-efficient web information extraction.</li>
</ul>

<h3>Title: Leveraging Prior Knowledge of Diffusion Model for Person Search</h3>
<ul>
<li><strong>Authors: </strong>Giyeol Kim, Sooyoung Yang, Jihyong Oh, Myungjoo Kang, Chanho Eom</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01841">https://arxiv.org/abs/2510.01841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01841">https://arxiv.org/pdf/2510.01841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01841]] Leveraging Prior Knowledge of Diffusion Model for Person Search(https://arxiv.org/abs/2510.01841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.</li>
</ul>

<h3>Title: Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets</h3>
<ul>
<li><strong>Authors: </strong>Yannis Belkhiter, Seshu Tirupathi, Giulio Zizzo, Sachin Sharma, John D. Kelleher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01842">https://arxiv.org/abs/2510.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01842">https://arxiv.org/pdf/2510.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01842]] Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets(https://arxiv.org/abs/2510.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The field of AutoML has made remarkable progress in post-hoc model selection, with libraries capable of automatically identifying the most performing models for a given dataset. Nevertheless, these methods often rely on exhaustive hyperparameter searches, where methods automatically train and test different types of models on the target dataset. Contrastingly, pre-hoc prediction emerges as a promising alternative, capable of bypassing exhaustive search through intelligent pre-selection of models. Despite its potential, pre-hoc prediction remains under-explored in the literature. This paper explores the intersection of AutoML and pre-hoc model selection by leveraging traditional models and Large Language Model (LLM) agents to reduce the search space of AutoML libraries. By relying on dataset descriptions and statistical information, we reduce the AutoML search space. Our methodology is applied to the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark containing 175 tabular classification datasets available on OpenML. The proposed approach offers a shift in AutoML workflows, significantly reducing computational overhead, while still selecting the best model for the given dataset.</li>
</ul>

<h3>Title: Compositional meta-learning through probabilistic task inference</h3>
<ul>
<li><strong>Authors: </strong>Jacob J. W. Bakermans, Pablo Tano, Reidar Riveland, Charles Findling, Alexandre Pouget</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01858">https://arxiv.org/abs/2510.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01858">https://arxiv.org/pdf/2510.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01858]] Compositional meta-learning through probabilistic task inference(https://arxiv.org/abs/2510.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To solve a new task from minimal experience, it is essential to effectively reuse knowledge from previous tasks, a problem known as meta-learning. Compositional solutions, where common elements of computation are flexibly recombined into new configurations, are particularly well-suited for meta-learning. Here, we propose a compositional meta-learning model that explicitly represents tasks as structured combinations of reusable computations. We achieve this by learning a generative model that captures the underlying components and their statistics shared across a family of tasks. This approach transforms learning a new task into a probabilistic inference problem, which allows for finding solutions without parameter updates through highly constrained hypothesis testing. Our model successfully recovers ground truth components and statistics in rule learning and motor learning tasks. We then demonstrate its ability to quickly infer new solutions from just single examples. Together, our framework joins the expressivity of neural networks with the data-efficiency of probabilistic inference to achieve rapid compositional meta-learning.</li>
</ul>

<h3>Title: Randomized Gradient Subspaces for Efficient Large Language Model Training</h3>
<ul>
<li><strong>Authors: </strong>Sahar Rajabi, Nayeema Nonta, Samanvay Vajpayee, Sirisha Rambhatla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01878">https://arxiv.org/abs/2510.01878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01878">https://arxiv.org/pdf/2510.01878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01878]] Randomized Gradient Subspaces for Efficient Large Language Model Training(https://arxiv.org/abs/2510.01878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.</li>
</ul>

<h3>Title: REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration</h3>
<ul>
<li><strong>Authors: </strong>Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, Xuming Ran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01879">https://arxiv.org/abs/2510.01879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01879">https://arxiv.org/pdf/2510.01879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01879]] REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration(https://arxiv.org/abs/2510.01879)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.</li>
</ul>

<h3>Title: Multi-marginal temporal SchrÃ¶dinger Bridge Matching for video generation from unpaired data</h3>
<ul>
<li><strong>Authors: </strong>Thomas Gravier, Thomas Boyer, Auguste Genovesio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01894">https://arxiv.org/abs/2510.01894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01894">https://arxiv.org/pdf/2510.01894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01894]] Multi-marginal temporal SchrÃ¶dinger Bridge Matching for video generation from unpaired data(https://arxiv.org/abs/2510.01894)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose \textit{\textbf{Multi-Marginal temporal SchrÃ¶dinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video generation from unpaired data}, extending the theoretical guarantees and empirical efficiency of Diffusion SchrÃ¶dinger Bridge Matching (arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real world datasets such as transcriptomic trajectory inference in 100 dimensions, and for the first time recovers couplings and dynamics in very high dimensional image settings. Our work establishes multi-marginal SchrÃ¶dinger bridges as a practical and principled approach for recovering hidden dynamics from static data.</li>
</ul>

<h3>Title: Multimodal Foundation Models for Early Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Talha Mohsin, Ismail Abdulrashid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01899">https://arxiv.org/abs/2510.01899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01899">https://arxiv.org/pdf/2510.01899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01899]] Multimodal Foundation Models for Early Disease Detection(https://arxiv.org/abs/2510.01899)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Healthcare generates diverse streams of data, including electronic health records (EHR), medical imaging, genetics, and ongoing monitoring from wearable devices. Traditional diagnostic models frequently analyze these sources in isolation, which constrains their capacity to identify cross-modal correlations essential for early disease diagnosis. Our research presents a multimodal foundation model that consolidates diverse patient data through an attention-based transformer framework. At first, dedicated encoders put each modality into a shared latent space. Then, they combine them using multi-head attention and residual normalization. The architecture is made for pretraining on many tasks, which makes it easy to adapt to new diseases and datasets with little extra work. We provide an experimental strategy that uses benchmark datasets in oncology, cardiology, and neurology, with the goal of testing early detection tasks. The framework includes data governance and model management tools in addition to technological performance to improve transparency, reliability, and clinical interpretability. The suggested method works toward a single foundation model for precision diagnostics, which could improve the accuracy of predictions and help doctors make decisions.</li>
</ul>

<h3>Title: A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine</h3>
<ul>
<li><strong>Authors: </strong>Mayur Kishor Shende, Ole-Christoffer Granmo, Runar Helin, Vladimir I. Zadorozhny, Rishad Shafik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01906">https://arxiv.org/abs/2510.01906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01906">https://arxiv.org/pdf/2510.01906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01906]] A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine(https://arxiv.org/abs/2510.01906)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The Tsetlin Machine (TM) is a novel machine learning paradigm that employs finite-state automata for learning and utilizes propositional logic to represent patterns. Due to its simplistic approach, TMs are inherently more interpretable than learning algorithms based on Neural Networks. The Convolutional TM has shown comparable performance on various datasets such as MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the applicability of the TM architecture for large-scale multi-channel (RGB) image classification. We propose a methodology to generate both local interpretations and global class representations. The local interpretations can be used to explain the model predictions while the global class representations aggregate important patterns for each class. These interpretations summarize the knowledge captured by the convolutional clauses, which can be visualized as images. We evaluate our methods on MNIST and CelebA datasets, using models that achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to 88.07\% for ResNet50) respectively. We show that the TM performs competitively to this deep learning model while maintaining its interpretability, even in large-scale complex training environments. This contributes to a better understanding of TM clauses and provides insights into how these models can be applied to more complex and diverse datasets.</li>
</ul>

<h3>Title: Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyan Wang, Zheng Gao, Arogya Kharel, In-Young Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01910">https://arxiv.org/abs/2510.01910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01910">https://arxiv.org/pdf/2510.01910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01910]] Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement(https://arxiv.org/abs/2510.01910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are widely adopted in Web-related applications, serving as a core technique for learning from graph-structured data, such as text-attributed graphs. Yet in real-world scenarios, such graphs exhibit deficiencies that substantially undermine GNN performance. While prior GNN-based augmentation studies have explored robustness against individual imperfections, a systematic understanding of how graph-native and Large Language Models (LLMs) enhanced methods behave under compound deficiencies is still missing. Specifically, there has been no comprehensive investigation comparing conventional approaches and recent LLM-on-graph frameworks, leaving their merits unclear. To fill this gap, we conduct the first empirical study that benchmarks these two lines of methods across diverse graph deficiencies, revealing overlooked vulnerabilities and challenging the assumption that LLM augmentation is consistently superior. Building on empirical findings, we propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is the first iterative paradigm that leverages Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations by supplying class-consistent, diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning. It transforms LLM augmentation for graphs from static signal injection into dynamic refinement. Extensive experiments demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement.</li>
</ul>

<h3>Title: Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yi Ai, Yuanhao Cai, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01912">https://arxiv.org/abs/2510.01912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01912">https://arxiv.org/pdf/2510.01912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01912]] Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction(https://arxiv.org/abs/2510.01912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) provides rich spatial-spectral information but remains costly to acquire due to hardware limitations and the difficulty of reconstructing three-dimensional data from compressed measurements. Although compressive sensing systems such as CASSI improve efficiency, accurate reconstruction is still challenged by severe degradation and loss of fine spectral details. We propose the Flow-Matching-guided Unfolding network (FMU), which, to our knowledge, is the first to integrate flow matching into HSI reconstruction by embedding its generative prior within a deep unfolding framework. To further strengthen the learned dynamics, we introduce a mean velocity loss that enforces global consistency of the flow, leading to a more robust and accurate reconstruction. This hybrid design leverages the interpretability of optimization-based methods and the generative capacity of flow matching. Extensive experiments on both simulated and real datasets show that FMU significantly outperforms existing approaches in reconstruction quality. Code and models will be available at this https URL.</li>
</ul>

<h3>Title: Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Liu, Hao Xu, Xuhong Chen, Wei Chen, Yee Whye Teh, Ning Miao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01925">https://arxiv.org/abs/2510.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01925">https://arxiv.org/pdf/2510.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01925]] Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey(https://arxiv.org/abs/2510.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we address critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning.</li>
</ul>

<h3>Title: Inverse Language Modeling towards Robust and Grounded LLMs</h3>
<ul>
<li><strong>Authors: </strong>Davide Gabrielli, Simone Sestito, Iacopo Masi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01929">https://arxiv.org/abs/2510.01929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01929">https://arxiv.org/pdf/2510.01929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01929]] Inverse Language Modeling towards Robust and Grounded LLMs(https://arxiv.org/abs/2510.01929)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The current landscape of defensive mechanisms for LLMs is fragmented and underdeveloped, unlike prior work on classifiers. To further promote adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a unified framework that simultaneously 1) improves the robustness of LLMs to input perturbations, and, at the same time, 2) enables native grounding by inverting model outputs to identify potentially toxic or unsafe input triggers. ILM transforms LLMs from static generators into analyzable and robust systems, potentially helping RED teaming. ILM can lay the foundation for next-generation LLMs that are not only robust and grounded but also fundamentally more controllable and trustworthy. The code is publicly available at this http URL.</li>
</ul>

<h3>Title: Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Qi He, Cheng Qian, Xiusi Chen, Bingxiang He, Yi R. (May)Fung, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01932">https://arxiv.org/abs/2510.01932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01932">https://arxiv.org/pdf/2510.01932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01932]] Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning(https://arxiv.org/abs/2510.01932)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Claim verification with large language models (LLMs) has recently attracted considerable attention, owing to their superior reasoning capabilities and transparent verification pathways compared to traditional answer-only judgments. Online claim verification requires iterative evidence retrieval and reasoning, yet existing approaches mainly rely on prompt engineering or predesigned reasoning workflows without offering a unified training paradigm to improve necessary skills. Therefore, we introduce Veri-R1, an online reinforcement learning (RL) framework that enables an LLM to interact with a search engine and to receive reward signals that explicitly shape its planning, retrieval, and reasoning behaviors. The dynamic interaction between models and retrieval systems more accurately reflects real-world verification scenarios and fosters comprehensive verification skills. Empirical results show that Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often surpassing larger-scale counterparts. Ablation studies further reveal the impact of reward components and the link between output logits and label accuracy. Our results highlight the effectiveness of online RL for precise and faithful claim verification and provide a foundation for future research. We release our code to support community progress in LLM empowered claim verification.</li>
</ul>

<h3>Title: ClustViT: Clustering-based Token Merging for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fabio Montello, Ronja GÃ¼ldenring, Lazaros Nalpantidis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01948">https://arxiv.org/abs/2510.01948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01948">https://arxiv.org/pdf/2510.01948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01948]] ClustViT: Clustering-based Token Merging for Semantic Segmentation(https://arxiv.org/abs/2510.01948)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers can achieve high accuracy and strong generalization across various contexts, but their practical applicability on real-world robotic systems is limited due to their quadratic attention complexity. Recent works have focused on dynamically merging tokens according to the image complexity. Token merging works well for classification but is less suited to dense prediction. We propose ClustViT, where we expand upon the Vision Transformer (ViT) backbone and address semantic segmentation. Within our architecture, a trainable Cluster module merges similar tokens along the network guided by pseudo-clusters from segmentation masks. Subsequently, a Regenerator module restores fine details for downstream heads. Our approach achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different datasets, with comparable segmentation accuracy. Our code and models will be made publicly available.</li>
</ul>

<h3>Title: Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Chen Li, Nancy F. Chen, Shuicheng Yan, Xulei Yang, Xun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01954">https://arxiv.org/abs/2510.01954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01954">https://arxiv.org/pdf/2510.01954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01954]] Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs(https://arxiv.org/abs/2510.01954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at this https URL.</li>
</ul>

<h3>Title: ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs</h3>
<ul>
<li><strong>Authors: </strong>Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01967">https://arxiv.org/abs/2510.01967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01967">https://arxiv.org/pdf/2510.01967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01967]] ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs(https://arxiv.org/abs/2510.01967)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.</li>
</ul>

<h3>Title: Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions</h3>
<ul>
<li><strong>Authors: </strong>Camilo AndrÃ©s GarcÃ­a Trillos, NicolÃ¡s GarcÃ­a Trillos</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01969">https://arxiv.org/abs/2510.01969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01969">https://arxiv.org/pdf/2510.01969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01969]] Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions(https://arxiv.org/abs/2510.01969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>We consider adversarially robust classification in a multiclass setting under arbitrary loss functions and derive dual and barycentric reformulations of the corresponding learner-agnostic robust risk minimization problem. We provide explicit characterizations for important cases such as the cross-entropy loss, loss functions with a power form, and the quadratic loss, extending in this way available results for the 0-1 loss. These reformulations enable efficient computation of sharp lower bounds for adversarial risks and facilitate the design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers interesting connections between adversarial robustness, $\alpha$-fair packing problems, and generalized barycenter problems for arbitrary positive measures where Kullback-Leibler and Tsallis entropies are used as penalties. Our theoretical results are accompanied with illustrative numerical experiments where we obtain tighter lower bounds for adversarial risks with the cross-entropy loss function.</li>
</ul>

<h3>Title: Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Yao, Yuhan Shi, Lu Chen, Ziquan Fang, Yunjun Gao, Leong Hou U, Yushuai Li, Tianyi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01970">https://arxiv.org/abs/2510.01970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01970">https://arxiv.org/pdf/2510.01970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01970]] Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2510.01970)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) anomaly detection identifies abnormal patterns where each timestamp contains multiple variables. Existing MTS anomaly detection methods fall into three categories: reconstruction-based, prediction-based, and classifier-based methods. However, these methods face two key challenges: (1) Unsupervised learning methods, such as reconstruction-based and prediction-based methods, rely on error thresholds, which can lead to inaccuracies; (2) Semi-supervised methods mainly model normal data and often underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised learning methods, such as classifier-based approaches, often fail to capture local relationships, incur high computational costs, and are constrained by the scarcity of labeled data. To address these limitations, we propose Moon, a supervised modality conversion-based multivariate time series anomaly detection framework. Moon enhances the efficiency and accuracy of anomaly detection while providing detailed anomaly analysis reports. First, Moon introduces a novel multivariate Markov Transition Field (MV-MTF) technique to convert numeric time series data into image representations, capturing relationships across variables and timestamps. Since numeric data retains unique patterns that cannot be fully captured by image conversion alone, Moon employs a Multimodal-CNN to integrate numeric and image data through a feature fusion model with parameter sharing, enhancing training efficiency. Finally, a SHAP-based anomaly explainer identifies key variables contributing to anomalies, improving interpretability. Extensive experiments on six real-world MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation performance.</li>
</ul>

<h3>Title: $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01982">https://arxiv.org/abs/2510.01982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01982">https://arxiv.org/pdf/2510.01982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01982]] $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models(https://arxiv.org/abs/2510.01982)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.</li>
</ul>

<h3>Title: Private Federated Multiclass Post-hoc Calibration</h3>
<ul>
<li><strong>Authors: </strong>Samuel Maddock, Graham Cormode, Carsten Maple</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01987">https://arxiv.org/abs/2510.01987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01987">https://arxiv.org/pdf/2510.01987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01987]] Private Federated Multiclass Post-hoc Calibration(https://arxiv.org/abs/2510.01987)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Calibrating machine learning models so that predicted probabilities better reflect the true outcome frequencies is crucial for reliable decision-making across many applications. In Federated Learning (FL), the goal is to train a global model on data which is distributed across multiple clients and cannot be centralized due to privacy concerns. FL is applied in key areas such as healthcare and finance where calibration is strongly required, yet federated private calibration has been largely overlooked. This work introduces the integration of post-hoc model calibration techniques within FL. Specifically, we transfer traditional centralized calibration methods such as histogram binning and temperature scaling into federated environments and define new methods to operate them under strong client heterogeneity. We study (1) a federated setting and (2) a user-level Differential Privacy (DP) setting and demonstrate how both federation and DP impacts calibration accuracy. We propose strategies to mitigate degradation commonly observed under heterogeneity and our findings highlight that our federated temperature scaling works best for DP-FL whereas our weighted binning approach is best when DP is not required.</li>
</ul>

<h3>Title: PepCompass: Navigating peptide embedding spaces using Riemannian Geometry</h3>
<ul>
<li><strong>Authors: </strong>Marcin MoÅ¼ejko (1), Adam Bielecki (1), Jurand PrÄdzyÅski (1), Marcin Traskowski (1), Antoni Janowski (1), Karol Jurasz (1), MichaÅ Kucharczyk (1), Hyun-Su Lee (2), Marcelo Der Torossian Torres (2), Cesar de la Fuente-Nunez (2), Paulina Szymczak (3), MichaÅ Kmicikiewicz (3), Ewa Szczurek (1 and 3) ((1) University of Warsaw, (2) University of Pennsylvania, (3) Hemholtz Center Munich)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01988">https://arxiv.org/abs/2510.01988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01988">https://arxiv.org/pdf/2510.01988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01988]] PepCompass: Navigating peptide embedding spaces using Riemannian Geometry(https://arxiv.org/abs/2510.01988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent "maps" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</li>
</ul>

<h3>Title: Exploring Database Normalization Effects on SQL Generation</h3>
<ul>
<li><strong>Authors: </strong>Ryosuke Kohita</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01989">https://arxiv.org/abs/2510.01989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01989">https://arxiv.org/pdf/2510.01989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01989]] Exploring Database Normalization Effects on SQL Generation(https://arxiv.org/abs/2510.01989)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Schema design, particularly normalization, is a critical yet often overlooked factor in natural language to SQL (NL2SQL) systems. Most prior research evaluates models on fixed schemas, overlooking the influence of design on performance. We present the first systematic study of schema normalization's impact, evaluating eight leading large language models on synthetic and real-world datasets with varied normalization levels. We construct controlled synthetic datasets with formal normalization (1NF-3NF) and real academic paper datasets with practical schemes. Our results show that denormalized schemas offer high accuracy on simple retrieval queries, even with cost-effective models in zero-shot settings. In contrast, normalized schemas (2NF/3NF) introduce challenges such as errors in base table selection and join type prediction; however, these issues are substantially mitigated by providing few-shot examples. For aggregation queries, normalized schemas yielded better performance, mainly due to their robustness against the data duplication and NULL value issues that cause errors in denormalized schemas. These findings suggest that the optimal schema design for NL2SQL applications depends on the types of queries to be supported. Our study demonstrates the importance of considering schema design when developing NL2SQL interfaces and integrating adaptive schema selection for real-world scenarios.</li>
</ul>

<h3>Title: LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target</h3>
<ul>
<li><strong>Authors: </strong>Md Arid Hasan, Firoj Alam, Md Fahad Hossain, Usman Naseem, Syed Ishtiaque Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01995">https://arxiv.org/abs/2510.01995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01995">https://arxiv.org/pdf/2510.01995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01995]] LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target(https://arxiv.org/abs/2510.01995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Online social media platforms are central to everyday communication and information seeking. While these platforms serve positive purposes, they also provide fertile ground for the spread of hate speech, offensive language, and bullying content targeting individuals, organizations, and communities. Such content undermines safety, participation, and equity online. Reliable detection systems are therefore needed, especially for low-resource languages where moderation tools are limited. In Bangla, prior work has contributed resources and models, but most are single-task (e.g., binary hate/offense) with limited coverage of multi-facet signals (type, severity, target). We address these gaps by introducing the first multi-task Bangla hate-speech dataset, BanglaMultiHate, one of the largest manually annotated corpus to date. Building on this resource, we conduct a comprehensive, controlled comparison spanning classical baselines, monolingual pretrained models, and LLMs under zero-shot prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs are competitive with BanglaBERT, culturally and linguistically grounded pretraining remains critical for robust performance. Together, our dataset and findings establish a stronger benchmark for developing culturally aligned moderation tools in low-resource contexts. For reproducibility, we will release the dataset and all related scripts.</li>
</ul>

<h3>Title: Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework</h3>
<ul>
<li><strong>Authors: </strong>Nanaka Hosokawa, Ryo Takahashi, Tomoya Kitano, Yukihiro Iida, Chisako Muramatsu, Tatsuro Hayashi, Yuta Seino, Xiangrong Zhou, Takeshi Hara, Akitoshi Katsumata, Hiroshi Fujita</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02001">https://arxiv.org/abs/2510.02001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02001">https://arxiv.org/pdf/2510.02001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02001]] Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework(https://arxiv.org/abs/2510.02001)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs. To improve accuracy, we constructed a Self-correction Loop with Structured Output (SLSO) framework and verified its effectiveness. A 10-step process was implemented for 22 cases of jaw cysts, including image input and analysis, structured data generation, tooth number extraction and consistency checking, iterative regeneration when inconsistencies were detected, and finding generation with subsequent restructuring and consistency verification. A comparative experiment was conducted using the conventional Chain-of-Thought (CoT) method across seven evaluation items: transparency, internal structure, borders, root resorption, tooth movement, relationships with other structures, and tooth number. The results showed that the proposed SLSO framework improved output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates for tooth number, tooth movement, and root resorption, respectively. In the successful cases, a consistently structured output was achieved after up to five regenerations. Although statistical significance was not reached because of the small size of the dataset, the overall SLSO framework enforced negative finding descriptions, suppressed hallucinations, and improved tooth number identification accuracy. However, the accurate identification of extensive lesions spanning multiple teeth is limited. Nevertheless, further refinement is required to enhance overall performance and move toward a practical finding generation system.</li>
</ul>

<h3>Title: FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Aida Tayebi, Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02017">https://arxiv.org/abs/2510.02017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02017">https://arxiv.org/pdf/2510.02017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02017]] FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data(https://arxiv.org/abs/2510.02017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>As AI systems become more embedded in everyday life, the development of fair and unbiased models becomes more critical. Considering the social impact of AI systems is not merely a technical challenge but a moral imperative. As evidenced in numerous research studies, learning fair and robust representations has proven to be a powerful approach to effectively debiasing algorithms and improving fairness while maintaining essential information for prediction tasks. Representation learning frameworks, particularly those that utilize self-supervised and contrastive learning, have demonstrated superior robustness and generalizability across various domains. Despite the growing interest in applying these approaches to tabular data, the issue of fairness in these learned representations remains underexplored. In this study, we introduce a contrastive learning framework specifically designed to address bias and learn fair representations in tabular datasets. By strategically selecting positive pair samples and employing supervised and self-supervised contrastive learning, we significantly reduce bias compared to existing state-of-the-art contrastive learning models for tabular data. Our results demonstrate the efficacy of our approach in mitigating bias with minimum trade-off in accuracy and leveraging the learned fair representations in various downstream tasks.</li>
</ul>

<h3>Title: Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Donghoon Jung, Jiwoo Choi, Songeun Chae, Seohyon Jung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02025">https://arxiv.org/abs/2510.02025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02025">https://arxiv.org/pdf/2510.02025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02025]] Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models(https://arxiv.org/abs/2510.02025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.</li>
</ul>

<h3>Title: Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers</h3>
<ul>
<li><strong>Authors: </strong>Sahil Bhandary Karnoor, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02043">https://arxiv.org/abs/2510.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02043">https://arxiv.org/pdf/2510.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02043]] Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers(https://arxiv.org/abs/2510.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.</li>
</ul>

<h3>Title: Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Wiriyapong, Oktay KarakuÅ, Kirill Sidorov</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02056">https://arxiv.org/abs/2510.02056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02056">https://arxiv.org/pdf/2510.02056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02056]] Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference(https://arxiv.org/abs/2510.02056)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Normalising-flow variational inference (VI) can approximate complex posteriors, yet single-flow models often behave inconsistently across qualitatively different distributions. We propose Adaptive Mixture Flow Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows (MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of individual flows, and (ii) adaptive global weight estimation via likelihood-driven updates, without per-sample gating or architectural changes. Evaluated on six canonical posterior families of banana, X-shape, two-moons, rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower negative log-likelihood than each single-flow baseline and delivers stable gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD), indicating improved robustness across shapes and modalities. The procedure is efficient and architecture-agnostic, incurring minimal overhead relative to standard flow training, and demonstrates that adaptive mixtures of diverse flows provide a reliable route to robust VI across diverse posterior families whilst preserving each expert's inductive bias.</li>
</ul>

<h3>Title: Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference</h3>
<ul>
<li><strong>Authors: </strong>Jens Behrmann, Maria R. Cervera, Antoine Wehenkel, Andrew C. Miller, Albert Cerussi, Pranay Jain, Vivek Venugopal, Shijie Yan, Guillermo Sapiro, Luca Pegolotti, JÃ¶rn-Henrik Jacobsen</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02073">https://arxiv.org/abs/2510.02073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02073">https://arxiv.org/pdf/2510.02073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02073]] Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference(https://arxiv.org/abs/2510.02073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Smart wearables enable continuous tracking of established biomarkers such as heart rate, heart rate variability, and blood oxygen saturation via photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer physiological information, as recent deep learning (DL) studies demonstrate. However, DL models often rely on features with unclear physiological meaning, creating a tension between predictive power, clinical interpretability, and sensor design. We address this gap by introducing PPGen, a biophysical model that relates PPG signals to interpretable physiological and optical parameters. Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast, robust, and scalable estimation of relevant physiological parameters from PPG signals while correcting for model misspecification. In extensive in-silico experiments, we show that HAI can accurately infer physiological parameters under diverse noise and sensor conditions. Our results illustrate a path toward PPG models that retain the fidelity needed for DL-based features while supporting clinical interpretation and informed hardware design.</li>
</ul>

<h3>Title: Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Li, Jingtao Ding, Yong Li, Shihua Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02081">https://arxiv.org/abs/2510.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02081">https://arxiv.org/pdf/2510.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02081]] Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions(https://arxiv.org/abs/2510.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) algorithm achieves remarkable results in generative tasks especially in robotic manipulation. Building upon the foundations of diffusion models, the simulation-free paradigm of FM enables simple and efficient training, but inherently introduces a train-inference gap. Specifically, we cannot assess the model's output during the training phase. In contrast, other generative models including Variational Autoencoder (VAE), Normalizing Flow and Generative Adversarial Networks (GANs) directly optimize on the reconstruction loss. Such a gap is particularly evident in scenarios that demand high precision, such as robotic manipulation. Moreover, we show that FM's over-pursuit of straight predefined paths may introduce some serious problems such as stiffness into the system. These motivate us to fine-tune FM via Maximum Likelihood Estimation of reconstructions - an approach made feasible by FM's underlying smooth ODE formulation, in contrast to the stochastic differential equations (SDEs) used in diffusion models. This paper first theoretically analyzes the relation between training loss and inference error in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood Estimation of reconstructions, which includes both straightforward fine-tuning and residual-based fine-tuning approaches. Furthermore, through specifically designed architectures, the residual-based fine-tuning can incorporate the contraction property into the model, which is crucial for the model's robustness and interpretability. Experimental results in image generation and robotic manipulation verify that our method reliably improves the inference performance of FM.</li>
</ul>

<h3>Title: VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Arman Behnam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02086">https://arxiv.org/abs/2510.02086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02086">https://arxiv.org/pdf/2510.02086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02086]] VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation(https://arxiv.org/abs/2510.02086)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries. In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details. This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.</li>
</ul>

<h3>Title: When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos</h3>
<ul>
<li><strong>Authors: </strong>Woowon Jang, Jiwon Im, Juseung Choi, Niki Rashidian, Wesley De Neve, Utku Ozbulak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02100">https://arxiv.org/abs/2510.02100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02100">https://arxiv.org/pdf/2510.02100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02100]] When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos(https://arxiv.org/abs/2510.02100)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Video object segmentation (VOS) models such as SAM2 offer promising zero-shot tracking capabilities for surgical videos using minimal user input. Among the available input types, point-based tracking offers an efficient and low-cost alternative, yet its reliability and failure cases in complex surgical environments are not well understood. In this work, we systematically analyze the failure modes of point-based tracking in laparoscopic cholecystectomy videos. Focusing on three surgical targets, the gallbladder, grasper, and L-hook electrocautery, we compare the performance of point-based tracking with segmentation mask initialization. Our results show that point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets, where tissue similarity and ambiguous boundaries lead to failure. Through qualitative analysis, we reveal key factors influencing tracking outcomes and provide several actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis.</li>
</ul>

<h3>Title: FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ding-Ruei Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02114">https://arxiv.org/abs/2510.02114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02114">https://arxiv.org/pdf/2510.02114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02114]] FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation(https://arxiv.org/abs/2510.02114)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research.</li>
</ul>

<h3>Title: Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Milad Firoozeh, Nader Dashti, Mohammad Ali Hatefi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02115">https://arxiv.org/abs/2510.02115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02115">https://arxiv.org/pdf/2510.02115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02115]] Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data(https://arxiv.org/abs/2510.02115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Today, natural gas, as a clean fuel and the best alternative to crude oil, covers a significant part of global demand. Iran is one of the largest countries with energy resources and in terms of gas is the second-largest country in the world. But, due to the increase in population and energy consumption, it faces problems such as pressure drops and gas outages yearly in cold seasons and therefore it is necessary to control gas consumption, especially in the residential sector, which has the largest share in Iran. This study aims to analyze and predict gas consumption for residential customers in Zanjan province, Iran, using machine learning models, including LSTM, GRU, and a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and meteorology data collected over six years, from 2017 to 2022. The models were trained and evaluated based on their ability to accurately predict consumption patterns. The results indicate that the hybrid BiLSTM-XGBoost model outperformed the other models in terms of accuracy, with lower Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust performance, particularly in scenarios with limited data. The findings suggest that machine learning approaches, particularly hybrid models, can be effectively utilized to manage and predict gas consumption, contributing to more efficient resource management and reducing seasonal shortages. This study highlights the importance of incorporating geographical and climatic factors in predictive modeling, as these significantly influence gas usage across different regions.</li>
</ul>

<h3>Title: DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding</h3>
<ul>
<li><strong>Authors: </strong>Samhita Pal, James O'quinn, Kaveh Aryan, Heather Pua, James P. Long, Amir Asiaee</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02117">https://arxiv.org/abs/2510.02117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02117">https://arxiv.org/pdf/2510.02117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02117]] DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding(https://arxiv.org/abs/2510.02117)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study structure learning for linear Gaussian SEMs in the presence of latent confounding. Existing continuous methods excel when errors are independent, while deconfounding-first pipelines rely on pervasive factor structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based and fully differentiable estimator that jointly learns a DAG and a correlated noise model. Our theory gives simple sufficient conditions for global parameter identifiability: if the mixed graph is bow free and the noise covariance has a uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the observational covariance is injective, so both the directed structure and the noise are uniquely determined. The estimator alternates a smooth-acyclic graph update with a convex noise update and can include a light bow complementarity penalty or a post hoc reconciliation step. On synthetic benchmarks that vary confounding density, graph density, latent rank, and dimension with $n<p$, \textsc{DECOR} matches or outperforms strong baselines and is especially robust when confounding is non-pervasive, while remaining competitive under pervasiveness.</li>
</ul>

<h3>Title: The Disparate Impacts of Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jameson Sandler, Ahmet ÃstÃ¼n, Marco Romanelli, Sara Hooker, Ferdinando Fioretto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02128">https://arxiv.org/abs/2510.02128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02128">https://arxiv.org/pdf/2510.02128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02128]] The Disparate Impacts of Speculative Decoding(https://arxiv.org/abs/2510.02128)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The practice of speculative decoding, whereby inference is probabilistically supported by a smaller, cheaper, ``drafter'' model, has become a standard technique for systematically reducing the decoding time of large language models. This paper conducts an analysis of speculative decoding through the lens of its potential disparate speed-up rates across tasks. Crucially, the paper shows that speed-up gained from speculative decoding is not uniformly distributed across tasks, consistently diminishing for under-fit, and often underrepresented tasks. To better understand this phenomenon, we derive an analysis to quantify this observed ``unfairness'' and draw attention to the factors that motivate such disparate speed-ups to emerge. Further, guided by these insights, the paper proposes a mitigation strategy designed to reduce speed-up disparities and validates the approach across several model pairs, revealing on average a 12% improvement in our fairness metric.</li>
</ul>

<h3>Title: Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study</h3>
<ul>
<li><strong>Authors: </strong>Lena Podina, Christina Humer, Alexandre Duval, Victor Schmidt, Ali Ramlaoui, Shahana Chatterjee, Yoshua Bengio, Alex Hernandez-Garcia, David Rolnick, FÃ©lix Therrien</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02142">https://arxiv.org/abs/2510.02142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02142">https://arxiv.org/pdf/2510.02142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02142]] Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study(https://arxiv.org/abs/2510.02142)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Efficient and inexpensive energy storage is essential for accelerating the adoption of renewable energy and ensuring a stable supply, despite fluctuations in sources such as wind and solar. Electrocatalysts play a key role in hydrogen energy storage (HES), allowing the energy to be stored as hydrogen. However, the development of affordable and high-performance catalysts for this process remains a significant challenge. We introduce Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts. We demonstrate the performance of the model through a proof-of-concept application to the hydrogen evolution reaction, a key reaction in HES, for which we successfully identified platinum as the most efficient known catalyst. In future work, we aim to extend this approach to the oxygen evolution reaction, where current optimal catalysts are expensive metal oxides, and open the search space to discover new materials. This generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts.</li>
</ul>

<h3>Title: Policy Gradient Guidance Enables Test Time Control</h3>
<ul>
<li><strong>Authors: </strong>Jianing Qi, Hao Tang, Zhigang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02148">https://arxiv.org/abs/2510.02148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02148">https://arxiv.org/pdf/2510.02148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02148]] Policy Gradient Guidance Enables Test Time Control(https://arxiv.org/abs/2510.02148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Policy Gradient Guidance (PGG), a simple extension of classifier-free guidance from diffusion models to classical policy gradient methods. PGG augments the policy gradient with an unconditional branch and interpolates conditional and unconditional branches, yielding a test-time control knob that modulates behavior without retraining. We provide a theoretical derivation showing that the additional normalization term vanishes under advantage estimation, leading to a clean guided policy gradient update. Empirically, we evaluate PGG on discrete and continuous control benchmarks. We find that conditioning dropout-central to diffusion guidance-offers gains in simple discrete tasks and low sample regimes, but dropout destabilizes continuous control. Training with modestly larger guidance ($\gamma>1$) consistently improves stability, sample efficiency, and controllability. Our results show that guidance, previously confined to diffusion policies, can be adapted to standard on-policy methods, opening new directions for controllable online reinforcement learning.</li>
</ul>

<h3>Title: Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Junjie Su, Weifei Jin, Yuxin Cao, Derui Wang, Kai Ye, Jie Hao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02158">https://arxiv.org/abs/2510.02158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02158">https://arxiv.org/pdf/2510.02158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02158]] Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems(https://arxiv.org/abs/2510.02158)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Sound Event Detection (SED) systems are increasingly deployed in safety-critical applications such as industrial monitoring and audio surveillance. However, their robustness against adversarial attacks has not been well explored. Existing audio adversarial attacks targeting SED systems, which incorporate both detection and localization capabilities, often lack effectiveness due to SED's strong contextual dependencies or lack precision by focusing solely on misclassifying the target region as the target event, inadvertently affecting non-target regions. To address these challenges, we propose the Mirage and Mute Attack (M2A) framework, which is designed for targeted adversarial attacks on polyphonic SED systems. In our optimization process, we impose specific constraints on the non-target output, which we refer to as preservation loss, ensuring that our attack does not alter the model outputs for non-target region, thus achieving precise attacks. Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that balances effectiveness and precision, enabling our method to simultaneously enhance both. Comprehensive experiments show that M2A achieves 94.56% and 99.11% EP on two state-of-the-art SED models, demonstrating that the framework is sufficiently effective while significantly enhancing attack precision.</li>
</ul>

<h3>Title: NoMod: A Non-modular Attack on Module Learning With Errors</h3>
<ul>
<li><strong>Authors: </strong>Cristian Bassotto, Ermes Franch, Marina KrÄek, Stjepan Picek</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02162">https://arxiv.org/abs/2510.02162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02162">https://arxiv.org/pdf/2510.02162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02162]] NoMod: A Non-modular Attack on Module Learning With Errors(https://arxiv.org/abs/2510.02162)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The advent of quantum computing threatens classical public-key cryptography, motivating NIST's adoption of post-quantum schemes such as those based on the Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a hybrid white-box cryptanalytic method that circumvents the challenge of modeling modular reduction by treating wrap-arounds as statistical corruption and casting secret recovery as robust linear estimation. Our approach combines optimized lattice preprocessing--including reduced-vector saving and algebraic amplification--with robust estimators trained via Tukey's Biweight loss. Experiments show NoMod achieves full recovery of binary secrets for dimension $n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) = (128, 3)$ and $(256, 2)$. We release our implementation in an anonymous repository this https URL.</li>
</ul>

<h3>Title: Learning to Reason for Hallucination Span Detection</h3>
<ul>
<li><strong>Authors: </strong>Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Kundan Krishna, Hadi Pouransari, Cheng-Yu Hsieh, Cem Koc, Joseph Yitan Cheng, Oncel Tuzel, Raviteja Vemulapalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02173">https://arxiv.org/abs/2510.02173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02173">https://arxiv.org/pdf/2510.02173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02173]] Learning to Reason for Hallucination Span Detection(https://arxiv.org/abs/2510.02173)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.</li>
</ul>

<h3>Title: Flatness-Aware Stochastic Gradient Langevin Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Stefano Bruno, Youngsik Hwang, Jaehyeon An, Sotirios Sabanis, Dong-Young Lim</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02174">https://arxiv.org/abs/2510.02174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02174">https://arxiv.org/pdf/2510.02174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02174]] Flatness-Aware Stochastic Gradient Langevin Dynamics(https://arxiv.org/abs/2510.02174)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalization in deep learning is closely tied to the pursuit of flat minima in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics (SGLD) offers no mechanism to bias its dynamics toward such low-curvature solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD), designed to efficiently and provably seek flat minima in high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian noise, commonly referred to as Random Weight Perturbation (RWP), thereby optimizing a randomized-smoothing objective that implicitly captures curvature information. Leveraging these properties, we prove that the invariant measure of fSGLD stays close to a stationary measure concentrated on the global minimizers of a loss function regularized by the Hessian trace whenever the inverse temperature and the scale of random weight perturbation are properly coupled. This result provides a rigorous theoretical explanation for the benefits of random weight perturbation. In particular, we establish non-asymptotic convergence guarantees in Wasserstein distance with the best known rate and derive an excess-risk bound for the Hessian-trace regularized objective. Extensive experiments on noisy-label and large-scale vision tasks, in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD achieves superior or comparable generalization and robustness to baseline algorithms while maintaining the computational cost of SGD, about half that of SAM. Hessian-spectrum analysis further confirms that fSGLD converges to significantly flatter minima.</li>
</ul>

<h3>Title: GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02180">https://arxiv.org/abs/2510.02180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02180">https://arxiv.org/pdf/2510.02180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02180]] GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning(https://arxiv.org/abs/2510.02180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield "black-box" models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.</li>
</ul>

<h3>Title: Testing Stability and Robustness in Three Cryptographic Chaotic Systems</h3>
<ul>
<li><strong>Authors: </strong>N. A. Anagnostopoulos, K. Konstantinidis, A. N. Miliou, S. G. Stavrinides</a></li>
<li><strong>Subjects: </strong>cs.CR, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02184">https://arxiv.org/abs/2510.02184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02184">https://arxiv.org/pdf/2510.02184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02184]] Testing Stability and Robustness in Three Cryptographic Chaotic Systems(https://arxiv.org/abs/2510.02184)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>In practical applications, it is crucial that the drive-response systems, although identical in all respects, are synchronized at all times, even if there is noise present. In this work, we test the stability and robustness of three distinct and well-known cryptographic chaotic systems, and compare the results in relation to the desired security.</li>
</ul>

<h3>Title: GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02186">https://arxiv.org/abs/2510.02186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02186">https://arxiv.org/pdf/2510.02186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02186]] GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation(https://arxiv.org/abs/2510.02186)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to 3D semantic segmentation expose a persistent trade-off. Directly projecting 2D features into 3D yields noisy and fragmented predictions, whereas enforcing geometric coherence necessitates costly training pipelines and large-scale annotated 3D data. We argue that this limitation stems from the dominant segmentation-and-matching paradigm, which fails to reconcile 2D semantics with 3D geometric structure. The geometric cues are not eliminated during the 2D-to-3D transfer but remain latent within the noisy and view-aggregated features. To exploit this property, we propose GeoPurify that applies a small Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors distilled from a 3D self-supervised teacher model. During inference, we devise a Geometry-Guided Pooling module to further denoise the point cloud and ensure the semantic and structural consistency. Benefiting from latent geometric information and the learned affinity network, GeoPurify effectively mitigates the trade-off and achieves superior data efficiency. Extensive experiments on major 3D benchmarks demonstrate that GeoPurify achieves or surpasses state-of-the-art performance while utilizing only about 1.5% of the training data. Our codes and checkpoints are available at [this https URL](this https URL).</li>
</ul>

<h3>Title: Authentication Security of PRF GNSS Ranging</h3>
<ul>
<li><strong>Authors: </strong>Jason Anderson</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02196">https://arxiv.org/abs/2510.02196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02196">https://arxiv.org/pdf/2510.02196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02196]] Authentication Security of PRF GNSS Ranging(https://arxiv.org/abs/2510.02196)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This work derives the authentication security of pseudorandom function (PRF) GNSS ranging under multiple GNSS spoofing models, including the Security Code Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF utilizing a secret known only to the broadcaster, the spoofer cannot predict the ranging code before broadcast. Therefore, PRF ranging can be used to establish trust in the GNSS pseudoranges and the resulting receiver position, navigation, and timing (PNT) solution. I apply the methods herein to Galileo's Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit authentication security under non-SCER models. For the SCER adversary, I predict the adversary's needed receiving radio equipment to break authentication security. One can use this work to design a PRF GNSS ranging protocol to meet useful authentication security requirements by computing the probability of missed detection.</li>
</ul>

<h3>Title: Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications</h3>
<ul>
<li><strong>Authors: </strong>Emmanuel Nsengiyumvaa, Leonard Niyitegekaa, Eric Umuhoza</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02197">https://arxiv.org/abs/2510.02197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02197">https://arxiv.org/pdf/2510.02197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02197]] Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications(https://arxiv.org/abs/2510.02197)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Accurate livestock identification is a cornerstone of modern farming: it supports health monitoring, breeding programs, and productivity tracking. However, common pig identification methods, such as ear tags and microchips, are often unreliable, costly, target pure breeds, and thus impractical for small-scale farmers. To address this gap, we propose a noninvasive biometric identification approach that leverages uniqueness of the auricular vein patterns. To this end, we have collected 800 ear images from 20 mixed-breed pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a standard smartphone and simple back lighting. A multistage computer vision pipeline was developed to enhance vein visibility, extract structural and spatial features, and generate biometric signatures. These features were then classified using machine learning models. Support Vector Machines (SVM) achieved the highest accuracy: correctly identifying pigs with 98.12% precision across mixed-breed populations. The entire process from image processing to classification was completed in an average of 8.3 seconds, demonstrating feasibility for real-time farm deployment. We believe that by replacing fragile physical identifiers with permanent biological markers, this system provides farmers with a cost-effective and stress-free method of animal identification. More broadly, the findings confirm the practicality of auricular vein biometrics for digitizing livestock management, reinforcing its potential to extend the benefits of precision farming to resource-constrained agricultural communities.</li>
</ul>

<h3>Title: ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities</h3>
<ul>
<li><strong>Authors: </strong>Felix Brei, Lorenz BÃ¼hmann, Johannes Frey, Daniel Gerber, Lars-Peter Meyer, Claus Stadler, Kirill Bulert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02200">https://arxiv.org/abs/2510.02200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02200">https://arxiv.org/pdf/2510.02200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02200]] ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities(https://arxiv.org/abs/2510.02200)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.</li>
</ul>

<h3>Title: Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025</h3>
<ul>
<li><strong>Authors: </strong>Matthew A. Reyna (1), Zuzana Koscova (1), Jan Pavlus (1), Soheil Saghafi (1), James Weigle (1), Andoni Elola (1,2), Salman Seyedi (1), Kiersten Campbell (1), Qiao Li (1), Ali Bahrami Rad (1), AntÃ´nio H. Ribeiro (3), Antonio Luiz P. Ribeiro (4,5), Reza Sameni (1,6), Gari D. Clifford (1,6) ((1) Department of Biomedical Informatics, Emory University, Atlanta, USA, (2) Department of Electronic Technology, University of the Basque Country UPV/EHU, Spain, (3) Department of Information Technology, Uppsala University, Uppsala, Sweden, (4) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil, (5) Telehealth Center from Hospital das Clinicas, Universidade Federal de Minas Gerais, Belo Horizonte, Brazil, (6) Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, Atlanta, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02202">https://arxiv.org/abs/2510.02202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02202">https://arxiv.org/pdf/2510.02202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02202]] Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025(https://arxiv.org/abs/2510.02202)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Objective: Chagas disease is a parasitic infection that is endemic to South America, Central America, and, more recently, the U.S., primarily transmitted by insects. Chronic Chagas disease can cause cardiovascular diseases and digestive problems. Serological testing capacities for Chagas disease are limited, but Chagas cardiomyopathy often manifests in ECGs, providing an opportunity to prioritize patients for testing and treatment. Approach: The George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic approaches for identifying Chagas disease from electrocardiograms (ECGs). Main results: This Challenge provides multiple innovations. First, we leveraged several datasets with labels from patient reports and serological testing, provided a large dataset with weak labels and smaller datasets with strong labels. Second, we augmented the data to support model robustness and generalizability to unseen data sources. Third, we applied an evaluation metric that captured the local serological testing capacity for Chagas disease to frame the machine learning problem as a triage task. Significance: Over 630 participants from 111 teams submitted over 1300 entries during the Challenge, representing diverse approaches from academia and industry worldwide.</li>
</ul>

<h3>Title: Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gallo FernÃ¡ndez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02206">https://arxiv.org/abs/2510.02206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02206">https://arxiv.org/pdf/2510.02206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02206]] Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling(https://arxiv.org/abs/2510.02206)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sequence-to-sequence models have become central in Artificial Intelligence, particularly following the introduction of the transformer architecture. While initially developed for Natural Language Processing, these models have demonstrated utility across domains, including Computer Vision. Such models require mechanisms to exchange information along the time dimension, typically using recurrent or self-attention layers. However, self-attention scales quadratically with sequence length, limiting its practicality for very long sequences. We introduce Poolformer, a sequence-to-sequence model that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. Poolformer is defined recursively using SkipBlocks, which contain residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. We conduct extensive experiments to support our architectural choices. Our results show that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Our experiments also suggest that long-range dependencies are handled by deep layers, while shallow layers take care of short-term features. Evaluated on raw audio, which naturally features long sequence lengths, Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba. Future directions include applications to text and vision, as well as multi-modal scenarios, where a Poolformer-based LLM could effectively process dense representations of images and videos.</li>
</ul>

<h3>Title: StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?</h3>
<ul>
<li><strong>Authors: </strong>Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02209">https://arxiv.org/abs/2510.02209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02209">https://arxiv.org/pdf/2510.02209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02209]] StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?(https://arxiv.org/abs/2510.02209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.</li>
</ul>

<h3>Title: DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02212">https://arxiv.org/abs/2510.02212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02212">https://arxiv.org/pdf/2510.02212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02212]] DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning(https://arxiv.org/abs/2510.02212)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.</li>
</ul>

<h3>Title: MMDEW: Multipurpose Multiclass Density Estimation in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Villanelle O'Reilly, Jonathan Cox, Georgios Leontidis, Marc Hanheide, Petra Bosilj, James Brown</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02213">https://arxiv.org/abs/2510.02213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02213">https://arxiv.org/pdf/2510.02213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02213]] MMDEW: Multipurpose Multiclass Density Estimation in the Wild(https://arxiv.org/abs/2510.02213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Density map estimation can be used to estimate object counts in dense and occluded scenes where discrete counting-by-detection methods fail. We propose a multicategory counting framework that leverages a Twins pyramid vision-transformer backbone and a specialised multi-class counting head built on a state-of-the-art multiscale decoding approach. A two-task design adds a segmentation-based Category Focus Module, suppressing inter-category cross-talk at training time. Training and evaluation on the VisDrone and iSAID benchmarks demonstrates superior performance versus prior multicategory crowd-counting approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11 underscores the necessity of crowd counting methods in dense scenes. The method's regional loss opens up multi-class crowd counting to new domains, demonstrated through the application to a biodiversity monitoring dataset, highlighting its capacity to inform conservation efforts and enable scalable ecological insights.</li>
</ul>

<h3>Title: Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Ye, Minshuo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02216">https://arxiv.org/abs/2510.02216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02216">https://arxiv.org/pdf/2510.02216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02216]] Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification(https://arxiv.org/abs/2510.02216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance.</li>
</ul>

<h3>Title: TempoControl: Temporal Attention Guidance for Text-to-Video Models</h3>
<ul>
<li><strong>Authors: </strong>Shira Schiber, Ofir Lindenbaum, Idan Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02226">https://arxiv.org/abs/2510.02226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02226">https://arxiv.org/pdf/2510.02226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02226]] TempoControl: Temporal Attention Guidance for Text-to-Video Models(https://arxiv.org/abs/2510.02226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.</li>
</ul>

<h3>Title: More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02227">https://arxiv.org/abs/2510.02227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02227">https://arxiv.org/pdf/2510.02227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02227]] More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration(https://arxiv.org/abs/2510.02227)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at this https URL.</li>
</ul>

<h3>Title: xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Beck, Kajetan Schweighofer, Sebastian BÃ¶ck, Sebastian Lehner, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02228">https://arxiv.org/abs/2510.02228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02228">https://arxiv.org/pdf/2510.02228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02228]] xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity(https://arxiv.org/abs/2510.02228)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.</li>
</ul>

<h3>Title: Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches</h3>
<ul>
<li><strong>Authors: </strong>Ebtesam Jaber Aljohani, Wael M. S. Yafoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02232">https://arxiv.org/abs/2510.02232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02232">https://arxiv.org/pdf/2510.02232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02232]] Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches(https://arxiv.org/abs/2510.02232)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent technological advances in smartphones and communications, including the growth of such online platforms as massive social media networks such as X (formerly known as Twitter) endangers young people and their emotional well-being by exposing them to cyberbullying, taunting, and bullying content. Most proposed approaches for automatically detecting cyberbullying have been developed around the English language, and methods for detecting Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language cyberbullying are especially scarce. This paper aims to enhance the effectiveness of methods for detecting cyberbullying in Arabic-language content. We assembled a dataset of 10,662 X posts, pre-processed the data, and used the kappa tool to verify and enhance the quality of our annotations. We conducted four experiments to test numerous deep learning models for automatically detecting Arabic-language cyberbullying. We first tested a long short-term memory (LSTM) model and a bidirectional long short-term memory (Bi-LSTM) model with several experimental word embeddings. We also tested the LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from representations (BERT) and then tested them on a different experimental models BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM with FastText embedding word performed even better, achieving 98% accuracy. As a result, the outcomes are generalize</li>
</ul>

<h3>Title: PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, Chadi Assi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02236">https://arxiv.org/abs/2510.02236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02236">https://arxiv.org/pdf/2510.02236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02236]] PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks(https://arxiv.org/abs/2510.02236)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust</a></li>
<li><strong>Abstract: </strong>Network Slices (NSs) are virtual networks operating over a shared physical infrastructure, each designed to meet specific application requirements while maintaining consistent Quality of Service (QoS). In Fifth Generation (5G) networks, User Equipment (UE) can connect to and seamlessly switch between multiple NSs to access diverse services. However, this flexibility, known as Inter-Slice Switching (ISS), introduces a potential vulnerability that can be exploited to launch Distributed Slice Mobility (DSM) attacks, a form of Distributed Denial of Service (DDoS) attack. To secure 5G networks and their NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and incorporates a combination of Long Short-Term Memory Autoencoders and K-Means clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership Project (3GPP) key performance indicators and performance measurement counters as features for its machine learning models to detect DSM attack variants while maintaining robustness in the presence of contaminated training data. When evaluated on data collected from our 5G testbed based on the open-source free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator; PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, consistently outperforming its counterpart Inter-Slice Defender and other PUL based solutions combining One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.</li>
</ul>

<h3>Title: RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Feng, Kaiwen Tuo, Song Wang, Lingdong Kong, Jianke Zhu, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02240">https://arxiv.org/abs/2510.02240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02240">https://arxiv.org/pdf/2510.02240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02240]] RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning(https://arxiv.org/abs/2510.02240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.</li>
</ul>

<h3>Title: ExGRPO: Learning to Reason from Experience</h3>
<ul>
<li><strong>Authors: </strong>Runzhe Zhan, Yafu Li, Zhi Wang, Xiaoye Qu, Dongrui Liu, Jing Shao, Derek F. Wong, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02245">https://arxiv.org/abs/2510.02245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02245">https://arxiv.org/pdf/2510.02245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02245]] ExGRPO: Learning to Reason from Experience(https://arxiv.org/abs/2510.02245)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.</li>
</ul>

<h3>Title: Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02249">https://arxiv.org/abs/2510.02249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02249">https://arxiv.org/pdf/2510.02249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02249]] Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation(https://arxiv.org/abs/2510.02249)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.</li>
</ul>

<h3>Title: DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02253">https://arxiv.org/abs/2510.02253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02253">https://arxiv.org/pdf/2510.02253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02253]] DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing(https://arxiv.org/abs/2510.02253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.</li>
</ul>

<h3>Title: Transformers Discover Molecular Structure Without Graph Priors</h3>
<ul>
<li><strong>Authors: </strong>Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02259">https://arxiv.org/abs/2510.02259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02259">https://arxiv.org/pdf/2510.02259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02259]] Transformers Discover Molecular Structure Without Graph Priors(https://arxiv.org/abs/2510.02259)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\unicode{x2013}$without predefined graphs or physical priors$\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.</li>
</ul>

<h3>Title: From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02262">https://arxiv.org/abs/2510.02262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02262">https://arxiv.org/pdf/2510.02262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02262]] From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding(https://arxiv.org/abs/2510.02262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the "needle in a haystack" problem: the massive number of visual tokens produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at this https URL .</li>
</ul>

<h3>Title: Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities</h3>
<ul>
<li><strong>Authors: </strong>Mario Medrano-Paredes, Carmen FernÃ¡ndez-GonzÃ¡lez, Francisco-Javier DÃ­az-Pernas, Hichem Saoudi, Javier GonzÃ¡lez-Alonso, Mario MartÃ­nez-Zarzuela</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02264">https://arxiv.org/abs/2510.02264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02264">https://arxiv.org/pdf/2510.02264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02264]] Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities(https://arxiv.org/abs/2510.02264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Advances in machine learning and wearable sensors offer new opportunities for capturing and analyzing human movement outside specialized laboratories. Accurate assessment of human movement under real-world conditions is essential for telemedicine, sports science, and rehabilitation. This preclinical benchmark compares monocular video-based 3D human pose estimation models with inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a total of 13 clinically relevant daily activities which were captured using both commodity video cameras and five IMUs. During this initial study only healthy subjects were recorded, so results cannot be generalized to pathological cohorts. Joint angles derived from state-of-the-art deep learning frameworks (MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA BodyTrack) were evaluated against joint angles computed from IMU data using OpenSim inverse kinematics following the Human3.6M dataset format with 17 keypoints. Among them, MotionAGFormer demonstrated superior performance, achieving the lowest overall RMSE ($9.27Â°\pm 4.80Â°$) and MAE ($7.86Â°\pm 4.18Â°$), as well as the highest Pearson correlation ($0.86 \pm 0.15$) and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The results reveal that both technologies are viable for out-of-the-lab kinematic assessment. However, they also highlight key trade-offs between video- and sensor-based approaches including costs, accessibility, and precision. This study clarifies where off-the-shelf video models already provide clinically promising kinematics in healthy adults and where they lag behind IMU-based estimates while establishing valuable guidelines for researchers and clinicians seeking to develop robust, cost-effective, and user-friendly solutions for telehealth and remote patient monitoring.</li>
</ul>

<h3>Title: How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, Sastry Kompella</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02265">https://arxiv.org/abs/2510.02265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02265">https://arxiv.org/pdf/2510.02265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02265]] How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning(https://arxiv.org/abs/2510.02265)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper studies the problem of mitigating reactive jamming, where a jammer adopts a dynamic policy of selecting channels and sensing thresholds to detect and jam ongoing transmissions. The transmitter-receiver pair learns to avoid jamming and optimize throughput over time (without prior knowledge of channel conditions or jamming strategies) by using reinforcement learning (RL) to adapt transmit power, modulation, and channel selection. Q-learning is employed for discrete jamming-event states, while Deep Q-Networks (DQN) are employed for continuous states based on received power. Through different reward functions and action sets, the results show that RL can adapt rapidly to spectrum dynamics and sustain high rates as channels and jamming policies change over time.</li>
</ul>

<h3>Title: NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Zhang, Dong Liang, Yihang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02266">https://arxiv.org/abs/2510.02266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02266">https://arxiv.org/pdf/2510.02266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02266]] NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes(https://arxiv.org/abs/2510.02266)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.</li>
</ul>

<h3>Title: microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02270">https://arxiv.org/abs/2510.02270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02270">https://arxiv.org/pdf/2510.02270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02270]] microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification(https://arxiv.org/abs/2510.02270)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at this https URL.</li>
</ul>

<h3>Title: Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps</h3>
<ul>
<li><strong>Authors: </strong>Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02274">https://arxiv.org/abs/2510.02274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02274">https://arxiv.org/pdf/2510.02274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02274]] Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps(https://arxiv.org/abs/2510.02274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modeling radio frequency (RF) signal propagation is essential for understanding the environment, as RF signals offer valuable insights beyond the capabilities of RGB cameras, which are limited by the visible-light spectrum, lens coverage, and occlusions. It is also useful for supporting wireless diagnosis, deployment, and optimization. However, accurately predicting RF signals in complex environments remains a challenge due to interactions with obstacles such as absorption and reflection. We introduce Diffusion^2, a diffusion-based approach that uses 3D point clouds to model the propagation of RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves. To effectively capture RF-related features from 3D data, we present the RF-3D Encoder, which encapsulates the complexities of 3D geometry along with signal-specific details. These features undergo multi-scale embedding to simulate the actual RF signal dissemination process. Our evaluation, based on synthetic and real-world measurements, demonstrates that Diffusion^2 accurately estimates the behavior of RF signals in various frequency bands and environmental conditions, with an error margin of just 1.9 dB and 27x faster than existing methods, marking a significant advancement in the field. Refer to this https URL for more information.</li>
</ul>

<h3>Title: Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02279">https://arxiv.org/abs/2510.02279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02279">https://arxiv.org/pdf/2510.02279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02279]] Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation(https://arxiv.org/abs/2510.02279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.</li>
</ul>

<h3>Title: VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</h3>
<ul>
<li><strong>Authors: </strong>Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02282">https://arxiv.org/abs/2510.02282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02282">https://arxiv.org/pdf/2510.02282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02282]] VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL(https://arxiv.org/abs/2510.02282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02283">https://arxiv.org/abs/2510.02283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02283">https://arxiv.org/pdf/2510.02283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02283]] Self-Forcing++: Towards Minute-Scale High-Quality Video Generation(https://arxiv.org/abs/2510.02283)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at this https URL</li>
</ul>

<h3>Title: Learning to Generate Object Interactions with Physics-Guided Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02284">https://arxiv.org/abs/2510.02284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02284">https://arxiv.org/pdf/2510.02284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02284]] Learning to Generate Object Interactions with Physics-Guided Video Diffusion(https://arxiv.org/abs/2510.02284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.</li>
</ul>

<h3>Title: Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02286">https://arxiv.org/abs/2510.02286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02286">https://arxiv.org/pdf/2510.02286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02286]] Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks(https://arxiv.org/abs/2510.02286)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.</li>
</ul>

<h3>Title: MultiModal Action Conditioned Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Li, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02287">https://arxiv.org/abs/2510.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02287">https://arxiv.org/pdf/2510.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02287]] MultiModal Action Conditioned Video Generation(https://arxiv.org/abs/2510.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.</li>
</ul>

<h3>Title: Test-Time Anchoring for Discrete Diffusion Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02291">https://arxiv.org/abs/2510.02291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02291">https://arxiv.org/pdf/2510.02291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02291]] Test-Time Anchoring for Discrete Diffusion Posterior Sampling(https://arxiv.org/abs/2510.02291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing.</li>
</ul>

<h3>Title: From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</h3>
<ul>
<li><strong>Authors: </strong>Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02292">https://arxiv.org/abs/2510.02292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02292">https://arxiv.org/pdf/2510.02292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02292]] From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens(https://arxiv.org/abs/2510.02292)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking, analysis, and interpretation of vision-language models (VLMs) by supporting the extraction of intermediate outputs from any layer during the forward pass of open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that abstracts away model-specific complexities and supports user-friendly operation across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and their over 30 variants, and is extensible to accommodate new models without changing the core logic. The toolkit integrates easily with various interpretability and analysis methods. We demonstrate its usage with two simple analytical experiments, revealing systematic differences in the hidden representations of VLMs across layers and target concepts. VLM-Lens is released as an open-sourced project to accelerate community efforts in understanding and improving VLMs.</li>
</ul>

<h3>Title: F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data</h3>
<ul>
<li><strong>Authors: </strong>Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02294">https://arxiv.org/abs/2510.02294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02294">https://arxiv.org/pdf/2510.02294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02294]] F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data(https://arxiv.org/abs/2510.02294)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.</li>
</ul>

<h3>Title: Continual Personalization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02296">https://arxiv.org/abs/2510.02296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02296">https://arxiv.org/pdf/2510.02296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02296]] Continual Personalization for Diffusion Models(https://arxiv.org/abs/2510.02296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.</li>
</ul>

<h3>Title: Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Runqian Wang, Yilun Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02300">https://arxiv.org/abs/2510.02300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02300">https://arxiv.org/pdf/2510.02300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02300]] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models(https://arxiv.org/abs/2510.02300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.</li>
</ul>

<h3>Title: Knowledge Distillation Detection for Open-weights Models</h3>
<ul>
<li><strong>Authors: </strong>Qin Shi, Amber Yijia Zheng, Qifan Song, Raymond A. Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02302">https://arxiv.org/abs/2510.02302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02302">https://arxiv.org/pdf/2510.02302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02302]] Knowledge Distillation Detection for Open-weights Models(https://arxiv.org/abs/2510.02302)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, generative</a></li>
<li><strong>Abstract: </strong>We propose the task of knowledge distillation detection, which aims to determine whether a student model has been distilled from a given teacher, under a practical setting where only the student's weights and the teacher's API are available. This problem is motivated by growing concerns about model provenance and unauthorized replication through distillation. To address this task, we introduce a model-agnostic framework that combines data-free input synthesis and statistical score computation for detecting distillation. Our approach is applicable to both classification and generative models. Experiments on diverse architectures for image classification and text-to-image generation show that our method improves detection accuracy over the strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation. The code is available at this https URL.</li>
</ul>

<h3>Title: Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive</h3>
<ul>
<li><strong>Authors: </strong>Tyler Farghly, Peter Potaptchik, Samuel Howard, George Deligiannidis, Jakiw Pidstrigach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02305">https://arxiv.org/abs/2510.02305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02305">https://arxiv.org/pdf/2510.02305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02305]] Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive(https://arxiv.org/abs/2510.02305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function -- or equivalently, smoothing in the log-density domain -- produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing.</li>
</ul>

<h3>Title: Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Raphael Tang, Crystina Zhang, Wenyan Li, Carmen Lai, Pontus Stenetorp, Yao Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02306">https://arxiv.org/abs/2510.02306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02306">https://arxiv.org/pdf/2510.02306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02306]] Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation(https://arxiv.org/abs/2510.02306)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the "battle" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.</li>
</ul>

<h3>Title: NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruozhen He, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02307">https://arxiv.org/abs/2510.02307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02307">https://arxiv.org/pdf/2510.02307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02307]] NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation(https://arxiv.org/abs/2510.02307)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.</li>
</ul>

<h3>Title: Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, Alexander Cloninger</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02308">https://arxiv.org/abs/2510.02308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02308">https://arxiv.org/pdf/2510.02308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02308]] Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization(https://arxiv.org/abs/2510.02308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold's tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation.</li>
</ul>

<h3>Title: Inferring Dynamic Physical Properties from Video Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02311">https://arxiv.org/abs/2510.02311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02311">https://arxiv.org/pdf/2510.02311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02311]] Inferring Dynamic Physical Properties from Video Foundation Models(https://arxiv.org/abs/2510.02311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.</li>
</ul>

<h3>Title: KaVa: Latent Reasoning via Compressed KV-Cache Distillation</h3>
<ul>
<li><strong>Authors: </strong>Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02312">https://arxiv.org/abs/2510.02312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02312">https://arxiv.org/pdf/2510.02312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02312]] KaVa: Latent Reasoning via Compressed KV-Cache Distillation(https://arxiv.org/abs/2510.02312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.</li>
</ul>

<h3>Title: Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Yang, Yiming Chen, Haozheng Pei, Siddhant Agarwal, Arun Balajee Vasudevan, James Hays</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02313">https://arxiv.org/abs/2510.02313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02313">https://arxiv.org/pdf/2510.02313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02313]] Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions(https://arxiv.org/abs/2510.02313)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Can a model distinguish between the sound of a spoon hitting a hardwood floor versus a carpeted one? Everyday object interactions produce sounds unique to the objects involved. We introduce the sounding object detection task to evaluate a model's ability to link these sounds to the objects directly involved. Inspired by human perception, our multimodal object-aware framework learns from in-the-wild egocentric videos. To encourage an object-centric approach, we first develop an automatic pipeline to compute segmentation masks of the objects involved to guide the model's focus during training towards the most informative regions of the interaction. A slot attention visual encoder is used to further enforce an object prior. We demonstrate state of the art performance on our new task along with existing multimodal action understanding tasks.</li>
</ul>

<h3>Title: StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions</h3>
<ul>
<li><strong>Authors: </strong>Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02314">https://arxiv.org/abs/2510.02314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02314">https://arxiv.org/pdf/2510.02314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02314]] StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions(https://arxiv.org/abs/2510.02314)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal</a></li>
<li><strong>Abstract: </strong>3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: this https URL</li>
</ul>

<h3>Title: Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Eric Tillmann Bill, Enis Simsar, Thomas Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02315">https://arxiv.org/abs/2510.02315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02315">https://arxiv.org/pdf/2510.02315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02315]] Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity(https://arxiv.org/abs/2510.02315)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
